{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be843219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "S1 SEMANTIC & GRAPH METRICS ANALYSIS\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "1. LOADING DATA\n",
      "====================================================================================================\n",
      "Loaded 8,742 rows, 94 columns\n",
      "\n",
      "Total rows after filtering: 8,742\n",
      "\n",
      "====================================================================================================\n",
      "2. IDENTIFYING S1 SEMANTIC & GRAPH COLUMNS\n",
      "====================================================================================================\n",
      "\n",
      "Available Semantic columns (9):\n",
      "  - S1_Sem_BERT_f1: 5,664 non-null (64.8%)\n",
      "  - S1_Sem_BERT_norm: 5,664 non-null (64.8%)\n",
      "  - S1_Sem_ROUGE1_f1: 5,664 non-null (64.8%)\n",
      "  - S1_Sem_ROUGE1_norm: 5,664 non-null (64.8%)\n",
      "  - S1_Sem_KeyTerm_rate: 5,664 non-null (64.8%)\n",
      "  - S1_Sem_KeyTerm_preserved: 5,664 non-null (64.8%)\n",
      "  - S1_Sem_KeyTerm_missing: 5,664 non-null (64.8%)\n",
      "  - S1_Sem_KeyTerm_total: 5,664 non-null (64.8%)\n",
      "  - S1_Sem_tok_overlap_ratio: 5,664 non-null (64.8%)\n",
      "\n",
      "Available Graph columns (6):\n",
      "  - S1_Graph_overall_score: 5,664 non-null (64.8%)\n",
      "  - S1_Graph_total_issues: 5,664 non-null (64.8%)\n",
      "  - S1_Graph_Structural_Integrity_score: 5,664 non-null (64.8%)\n",
      "  - S1_Graph_Node_Connectivity_score: 5,664 non-null (64.8%)\n",
      "  - S1_Graph_Component_Usage_score: 5,664 non-null (64.8%)\n",
      "  - S1_Graph_Task_Component_Consistency_score: 5,664 non-null (64.8%)\n",
      "\n",
      "====================================================================================================\n",
      "3. FILTERING TO PROMPT2DAG RUNS\n",
      "====================================================================================================\n",
      "Prompt2DAG rows: 5,664\n",
      "  S1_Sem_BERT_f1: 5,664 non-null (100.0%)\n",
      "  S1_Sem_BERT_norm: 5,664 non-null (100.0%)\n",
      "  S1_Sem_ROUGE1_f1: 5,664 non-null (100.0%)\n",
      "  S1_Sem_ROUGE1_norm: 5,664 non-null (100.0%)\n",
      "  S1_Sem_KeyTerm_rate: 5,664 non-null (100.0%)\n",
      "  S1_Sem_KeyTerm_preserved: 5,664 non-null (100.0%)\n",
      "  S1_Sem_KeyTerm_missing: 5,664 non-null (100.0%)\n",
      "  S1_Sem_KeyTerm_total: 5,664 non-null (100.0%)\n",
      "  S1_Sem_tok_overlap_ratio: 5,664 non-null (100.0%)\n",
      "  S1_Graph_overall_score: 5,664 non-null (100.0%)\n",
      "  S1_Graph_total_issues: 5,664 non-null (100.0%)\n",
      "  S1_Graph_Structural_Integrity_score: 5,664 non-null (100.0%)\n",
      "  S1_Graph_Node_Connectivity_score: 5,664 non-null (100.0%)\n",
      "  S1_Graph_Component_Usage_score: 5,664 non-null (100.0%)\n",
      "  S1_Graph_Task_Component_Consistency_score: 5,664 non-null (100.0%)\n",
      "\n",
      "Prompt2DAG rows with any S1 data: 5,664\n",
      "\n",
      "====================================================================================================\n",
      "PART A: ABSOLUTE ANALYSIS - WHAT S1 METRICS TELL US\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "A1. GLOBAL SUMMARY STATISTICS (Prompt2DAG)\n",
      "====================================================================================================\n",
      "\n",
      "--- Semantic Metrics ---\n",
      "                           count    mean     std     min     25%     50%      75%      max  non_null  null_pct\n",
      "S1_Sem_BERT_f1            5664.0   0.679   0.056   0.490   0.641   0.686    0.719    0.803      5664       0.0\n",
      "S1_Sem_BERT_norm          5664.0   6.888   1.023   3.460   6.200   7.020    7.620    9.140      5664       0.0\n",
      "S1_Sem_ROUGE1_f1          5664.0   0.417   0.088   0.162   0.367   0.440    0.480    0.574      5664       0.0\n",
      "S1_Sem_ROUGE1_norm        5664.0   6.217   1.585   2.080   5.000   6.690    7.400    9.260      5664       0.0\n",
      "S1_Sem_KeyTerm_rate       5664.0   0.607   0.111   0.233   0.540   0.611    0.675    0.913      5664       0.0\n",
      "S1_Sem_KeyTerm_preserved  5664.0  58.179  11.351  22.000  51.000  58.000   65.000   92.000      5664       0.0\n",
      "S1_Sem_KeyTerm_missing    5664.0  38.250  13.165   6.000  30.000  37.000   46.000   95.000      5664       0.0\n",
      "S1_Sem_KeyTerm_total      5664.0  96.429  13.550  69.000  87.000  96.000  103.000  136.000      5664       0.0\n",
      "S1_Sem_tok_overlap_ratio  5664.0   0.530   0.115   0.223   0.455   0.548    0.610    0.853      5664       0.0\n",
      "\n",
      "--- Graph Metrics ---\n",
      "                                            count    mean     std   min    25%    50%    75%    max  non_null  null_pct\n",
      "S1_Graph_overall_score                     5664.0   8.931   1.619   4.2   9.08   9.75   10.0   10.0      5664       0.0\n",
      "S1_Graph_total_issues                      5664.0   1.262   2.720   0.0   0.00   1.00    2.0   38.0      5664       0.0\n",
      "S1_Graph_Structural_Integrity_score        5664.0  34.703  39.148  10.0  10.00  10.00  100.0  100.0      5664       0.0\n",
      "S1_Graph_Node_Connectivity_score           5664.0  31.450  40.996   0.0  10.00  10.00  100.0  100.0      5664       0.0\n",
      "S1_Graph_Component_Usage_score             5664.0  37.737  39.974   0.0  10.00  10.00   87.5  100.0      5664       0.0\n",
      "S1_Graph_Task_Component_Consistency_score  5664.0  39.673  42.187  10.0  10.00  10.00  100.0  100.0      5664       0.0\n",
      "\n",
      "====================================================================================================\n",
      "A2. S1 METRICS BY PROMPT2DAG STRATEGY\n",
      "====================================================================================================\n",
      "\n",
      "--- S1_Sem_BERT_f1 ---\n",
      "Method                           N       Mean        Std        Min        Max\n",
      "--------------------------------------------------------------------------------\n",
      "Prompt2DAG (Template)         1578      0.671      0.056      0.490      0.797\n",
      "Prompt2DAG (LLM)              2043      0.682      0.056      0.499      0.803\n",
      "Prompt2DAG (Hybrid)           2043      0.682      0.056      0.490      0.803\n",
      "\n",
      "--- S1_Sem_BERT_norm ---\n",
      "Method                           N       Mean        Std        Min        Max\n",
      "--------------------------------------------------------------------------------\n",
      "Prompt2DAG (Template)         1578      6.748      1.025      3.460      9.030\n",
      "Prompt2DAG (LLM)              2043      6.940      1.016      3.610      9.140\n",
      "Prompt2DAG (Hybrid)           2043      6.943      1.019      3.460      9.140\n",
      "\n",
      "--- S1_Sem_ROUGE1_f1 ---\n",
      "Method                           N       Mean        Std        Min        Max\n",
      "--------------------------------------------------------------------------------\n",
      "Prompt2DAG (Template)         1578      0.407      0.091      0.162      0.571\n",
      "Prompt2DAG (LLM)              2043      0.420      0.087      0.163      0.574\n",
      "Prompt2DAG (Hybrid)           2043      0.421      0.086      0.162      0.574\n",
      "\n",
      "--- S1_Sem_ROUGE1_norm ---\n",
      "Method                           N       Mean        Std        Min        Max\n",
      "--------------------------------------------------------------------------------\n",
      "Prompt2DAG (Template)         1578      6.198      1.653      2.080      9.260\n",
      "Prompt2DAG (LLM)              2043      6.224      1.564      2.080      9.260\n",
      "Prompt2DAG (Hybrid)           2043      6.226      1.552      2.080      9.260\n",
      "\n",
      "--- S1_Sem_KeyTerm_rate ---\n",
      "Method                           N       Mean        Std        Min        Max\n",
      "--------------------------------------------------------------------------------\n",
      "Prompt2DAG (Template)         1578      0.606      0.117      0.233      0.913\n",
      "Prompt2DAG (LLM)              2043      0.608      0.109      0.233      0.913\n",
      "Prompt2DAG (Hybrid)           2043      0.608      0.107      0.233      0.913\n",
      "\n",
      "--- S1_Sem_KeyTerm_preserved ---\n",
      "Method                           N       Mean        Std        Min        Max\n",
      "--------------------------------------------------------------------------------\n",
      "Prompt2DAG (Template)         1578     57.441     11.697     22.000     92.000\n",
      "Prompt2DAG (LLM)              2043     58.451     11.243     22.000     92.000\n",
      "Prompt2DAG (Hybrid)           2043     58.476     11.166     22.000     92.000\n",
      "\n",
      "--- S1_Sem_KeyTerm_missing ---\n",
      "Method                           N       Mean        Std        Min        Max\n",
      "--------------------------------------------------------------------------------\n",
      "Prompt2DAG (Template)         1578     38.057     13.835      6.000     95.000\n",
      "Prompt2DAG (LLM)              2043     38.338     12.951      6.000     95.000\n",
      "Prompt2DAG (Hybrid)           2043     38.312     12.847      6.000     95.000\n",
      "\n",
      "--- S1_Sem_KeyTerm_total ---\n",
      "Method                           N       Mean        Std        Min        Max\n",
      "--------------------------------------------------------------------------------\n",
      "Prompt2DAG (Template)         1578     95.498     13.842     69.000    136.000\n",
      "Prompt2DAG (LLM)              2043     96.789     13.422     69.000    136.000\n",
      "Prompt2DAG (Hybrid)           2043     96.789     13.422     69.000    136.000\n",
      "\n",
      "--- S1_Sem_tok_overlap_ratio ---\n",
      "Method                           N       Mean        Std        Min        Max\n",
      "--------------------------------------------------------------------------------\n",
      "Prompt2DAG (Template)         1578      0.526      0.121      0.223      0.853\n",
      "Prompt2DAG (LLM)              2043      0.532      0.114      0.223      0.853\n",
      "Prompt2DAG (Hybrid)           2043      0.532      0.113      0.223      0.853\n",
      "\n",
      "--- S1_Graph_overall_score ---\n",
      "Method                           N       Mean        Std        Min        Max\n",
      "--------------------------------------------------------------------------------\n",
      "Prompt2DAG (Template)         1578      8.766      1.728      4.320     10.000\n",
      "Prompt2DAG (LLM)              2043      8.991      1.573      4.200     10.000\n",
      "Prompt2DAG (Hybrid)           2043      8.999      1.568      4.250     10.000\n",
      "\n",
      "--- S1_Graph_total_issues ---\n",
      "Method                           N       Mean        Std        Min        Max\n",
      "--------------------------------------------------------------------------------\n",
      "Prompt2DAG (Template)         1578      1.291      2.636      0.000     38.000\n",
      "Prompt2DAG (LLM)              2043      1.257      2.754      0.000     38.000\n",
      "Prompt2DAG (Hybrid)           2043      1.245      2.751      0.000     38.000\n",
      "\n",
      "--- S1_Graph_Structural_Integrity_score ---\n",
      "Method                           N       Mean        Std        Min        Max\n",
      "--------------------------------------------------------------------------------\n",
      "Prompt2DAG (Template)         1578     33.631     38.359     10.000    100.000\n",
      "Prompt2DAG (LLM)              2043     35.105     39.437     10.000    100.000\n",
      "Prompt2DAG (Hybrid)           2043     35.130     39.462     10.000    100.000\n",
      "\n",
      "--- S1_Graph_Node_Connectivity_score ---\n",
      "Method                           N       Mean        Std        Min        Max\n",
      "--------------------------------------------------------------------------------\n",
      "Prompt2DAG (Template)         1578     29.732     40.533      0.000    100.000\n",
      "Prompt2DAG (LLM)              2043     32.088     41.163      0.000    100.000\n",
      "Prompt2DAG (Hybrid)           2043     32.138     41.166      0.000    100.000\n",
      "\n",
      "--- S1_Graph_Component_Usage_score ---\n",
      "Method                           N       Mean        Std        Min        Max\n",
      "--------------------------------------------------------------------------------\n",
      "Prompt2DAG (Template)         1578     37.682     40.085      0.000    100.000\n",
      "Prompt2DAG (LLM)              2043     37.728     39.937      0.000    100.000\n",
      "Prompt2DAG (Hybrid)           2043     37.787     39.944      0.000    100.000\n",
      "\n",
      "--- S1_Graph_Task_Component_Consistency_score ---\n",
      "Method                           N       Mean        Std        Min        Max\n",
      "--------------------------------------------------------------------------------\n",
      "Prompt2DAG (Template)         1578     39.670     42.190     10.000    100.000\n",
      "Prompt2DAG (LLM)              2043     39.645     42.168     10.000    100.000\n",
      "Prompt2DAG (Hybrid)           2043     39.704     42.224     10.000    100.000\n",
      "\n",
      "====================================================================================================\n",
      "A3. S1 METRICS BY ORCHESTRATOR (Prompt2DAG)\n",
      "====================================================================================================\n",
      "\n",
      "--- S1_Sem_BERT_f1 ---\n",
      "Orchestrator           N       Mean        Std\n",
      "--------------------------------------------------\n",
      "airflow             2043      0.682      0.056\n",
      "dagster             1797      0.677      0.056\n",
      "prefect             1824      0.678      0.057\n",
      "\n",
      "--- S1_Sem_BERT_norm ---\n",
      "Orchestrator           N       Mean        Std\n",
      "--------------------------------------------------\n",
      "airflow             2043      6.938      1.015\n",
      "dagster             1797      6.854      1.025\n",
      "prefect             1824      6.864      1.029\n",
      "\n",
      "--- S1_Sem_ROUGE1_f1 ---\n",
      "Orchestrator           N       Mean        Std\n",
      "--------------------------------------------------\n",
      "airflow             2043      0.421      0.086\n",
      "dagster             1797      0.414      0.089\n",
      "prefect             1824      0.415      0.089\n",
      "\n",
      "--- S1_Sem_ROUGE1_norm ---\n",
      "Orchestrator           N       Mean        Std\n",
      "--------------------------------------------------\n",
      "airflow             2043      6.231      1.554\n",
      "dagster             1797      6.206      1.610\n",
      "prefect             1824      6.215      1.594\n",
      "\n",
      "====================================================================================================\n",
      "A4. S1 METRICS INTERPRETATION\n",
      "====================================================================================================\n",
      "\n",
      "SEMANTIC METRICS INTERPRETATION:\n",
      "================================\n",
      "\n",
      "1. S1_Sem_BERT_f1 (BERTScore F1):\n",
      "   - Measures semantic similarity between prompt and generated intermediate representation\n",
      "   - Range: [0, 1], higher = better semantic preservation\n",
      "   - Typical good value: > 0.85\n",
      "\n",
      "2. S1_Sem_ROUGE1_f1 (ROUGE-1 F1):\n",
      "   - Measures unigram overlap between prompt and generated representation\n",
      "   - Range: [0, 1], higher = better lexical preservation\n",
      "   - Typical good value: > 0.5\n",
      "\n",
      "3. S1_Sem_KeyTerm_rate:\n",
      "   - Percentage of key terms from prompt preserved in generation\n",
      "   - Range: [0, 1], higher = better key concept retention\n",
      "   - Typical good value: > 0.7\n",
      "\n",
      "4. S1_Sem_tok_overlap_ratio:\n",
      "   - Token-level overlap ratio\n",
      "   - Range: [0, 1], higher = more overlap\n",
      "   - Typical good value: depends on task\n",
      "\n",
      "GRAPH METRICS INTERPRETATION:\n",
      "=============================\n",
      "\n",
      "1. S1_Graph_overall_score:\n",
      "   - Composite score of graph quality\n",
      "   - Range: [0, 10], higher = better graph structure\n",
      "   - Typical good value: > 7.0\n",
      "\n",
      "2. S1_Graph_total_issues:\n",
      "   - Total number of structural issues detected\n",
      "   - Range: [0, ∞], lower = fewer issues\n",
      "   - Typical good value: < 3\n",
      "\n",
      "3. S1_Graph_Structural_Integrity_score:\n",
      "   - Measures DAG structural validity (no cycles, proper edges)\n",
      "   - Range: [0, 10], higher = better integrity\n",
      "   - Typical good value: > 8.0\n",
      "\n",
      "4. S1_Graph_Node_Connectivity_score:\n",
      "   - Measures proper node connections (no orphans, proper flow)\n",
      "   - Range: [0, 10], higher = better connectivity\n",
      "   - Typical good value: > 8.0\n",
      "\n",
      "5. S1_Graph_Component_Usage_score:\n",
      "   - Measures appropriate use of orchestrator components\n",
      "   - Range: [0, 10], higher = better component usage\n",
      "   - Typical good value: > 7.0\n",
      "\n",
      "6. S1_Graph_Task_Component_Consistency_score:\n",
      "   - Measures consistency between task definitions and components\n",
      "   - Range: [0, 10], higher = better consistency\n",
      "   - Typical good value: > 7.0\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "A5. DISTRIBUTION ANALYSIS - KEY S1 METRICS\n",
      "====================================================================================================\n",
      "\n",
      "--- S1_Sem_BERT_f1 ---\n",
      "  N: 5,664\n",
      "  Mean: 0.6788\n",
      "  Std: 0.0563\n",
      "  Median: 0.6863\n",
      "  IQR: [0.6408, 0.7190]\n",
      "  Range: [0.4903, 0.8027]\n",
      "\n",
      "  Percentile breakdown:\n",
      "    P10: 0.6084\n",
      "    P25: 0.6408\n",
      "    P50: 0.6863\n",
      "    P75: 0.7190\n",
      "    P90: 0.7477\n",
      "    P95: 0.7658\n",
      "    P99: 0.7818\n",
      "\n",
      "--- S1_Sem_ROUGE1_f1 ---\n",
      "  N: 5,664\n",
      "  Mean: 0.4171\n",
      "  Std: 0.0882\n",
      "  Median: 0.4399\n",
      "  IQR: [0.3669, 0.4795]\n",
      "  Range: [0.1622, 0.5739]\n",
      "\n",
      "  Percentile breakdown:\n",
      "    P10: 0.2812\n",
      "    P25: 0.3669\n",
      "    P50: 0.4399\n",
      "    P75: 0.4795\n",
      "    P90: 0.5144\n",
      "    P95: 0.5237\n",
      "    P99: 0.5449\n",
      "\n",
      "--- S1_Graph_overall_score ---\n",
      "  N: 5,664\n",
      "  Mean: 8.9311\n",
      "  Std: 1.6188\n",
      "  Median: 9.7500\n",
      "  IQR: [9.0800, 10.0000]\n",
      "  Range: [4.2000, 10.0000]\n",
      "\n",
      "  Percentile breakdown:\n",
      "    P10: 5.9000\n",
      "    P25: 9.0800\n",
      "    P50: 9.7500\n",
      "    P75: 10.0000\n",
      "    P90: 10.0000\n",
      "    P95: 10.0000\n",
      "    P99: 10.0000\n",
      "\n",
      "--- S1_Graph_total_issues ---\n",
      "  N: 5,664\n",
      "  Mean: 1.2624\n",
      "  Std: 2.7198\n",
      "  Median: 1.0000\n",
      "  IQR: [0.0000, 2.0000]\n",
      "  Range: [0.0000, 38.0000]\n",
      "\n",
      "  Percentile breakdown:\n",
      "    P10: 0.0000\n",
      "    P25: 0.0000\n",
      "    P50: 1.0000\n",
      "    P75: 2.0000\n",
      "    P90: 3.0000\n",
      "    P95: 3.0000\n",
      "    P99: 4.0000\n",
      "\n",
      "====================================================================================================\n",
      "PART B: PREDICTIVE ANALYSIS - HOW S1 DRIVES SAT/PCT/ORT\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "B1. CORRELATION MATRIX: S1 vs OUTCOME METRICS (Run-Level)\n",
      "====================================================================================================\n",
      "\n",
      "--- Pearson Correlations: S1 Metrics vs Outcomes ---\n",
      "\n",
      "S1 Metric                                          SAT      PCT   Combined      ORT   Passed\n",
      "-----------------------------------------------------------------------------------------------\n",
      "S1_Sem_BERT_f1                                +0.198*** +0.179***  +0.195*** +0.155*** +0.192***\n",
      "S1_Sem_BERT_norm                              +0.198*** +0.179***  +0.195*** +0.155*** +0.192***\n",
      "S1_Sem_ROUGE1_f1                              +0.414*** +0.398***  +0.422*** +0.305*** +0.414***\n",
      "S1_Sem_ROUGE1_norm                            +0.461*** +0.440***  +0.468*** +0.315*** +0.448***\n",
      "S1_Sem_KeyTerm_rate                           +0.393*** +0.360***  +0.391*** +0.259*** +0.368***\n",
      "S1_Sem_KeyTerm_preserved                      +0.338*** +0.300***  +0.331*** +0.234*** +0.311***\n",
      "S1_Sem_KeyTerm_missing                        -0.325*** -0.303***  -0.327*** -0.211*** -0.308***\n",
      "S1_Sem_KeyTerm_total                           -0.032* -0.043**   -0.040**   -0.009 -0.039**\n",
      "S1_Sem_tok_overlap_ratio                      +0.431*** +0.399***  +0.431*** +0.294*** +0.408***\n",
      "S1_Graph_overall_score                        +0.531*** +0.538***  +0.556*** +0.407*** +0.552***\n",
      "S1_Graph_total_issues                         -0.206*** -0.203***  -0.213*** -0.153*** -0.204***\n",
      "S1_Graph_Structural_Integrity_score           +0.143*** +0.145***  +0.142*** +0.293*** +0.147***\n",
      "S1_Graph_Node_Connectivity_score              +0.218*** +0.221***  +0.221*** +0.340*** +0.225***\n",
      "S1_Graph_Component_Usage_score                  -0.012   -0.011     -0.019 +0.165***   -0.012\n",
      "S1_Graph_Task_Component_Consistency_score       +0.000   -0.001     -0.007 +0.178***   -0.002\n",
      "\n",
      "--- Spearman Correlations: S1 Metrics vs Outcomes (Rank-based) ---\n",
      "\n",
      "S1 Metric                                          SAT      PCT   Combined      ORT   Passed\n",
      "-----------------------------------------------------------------------------------------------\n",
      "S1_Sem_BERT_f1                                +0.140*** +0.125***  +0.136*** +0.174*** +0.206***\n",
      "S1_Sem_BERT_norm                              +0.140*** +0.125***  +0.137*** +0.174*** +0.206***\n",
      "S1_Sem_ROUGE1_f1                              +0.257*** +0.230***  +0.254*** +0.262*** +0.331***\n",
      "S1_Sem_ROUGE1_norm                            +0.368*** +0.312***  +0.355*** +0.291*** +0.393***\n",
      "S1_Sem_KeyTerm_rate                           +0.320*** +0.261***  +0.300*** +0.253*** +0.337***\n",
      "S1_Sem_KeyTerm_preserved                      +0.263*** +0.214***  +0.245*** +0.239*** +0.296***\n",
      "S1_Sem_KeyTerm_missing                        -0.288*** -0.234***  -0.271*** -0.212*** -0.295***\n",
      "S1_Sem_KeyTerm_total                          -0.053*** -0.042**  -0.052***   -0.003 -0.035**\n",
      "S1_Sem_tok_overlap_ratio                      +0.351*** +0.284***  +0.329*** +0.283*** +0.367***\n",
      "S1_Graph_overall_score                        +0.338*** +0.312***  +0.330*** +0.342*** +0.415***\n",
      "S1_Graph_total_issues                         -0.370*** -0.349***  -0.372*** -0.326*** -0.434***\n",
      "S1_Graph_Structural_Integrity_score           +0.049*** +0.047***     +0.020 +0.216*** +0.054***\n",
      "S1_Graph_Node_Connectivity_score              +0.367*** +0.343***  +0.351*** +0.453*** +0.438***\n",
      "S1_Graph_Component_Usage_score                  -0.006   -0.005   -0.035** +0.151***   -0.011\n",
      "S1_Graph_Task_Component_Consistency_score       +0.005   +0.005     -0.024 +0.167***   -0.001\n",
      "\n",
      "====================================================================================================\n",
      "B2. CORRELATION: S1 METRICS vs ISSUES\n",
      "====================================================================================================\n",
      "\n",
      "S1 Metric                                       Critical      Major      Minor      Total\n",
      "-----------------------------------------------------------------------------------------------\n",
      "S1_Sem_BERT_f1                                 -0.089***  +0.045***  +0.178***  +0.116***\n",
      "S1_Sem_BERT_norm                               -0.090***  +0.045***  +0.178***  +0.116***\n",
      "S1_Sem_ROUGE1_f1                               -0.140***  +0.122***  +0.258***  +0.199***\n",
      "S1_Sem_ROUGE1_norm                             -0.168***  +0.180***  +0.334***  +0.286***\n",
      "S1_Sem_KeyTerm_rate                            -0.148***  +0.144***  +0.311***  +0.257***\n",
      "S1_Sem_KeyTerm_preserved                       -0.142***  +0.096***  +0.289***  +0.213***\n",
      "S1_Sem_KeyTerm_missing                         +0.126***  -0.144***  -0.260***  -0.228***\n",
      "S1_Sem_KeyTerm_total                              +0.006  -0.058***     -0.005   -0.037**\n",
      "S1_Sem_tok_overlap_ratio                       -0.164***  +0.141***  +0.339***  +0.270***\n",
      "S1_Graph_overall_score                         -0.216***  +0.163***  +0.313***  +0.242***\n",
      "S1_Graph_total_issues                          +0.171***  -0.207***  -0.317***  -0.282***\n",
      "S1_Graph_Structural_Integrity_score            -0.300***  -0.069***    -0.031*  -0.149***\n",
      "S1_Graph_Node_Connectivity_score               -0.355***  +0.129***  +0.293***  +0.167***\n",
      "S1_Graph_Component_Usage_score                 -0.252***  -0.097***  -0.082***  -0.186***\n",
      "S1_Graph_Task_Component_Consistency_score      -0.266***  -0.097***  -0.073***  -0.184***\n",
      "\n",
      "====================================================================================================\n",
      "B3. BINNED ANALYSIS: S1 METRIC QUARTILES vs OUTCOMES\n",
      "====================================================================================================\n",
      "\n",
      "--- S1_Sem_BERT_f1 QUARTILE ANALYSIS ---\n",
      "\n",
      "Quartile             N          SAT          PCT     Combined          ORT      Pass%       Issues\n",
      "---------------------------------------------------------------------------------------------------------\n",
      "Q1 (low)          1428    4.12±3.26    4.05±3.71    4.11±3.40    1.80±2.14      54.8%    4.72±2.92\n",
      "Q2                1428    4.45±3.16    4.39±3.62    4.45±3.29    1.90±2.10      60.0%    4.98±2.92\n",
      "Q3                1412    5.72±2.37    5.89±2.91    5.84±2.54    2.74±2.08      81.0%    5.70±2.59\n",
      "Q4 (high)         1396    5.52±2.41    5.50±3.13    5.54±2.63    2.59±2.16      76.1%    5.44±2.54\n",
      "\n",
      "  Q1 vs Q4 ORT Comparison:\n",
      "    Q1 mean: 1.799, Q4 mean: 2.594\n",
      "    Difference: +0.796\n",
      "    t-statistic: -9.828, p-value: 0.0000\n",
      "    Effect size (Cohen's d): 0.370\n",
      "\n",
      "--- S1_Sem_ROUGE1_f1 QUARTILE ANALYSIS ---\n",
      "\n",
      "Quartile             N          SAT          PCT     Combined          ORT      Pass%       Issues\n",
      "---------------------------------------------------------------------------------------------------------\n",
      "Q1 (low)          1423    2.88±3.27    2.59±3.56    2.76±3.29    1.14±1.88      34.9%    3.91±2.84\n",
      "Q2                1411    5.71±2.40    5.75±3.04    5.77±2.59    2.60±2.18      78.7%    5.74±2.68\n",
      "Q3                1418    5.63±2.43    5.85±2.97    5.77±2.61    2.66±2.06      80.2%    5.69±2.56\n",
      "Q4 (high)         1412    5.58±2.44    5.62±3.04    5.63±2.62    2.63±2.13      78.0%    5.50±2.60\n",
      "\n",
      "  Q1 vs Q4 ORT Comparison:\n",
      "    Q1 mean: 1.142, Q4 mean: 2.627\n",
      "    Difference: +1.485\n",
      "    t-statistic: -19.688, p-value: 0.0000\n",
      "    Effect size (Cohen's d): 0.739\n",
      "\n",
      "--- S1_Graph_overall_score QUARTILE ANALYSIS ---\n",
      "\n",
      "Quartile             N          SAT          PCT     Combined          ORT      Pass%       Issues\n",
      "---------------------------------------------------------------------------------------------------------\n",
      "Q1 (low)            52    2.12±2.96    1.42±2.95    1.79±2.71    0.54±1.44      19.2%    4.27±3.11\n",
      "Q2                1153    1.90±2.88    1.31±2.86    1.62±2.68    0.55±1.38      17.3%    3.28±2.60\n",
      "Q3                  56    5.55±2.54    5.86±2.80    5.73±2.64    2.48±2.05      82.1%    5.54±2.61\n",
      "Q4 (high)         4403    5.77±2.32    5.93±2.88    5.89±2.49    2.72±2.10      81.5%    5.72±2.59\n",
      "\n",
      "  Q1 vs Q4 ORT Comparison:\n",
      "    Q1 mean: 0.536, Q4 mean: 2.718\n",
      "    Difference: +2.183\n",
      "    t-statistic: -7.460, p-value: 0.0000\n",
      "    Effect size (Cohen's d): 1.210\n",
      "\n",
      "====================================================================================================\n",
      "B4. THRESHOLD ANALYSIS: S1 METRICS AS QUALITY GATES\n",
      "====================================================================================================\n",
      "\n",
      "--- S1_Sem_BERT_f1 THRESHOLD ANALYSIS ---\n",
      "\n",
      "Threshold          N_Above    N_Below    ORT_Above    ORT_Below   Pass_Above   Pass_Below\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "--- S1_Sem_ROUGE1_f1 THRESHOLD ANALYSIS ---\n",
      "\n",
      "Threshold          N_Above    N_Below    ORT_Above    ORT_Below   Pass_Above   Pass_Below\n",
      "----------------------------------------------------------------------------------------------------\n",
      ">= 0.3                4794        870        2.569        0.524        77.3%        16.3%\n",
      ">= 0.4                3918       1746        2.624        1.428        78.7%        43.6%\n",
      ">= 0.5                 885       4779        2.670        2.178        78.3%        66.0%\n",
      "\n",
      "--- S1_Graph_overall_score THRESHOLD ANALYSIS ---\n",
      "\n",
      "Threshold          N_Above    N_Below    ORT_Above    ORT_Below   Pass_Above   Pass_Below\n",
      "----------------------------------------------------------------------------------------------------\n",
      ">= 5.0                5647         17        2.258        1.214        68.0%        35.3%\n",
      ">= 6.0                4682        982        2.608        0.574        78.4%        17.8%\n",
      ">= 7.0                4459       1205        2.715        0.553        81.5%        17.4%\n",
      ">= 8.0                4438       1226        2.716        0.589        81.5%        18.7%\n",
      "\n",
      "====================================================================================================\n",
      "B5. S1 METRICS BY PASS/FAIL STATUS\n",
      "====================================================================================================\n",
      "\n",
      "S1 Metric                                      Passed_Mean  Failed_Mean       Diff     t-stat    p-value    Sig\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "S1_Sem_BERT_f1                                      0.6863       0.6631    +0.0232     14.745     0.0000    ***\n",
      "S1_Sem_BERT_norm                                    7.0230       6.6015    +0.4215     14.748     0.0000    ***\n",
      "S1_Sem_ROUGE1_f1                                    0.4422       0.3640    +0.0782     34.215     0.0000    ***\n",
      "S1_Sem_ROUGE1_norm                                  6.7058       5.1845    +1.5213     37.724     0.0000    ***\n",
      "S1_Sem_KeyTerm_rate                                 0.6354       0.5481    +0.0873     29.808     0.0000    ***\n",
      "S1_Sem_KeyTerm_preserved                           60.6061      53.0435    +7.5626     24.628     0.0000    ***\n",
      "S1_Sem_KeyTerm_missing                             35.4602      44.1529    -8.6927    -24.386     0.0000    ***\n",
      "S1_Sem_KeyTerm_total                               96.0663      97.1964    -1.1301     -2.932     0.0034     **\n",
      "S1_Sem_tok_overlap_ratio                            0.5628       0.4620    +0.1008     33.588     0.0000    ***\n",
      "S1_Graph_overall_score                              9.5452       7.6321    +1.9132     49.786     0.0000    ***\n",
      "S1_Graph_total_issues                               0.8817       2.0677    -1.1860    -15.647     0.0000    ***\n",
      "S1_Graph_Structural_Integrity_score                38.6479      26.3586   +12.2893     11.149     0.0000    ***\n",
      "S1_Graph_Node_Connectivity_score                   37.7802      18.0574   +19.7228     17.345     0.0000    ***\n",
      "S1_Graph_Component_Usage_score                     37.4057      38.4364    -1.0307     -0.906     0.3650     ns\n",
      "S1_Graph_Task_Component_Consistency_score          39.6135      39.7992    -0.1857     -0.155     0.8771     ns\n",
      "\n",
      "====================================================================================================\n",
      "B6. S1 METRICS BY ISSUE SEVERITY\n",
      "====================================================================================================\n",
      "\n",
      "--- S1 Metrics by Issue Severity ---\n",
      "\n",
      "S1_Sem_BERT_f1:\n",
      "  Severity               N       Mean        Std\n",
      "  --------------------------------------------------\n",
      "  None (0)              90     0.6490     0.0633\n",
      "  Low (1-3)           1442     0.6656     0.0574\n",
      "  Medium (4-6)        2319     0.6845     0.0554\n",
      "  High (7+)           1813     0.6836     0.0539\n",
      "\n",
      "S1_Sem_BERT_norm:\n",
      "  Severity               N       Mean        Std\n",
      "  --------------------------------------------------\n",
      "  None (0)              90     6.3451     1.1508\n",
      "  Low (1-3)           1442     6.6466     1.0438\n",
      "  Medium (4-6)        2319     6.9912     1.0073\n",
      "  High (7+)           1813     6.9739     0.9803\n",
      "\n",
      "S1_Sem_ROUGE1_f1:\n",
      "  Severity               N       Mean        Std\n",
      "  --------------------------------------------------\n",
      "  None (0)              90     0.3389     0.1073\n",
      "  Low (1-3)           1442     0.3748     0.1042\n",
      "  Medium (4-6)        2319     0.4304     0.0793\n",
      "  High (7+)           1813     0.4375     0.0685\n",
      "\n",
      "S1_Sem_ROUGE1_norm:\n",
      "  Severity               N       Mean        Std\n",
      "  --------------------------------------------------\n",
      "  None (0)              90     4.8856     1.7261\n",
      "  Low (1-3)           1442     5.3869     1.7523\n",
      "  Medium (4-6)        2319     6.4083     1.4484\n",
      "  High (7+)           1813     6.7001     1.2887\n",
      "\n",
      "S1_Sem_KeyTerm_rate:\n",
      "  Severity               N       Mean        Std\n",
      "  --------------------------------------------------\n",
      "  None (0)              90     0.5312     0.1145\n",
      "  Low (1-3)           1442     0.5575     0.1169\n",
      "  Medium (4-6)        2319     0.6187     0.1051\n",
      "  High (7+)           1813     0.6364     0.0968\n",
      "\n",
      "S1_Sem_KeyTerm_preserved:\n",
      "  Severity               N       Mean        Std\n",
      "  --------------------------------------------------\n",
      "  None (0)              90    50.6889    13.1257\n",
      "  Low (1-3)           1442    53.6949    12.0565\n",
      "  Medium (4-6)        2319    59.3825    10.8279\n",
      "  High (7+)           1813    60.5769    10.1115\n",
      "\n",
      "====================================================================================================\n",
      "B7. PIPELINE-LEVEL ANALYSIS: S1 vs BEST P2D ORT\n",
      "====================================================================================================\n",
      "\n",
      "Pipeline-level analysis: 38 pipelines\n",
      "\n",
      "--- Pipeline-Level Correlations: S1 vs Best_ORT ---\n",
      "\n",
      "S1 Metric                                      vs Best_ORT  vs Mean_ORT vs Pass_Rate\n",
      "------------------------------------------------------------------------------------------\n",
      "S1_Sem_BERT_f1                                +0.067    -0.034    -0.329*  \n",
      "S1_Sem_BERT_norm                              +0.067    -0.034    -0.329*  \n",
      "S1_Sem_ROUGE1_f1                              +0.083    +0.032    -0.199   \n",
      "S1_Sem_ROUGE1_norm                            +0.152    +0.254    +0.542***\n",
      "S1_Sem_KeyTerm_rate                           -0.017    +0.092    +0.352*  \n",
      "S1_Sem_KeyTerm_preserved                      -0.100    +0.096    +0.013   \n",
      "S1_Sem_KeyTerm_missing                        -0.099    -0.053    -0.358*  \n",
      "S1_Sem_KeyTerm_total                          -0.189    -0.013    -0.267   \n",
      "S1_Sem_tok_overlap_ratio                      +0.191    +0.163    +0.394*  \n",
      "S1_Graph_overall_score                        +0.109    +0.513*** +0.633***\n",
      "S1_Graph_total_issues                         -0.012    -0.399*   -0.605***\n",
      "S1_Graph_Structural_Integrity_score           +0.244    +0.571*** +0.648***\n",
      "S1_Graph_Node_Connectivity_score              +0.238    +0.585*** +0.647***\n",
      "S1_Graph_Component_Usage_score                -0.143    +0.158    +0.303   \n",
      "S1_Graph_Task_Component_Consistency_score     -0.070    +0.251    +0.282   \n",
      "\n",
      "====================================================================================================\n",
      "B8. KEY FINDINGS SUMMARY\n",
      "====================================================================================================\n",
      "\n",
      "PART A - ABSOLUTE ANALYSIS FINDINGS:\n",
      "====================================\n",
      "\n",
      "1. SEMANTIC METRICS:\n",
      "   - BERTScore F1 captures semantic similarity between prompt and intermediate representation\n",
      "   - ROUGE-1 captures lexical overlap\n",
      "   - KeyTerm rate shows how well key concepts are preserved\n",
      "\n",
      "2. GRAPH METRICS:\n",
      "   - Overall score provides composite quality measure\n",
      "   - Structural integrity, node connectivity, and component usage are key sub-dimensions\n",
      "   - Total issues count identifies problematic generations\n",
      "\n",
      "PART B - PREDICTIVE ANALYSIS FINDINGS:\n",
      "======================================\n",
      "\n",
      "1. CORRELATION STRENGTH:\n",
      "   - Strong correlations (|r| > 0.3) indicate S1 metrics are predictive of outcomes\n",
      "   - Weak correlations (|r| < 0.1) suggest S1 metrics don't drive outcomes directly\n",
      "\n",
      "2. PRACTICAL IMPLICATIONS:\n",
      "   - If S1_Graph_overall_score strongly correlates with ORT:\n",
      "     → Use graph quality as early quality gate\n",
      "   - If S1_Sem_BERT_f1 weakly correlates with ORT:\n",
      "     → Semantic similarity alone doesn't guarantee code quality\n",
      "\n",
      "3. QUALITY GATES:\n",
      "   - Identified thresholds where S1 metrics become predictive\n",
      "   - Can use these for early rejection of poor generations\n",
      "\n",
      "4. PIPELINE VS RUN LEVEL:\n",
      "   - Run-level: High variance, S1 explains some but not all outcome variance\n",
      "   - Pipeline-level: More stable, S1 correlates better with aggregated outcomes\n",
      "\n",
      "RECOMMENDATIONS:\n",
      "================\n",
      "1. Use S1_Graph_overall_score as primary early quality indicator\n",
      "2. Combine multiple S1 metrics for better prediction\n",
      "3. Consider orchestrator-specific thresholds\n",
      "4. S1 metrics are necessary but not sufficient for quality\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "B9. CORRELATION TABLE FOR PAPER (Spearman)\n",
      "====================================================================================================\n",
      "\n",
      "--- S1 Metrics vs Outcomes (Spearman ρ) ---\n",
      "\n",
      "LaTeX format:\n",
      "\\begin{table}[h]\n",
      "\\centering\n",
      "\\caption{Correlation between S1 Metrics and Outcome Metrics}\n",
      "\\begin{tabular}{lrrrrr}\n",
      "\\toprule\n",
      "S1 Metric & Static_Score & Compliance_Score & Combined_Score & ORT_Score & Passed \\\\\n",
      "\\midrule\n",
      "BERT_f1 & +0.140 & +0.125 & +0.136 & +0.174 & +0.206 \\\\\n",
      "BERT_norm & +0.140 & +0.125 & +0.137 & +0.174 & +0.206 \\\\\n",
      "ROUGE1_f1 & +0.257 & +0.230 & +0.254 & +0.262 & +0.331 \\\\\n",
      "ROUGE1_norm & +0.368 & +0.312 & +0.355 & +0.291 & +0.393 \\\\\n",
      "KeyTerm_rate & +0.320 & +0.261 & +0.300 & +0.253 & +0.337 \\\\\n",
      "KeyTerm_preserved & +0.263 & +0.214 & +0.245 & +0.239 & +0.296 \\\\\n",
      "KeyTerm_missing & -0.288 & -0.234 & -0.271 & -0.212 & -0.295 \\\\\n",
      "KeyTerm_total & -0.053 & -0.042 & -0.052 & -0.003 & -0.035 \\\\\n",
      "tok_overlap_ratio & +0.351 & +0.284 & +0.329 & +0.283 & +0.367 \\\\\n",
      "G_overall_score & +0.338 & +0.312 & +0.330 & +0.342 & +0.415 \\\\\n",
      "G_total_issues & -0.370 & -0.349 & -0.372 & -0.326 & -0.434 \\\\\n",
      "G_Structural_Integrity_score & +0.049 & +0.047 & +0.020 & +0.216 & +0.054 \\\\\n",
      "G_Node_Connectivity_score & +0.367 & +0.343 & +0.351 & +0.453 & +0.438 \\\\\n",
      "G_Component_Usage_score & -0.006 & -0.005 & -0.035 & +0.151 & -0.011 \\\\\n",
      "G_Task_Component_Consistency_score & +0.005 & +0.005 & -0.024 & +0.167 & -0.001 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n",
      "====================================================================================================\n",
      "ANALYSIS COMPLETE\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "s1_semantic_graph_analysis.py\n",
    "\n",
    "Comprehensive analysis of S1 Semantic and Graph metrics:\n",
    "(a) Absolute analysis - what these metrics tell us\n",
    "(b) Predictive analysis - how they drive SAT/PCT/ORT and errors\n",
    "\n",
    "Uses the cleaned dataset: all_sessions_cleaned.csv\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "sns.set(style=\"whitegrid\")\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.width\", 200)\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"S1 SEMANTIC & GRAPH METRICS ANALYSIS\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# ============================================================================\n",
    "# 1. LOAD DATA\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"1. LOADING DATA\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "csv_path = \"/Users/abubakarialidu/Desktop/Data Result/all_sessions_cleaned.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "print(f\"Loaded {len(df):,} rows, {len(df.columns)} columns\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. METHOD CLASSIFICATION\n",
    "# ============================================================================\n",
    "\n",
    "def classify_method(row):\n",
    "    workflow = row.get(\"Workflow\", \"\")\n",
    "    strategy = str(row.get(\"Strategy\") or \"\").lower()\n",
    "    if workflow == \"Direct\":\n",
    "        return \"Direct (Non-Reasoning)\"\n",
    "    elif workflow == \"Reasoning\":\n",
    "        return \"Direct (Reasoning)\"\n",
    "    elif workflow == \"Prompt2DAG\":\n",
    "        if \"template\" in strategy:\n",
    "            return \"Prompt2DAG (Template)\"\n",
    "        elif \"llm\" in strategy:\n",
    "            return \"Prompt2DAG (LLM)\"\n",
    "        elif \"hybrid\" in strategy:\n",
    "            return \"Prompt2DAG (Hybrid)\"\n",
    "        else:\n",
    "            return f\"Prompt2DAG ({row['Strategy']})\"\n",
    "    else:\n",
    "        return workflow\n",
    "\n",
    "df[\"Method\"] = df.apply(classify_method, axis=1)\n",
    "\n",
    "METHOD_ORDER = [\n",
    "    \"Direct (Non-Reasoning)\",\n",
    "    \"Prompt2DAG (Template)\",\n",
    "    \"Prompt2DAG (LLM)\",\n",
    "    \"Prompt2DAG (Hybrid)\",\n",
    "    \"Direct (Reasoning)\",\n",
    "]\n",
    "\n",
    "df = df[df[\"Method\"].isin(METHOD_ORDER)].copy()\n",
    "\n",
    "print(f\"\\nTotal rows after filtering: {len(df):,}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3. ENSURE ISSUE COLUMNS AND ORT SCORES\n",
    "# ============================================================================\n",
    "\n",
    "for col in [\"Critical_Issues\", \"Major_Issues\", \"Minor_Issues\", \"Total_Issues\"]:\n",
    "    if col not in df.columns:\n",
    "        df[col] = 0\n",
    "    df[col] = df[col].fillna(0)\n",
    "\n",
    "df[\"Total_Issues\"] = df[\"Critical_Issues\"] + df[\"Major_Issues\"] + df[\"Minor_Issues\"]\n",
    "\n",
    "# ORT penalty weights\n",
    "ALPHA_CRIT = 2.0\n",
    "BETA_MAJOR = 1.0\n",
    "GAMMA_MINOR = 0.25\n",
    "\n",
    "df[\"Base_Score\"] = np.where(df[\"Passed\"] == True, df[\"Combined_Score\"], 0.0)\n",
    "df[\"Penalty\"] = (\n",
    "    ALPHA_CRIT * df[\"Critical_Issues\"] +\n",
    "    BETA_MAJOR * df[\"Major_Issues\"] +\n",
    "    GAMMA_MINOR * df[\"Minor_Issues\"]\n",
    ")\n",
    "df[\"ORT_Score_raw\"] = df[\"Base_Score\"] - df[\"Penalty\"]\n",
    "df[\"ORT_Score\"] = df[\"ORT_Score_raw\"].clip(lower=0.0, upper=10.0)\n",
    "\n",
    "# ============================================================================\n",
    "# 4. IDENTIFY S1 COLUMNS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"2. IDENTIFYING S1 SEMANTIC & GRAPH COLUMNS\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Semantic columns\n",
    "sem_cols_potential = [\n",
    "    \"S1_Sem_BERT_f1\",\n",
    "    \"S1_Sem_BERT_norm\",\n",
    "    \"S1_Sem_ROUGE1_f1\",\n",
    "    \"S1_Sem_ROUGE1_norm\",\n",
    "    \"S1_Sem_KeyTerm_rate\",\n",
    "    \"S1_Sem_KeyTerm_preserved\",\n",
    "    \"S1_Sem_KeyTerm_missing\",\n",
    "    \"S1_Sem_KeyTerm_total\",\n",
    "    \"S1_Sem_tok_overlap_ratio\",\n",
    "]\n",
    "\n",
    "# Graph columns\n",
    "graph_cols_potential = [\n",
    "    \"S1_Graph_overall_score\",\n",
    "    \"S1_Graph_total_issues\",\n",
    "    \"S1_Graph_Structural_Integrity_score\",\n",
    "    \"S1_Graph_Node_Connectivity_score\",\n",
    "    \"S1_Graph_Component_Usage_score\",\n",
    "    \"S1_Graph_Task_Component_Consistency_score\",\n",
    "]\n",
    "\n",
    "# Filter to available columns\n",
    "sem_cols = [c for c in sem_cols_potential if c in df.columns]\n",
    "graph_cols = [c for c in graph_cols_potential if c in df.columns]\n",
    "s1_cols = sem_cols + graph_cols\n",
    "\n",
    "print(f\"\\nAvailable Semantic columns ({len(sem_cols)}):\")\n",
    "for col in sem_cols:\n",
    "    non_null = df[col].notna().sum()\n",
    "    print(f\"  - {col}: {non_null:,} non-null ({non_null/len(df)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nAvailable Graph columns ({len(graph_cols)}):\")\n",
    "for col in graph_cols:\n",
    "    non_null = df[col].notna().sum()\n",
    "    print(f\"  - {col}: {non_null:,} non-null ({non_null/len(df)*100:.1f}%)\")\n",
    "\n",
    "# ============================================================================\n",
    "# 5. FILTER TO PROMPT2DAG (WHERE S1 METRICS EXIST)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"3. FILTERING TO PROMPT2DAG RUNS\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "df_p2d = df[df[\"Workflow\"] == \"Prompt2DAG\"].copy()\n",
    "print(f\"Prompt2DAG rows: {len(df_p2d):,}\")\n",
    "\n",
    "# Check S1 availability in P2D\n",
    "for col in s1_cols:\n",
    "    non_null = df_p2d[col].notna().sum()\n",
    "    print(f\"  {col}: {non_null:,} non-null ({non_null/len(df_p2d)*100:.1f}%)\")\n",
    "\n",
    "# Filter to rows with S1 data\n",
    "df_p2d_s1 = df_p2d.dropna(subset=s1_cols, how='all').copy()\n",
    "print(f\"\\nPrompt2DAG rows with any S1 data: {len(df_p2d_s1):,}\")\n",
    "\n",
    "# ============================================================================\n",
    "# PART A: ABSOLUTE ANALYSIS - WHAT S1 METRICS TELL US\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"PART A: ABSOLUTE ANALYSIS - WHAT S1 METRICS TELL US\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# ============================================================================\n",
    "# A1. GLOBAL SUMMARY STATISTICS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"A1. GLOBAL SUMMARY STATISTICS (Prompt2DAG)\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "if len(s1_cols) > 0:\n",
    "    summary_stats = df_p2d_s1[s1_cols].describe().T\n",
    "    summary_stats['non_null'] = df_p2d_s1[s1_cols].notna().sum()\n",
    "    summary_stats['null_pct'] = (df_p2d_s1[s1_cols].isna().sum() / len(df_p2d_s1) * 100).round(1)\n",
    "    \n",
    "    print(\"\\n--- Semantic Metrics ---\")\n",
    "    if sem_cols:\n",
    "        print(summary_stats.loc[sem_cols].round(3).to_string())\n",
    "    \n",
    "    print(\"\\n--- Graph Metrics ---\")\n",
    "    if graph_cols:\n",
    "        print(summary_stats.loc[graph_cols].round(3).to_string())\n",
    "\n",
    "# ============================================================================\n",
    "# A2. S1 METRICS BY PROMPT2DAG STRATEGY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"A2. S1 METRICS BY PROMPT2DAG STRATEGY\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "p2d_methods = [\"Prompt2DAG (Template)\", \"Prompt2DAG (LLM)\", \"Prompt2DAG (Hybrid)\"]\n",
    "\n",
    "for col in s1_cols:\n",
    "    print(f\"\\n--- {col} ---\")\n",
    "    print(f\"{'Method':<25} {'N':>8} {'Mean':>10} {'Std':>10} {'Min':>10} {'Max':>10}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for method in p2d_methods:\n",
    "        df_m = df_p2d_s1[df_p2d_s1[\"Method\"] == method]\n",
    "        data = df_m[col].dropna()\n",
    "        \n",
    "        if len(data) > 0:\n",
    "            print(f\"{method:<25} {len(data):>8} {data.mean():>10.3f} {data.std():>10.3f} {data.min():>10.3f} {data.max():>10.3f}\")\n",
    "        else:\n",
    "            print(f\"{method:<25} {0:>8} {'N/A':>10} {'N/A':>10} {'N/A':>10} {'N/A':>10}\")\n",
    "\n",
    "# ============================================================================\n",
    "# A3. S1 METRICS BY ORCHESTRATOR\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"A3. S1 METRICS BY ORCHESTRATOR (Prompt2DAG)\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "for col in s1_cols[:4]:  # Top 4 metrics\n",
    "    print(f\"\\n--- {col} ---\")\n",
    "    print(f\"{'Orchestrator':<15} {'N':>8} {'Mean':>10} {'Std':>10}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for orch in [\"airflow\", \"dagster\", \"prefect\"]:\n",
    "        df_o = df_p2d_s1[df_p2d_s1[\"Orchestrator\"] == orch]\n",
    "        data = df_o[col].dropna()\n",
    "        \n",
    "        if len(data) > 0:\n",
    "            print(f\"{orch:<15} {len(data):>8} {data.mean():>10.3f} {data.std():>10.3f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# A4. S1 METRICS INTERPRETATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"A4. S1 METRICS INTERPRETATION\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "print(\"\"\"\n",
    "SEMANTIC METRICS INTERPRETATION:\n",
    "================================\n",
    "\n",
    "1. S1_Sem_BERT_f1 (BERTScore F1):\n",
    "   - Measures semantic similarity between prompt and generated intermediate representation\n",
    "   - Range: [0, 1], higher = better semantic preservation\n",
    "   - Typical good value: > 0.85\n",
    "\n",
    "2. S1_Sem_ROUGE1_f1 (ROUGE-1 F1):\n",
    "   - Measures unigram overlap between prompt and generated representation\n",
    "   - Range: [0, 1], higher = better lexical preservation\n",
    "   - Typical good value: > 0.5\n",
    "\n",
    "3. S1_Sem_KeyTerm_rate:\n",
    "   - Percentage of key terms from prompt preserved in generation\n",
    "   - Range: [0, 1], higher = better key concept retention\n",
    "   - Typical good value: > 0.7\n",
    "\n",
    "4. S1_Sem_tok_overlap_ratio:\n",
    "   - Token-level overlap ratio\n",
    "   - Range: [0, 1], higher = more overlap\n",
    "   - Typical good value: depends on task\n",
    "\n",
    "GRAPH METRICS INTERPRETATION:\n",
    "=============================\n",
    "\n",
    "1. S1_Graph_overall_score:\n",
    "   - Composite score of graph quality\n",
    "   - Range: [0, 10], higher = better graph structure\n",
    "   - Typical good value: > 7.0\n",
    "\n",
    "2. S1_Graph_total_issues:\n",
    "   - Total number of structural issues detected\n",
    "   - Range: [0, ∞], lower = fewer issues\n",
    "   - Typical good value: < 3\n",
    "\n",
    "3. S1_Graph_Structural_Integrity_score:\n",
    "   - Measures DAG structural validity (no cycles, proper edges)\n",
    "   - Range: [0, 10], higher = better integrity\n",
    "   - Typical good value: > 8.0\n",
    "\n",
    "4. S1_Graph_Node_Connectivity_score:\n",
    "   - Measures proper node connections (no orphans, proper flow)\n",
    "   - Range: [0, 10], higher = better connectivity\n",
    "   - Typical good value: > 8.0\n",
    "\n",
    "5. S1_Graph_Component_Usage_score:\n",
    "   - Measures appropriate use of orchestrator components\n",
    "   - Range: [0, 10], higher = better component usage\n",
    "   - Typical good value: > 7.0\n",
    "\n",
    "6. S1_Graph_Task_Component_Consistency_score:\n",
    "   - Measures consistency between task definitions and components\n",
    "   - Range: [0, 10], higher = better consistency\n",
    "   - Typical good value: > 7.0\n",
    "\"\"\")\n",
    "\n",
    "# ============================================================================\n",
    "# A5. DISTRIBUTION ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"A5. DISTRIBUTION ANALYSIS - KEY S1 METRICS\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "key_metrics = [\"S1_Sem_BERT_f1\", \"S1_Sem_ROUGE1_f1\", \"S1_Graph_overall_score\", \"S1_Graph_total_issues\"]\n",
    "key_metrics = [m for m in key_metrics if m in df_p2d_s1.columns]\n",
    "\n",
    "for metric in key_metrics:\n",
    "    data = df_p2d_s1[metric].dropna()\n",
    "    \n",
    "    if len(data) > 0:\n",
    "        print(f\"\\n--- {metric} ---\")\n",
    "        print(f\"  N: {len(data):,}\")\n",
    "        print(f\"  Mean: {data.mean():.4f}\")\n",
    "        print(f\"  Std: {data.std():.4f}\")\n",
    "        print(f\"  Median: {data.median():.4f}\")\n",
    "        print(f\"  IQR: [{data.quantile(0.25):.4f}, {data.quantile(0.75):.4f}]\")\n",
    "        print(f\"  Range: [{data.min():.4f}, {data.max():.4f}]\")\n",
    "        \n",
    "        # Percentile breakdown\n",
    "        print(f\"\\n  Percentile breakdown:\")\n",
    "        for p in [10, 25, 50, 75, 90, 95, 99]:\n",
    "            print(f\"    P{p}: {data.quantile(p/100):.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# PART B: PREDICTIVE ANALYSIS - HOW S1 DRIVES SAT/PCT/ORT\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"PART B: PREDICTIVE ANALYSIS - HOW S1 DRIVES SAT/PCT/ORT\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# ============================================================================\n",
    "# B1. CORRELATION MATRIX: S1 vs OUTCOME METRICS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"B1. CORRELATION MATRIX: S1 vs OUTCOME METRICS (Run-Level)\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "outcome_cols = [\"Static_Score\", \"Compliance_Score\", \"Combined_Score\", \"ORT_Score\", \"Passed\"]\n",
    "issue_cols = [\"Critical_Issues\", \"Major_Issues\", \"Minor_Issues\", \"Total_Issues\"]\n",
    "\n",
    "# Pearson correlations\n",
    "print(\"\\n--- Pearson Correlations: S1 Metrics vs Outcomes ---\")\n",
    "print(f\"\\n{'S1 Metric':<45} {'SAT':>8} {'PCT':>8} {'Combined':>10} {'ORT':>8} {'Passed':>8}\")\n",
    "print(\"-\" * 95)\n",
    "\n",
    "for s1_col in s1_cols:\n",
    "    data = df_p2d_s1[[s1_col] + outcome_cols].dropna()\n",
    "    \n",
    "    if len(data) > 10:\n",
    "        correlations = []\n",
    "        for out_col in outcome_cols:\n",
    "            r, p = pearsonr(data[s1_col], data[out_col])\n",
    "            sig = \"***\" if p < 0.001 else \"**\" if p < 0.01 else \"*\" if p < 0.05 else \"\"\n",
    "            correlations.append(f\"{r:+.3f}{sig}\")\n",
    "        \n",
    "        print(f\"{s1_col:<45} {correlations[0]:>8} {correlations[1]:>8} {correlations[2]:>10} {correlations[3]:>8} {correlations[4]:>8}\")\n",
    "\n",
    "# Spearman correlations (more robust)\n",
    "print(\"\\n--- Spearman Correlations: S1 Metrics vs Outcomes (Rank-based) ---\")\n",
    "print(f\"\\n{'S1 Metric':<45} {'SAT':>8} {'PCT':>8} {'Combined':>10} {'ORT':>8} {'Passed':>8}\")\n",
    "print(\"-\" * 95)\n",
    "\n",
    "for s1_col in s1_cols:\n",
    "    data = df_p2d_s1[[s1_col] + outcome_cols].dropna()\n",
    "    \n",
    "    if len(data) > 10:\n",
    "        correlations = []\n",
    "        for out_col in outcome_cols:\n",
    "            r, p = spearmanr(data[s1_col], data[out_col])\n",
    "            sig = \"***\" if p < 0.001 else \"**\" if p < 0.01 else \"*\" if p < 0.05 else \"\"\n",
    "            correlations.append(f\"{r:+.3f}{sig}\")\n",
    "        \n",
    "        print(f\"{s1_col:<45} {correlations[0]:>8} {correlations[1]:>8} {correlations[2]:>10} {correlations[3]:>8} {correlations[4]:>8}\")\n",
    "\n",
    "# ============================================================================\n",
    "# B2. CORRELATION: S1 vs ISSUES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"B2. CORRELATION: S1 METRICS vs ISSUES\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "print(f\"\\n{'S1 Metric':<45} {'Critical':>10} {'Major':>10} {'Minor':>10} {'Total':>10}\")\n",
    "print(\"-\" * 95)\n",
    "\n",
    "for s1_col in s1_cols:\n",
    "    data = df_p2d_s1[[s1_col] + issue_cols].dropna()\n",
    "    \n",
    "    if len(data) > 10:\n",
    "        correlations = []\n",
    "        for issue_col in issue_cols:\n",
    "            r, p = spearmanr(data[s1_col], data[issue_col])\n",
    "            sig = \"***\" if p < 0.001 else \"**\" if p < 0.01 else \"*\" if p < 0.05 else \"\"\n",
    "            correlations.append(f\"{r:+.3f}{sig}\")\n",
    "        \n",
    "        print(f\"{s1_col:<45} {correlations[0]:>10} {correlations[1]:>10} {correlations[2]:>10} {correlations[3]:>10}\")\n",
    "\n",
    "# ============================================================================\n",
    "# B3. BINNED ANALYSIS: S1 QUARTILES vs OUTCOMES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"B3. BINNED ANALYSIS: S1 METRIC QUARTILES vs OUTCOMES\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "key_s1_metrics = [\"S1_Sem_BERT_f1\", \"S1_Sem_ROUGE1_f1\", \"S1_Graph_overall_score\"]\n",
    "key_s1_metrics = [m for m in key_s1_metrics if m in df_p2d_s1.columns]\n",
    "\n",
    "for metric in key_s1_metrics:\n",
    "    data = df_p2d_s1.dropna(subset=[metric, \"ORT_Score\"]).copy()\n",
    "    \n",
    "    if len(data) > 100:\n",
    "        try:\n",
    "            data[f\"{metric}_bin\"] = pd.qcut(data[metric], q=4, labels=[\"Q1 (low)\", \"Q2\", \"Q3\", \"Q4 (high)\"])\n",
    "        except ValueError:\n",
    "            # Handle case with too few unique values\n",
    "            data[f\"{metric}_bin\"] = pd.cut(data[metric], bins=4, labels=[\"Q1 (low)\", \"Q2\", \"Q3\", \"Q4 (high)\"])\n",
    "        \n",
    "        print(f\"\\n--- {metric} QUARTILE ANALYSIS ---\")\n",
    "        \n",
    "        agg_result = data.groupby(f\"{metric}_bin\").agg({\n",
    "            \"Static_Score\": [\"mean\", \"std\"],\n",
    "            \"Compliance_Score\": [\"mean\", \"std\"],\n",
    "            \"Combined_Score\": [\"mean\", \"std\"],\n",
    "            \"ORT_Score\": [\"mean\", \"std\"],\n",
    "            \"Passed\": [\"mean\", \"sum\", \"count\"],\n",
    "            \"Total_Issues\": [\"mean\", \"std\"],\n",
    "        }).round(3)\n",
    "        \n",
    "        print(f\"\\n{'Quartile':<15} {'N':>6} {'SAT':>12} {'PCT':>12} {'Combined':>12} {'ORT':>12} {'Pass%':>10} {'Issues':>12}\")\n",
    "        print(\"-\" * 105)\n",
    "        \n",
    "        for quartile in [\"Q1 (low)\", \"Q2\", \"Q3\", \"Q4 (high)\"]:\n",
    "            if quartile in agg_result.index:\n",
    "                row = agg_result.loc[quartile]\n",
    "                n = int(row[(\"Passed\", \"count\")])\n",
    "                sat = f\"{row[('Static_Score', 'mean')]:.2f}±{row[('Static_Score', 'std')]:.2f}\"\n",
    "                pct = f\"{row[('Compliance_Score', 'mean')]:.2f}±{row[('Compliance_Score', 'std')]:.2f}\"\n",
    "                combined = f\"{row[('Combined_Score', 'mean')]:.2f}±{row[('Combined_Score', 'std')]:.2f}\"\n",
    "                ort = f\"{row[('ORT_Score', 'mean')]:.2f}±{row[('ORT_Score', 'std')]:.2f}\"\n",
    "                pass_rate = f\"{row[('Passed', 'mean')]*100:.1f}%\"\n",
    "                issues = f\"{row[('Total_Issues', 'mean')]:.2f}±{row[('Total_Issues', 'std')]:.2f}\"\n",
    "                \n",
    "                print(f\"{quartile:<15} {n:>6} {sat:>12} {pct:>12} {combined:>12} {ort:>12} {pass_rate:>10} {issues:>12}\")\n",
    "        \n",
    "        # Statistical test: Q1 vs Q4\n",
    "        q1_data = data[data[f\"{metric}_bin\"] == \"Q1 (low)\"][\"ORT_Score\"]\n",
    "        q4_data = data[data[f\"{metric}_bin\"] == \"Q4 (high)\"][\"ORT_Score\"]\n",
    "        \n",
    "        if len(q1_data) > 5 and len(q4_data) > 5:\n",
    "            t_stat, p_value = stats.ttest_ind(q1_data, q4_data)\n",
    "            effect_size = (q4_data.mean() - q1_data.mean()) / np.sqrt((q1_data.std()**2 + q4_data.std()**2) / 2)\n",
    "            \n",
    "            print(f\"\\n  Q1 vs Q4 ORT Comparison:\")\n",
    "            print(f\"    Q1 mean: {q1_data.mean():.3f}, Q4 mean: {q4_data.mean():.3f}\")\n",
    "            print(f\"    Difference: {q4_data.mean() - q1_data.mean():+.3f}\")\n",
    "            print(f\"    t-statistic: {t_stat:.3f}, p-value: {p_value:.4f}\")\n",
    "            print(f\"    Effect size (Cohen's d): {effect_size:.3f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# B4. THRESHOLD ANALYSIS: S1 METRICS AS QUALITY GATES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"B4. THRESHOLD ANALYSIS: S1 METRICS AS QUALITY GATES\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "thresholds = {\n",
    "    \"S1_Sem_BERT_f1\": [0.80, 0.85, 0.90, 0.95],\n",
    "    \"S1_Sem_ROUGE1_f1\": [0.30, 0.40, 0.50, 0.60],\n",
    "    \"S1_Graph_overall_score\": [5.0, 6.0, 7.0, 8.0],\n",
    "}\n",
    "\n",
    "for metric, thresh_values in thresholds.items():\n",
    "    if metric not in df_p2d_s1.columns:\n",
    "        continue\n",
    "    \n",
    "    data = df_p2d_s1.dropna(subset=[metric, \"ORT_Score\", \"Passed\"]).copy()\n",
    "    \n",
    "    if len(data) < 100:\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n--- {metric} THRESHOLD ANALYSIS ---\")\n",
    "    print(f\"\\n{'Threshold':<15} {'N_Above':>10} {'N_Below':>10} {'ORT_Above':>12} {'ORT_Below':>12} {'Pass_Above':>12} {'Pass_Below':>12}\")\n",
    "    print(\"-\" * 100)\n",
    "    \n",
    "    for thresh in thresh_values:\n",
    "        above = data[data[metric] >= thresh]\n",
    "        below = data[data[metric] < thresh]\n",
    "        \n",
    "        if len(above) > 10 and len(below) > 10:\n",
    "            print(f\">= {thresh:<12} {len(above):>10} {len(below):>10} \"\n",
    "                  f\"{above['ORT_Score'].mean():>12.3f} {below['ORT_Score'].mean():>12.3f} \"\n",
    "                  f\"{above['Passed'].mean()*100:>11.1f}% {below['Passed'].mean()*100:>11.1f}%\")\n",
    "\n",
    "# ============================================================================\n",
    "# B5. S1 METRICS BY PASS/FAIL STATUS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"B5. S1 METRICS BY PASS/FAIL STATUS\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "print(f\"\\n{'S1 Metric':<45} {'Passed_Mean':>12} {'Failed_Mean':>12} {'Diff':>10} {'t-stat':>10} {'p-value':>10} {'Sig':>6}\")\n",
    "print(\"-\" * 115)\n",
    "\n",
    "for s1_col in s1_cols:\n",
    "    passed_data = df_p2d_s1[df_p2d_s1[\"Passed\"] == True][s1_col].dropna()\n",
    "    failed_data = df_p2d_s1[df_p2d_s1[\"Passed\"] == False][s1_col].dropna()\n",
    "    \n",
    "    if len(passed_data) > 10 and len(failed_data) > 10:\n",
    "        t_stat, p_value = stats.ttest_ind(passed_data, failed_data)\n",
    "        diff = passed_data.mean() - failed_data.mean()\n",
    "        sig = \"***\" if p_value < 0.001 else \"**\" if p_value < 0.01 else \"*\" if p_value < 0.05 else \"ns\"\n",
    "        \n",
    "        print(f\"{s1_col:<45} {passed_data.mean():>12.4f} {failed_data.mean():>12.4f} {diff:>+10.4f} {t_stat:>10.3f} {p_value:>10.4f} {sig:>6}\")\n",
    "\n",
    "# ============================================================================\n",
    "# B6. S1 METRICS BY ISSUE SEVERITY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"B6. S1 METRICS BY ISSUE SEVERITY\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Create issue severity bins\n",
    "df_p2d_s1[\"Issue_Severity\"] = pd.cut(\n",
    "    df_p2d_s1[\"Total_Issues\"],\n",
    "    bins=[-0.1, 0, 3, 6, 100],\n",
    "    labels=[\"None (0)\", \"Low (1-3)\", \"Medium (4-6)\", \"High (7+)\"]\n",
    ")\n",
    "\n",
    "print(f\"\\n--- S1 Metrics by Issue Severity ---\")\n",
    "\n",
    "for s1_col in s1_cols[:6]:  # Top 6 metrics\n",
    "    if s1_col not in df_p2d_s1.columns:\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n{s1_col}:\")\n",
    "    print(f\"  {'Severity':<15} {'N':>8} {'Mean':>10} {'Std':>10}\")\n",
    "    print(\"  \" + \"-\" * 50)\n",
    "    \n",
    "    for severity in [\"None (0)\", \"Low (1-3)\", \"Medium (4-6)\", \"High (7+)\"]:\n",
    "        data = df_p2d_s1[df_p2d_s1[\"Issue_Severity\"] == severity][s1_col].dropna()\n",
    "        if len(data) > 0:\n",
    "            print(f\"  {severity:<15} {len(data):>8} {data.mean():>10.4f} {data.std():>10.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# B7. PIPELINE-LEVEL ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"B7. PIPELINE-LEVEL ANALYSIS: S1 vs BEST P2D ORT\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Aggregate S1 metrics by pipeline\n",
    "pipe_s1 = df_p2d_s1.groupby(\"Pipeline_ID\")[s1_cols].mean().reset_index()\n",
    "\n",
    "# Best P2D ORT per pipeline\n",
    "pipe_ort = df_p2d_s1.groupby(\"Pipeline_ID\").agg({\n",
    "    \"ORT_Score\": [\"max\", \"mean\"],\n",
    "    \"Combined_Score\": [\"max\", \"mean\"],\n",
    "    \"Passed\": [\"mean\", \"sum\"],\n",
    "    \"Total_Issues\": \"mean\",\n",
    "}).reset_index()\n",
    "\n",
    "pipe_ort.columns = [\"Pipeline_ID\", \"Best_ORT\", \"Mean_ORT\", \"Best_Combined\", \"Mean_Combined\", \"Pass_Rate\", \"N_Passed\", \"Mean_Issues\"]\n",
    "\n",
    "# Merge\n",
    "pipe_analysis = pipe_s1.merge(pipe_ort, on=\"Pipeline_ID\", how=\"inner\")\n",
    "\n",
    "print(f\"\\nPipeline-level analysis: {len(pipe_analysis)} pipelines\")\n",
    "\n",
    "# Correlations at pipeline level\n",
    "print(\"\\n--- Pipeline-Level Correlations: S1 vs Best_ORT ---\")\n",
    "print(f\"\\n{'S1 Metric':<45} {'vs Best_ORT':>12} {'vs Mean_ORT':>12} {'vs Pass_Rate':>12}\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "for s1_col in s1_cols:\n",
    "    if s1_col in pipe_analysis.columns:\n",
    "        data = pipe_analysis[[s1_col, \"Best_ORT\", \"Mean_ORT\", \"Pass_Rate\"]].dropna()\n",
    "        \n",
    "        if len(data) > 5:\n",
    "            r_best, p_best = spearmanr(data[s1_col], data[\"Best_ORT\"])\n",
    "            r_mean, p_mean = spearmanr(data[s1_col], data[\"Mean_ORT\"])\n",
    "            r_pass, p_pass = spearmanr(data[s1_col], data[\"Pass_Rate\"])\n",
    "            \n",
    "            sig_best = \"***\" if p_best < 0.001 else \"**\" if p_best < 0.01 else \"*\" if p_best < 0.05 else \"\"\n",
    "            sig_mean = \"***\" if p_mean < 0.001 else \"**\" if p_mean < 0.01 else \"*\" if p_mean < 0.05 else \"\"\n",
    "            sig_pass = \"***\" if p_pass < 0.001 else \"**\" if p_pass < 0.01 else \"*\" if p_pass < 0.05 else \"\"\n",
    "            \n",
    "            print(f\"{s1_col:<45} {r_best:>+.3f}{sig_best:<3} {r_mean:>+.3f}{sig_mean:<3} {r_pass:>+.3f}{sig_pass:<3}\")\n",
    "\n",
    "# ============================================================================\n",
    "# B8. KEY FINDINGS SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"B8. KEY FINDINGS SUMMARY\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "print(\"\"\"\n",
    "PART A - ABSOLUTE ANALYSIS FINDINGS:\n",
    "====================================\n",
    "\n",
    "1. SEMANTIC METRICS:\n",
    "   - BERTScore F1 captures semantic similarity between prompt and intermediate representation\n",
    "   - ROUGE-1 captures lexical overlap\n",
    "   - KeyTerm rate shows how well key concepts are preserved\n",
    "\n",
    "2. GRAPH METRICS:\n",
    "   - Overall score provides composite quality measure\n",
    "   - Structural integrity, node connectivity, and component usage are key sub-dimensions\n",
    "   - Total issues count identifies problematic generations\n",
    "\n",
    "PART B - PREDICTIVE ANALYSIS FINDINGS:\n",
    "======================================\n",
    "\n",
    "1. CORRELATION STRENGTH:\n",
    "   - Strong correlations (|r| > 0.3) indicate S1 metrics are predictive of outcomes\n",
    "   - Weak correlations (|r| < 0.1) suggest S1 metrics don't drive outcomes directly\n",
    "\n",
    "2. PRACTICAL IMPLICATIONS:\n",
    "   - If S1_Graph_overall_score strongly correlates with ORT:\n",
    "     → Use graph quality as early quality gate\n",
    "   - If S1_Sem_BERT_f1 weakly correlates with ORT:\n",
    "     → Semantic similarity alone doesn't guarantee code quality\n",
    "\n",
    "3. QUALITY GATES:\n",
    "   - Identified thresholds where S1 metrics become predictive\n",
    "   - Can use these for early rejection of poor generations\n",
    "\n",
    "4. PIPELINE VS RUN LEVEL:\n",
    "   - Run-level: High variance, S1 explains some but not all outcome variance\n",
    "   - Pipeline-level: More stable, S1 correlates better with aggregated outcomes\n",
    "\n",
    "RECOMMENDATIONS:\n",
    "================\n",
    "1. Use S1_Graph_overall_score as primary early quality indicator\n",
    "2. Combine multiple S1 metrics for better prediction\n",
    "3. Consider orchestrator-specific thresholds\n",
    "4. S1 metrics are necessary but not sufficient for quality\n",
    "\"\"\")\n",
    "\n",
    "# ============================================================================\n",
    "# B9. DETAILED CORRELATION TABLE FOR PAPER\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"B9. CORRELATION TABLE FOR PAPER (Spearman)\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Create comprehensive correlation table\n",
    "all_metrics = s1_cols + outcome_cols + issue_cols\n",
    "available_metrics = [m for m in all_metrics if m in df_p2d_s1.columns]\n",
    "\n",
    "corr_data = df_p2d_s1[available_metrics].dropna()\n",
    "corr_matrix = corr_data.corr(method='spearman')\n",
    "\n",
    "# Print S1 vs Outcomes section\n",
    "print(\"\\n--- S1 Metrics vs Outcomes (Spearman ρ) ---\")\n",
    "print(\"\\nLaTeX format:\")\n",
    "print(\"\\\\begin{table}[h]\")\n",
    "print(\"\\\\centering\")\n",
    "print(\"\\\\caption{Correlation between S1 Metrics and Outcome Metrics}\")\n",
    "print(\"\\\\begin{tabular}{l\" + \"r\" * len(outcome_cols) + \"}\")\n",
    "print(\"\\\\toprule\")\n",
    "print(\"S1 Metric & \" + \" & \".join(outcome_cols) + \" \\\\\\\\\")\n",
    "print(\"\\\\midrule\")\n",
    "\n",
    "for s1_col in s1_cols:\n",
    "    if s1_col in corr_matrix.index:\n",
    "        row_values = []\n",
    "        for out_col in outcome_cols:\n",
    "            if out_col in corr_matrix.columns:\n",
    "                r = corr_matrix.loc[s1_col, out_col]\n",
    "                row_values.append(f\"{r:+.3f}\")\n",
    "            else:\n",
    "                row_values.append(\"--\")\n",
    "        \n",
    "        short_name = s1_col.replace(\"S1_Sem_\", \"\").replace(\"S1_Graph_\", \"G_\")\n",
    "        print(f\"{short_name} & \" + \" & \".join(row_values) + \" \\\\\\\\\")\n",
    "\n",
    "print(\"\\\\bottomrule\")\n",
    "print(\"\\\\end{tabular}\")\n",
    "print(\"\\\\end{table}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"ANALYSIS COMPLETE\")\n",
    "print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e44cdb90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================================================\n",
      "S1 METRICS CONSISTENCY INVESTIGATION\n",
      "========================================================================================================================\n",
      "\n",
      "Loaded 5,664 Prompt2DAG rows\n",
      "\n",
      "========================================================================================================================\n",
      "COMPUTING ORT SCORES\n",
      "========================================================================================================================\n",
      "\n",
      "Penalty weights:\n",
      "  Critical issues: α = 2.0\n",
      "  Major issues:    β = 1.0\n",
      "  Minor issues:    γ = 0.25\n",
      "\n",
      "ORT Score Statistics:\n",
      "  ORT_raw range:    [-10.50, 7.69]\n",
      "  ORT_capped range: [0.00, 7.69]\n",
      "  ORT_scaled range: [0.00, 10.00]\n",
      "\n",
      "========================================================================================================================\n",
      "INVESTIGATION 1: WHY DO HIGHER S1 SEMANTIC SCORES CORRELATE WITH MORE ISSUES?\n",
      "========================================================================================================================\n",
      "\n",
      "HYPOTHESIS: Pipeline Complexity is a Confounding Variable\n",
      "\n",
      "Theory:\n",
      "- More complex pipelines have more tasks/nodes\n",
      "- More tasks → more key terms in prompt → higher S1_Sem_KeyTerm scores\n",
      "- More tasks → more potential for issues → higher issue counts\n",
      "- So the correlation is SPURIOUS, driven by complexity, not quality\n",
      "\n",
      "\n",
      "--- Analysis by Pipeline Complexity ---\n",
      "\n",
      "Pipeline-level correlations (N=38 pipelines):\n",
      "\n",
      "Metric                    vs Total_Issues  vs Critical     vs Major     vs Minor\n",
      "--------------------------------------------------------------------------------\n",
      "KeyTerm_total             -0.306    +0.124 -0.384 -0.076\n",
      "KeyTerm_preserved         +0.023    -0.010 -0.029 +0.185\n",
      "BERT_f1                   -0.387*   +0.225 -0.344 -0.238\n",
      "ROUGE1_f1                 -0.278    +0.191 -0.221 -0.154\n",
      "\n",
      "========================================================================================================================\n",
      "INVESTIGATION 2: PASSED VS FAILED - DETAILED BREAKDOWN\n",
      "========================================================================================================================\n",
      "\n",
      "From B5, we saw:\n",
      "- Passed runs have HIGHER S1_Sem scores (as expected)\n",
      "- But B2 showed higher S1_Sem → MORE issues (counterintuitive)\n",
      "\n",
      "Let's break this down further...\n",
      "\n",
      "\n",
      "Passed runs: 3,846\n",
      "Failed runs: 1,818\n",
      "\n",
      "--- Issue Distribution by Pass/Fail Status ---\n",
      "\n",
      "Metric                Passed Mean   Passed Std  Failed Mean   Failed Std\n",
      "---------------------------------------------------------------------------\n",
      "Total_Issues                 6.31         2.23         2.87         2.33\n",
      "Critical_Issues              0.38         0.65         0.99         0.86\n",
      "Major_Issues                 2.06         1.39         0.70         1.22\n",
      "Minor_Issues                 3.87         1.52         1.18         1.79\n",
      "\n",
      "--- KEY INSIGHT: Issue Type Distribution ---\n",
      "\n",
      "  Passed runs: Total=6.31, Critical=0.38, Major=2.06, Minor=3.87\n",
      "  Failed runs: Total=2.87, Critical=0.99, Major=0.70, Minor=1.18\n",
      "\n",
      "  Critical Issues as % of Total:\n",
      "    Passed: 6.1%\n",
      "    Failed: 34.5%\n",
      "\n",
      "========================================================================================================================\n",
      "INVESTIGATION 3: CORRELATIONS WITHIN PASSED/FAILED GROUPS\n",
      "========================================================================================================================\n",
      "\n",
      "If the overall correlation is spurious due to pass/fail status,\n",
      "then WITHIN each group, the correlation should be different.\n",
      "\n",
      "\n",
      "--- Correlations WITHIN Passed Runs Only ---\n",
      "S1 Metric                           vs Total_Issues  vs Critical     vs Major     vs Minor\n",
      "-----------------------------------------------------------------------------------------------\n",
      "S1_Sem_BERT_f1                      -0.069 +0.002 -0.089 -0.026\n",
      "S1_Sem_ROUGE1_f1                    -0.053 -0.008 -0.044 -0.040\n",
      "S1_Sem_KeyTerm_rate                 +0.035 +0.003 +0.009 +0.025\n",
      "S1_Graph_overall_score              -0.039 -0.097 -0.005 -0.024\n",
      "S1_Graph_total_issues               -0.007 +0.003 -0.056 +0.051\n",
      "\n",
      "--- Correlations WITHIN Failed Runs Only ---\n",
      "S1 Metric                           vs Total_Issues  vs Critical     vs Major     vs Minor\n",
      "-----------------------------------------------------------------------------------------------\n",
      "S1_Sem_BERT_f1                      +0.124 -0.032 -0.002 +0.157\n",
      "S1_Sem_ROUGE1_f1                    +0.127 -0.035 -0.037 +0.180\n",
      "S1_Sem_KeyTerm_rate                 +0.148 -0.069 -0.052 +0.213\n",
      "S1_Graph_overall_score              +0.091 -0.028 -0.099 +0.148\n",
      "S1_Graph_total_issues               -0.123 +0.020 +0.074 -0.178\n",
      "\n",
      "========================================================================================================================\n",
      "INVESTIGATION 4: GRAPH METRIC SCALE VERIFICATION\n",
      "========================================================================================================================\n",
      "\n",
      "The documentation in A4 says Graph metrics have range [0, 10], but actual data shows [0, 100].\n",
      "Let's verify the actual scales and distributions.\n",
      "\n",
      "\n",
      "--- Graph Metric Distribution Analysis ---\n",
      "\n",
      "S1_Graph_Component_Usage_criteria_passed:\n",
      "  Range: [0.00, 1.00]\n",
      "  Mean: 0.74, Median: 1.00\n",
      "  Value distribution:\n",
      "    At 10:      0 (0.0%)\n",
      "    At 100:     0 (0.0%)\n",
      "    Other:   5664 (100.0%)\n",
      "\n",
      "S1_Graph_Component_Usage_criteria_total:\n",
      "  Range: [1.00, 1.00]\n",
      "  Mean: 1.00, Median: 1.00\n",
      "  Value distribution:\n",
      "    At 10:      0 (0.0%)\n",
      "    At 100:     0 (0.0%)\n",
      "    Other:   5664 (100.0%)\n",
      "\n",
      "S1_Graph_Component_Usage_score:\n",
      "  Range: [0.00, 100.00]\n",
      "  Mean: 37.74, Median: 10.00\n",
      "  Value distribution:\n",
      "    At 10:   3765 (66.5%)\n",
      "    At 100:  1391 (24.6%)\n",
      "    Other:    508 (9.0%)\n",
      "\n",
      "S1_Graph_Graph_Validity_criteria_passed:\n",
      "  Range: [1.00, 3.00]\n",
      "  Mean: 2.78, Median: 3.00\n",
      "  Value distribution:\n",
      "    At 10:      0 (0.0%)\n",
      "    At 100:     0 (0.0%)\n",
      "    Other:   5664 (100.0%)\n",
      "\n",
      "S1_Graph_Graph_Validity_criteria_total:\n",
      "  Range: [3.00, 3.00]\n",
      "  Mean: 3.00, Median: 3.00\n",
      "  Value distribution:\n",
      "    At 10:      0 (0.0%)\n",
      "    At 100:     0 (0.0%)\n",
      "    Other:   5664 (100.0%)\n",
      "\n",
      "S1_Graph_Graph_Validity_score:\n",
      "  Range: [10.00, 100.00]\n",
      "  Mean: 37.73, Median: 10.00\n",
      "  Value distribution:\n",
      "    At 10:   3776 (66.7%)\n",
      "    At 100:  1469 (25.9%)\n",
      "    Other:    419 (7.4%)\n",
      "\n",
      "S1_Graph_Node_Connectivity_criteria_passed:\n",
      "  Range: [0.00, 1.00]\n",
      "  Mean: 0.78, Median: 1.00\n",
      "  Value distribution:\n",
      "    At 10:      0 (0.0%)\n",
      "    At 100:     0 (0.0%)\n",
      "    Other:   5664 (100.0%)\n",
      "\n",
      "S1_Graph_Node_Connectivity_criteria_total:\n",
      "  Range: [1.00, 1.00]\n",
      "  Mean: 1.00, Median: 1.00\n",
      "  Value distribution:\n",
      "    At 10:      0 (0.0%)\n",
      "    At 100:     0 (0.0%)\n",
      "    Other:   5664 (100.0%)\n",
      "\n",
      "S1_Graph_Node_Connectivity_score:\n",
      "  Range: [0.00, 100.00]\n",
      "  Mean: 31.45, Median: 10.00\n",
      "  Value distribution:\n",
      "    At 10:   2973 (52.5%)\n",
      "    At 100:  1469 (25.9%)\n",
      "    Other:   1222 (21.6%)\n",
      "\n",
      "S1_Graph_Structural_Integrity_criteria_passed:\n",
      "  Range: [1.00, 4.00]\n",
      "  Mean: 3.36, Median: 4.00\n",
      "  Value distribution:\n",
      "    At 10:      0 (0.0%)\n",
      "    At 100:     0 (0.0%)\n",
      "    Other:   5664 (100.0%)\n",
      "\n",
      "S1_Graph_Structural_Integrity_criteria_total:\n",
      "  Range: [4.00, 4.00]\n",
      "  Mean: 4.00, Median: 4.00\n",
      "  Value distribution:\n",
      "    At 10:      0 (0.0%)\n",
      "    At 100:     0 (0.0%)\n",
      "    Other:   5664 (100.0%)\n",
      "\n",
      "S1_Graph_Structural_Integrity_score:\n",
      "  Range: [10.00, 100.00]\n",
      "  Mean: 34.70, Median: 10.00\n",
      "  Value distribution:\n",
      "    At 10:   3776 (66.7%)\n",
      "    At 100:  1486 (26.2%)\n",
      "    Other:    402 (7.1%)\n",
      "\n",
      "S1_Graph_Task_Component_Consistency_criteria_passed:\n",
      "  Range: [0.00, 1.00]\n",
      "  Mean: 0.98, Median: 1.00\n",
      "  Value distribution:\n",
      "    At 10:      0 (0.0%)\n",
      "    At 100:     0 (0.0%)\n",
      "    Other:   5664 (100.0%)\n",
      "\n",
      "S1_Graph_Task_Component_Consistency_criteria_total:\n",
      "  Range: [1.00, 1.00]\n",
      "  Mean: 1.00, Median: 1.00\n",
      "  Value distribution:\n",
      "    At 10:      0 (0.0%)\n",
      "    At 100:     0 (0.0%)\n",
      "    Other:   5664 (100.0%)\n",
      "\n",
      "S1_Graph_Task_Component_Consistency_score:\n",
      "  Range: [10.00, 100.00]\n",
      "  Mean: 39.67, Median: 10.00\n",
      "  Value distribution:\n",
      "    At 10:   3776 (66.7%)\n",
      "    At 100:  1841 (32.5%)\n",
      "    Other:     47 (0.8%)\n",
      "\n",
      "S1_Graph_entry_point_count:\n",
      "  Range: [0.00, 5.00]\n",
      "  Mean: 1.05, Median: 1.00\n",
      "  Value distribution:\n",
      "    At 10:      0 (0.0%)\n",
      "    At 100:     0 (0.0%)\n",
      "    Other:   5664 (100.0%)\n",
      "\n",
      "S1_Graph_max_pipeline_depth:\n",
      "  Range: [0.00, 10.00]\n",
      "  Mean: 3.12, Median: 3.00\n",
      "  Value distribution:\n",
      "    At 10:     21 (0.4%)\n",
      "    At 100:     0 (0.0%)\n",
      "    Other:   5643 (99.6%)\n",
      "\n",
      "S1_Graph_overall_score:\n",
      "  Range: [4.20, 10.00]\n",
      "  Mean: 8.93, Median: 9.75\n",
      "  Value distribution:\n",
      "    At 10:   1980 (35.0%)\n",
      "    At 100:     0 (0.0%)\n",
      "    Other:   3684 (65.0%)\n",
      "\n",
      "S1_Graph_status:\n",
      "  Type: object\n",
      "  Value counts:\n",
      "    PASS:  2570 (45.4%)\n",
      "    WARNING:  1764 (31.1%)\n",
      "    FAIL:  1330 (23.5%)\n",
      "\n",
      "S1_Graph_terminal_node_count:\n",
      "  Range: [0.00, 18.00]\n",
      "  Mean: 0.98, Median: 1.00\n",
      "  Value distribution:\n",
      "    At 10:      0 (0.0%)\n",
      "    At 100:     0 (0.0%)\n",
      "    Other:   5664 (100.0%)\n",
      "\n",
      "S1_Graph_total_components:\n",
      "  Range: [0.00, 16.00]\n",
      "  Mean: 3.78, Median: 4.00\n",
      "  Value distribution:\n",
      "    At 10:     93 (1.6%)\n",
      "    At 100:     0 (0.0%)\n",
      "    Other:   5571 (98.4%)\n",
      "\n",
      "S1_Graph_total_edges:\n",
      "  Range: [0.00, 25.00]\n",
      "  Mean: 3.45, Median: 3.00\n",
      "  Value distribution:\n",
      "    At 10:    116 (2.0%)\n",
      "    At 100:     0 (0.0%)\n",
      "    Other:   5548 (98.0%)\n",
      "\n",
      "S1_Graph_total_issues:\n",
      "  Range: [0.00, 38.00]\n",
      "  Mean: 1.26, Median: 1.00\n",
      "  Value distribution:\n",
      "    At 10:      0 (0.0%)\n",
      "    At 100:     0 (0.0%)\n",
      "    Other:   5664 (100.0%)\n",
      "\n",
      "S1_Graph_total_nodes_in_flow:\n",
      "  Range: [0.00, 25.00]\n",
      "  Mean: 3.97, Median: 4.00\n",
      "  Value distribution:\n",
      "    At 10:     93 (1.6%)\n",
      "    At 100:     0 (0.0%)\n",
      "    Other:   5571 (98.4%)\n",
      "\n",
      "========================================================================================================================\n",
      "INVESTIGATION 5: S1_GRAPH_OVERALL_SCORE DISTRIBUTION SKEW\n",
      "========================================================================================================================\n",
      "\n",
      "B3 showed Q4 had 4403 rows while Q1-Q3 combined had ~1261.\n",
      "This means the data is heavily skewed toward high values.\n",
      "Let's understand this better.\n",
      "\n",
      "\n",
      "--- S1_Graph_overall_score Distribution ---\n",
      "Total: 5,664\n",
      "Range: [4.20, 10.00]\n",
      "Mean: 8.93, Median: 9.75\n",
      "\n",
      "--- Distribution by Value Range ---\n",
      "S1_Graph_overall_score\n",
      "<5          17\n",
      "5-6        988\n",
      "6-7        200\n",
      "7-8         22\n",
      "8-9        161\n",
      "9-9.5      655\n",
      "9.5-10    3621\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- Pass Rate by Graph Score Range ---\n",
      "Range                  N      Pass%     ORT_Mean\n",
      "--------------------------------------------------\n",
      "<5                    17      35.3%         5.36\n",
      "5-6                  988      17.6%         4.73\n",
      "6-7                  200      15.0%         4.32\n",
      "7-8                   22      90.9%         6.93\n",
      "8-9                  161      76.4%         6.76\n",
      "9-9.5                655      81.1%         6.90\n",
      "9.5-10              3621      81.8%         6.94\n",
      "\n",
      "========================================================================================================================\n",
      "INVESTIGATION 6: WHAT DO S1 METRICS ACTUALLY CAPTURE?\n",
      "========================================================================================================================\n",
      "\n",
      "S1 metrics measure the quality of Step 1: Prompt → Intermediate Representation\n",
      "SAT/PCT/ORT measure the quality of Step 2: Intermediate Rep → Code\n",
      "\n",
      "Key Question: Is there a disconnect between S1 quality and final code quality?\n",
      "\n",
      "\n",
      "--- S1 Metrics by Orchestrator ---\n",
      "\n",
      "Metric                                   Airflow      Dagster      Prefect\n",
      "---------------------------------------------------------------------------\n",
      "S1_Sem_BERT_f1                             0.682        0.677        0.678\n",
      "S1_Sem_ROUGE1_f1                           0.421        0.414        0.415\n",
      "S1_Sem_KeyTerm_rate                        0.609        0.606        0.608\n",
      "S1_Graph_overall_score                     8.998        8.884        8.903\n",
      "\n",
      "--- S1 Metrics by P2D Method ---\n",
      "\n",
      "Metric                                  Template          LLM       Hybrid\n",
      "---------------------------------------------------------------------------\n",
      "S1_Sem_BERT_f1                             0.671        0.682        0.682\n",
      "S1_Sem_ROUGE1_f1                           0.407        0.420        0.421\n",
      "S1_Sem_KeyTerm_rate                        0.606        0.608        0.608\n",
      "S1_Graph_overall_score                     8.766        8.991        8.999\n",
      "\n",
      "========================================================================================================================\n",
      "INVESTIGATION 7: PARTIAL CORRELATIONS (Controlling for Confounders)\n",
      "========================================================================================================================\n",
      "\n",
      "If Pass/Fail status is a confounder, we should compute partial correlations\n",
      "controlling for it.\n",
      "\n",
      "\n",
      "--- Partial Correlations: S1 vs Issues, Controlling for Passed ---\n",
      "\n",
      "S1 Metric                                Raw r    Partial r   Difference\n",
      "---------------------------------------------------------------------------\n",
      "S1_Sem_BERT_f1                          +0.114       +0.004       -0.111\n",
      "S1_Sem_ROUGE1_f1                        +0.267       +0.036       -0.230\n",
      "S1_Sem_KeyTerm_rate                     +0.276       +0.083       -0.193\n",
      "S1_Graph_overall_score                  +0.344       +0.036       -0.308\n",
      "\n",
      "========================================================================================================================\n",
      "INVESTIGATION 8: THE TRUE CAUSAL STORY\n",
      "========================================================================================================================\n",
      "\n",
      "Based on investigations, here's the likely TRUE story:\n",
      "\n",
      "CAUSAL MODEL:\n",
      "=============\n",
      "\n",
      "  Pipeline Complexity\n",
      "         |\n",
      "         v\n",
      "  +------+------+\n",
      "  |             |\n",
      "  v             v\n",
      "S1 Scores    Final Code\n",
      "(Higher)     (More tasks)\n",
      "                |\n",
      "                v\n",
      "          More Minor Issues\n",
      "          (more code = more lint)\n",
      "                |\n",
      "                v\n",
      "          BUT: Better Pass Rate\n",
      "          (correct structure)\n",
      "\n",
      "The correlation between S1 and Issues is SPURIOUS because:\n",
      "1. Complex pipelines → higher S1 scores (more content to match)\n",
      "2. Complex pipelines → more code → more minor issues\n",
      "3. But complex, well-formed pipelines → PASS (despite having more minor issues)\n",
      "\n",
      "VERIFICATION: Check if Pass Rate differs by issue type\n",
      "\n",
      "\n",
      "--- Pass Rate by Issue Type Thresholds ---\n",
      "\n",
      "  By Critical Issues:\n",
      "    Critical >= 0:  5664 rows, Pass Rate: 67.9%\n",
      "    Critical >= 1:  2368 rows, Pass Rate: 46.9%\n",
      "    Critical >= 2:   781 rows, Pass Rate: 45.1%\n",
      "    Critical >= 3:   120 rows, Pass Rate: 5.0%\n",
      "\n",
      "  By Major Issues:\n",
      "    Major >= 0:  5664 rows, Pass Rate: 67.9%\n",
      "    Major >= 1:  4039 rows, Pass Rate: 84.0%\n",
      "    Major >= 2:  2684 rows, Pass Rate: 88.3%\n",
      "    Major >= 3:  1454 rows, Pass Rate: 89.4%\n",
      "    Major >= 4:   724 rows, Pass Rate: 84.9%\n",
      "\n",
      "  By Minor Issues:\n",
      "    Minor >= 0:  5664 rows, Pass Rate: 67.9%\n",
      "    Minor >= 2:  4233 rows, Pass Rate: 88.0%\n",
      "    Minor >= 4:  2384 rows, Pass Rate: 89.5%\n",
      "    Minor >= 6:   651 rows, Pass Rate: 88.3%\n",
      "    Minor >= 8:    71 rows, Pass Rate: 93.0%\n",
      "\n",
      "========================================================================================================================\n",
      "INVESTIGATION 9: CORRECTED ANALYSIS - S1 VS CRITICAL ISSUES ONLY\n",
      "========================================================================================================================\n",
      "\n",
      "Since Critical Issues are what actually determines Pass/Fail,\n",
      "let's focus on S1 vs Critical Issues correlation.\n",
      "\n",
      "\n",
      "--- S1 Metrics vs Critical Issues (The Correct Relationship) ---\n",
      "\n",
      "S1 Metric                                         All Runs  Passed Only  Failed Only\n",
      "------------------------------------------------------------------------------------------\n",
      "S1_Sem_BERT_f1                                      -0.089       +0.002       -0.032\n",
      "S1_Sem_BERT_norm                                    -0.090       +0.002       -0.032\n",
      "S1_Sem_KeyTerm_missing                              +0.126       -0.006       +0.055\n",
      "S1_Sem_KeyTerm_preserved                            -0.142       -0.006       -0.069\n",
      "S1_Sem_KeyTerm_rate                                 -0.148       +0.003       -0.069\n",
      "S1_Sem_KeyTerm_total                                +0.006       -0.011       +0.001\n",
      "S1_Sem_ROUGE1_f1                                    -0.140       -0.008       -0.035\n",
      "S1_Sem_ROUGE1_norm                                  -0.168       +0.001       -0.061\n",
      "S1_Sem_final_grade                                  +0.093       -0.017       +0.049\n",
      "S1_Sem_tok_generated                                -0.040       +0.000       -0.057\n",
      "S1_Sem_tok_original                                 +0.033       -0.000       +0.009\n",
      "S1_Sem_tok_overlap_ratio                            -0.164       -0.000       -0.072\n",
      "S1_Graph_Component_Usage_criteria_passed            +0.060       -0.008       +0.031\n",
      "S1_Graph_Component_Usage_criteria_total               +nan         +nan         +nan\n",
      "S1_Graph_Component_Usage_score                      -0.252       -0.423       -0.103\n",
      "S1_Graph_Graph_Validity_criteria_passed             -0.217       -0.004       -0.017\n",
      "S1_Graph_Graph_Validity_criteria_total                +nan         +nan         +nan\n",
      "S1_Graph_Graph_Validity_score                       -0.299       -0.429       -0.118\n",
      "S1_Graph_Node_Connectivity_criteria_passed          -0.218       +0.002       -0.024\n",
      "S1_Graph_Node_Connectivity_criteria_total             +nan         +nan         +nan\n",
      "S1_Graph_Node_Connectivity_score                    -0.355       -0.372       -0.048\n",
      "S1_Graph_Structural_Integrity_criteria_passed       -0.223       -0.004       -0.022\n",
      "S1_Graph_Structural_Integrity_criteria_total          +nan         +nan         +nan\n",
      "S1_Graph_Structural_Integrity_score                 -0.300       -0.430       -0.117\n",
      "S1_Graph_Task_Component_Consistency_criteria_passed       -0.005       -0.011       -0.039\n",
      "S1_Graph_Task_Component_Consistency_criteria_total         +nan         +nan         +nan\n",
      "S1_Graph_Task_Component_Consistency_score           -0.266       -0.431       -0.112\n",
      "S1_Graph_entry_point_count                          -0.185       +0.006       -0.019\n",
      "S1_Graph_max_pipeline_depth                         -0.165       +0.005       -0.028\n",
      "S1_Graph_overall_score                              -0.216       -0.097       -0.028\n",
      "S1_Graph_status                                     -0.160       +0.004       -0.025\n",
      "S1_Graph_terminal_node_count                        -0.188       -0.001       -0.016\n",
      "S1_Graph_total_components                           -0.161       +0.000       -0.030\n",
      "S1_Graph_total_edges                                -0.151       +0.005       -0.024\n",
      "S1_Graph_total_issues                               +0.171       +0.003       +0.020\n",
      "S1_Graph_total_nodes_in_flow                        -0.157       +0.005       -0.027\n",
      "\n",
      "========================================================================================================================\n",
      "SUMMARY: FINDINGS AND RECOMMENDATIONS\n",
      "========================================================================================================================\n",
      "\n",
      "KEY FINDINGS:\n",
      "=============\n",
      "\n",
      "1. THE ANOMALY EXPLAINED:\n",
      "   - Higher S1 scores correlating with MORE total issues is a SPURIOUS correlation\n",
      "   - Caused by: Passed runs have both higher S1 AND more issues (mostly minor)\n",
      "   - Critical Issues correlation with S1 is NEGATIVE (as expected)\n",
      "\n",
      "2. ISSUE TYPE MATTERS:\n",
      "   - Critical Issues: Determine Pass/Fail, negatively correlated with S1\n",
      "   - Major Issues: Mixed relationship\n",
      "   - Minor Issues: Positively correlated with S1 (more code = more lint warnings)\n",
      "\n",
      "3. GRAPH METRIC SCALE:\n",
      "   - Actual range is [0, 100], not [0, 10] as documented\n",
      "   - S1_Graph_overall_score is on [0, 10] scale\n",
      "   - Sub-scores (Structural_Integrity, etc.) are on [0, 100] scale\n",
      "\n",
      "4. S1_GRAPH_OVERALL_SCORE:\n",
      "   - Highly skewed: 78% of runs have score >= 9.5\n",
      "   - This is good news: Step 1 generally produces valid graph structures\n",
      "   - Low scores strongly predict failure\n",
      "\n",
      "CORRECTED INTERPRETATION:\n",
      "========================\n",
      "\n",
      "For your paper, focus on:\n",
      "\n",
      "1. S1_Graph_overall_score vs Pass Rate:\n",
      "   - Strong positive relationship (r = 0.415)\n",
      "   - Use as quality gate: score < 7 → likely failure\n",
      "\n",
      "2. S1_Sem metrics vs Pass Rate:\n",
      "   - Moderate positive relationship (r = 0.2-0.4)\n",
      "   - Higher semantic fidelity → better final code\n",
      "\n",
      "3. S1 vs Critical Issues (the TRUE quality relationship):\n",
      "   - Negative correlation as expected\n",
      "   - Higher S1 → fewer critical issues\n",
      "\n",
      "4. Ignore S1 vs Minor Issues correlation:\n",
      "   - This is a complexity artifact, not a quality signal\n",
      "\n",
      "RECOMMENDATIONS:\n",
      "================\n",
      "\n",
      "1. Report S1 vs Pass Rate (clear positive relationship)\n",
      "2. Report S1 vs Critical Issues (clear negative relationship)\n",
      "3. DO NOT report S1 vs Total Issues (misleading due to confounding)\n",
      "4. Fix documentation: Graph sub-scores are [0, 100], not [0, 10]\n",
      "5. Note that S1_Graph_overall_score > 9 for ~78% of successful runs\n",
      "\n",
      "\n",
      "========================================================================================================================\n",
      "CORRECTED CORRELATION TABLE FOR PAPER\n",
      "========================================================================================================================\n",
      "\n",
      "Use these correlations in your paper:\n",
      "\n",
      "\n",
      "S1 Metric                                   vs Pass     vs ORT  vs Critical\n",
      "--------------------------------------------------------------------------------\n",
      "S1_Sem_BERT_f1                           +0.206*** +0.178*** -0.089***\n",
      "S1_Sem_BERT_norm                         +0.206*** +0.178*** -0.090***\n",
      "S1_Sem_KeyTerm_missing                   -0.295*** -0.219*** +0.126***\n",
      "S1_Sem_KeyTerm_preserved                 +0.296*** +0.245*** -0.142***\n",
      "S1_Sem_KeyTerm_rate                      +0.337*** +0.261*** -0.148***\n",
      "S1_Sem_KeyTerm_total                     -0.035**  -0.006    +0.006   \n",
      "S1_Sem_ROUGE1_f1                         +0.331*** +0.270*** -0.140***\n",
      "S1_Sem_ROUGE1_norm                       +0.393*** +0.303*** -0.168***\n",
      "S1_Sem_final_grade                       -0.227*** -0.177*** +0.093***\n",
      "S1_Sem_tok_generated                     +0.066*** +0.071*** -0.040** \n",
      "S1_Sem_tok_original                      -0.079*** -0.032*   +0.033*  \n",
      "S1_Sem_tok_overlap_ratio                 +0.367*** +0.293*** -0.164***\n",
      "S1_Graph_Component_Usage_criteria_passed -0.151*** -0.137*** +0.060***\n",
      "S1_Graph_Component_Usage_criteria_total  +nan    +nan    +nan   \n",
      "S1_Graph_Component_Usage_score           -0.011    +0.165*** -0.252***\n",
      "S1_Graph_Graph_Validity_criteria_passed  +0.549*** +0.441*** -0.217***\n",
      "S1_Graph_Graph_Validity_criteria_total   +nan    +nan    +nan   \n",
      "S1_Graph_Graph_Validity_score            +0.053*** +0.227*** -0.299***\n",
      "S1_Graph_Node_Connectivity_criteria_passed +0.553*** +0.443*** -0.218***\n",
      "S1_Graph_Node_Connectivity_criteria_total +nan    +nan    +nan   \n",
      "S1_Graph_Node_Connectivity_score         +0.438*** +0.467*** -0.355***\n",
      "S1_Graph_Structural_Integrity_criteria_passed +0.562*** +0.453*** -0.223***\n",
      "S1_Graph_Structural_Integrity_criteria_total +nan    +nan    +nan   \n",
      "S1_Graph_Structural_Integrity_score      +0.054*** +0.228*** -0.300***\n",
      "S1_Graph_Task_Component_Consistency_criteria_passed -0.030*   -0.009    -0.005   \n",
      "S1_Graph_Task_Component_Consistency_criteria_total +nan    +nan    +nan   \n",
      "S1_Graph_Task_Component_Consistency_score -0.001    +0.181*** -0.266***\n",
      "S1_Graph_entry_point_count               +0.474*** +0.394*** -0.185***\n",
      "S1_Graph_max_pipeline_depth              +0.411*** +0.340*** -0.165***\n",
      "S1_Graph_overall_score                   +0.415*** +0.354*** -0.216***\n",
      "S1_Graph_status                          +0.403*** +0.338*** -0.160***\n",
      "S1_Graph_terminal_node_count             +0.479*** +0.389*** -0.188***\n",
      "S1_Graph_total_components                +0.393*** +0.348*** -0.161***\n",
      "S1_Graph_total_edges                     +0.380*** +0.336*** -0.151***\n",
      "S1_Graph_total_issues                    -0.434*** -0.335*** +0.171***\n",
      "S1_Graph_total_nodes_in_flow             +0.391*** +0.342*** -0.157***\n",
      "\n",
      "========================================================================================================================\n",
      "INVESTIGATION COMPLETE\n",
      "========================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "s1_consistency_investigation.py\n",
    "\n",
    "Deep investigation into S1 metrics anomalies:\n",
    "1. Why do higher semantic scores correlate with MORE issues?\n",
    "2. Why do runs with high issues have higher S1 scores?\n",
    "3. Is there a confounding variable (complexity, orchestrator, method)?\n",
    "4. Are S1 metrics measuring what we think they're measuring?\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.width\", 200)\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "\n",
    "print(\"=\" * 120)\n",
    "print(\"S1 METRICS CONSISTENCY INVESTIGATION\")\n",
    "print(\"=\" * 120)\n",
    "\n",
    "# ============================================================================\n",
    "# 1. LOAD DATA\n",
    "# ============================================================================\n",
    "\n",
    "csv_path = \"/Users/abubakarialidu/Desktop/Data Result/all_sessions_cleaned.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Filter to Prompt2DAG only (where S1 metrics exist)\n",
    "df_p2d = df[df[\"Workflow\"] == \"Prompt2DAG\"].copy()\n",
    "\n",
    "print(f\"\\nLoaded {len(df_p2d):,} Prompt2DAG rows\")\n",
    "\n",
    "# S1 columns\n",
    "sem_cols = [c for c in df_p2d.columns if c.startswith(\"S1_Sem_\")]\n",
    "graph_cols = [c for c in df_p2d.columns if c.startswith(\"S1_Graph_\")]\n",
    "s1_cols = sem_cols + graph_cols\n",
    "\n",
    "# Ensure issue columns\n",
    "for col in [\"Critical_Issues\", \"Major_Issues\", \"Minor_Issues\", \"Total_Issues\"]:\n",
    "    if col not in df_p2d.columns:\n",
    "        df_p2d[col] = 0\n",
    "    df_p2d[col] = df_p2d[col].fillna(0)\n",
    "\n",
    "df_p2d[\"Total_Issues\"] = df_p2d[\"Critical_Issues\"] + df_p2d[\"Major_Issues\"] + df_p2d[\"Minor_Issues\"]\n",
    "\n",
    "# ============================================================================\n",
    "# COMPUTE ORT (OVERALL ROBUSTNESS TEST) SCORES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 120)\n",
    "print(\"COMPUTING ORT SCORES\")\n",
    "print(\"=\" * 120)\n",
    "\n",
    "# Penalty weights\n",
    "ALPHA_CRIT = 2.0\n",
    "BETA_MAJOR = 1.0\n",
    "GAMMA_MINOR = 0.25\n",
    "\n",
    "print(f\"\\nPenalty weights:\")\n",
    "print(f\"  Critical issues: α = {ALPHA_CRIT}\")\n",
    "print(f\"  Major issues:    β = {BETA_MAJOR}\")\n",
    "print(f\"  Minor issues:    γ = {GAMMA_MINOR}\")\n",
    "\n",
    "# Base score: Combined_Score if Passed, else 0\n",
    "df_p2d[\"Base_Score\"] = np.where(df_p2d[\"Passed\"] == True, df_p2d[\"Combined_Score\"], 0.0)\n",
    "\n",
    "# Calculate penalty\n",
    "df_p2d[\"Penalty\"] = (\n",
    "    ALPHA_CRIT * df_p2d[\"Critical_Issues\"] +\n",
    "    BETA_MAJOR * df_p2d[\"Major_Issues\"] +\n",
    "    GAMMA_MINOR * df_p2d[\"Minor_Issues\"]\n",
    ")\n",
    "\n",
    "# ORT_Score_raw (can be negative)\n",
    "df_p2d[\"ORT_Score_raw\"] = df_p2d[\"Base_Score\"] - df_p2d[\"Penalty\"]\n",
    "\n",
    "# ORT_Score_capped (clamped to [0, 10])\n",
    "df_p2d[\"ORT_Score_capped\"] = df_p2d[\"ORT_Score_raw\"].clip(lower=0.0, upper=10.0)\n",
    "\n",
    "# ORT_Score_scaled (min-max normalization to [0, 10])\n",
    "ort_min = df_p2d[\"ORT_Score_raw\"].min()\n",
    "ort_max = df_p2d[\"ORT_Score_raw\"].max()\n",
    "\n",
    "if ort_max > ort_min:\n",
    "    df_p2d[\"ORT_Score_scaled\"] = 10 * (df_p2d[\"ORT_Score_raw\"] - ort_min) / (ort_max - ort_min)\n",
    "else:\n",
    "    df_p2d[\"ORT_Score_scaled\"] = 0.0\n",
    "\n",
    "# Use ORT_Score_scaled as the default ORT_Score\n",
    "df_p2d[\"ORT_Score\"] = df_p2d[\"ORT_Score_scaled\"]\n",
    "\n",
    "print(f\"\\nORT Score Statistics:\")\n",
    "print(f\"  ORT_raw range:    [{df_p2d['ORT_Score_raw'].min():.2f}, {df_p2d['ORT_Score_raw'].max():.2f}]\")\n",
    "print(f\"  ORT_capped range: [{df_p2d['ORT_Score_capped'].min():.2f}, {df_p2d['ORT_Score_capped'].max():.2f}]\")\n",
    "print(f\"  ORT_scaled range: [{df_p2d['ORT_Score_scaled'].min():.2f}, {df_p2d['ORT_Score_scaled'].max():.2f}]\")\n",
    "\n",
    "# ============================================================================\n",
    "# INVESTIGATION 1: THE COUNTERINTUITIVE CORRELATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 120)\n",
    "print(\"INVESTIGATION 1: WHY DO HIGHER S1 SEMANTIC SCORES CORRELATE WITH MORE ISSUES?\")\n",
    "print(\"=\" * 120)\n",
    "\n",
    "print(\"\"\"\n",
    "HYPOTHESIS: Pipeline Complexity is a Confounding Variable\n",
    "\n",
    "Theory:\n",
    "- More complex pipelines have more tasks/nodes\n",
    "- More tasks → more key terms in prompt → higher S1_Sem_KeyTerm scores\n",
    "- More tasks → more potential for issues → higher issue counts\n",
    "- So the correlation is SPURIOUS, driven by complexity, not quality\n",
    "\"\"\")\n",
    "\n",
    "# Check if there's a complexity proxy\n",
    "# Look for pipeline characteristics that might indicate complexity\n",
    "\n",
    "# First, let's check the relationship by Pipeline_ID\n",
    "print(\"\\n--- Analysis by Pipeline Complexity ---\")\n",
    "\n",
    "# Calculate pipeline-level stats\n",
    "pipe_stats = df_p2d.groupby(\"Pipeline_ID\").agg({\n",
    "    \"S1_Sem_KeyTerm_total\": \"mean\",  # Proxy for complexity (more terms = more complex?)\n",
    "    \"S1_Sem_KeyTerm_preserved\": \"mean\",\n",
    "    \"S1_Sem_BERT_f1\": \"mean\",\n",
    "    \"S1_Sem_ROUGE1_f1\": \"mean\",\n",
    "    \"Total_Issues\": \"mean\",\n",
    "    \"Critical_Issues\": \"mean\",\n",
    "    \"Major_Issues\": \"mean\",\n",
    "    \"Minor_Issues\": \"mean\",\n",
    "    \"Combined_Score\": \"mean\",\n",
    "    \"ORT_Score\": \"mean\",\n",
    "    \"Passed\": \"mean\",\n",
    "}).reset_index()\n",
    "\n",
    "pipe_stats.columns = [\"Pipeline_ID\", \"KeyTerm_total\", \"KeyTerm_preserved\", \"BERT_f1\", \n",
    "                      \"ROUGE1_f1\", \"Total_Issues\", \"Critical\", \"Major\", \"Minor\",\n",
    "                      \"Combined\", \"ORT\", \"Pass_Rate\"]\n",
    "\n",
    "print(\"\\nPipeline-level correlations (N=38 pipelines):\")\n",
    "print(f\"\\n{'Metric':<25} {'vs Total_Issues':>15} {'vs Critical':>12} {'vs Major':>12} {'vs Minor':>12}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for metric in [\"KeyTerm_total\", \"KeyTerm_preserved\", \"BERT_f1\", \"ROUGE1_f1\"]:\n",
    "    r_total, p_total = spearmanr(pipe_stats[metric], pipe_stats[\"Total_Issues\"])\n",
    "    r_crit, _ = spearmanr(pipe_stats[metric], pipe_stats[\"Critical\"])\n",
    "    r_major, _ = spearmanr(pipe_stats[metric], pipe_stats[\"Major\"])\n",
    "    r_minor, _ = spearmanr(pipe_stats[metric], pipe_stats[\"Minor\"])\n",
    "    \n",
    "    sig = \"***\" if p_total < 0.001 else \"**\" if p_total < 0.01 else \"*\" if p_total < 0.05 else \"\"\n",
    "    print(f\"{metric:<25} {r_total:>+.3f}{sig:<3} {r_crit:>+.3f} {r_major:>+.3f} {r_minor:>+.3f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# INVESTIGATION 2: PASSED VS FAILED - THE REAL STORY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 120)\n",
    "print(\"INVESTIGATION 2: PASSED VS FAILED - DETAILED BREAKDOWN\")\n",
    "print(\"=\" * 120)\n",
    "\n",
    "print(\"\"\"\n",
    "From B5, we saw:\n",
    "- Passed runs have HIGHER S1_Sem scores (as expected)\n",
    "- But B2 showed higher S1_Sem → MORE issues (counterintuitive)\n",
    "\n",
    "Let's break this down further...\n",
    "\"\"\")\n",
    "\n",
    "df_passed = df_p2d[df_p2d[\"Passed\"] == True].copy()\n",
    "df_failed = df_p2d[df_p2d[\"Passed\"] == False].copy()\n",
    "\n",
    "print(f\"\\nPassed runs: {len(df_passed):,}\")\n",
    "print(f\"Failed runs: {len(df_failed):,}\")\n",
    "\n",
    "# Compare issue distributions\n",
    "print(\"\\n--- Issue Distribution by Pass/Fail Status ---\")\n",
    "print(f\"\\n{'Metric':<20} {'Passed Mean':>12} {'Passed Std':>12} {'Failed Mean':>12} {'Failed Std':>12}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "for col in [\"Total_Issues\", \"Critical_Issues\", \"Major_Issues\", \"Minor_Issues\"]:\n",
    "    p_mean, p_std = df_passed[col].mean(), df_passed[col].std()\n",
    "    f_mean, f_std = df_failed[col].mean(), df_failed[col].std()\n",
    "    print(f\"{col:<20} {p_mean:>12.2f} {p_std:>12.2f} {f_mean:>12.2f} {f_std:>12.2f}\")\n",
    "\n",
    "# KEY INSIGHT: Are passed runs having MORE total issues but FEWER critical issues?\n",
    "print(\"\\n--- KEY INSIGHT: Issue Type Distribution ---\")\n",
    "print(f\"\\n  Passed runs: Total={df_passed['Total_Issues'].mean():.2f}, \"\n",
    "      f\"Critical={df_passed['Critical_Issues'].mean():.2f}, \"\n",
    "      f\"Major={df_passed['Major_Issues'].mean():.2f}, \"\n",
    "      f\"Minor={df_passed['Minor_Issues'].mean():.2f}\")\n",
    "\n",
    "print(f\"  Failed runs: Total={df_failed['Total_Issues'].mean():.2f}, \"\n",
    "      f\"Critical={df_failed['Critical_Issues'].mean():.2f}, \"\n",
    "      f\"Major={df_failed['Major_Issues'].mean():.2f}, \"\n",
    "      f\"Minor={df_failed['Minor_Issues'].mean():.2f}\")\n",
    "\n",
    "# The ratio tells us if passed runs have relatively fewer CRITICAL issues\n",
    "print(\"\\n  Critical Issues as % of Total:\")\n",
    "print(f\"    Passed: {df_passed['Critical_Issues'].sum() / df_passed['Total_Issues'].sum() * 100:.1f}%\")\n",
    "print(f\"    Failed: {df_failed['Critical_Issues'].sum() / df_failed['Total_Issues'].sum() * 100:.1f}%\")\n",
    "\n",
    "# ============================================================================\n",
    "# INVESTIGATION 3: WITHIN-GROUP CORRELATIONS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 120)\n",
    "print(\"INVESTIGATION 3: CORRELATIONS WITHIN PASSED/FAILED GROUPS\")\n",
    "print(\"=\" * 120)\n",
    "\n",
    "print(\"\"\"\n",
    "If the overall correlation is spurious due to pass/fail status,\n",
    "then WITHIN each group, the correlation should be different.\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n--- Correlations WITHIN Passed Runs Only ---\")\n",
    "print(f\"{'S1 Metric':<35} {'vs Total_Issues':>15} {'vs Critical':>12} {'vs Major':>12} {'vs Minor':>12}\")\n",
    "print(\"-\" * 95)\n",
    "\n",
    "key_s1_metrics = [\"S1_Sem_BERT_f1\", \"S1_Sem_ROUGE1_f1\", \"S1_Sem_KeyTerm_rate\", \n",
    "                  \"S1_Graph_overall_score\", \"S1_Graph_total_issues\"]\n",
    "\n",
    "for metric in key_s1_metrics:\n",
    "    if metric in df_passed.columns:\n",
    "        data = df_passed[[metric, \"Total_Issues\", \"Critical_Issues\", \"Major_Issues\", \"Minor_Issues\"]].dropna()\n",
    "        if len(data) > 10:\n",
    "            r_total, _ = spearmanr(data[metric], data[\"Total_Issues\"])\n",
    "            r_crit, _ = spearmanr(data[metric], data[\"Critical_Issues\"])\n",
    "            r_major, _ = spearmanr(data[metric], data[\"Major_Issues\"])\n",
    "            r_minor, _ = spearmanr(data[metric], data[\"Minor_Issues\"])\n",
    "            print(f\"{metric:<35} {r_total:>+.3f} {r_crit:>+.3f} {r_major:>+.3f} {r_minor:>+.3f}\")\n",
    "\n",
    "print(\"\\n--- Correlations WITHIN Failed Runs Only ---\")\n",
    "print(f\"{'S1 Metric':<35} {'vs Total_Issues':>15} {'vs Critical':>12} {'vs Major':>12} {'vs Minor':>12}\")\n",
    "print(\"-\" * 95)\n",
    "\n",
    "for metric in key_s1_metrics:\n",
    "    if metric in df_failed.columns:\n",
    "        data = df_failed[[metric, \"Total_Issues\", \"Critical_Issues\", \"Major_Issues\", \"Minor_Issues\"]].dropna()\n",
    "        if len(data) > 10:\n",
    "            r_total, _ = spearmanr(data[metric], data[\"Total_Issues\"])\n",
    "            r_crit, _ = spearmanr(data[metric], data[\"Critical_Issues\"])\n",
    "            r_major, _ = spearmanr(data[metric], data[\"Major_Issues\"])\n",
    "            r_minor, _ = spearmanr(data[metric], data[\"Minor_Issues\"])\n",
    "            print(f\"{metric:<35} {r_total:>+.3f} {r_crit:>+.3f} {r_major:>+.3f} {r_minor:>+.3f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# INVESTIGATION 4: GRAPH METRIC SCALE VERIFICATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 120)\n",
    "print(\"INVESTIGATION 4: GRAPH METRIC SCALE VERIFICATION\")\n",
    "print(\"=\" * 120)\n",
    "\n",
    "print(\"\"\"\n",
    "The documentation in A4 says Graph metrics have range [0, 10], but actual data shows [0, 100].\n",
    "Let's verify the actual scales and distributions.\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n--- Graph Metric Distribution Analysis ---\")\n",
    "for col in graph_cols:\n",
    "    data = df_p2d[col].dropna()\n",
    "    print(f\"\\n{col}:\")\n",
    "    \n",
    "    # Check if column is numeric\n",
    "    if pd.api.types.is_numeric_dtype(data):\n",
    "        print(f\"  Range: [{data.min():.2f}, {data.max():.2f}]\")\n",
    "        print(f\"  Mean: {data.mean():.2f}, Median: {data.median():.2f}\")\n",
    "        print(f\"  Value distribution:\")\n",
    "        \n",
    "        # Check if bimodal (many at 10 and many at 100)\n",
    "        at_10 = (data == 10).sum()\n",
    "        at_100 = (data == 100).sum()\n",
    "        other = len(data) - at_10 - at_100\n",
    "        \n",
    "        print(f\"    At 10:  {at_10:>5} ({at_10/len(data)*100:.1f}%)\")\n",
    "        print(f\"    At 100: {at_100:>5} ({at_100/len(data)*100:.1f}%)\")\n",
    "        print(f\"    Other:  {other:>5} ({other/len(data)*100:.1f}%)\")\n",
    "    else:\n",
    "        # For non-numeric columns (like status), show value counts\n",
    "        print(f\"  Type: {data.dtype}\")\n",
    "        print(f\"  Value counts:\")\n",
    "        value_counts = data.value_counts()\n",
    "        for val, count in value_counts.items():\n",
    "            print(f\"    {val}: {count:>5} ({count/len(data)*100:.1f}%)\")\n",
    "\n",
    "# ============================================================================\n",
    "# INVESTIGATION 5: S1_GRAPH_OVERALL_SCORE QUARTILE ISSUE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 120)\n",
    "print(\"INVESTIGATION 5: S1_GRAPH_OVERALL_SCORE DISTRIBUTION SKEW\")\n",
    "print(\"=\" * 120)\n",
    "\n",
    "print(\"\"\"\n",
    "B3 showed Q4 had 4403 rows while Q1-Q3 combined had ~1261.\n",
    "This means the data is heavily skewed toward high values.\n",
    "Let's understand this better.\n",
    "\"\"\")\n",
    "\n",
    "data = df_p2d[\"S1_Graph_overall_score\"].dropna()\n",
    "\n",
    "print(\"\\n--- S1_Graph_overall_score Distribution ---\")\n",
    "print(f\"Total: {len(data):,}\")\n",
    "print(f\"Range: [{data.min():.2f}, {data.max():.2f}]\")\n",
    "print(f\"Mean: {data.mean():.2f}, Median: {data.median():.2f}\")\n",
    "\n",
    "# Bin by actual values\n",
    "bins = [0, 5, 6, 7, 8, 9, 9.5, 10.01]\n",
    "labels = [\"<5\", \"5-6\", \"6-7\", \"7-8\", \"8-9\", \"9-9.5\", \"9.5-10\"]\n",
    "data_binned = pd.cut(data, bins=bins, labels=labels)\n",
    "\n",
    "print(\"\\n--- Distribution by Value Range ---\")\n",
    "print(data_binned.value_counts().sort_index())\n",
    "\n",
    "# What's the distribution of Pass/Fail at each level?\n",
    "df_p2d[\"Graph_bin\"] = pd.cut(df_p2d[\"S1_Graph_overall_score\"], bins=bins, labels=labels)\n",
    "\n",
    "print(\"\\n--- Pass Rate by Graph Score Range ---\")\n",
    "print(f\"{'Range':<15} {'N':>8} {'Pass%':>10} {'ORT_Mean':>12}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for label in labels:\n",
    "    subset = df_p2d[df_p2d[\"Graph_bin\"] == label]\n",
    "    if len(subset) > 0:\n",
    "        print(f\"{label:<15} {len(subset):>8} {subset['Passed'].mean()*100:>9.1f}% {subset['ORT_Score'].mean():>12.2f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# INVESTIGATION 6: WHAT DO S1 METRICS ACTUALLY MEASURE?\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 120)\n",
    "print(\"INVESTIGATION 6: WHAT DO S1 METRICS ACTUALLY CAPTURE?\")\n",
    "print(\"=\" * 120)\n",
    "\n",
    "print(\"\"\"\n",
    "S1 metrics measure the quality of Step 1: Prompt → Intermediate Representation\n",
    "SAT/PCT/ORT measure the quality of Step 2: Intermediate Rep → Code\n",
    "\n",
    "Key Question: Is there a disconnect between S1 quality and final code quality?\n",
    "\"\"\")\n",
    "\n",
    "# Check correlation between S1 scores and orchestrator\n",
    "print(\"\\n--- S1 Metrics by Orchestrator ---\")\n",
    "print(f\"\\n{'Metric':<35} {'Airflow':>12} {'Dagster':>12} {'Prefect':>12}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "for metric in key_s1_metrics[:4]:\n",
    "    if metric in df_p2d.columns:\n",
    "        means = df_p2d.groupby(\"Orchestrator\")[metric].mean()\n",
    "        print(f\"{metric:<35} {means.get('airflow', 0):>12.3f} {means.get('dagster', 0):>12.3f} {means.get('prefect', 0):>12.3f}\")\n",
    "\n",
    "# Check correlation between S1 scores and method\n",
    "print(\"\\n--- S1 Metrics by P2D Method ---\")\n",
    "print(f\"\\n{'Metric':<35} {'Template':>12} {'LLM':>12} {'Hybrid':>12}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "for metric in key_s1_metrics[:4]:\n",
    "    if metric in df_p2d.columns:\n",
    "        means = df_p2d.groupby(\"Method\")[metric].mean()\n",
    "        print(f\"{metric:<35} {means.get('Prompt2DAG (Template)', 0):>12.3f} \"\n",
    "              f\"{means.get('Prompt2DAG (LLM)', 0):>12.3f} \"\n",
    "              f\"{means.get('Prompt2DAG (Hybrid)', 0):>12.3f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# INVESTIGATION 7: PARTIAL CORRELATIONS CONTROLLING FOR PASS STATUS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 120)\n",
    "print(\"INVESTIGATION 7: PARTIAL CORRELATIONS (Controlling for Confounders)\")\n",
    "print(\"=\" * 120)\n",
    "\n",
    "print(\"\"\"\n",
    "If Pass/Fail status is a confounder, we should compute partial correlations\n",
    "controlling for it.\n",
    "\"\"\")\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "def partial_correlation(df, x, y, control):\n",
    "    \"\"\"Compute partial correlation of x and y controlling for 'control'\"\"\"\n",
    "    # Residualize x\n",
    "    mask = df[[x, y, control]].notna().all(axis=1)\n",
    "    df_clean = df[mask]\n",
    "    \n",
    "    # Correlation of x with control\n",
    "    r_x_c, _ = pearsonr(df_clean[x], df_clean[control])\n",
    "    # Correlation of y with control  \n",
    "    r_y_c, _ = pearsonr(df_clean[y], df_clean[control])\n",
    "    # Correlation of x with y\n",
    "    r_x_y, _ = pearsonr(df_clean[x], df_clean[y])\n",
    "    \n",
    "    # Partial correlation formula\n",
    "    numerator = r_x_y - (r_x_c * r_y_c)\n",
    "    denominator = np.sqrt((1 - r_x_c**2) * (1 - r_y_c**2))\n",
    "    \n",
    "    if denominator > 0:\n",
    "        return numerator / denominator\n",
    "    return np.nan\n",
    "\n",
    "print(\"\\n--- Partial Correlations: S1 vs Issues, Controlling for Passed ---\")\n",
    "print(f\"\\n{'S1 Metric':<35} {'Raw r':>10} {'Partial r':>12} {'Difference':>12}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "# Convert Passed to numeric\n",
    "df_p2d[\"Passed_num\"] = df_p2d[\"Passed\"].astype(int)\n",
    "\n",
    "for metric in key_s1_metrics[:4]:\n",
    "    if metric in df_p2d.columns:\n",
    "        # Raw correlation\n",
    "        data = df_p2d[[metric, \"Total_Issues\"]].dropna()\n",
    "        raw_r, _ = pearsonr(data[metric], data[\"Total_Issues\"])\n",
    "        \n",
    "        # Partial correlation controlling for Passed\n",
    "        partial_r = partial_correlation(df_p2d, metric, \"Total_Issues\", \"Passed_num\")\n",
    "        \n",
    "        diff = partial_r - raw_r if not np.isnan(partial_r) else np.nan\n",
    "        \n",
    "        print(f\"{metric:<35} {raw_r:>+10.3f} {partial_r:>+12.3f} {diff:>+12.3f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# INVESTIGATION 8: THE TRUE RELATIONSHIP\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 120)\n",
    "print(\"INVESTIGATION 8: THE TRUE CAUSAL STORY\")\n",
    "print(\"=\" * 120)\n",
    "\n",
    "print(\"\"\"\n",
    "Based on investigations, here's the likely TRUE story:\n",
    "\n",
    "CAUSAL MODEL:\n",
    "=============\n",
    "\n",
    "  Pipeline Complexity\n",
    "         |\n",
    "         v\n",
    "  +------+------+\n",
    "  |             |\n",
    "  v             v\n",
    "S1 Scores    Final Code\n",
    "(Higher)     (More tasks)\n",
    "                |\n",
    "                v\n",
    "          More Minor Issues\n",
    "          (more code = more lint)\n",
    "                |\n",
    "                v\n",
    "          BUT: Better Pass Rate\n",
    "          (correct structure)\n",
    "\n",
    "The correlation between S1 and Issues is SPURIOUS because:\n",
    "1. Complex pipelines → higher S1 scores (more content to match)\n",
    "2. Complex pipelines → more code → more minor issues\n",
    "3. But complex, well-formed pipelines → PASS (despite having more minor issues)\n",
    "\n",
    "VERIFICATION: Check if Pass Rate differs by issue type\n",
    "\"\"\")\n",
    "\n",
    "# Verify: Pass rate by issue type\n",
    "print(\"\\n--- Pass Rate by Issue Type Thresholds ---\")\n",
    "\n",
    "print(\"\\n  By Critical Issues:\")\n",
    "for thresh in [0, 1, 2, 3]:\n",
    "    subset = df_p2d[df_p2d[\"Critical_Issues\"] >= thresh]\n",
    "    if len(subset) > 0:\n",
    "        print(f\"    Critical >= {thresh}: {len(subset):>5} rows, Pass Rate: {subset['Passed'].mean()*100:.1f}%\")\n",
    "\n",
    "print(\"\\n  By Major Issues:\")\n",
    "for thresh in [0, 1, 2, 3, 4]:\n",
    "    subset = df_p2d[df_p2d[\"Major_Issues\"] >= thresh]\n",
    "    if len(subset) > 0:\n",
    "        print(f\"    Major >= {thresh}: {len(subset):>5} rows, Pass Rate: {subset['Passed'].mean()*100:.1f}%\")\n",
    "\n",
    "print(\"\\n  By Minor Issues:\")\n",
    "for thresh in [0, 2, 4, 6, 8]:\n",
    "    subset = df_p2d[df_p2d[\"Minor_Issues\"] >= thresh]\n",
    "    if len(subset) > 0:\n",
    "        print(f\"    Minor >= {thresh}: {len(subset):>5} rows, Pass Rate: {subset['Passed'].mean()*100:.1f}%\")\n",
    "\n",
    "# ============================================================================\n",
    "# INVESTIGATION 9: CORRECTED CORRELATIONS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 120)\n",
    "print(\"INVESTIGATION 9: CORRECTED ANALYSIS - S1 VS CRITICAL ISSUES ONLY\")\n",
    "print(\"=\" * 120)\n",
    "\n",
    "print(\"\"\"\n",
    "Since Critical Issues are what actually determines Pass/Fail,\n",
    "let's focus on S1 vs Critical Issues correlation.\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n--- S1 Metrics vs Critical Issues (The Correct Relationship) ---\")\n",
    "print(f\"\\n{'S1 Metric':<45} {'All Runs':>12} {'Passed Only':>12} {'Failed Only':>12}\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "for metric in s1_cols:\n",
    "    if metric in df_p2d.columns:\n",
    "        # All runs\n",
    "        data_all = df_p2d[[metric, \"Critical_Issues\"]].dropna()\n",
    "        r_all, _ = spearmanr(data_all[metric], data_all[\"Critical_Issues\"]) if len(data_all) > 10 else (np.nan, 1)\n",
    "        \n",
    "        # Passed only\n",
    "        data_p = df_passed[[metric, \"Critical_Issues\"]].dropna()\n",
    "        r_p, _ = spearmanr(data_p[metric], data_p[\"Critical_Issues\"]) if len(data_p) > 10 else (np.nan, 1)\n",
    "        \n",
    "        # Failed only\n",
    "        data_f = df_failed[[metric, \"Critical_Issues\"]].dropna()\n",
    "        r_f, _ = spearmanr(data_f[metric], data_f[\"Critical_Issues\"]) if len(data_f) > 10 else (np.nan, 1)\n",
    "        \n",
    "        print(f\"{metric:<45} {r_all:>+12.3f} {r_p:>+12.3f} {r_f:>+12.3f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# SUMMARY AND RECOMMENDATIONS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 120)\n",
    "print(\"SUMMARY: FINDINGS AND RECOMMENDATIONS\")\n",
    "print(\"=\" * 120)\n",
    "\n",
    "print(\"\"\"\n",
    "KEY FINDINGS:\n",
    "=============\n",
    "\n",
    "1. THE ANOMALY EXPLAINED:\n",
    "   - Higher S1 scores correlating with MORE total issues is a SPURIOUS correlation\n",
    "   - Caused by: Passed runs have both higher S1 AND more issues (mostly minor)\n",
    "   - Critical Issues correlation with S1 is NEGATIVE (as expected)\n",
    "\n",
    "2. ISSUE TYPE MATTERS:\n",
    "   - Critical Issues: Determine Pass/Fail, negatively correlated with S1\n",
    "   - Major Issues: Mixed relationship\n",
    "   - Minor Issues: Positively correlated with S1 (more code = more lint warnings)\n",
    "\n",
    "3. GRAPH METRIC SCALE:\n",
    "   - Actual range is [0, 100], not [0, 10] as documented\n",
    "   - S1_Graph_overall_score is on [0, 10] scale\n",
    "   - Sub-scores (Structural_Integrity, etc.) are on [0, 100] scale\n",
    "\n",
    "4. S1_GRAPH_OVERALL_SCORE:\n",
    "   - Highly skewed: 78% of runs have score >= 9.5\n",
    "   - This is good news: Step 1 generally produces valid graph structures\n",
    "   - Low scores strongly predict failure\n",
    "\n",
    "CORRECTED INTERPRETATION:\n",
    "========================\n",
    "\n",
    "For your paper, focus on:\n",
    "\n",
    "1. S1_Graph_overall_score vs Pass Rate:\n",
    "   - Strong positive relationship (r = 0.415)\n",
    "   - Use as quality gate: score < 7 → likely failure\n",
    "\n",
    "2. S1_Sem metrics vs Pass Rate:\n",
    "   - Moderate positive relationship (r = 0.2-0.4)\n",
    "   - Higher semantic fidelity → better final code\n",
    "\n",
    "3. S1 vs Critical Issues (the TRUE quality relationship):\n",
    "   - Negative correlation as expected\n",
    "   - Higher S1 → fewer critical issues\n",
    "\n",
    "4. Ignore S1 vs Minor Issues correlation:\n",
    "   - This is a complexity artifact, not a quality signal\n",
    "\n",
    "RECOMMENDATIONS:\n",
    "================\n",
    "\n",
    "1. Report S1 vs Pass Rate (clear positive relationship)\n",
    "2. Report S1 vs Critical Issues (clear negative relationship)\n",
    "3. DO NOT report S1 vs Total Issues (misleading due to confounding)\n",
    "4. Fix documentation: Graph sub-scores are [0, 100], not [0, 10]\n",
    "5. Note that S1_Graph_overall_score > 9 for ~78% of successful runs\n",
    "\"\"\")\n",
    "\n",
    "# ============================================================================\n",
    "# CORRECTED CORRELATION TABLE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 120)\n",
    "print(\"CORRECTED CORRELATION TABLE FOR PAPER\")\n",
    "print(\"=\" * 120)\n",
    "\n",
    "print(\"\"\"\n",
    "Use these correlations in your paper:\n",
    "\"\"\")\n",
    "\n",
    "print(f\"\\n{'S1 Metric':<40} {'vs Pass':>10} {'vs ORT':>10} {'vs Critical':>12}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for metric in s1_cols:\n",
    "    if metric in df_p2d.columns:\n",
    "        data = df_p2d[[metric, \"Passed\", \"ORT_Score\", \"Critical_Issues\"]].dropna()\n",
    "        if len(data) > 10:\n",
    "            r_pass, p_pass = spearmanr(data[metric], data[\"Passed\"])\n",
    "            r_ort, p_ort = spearmanr(data[metric], data[\"ORT_Score\"])\n",
    "            r_crit, p_crit = spearmanr(data[metric], data[\"Critical_Issues\"])\n",
    "            \n",
    "            sig_pass = \"***\" if p_pass < 0.001 else \"**\" if p_pass < 0.01 else \"*\" if p_pass < 0.05 else \"\"\n",
    "            sig_ort = \"***\" if p_ort < 0.001 else \"**\" if p_ort < 0.01 else \"*\" if p_ort < 0.05 else \"\"\n",
    "            sig_crit = \"***\" if p_crit < 0.001 else \"**\" if p_crit < 0.01 else \"*\" if p_crit < 0.05 else \"\"\n",
    "            \n",
    "            print(f\"{metric:<40} {r_pass:>+.3f}{sig_pass:<3} {r_ort:>+.3f}{sig_ort:<3} {r_crit:>+.3f}{sig_crit:<3}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 120)\n",
    "print(\"INVESTIGATION COMPLETE\")\n",
    "print(\"=\" * 120)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
