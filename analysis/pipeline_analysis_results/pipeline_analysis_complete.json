{
  "analyses": [
    {
      "pipeline_name": "6587093polakorn.../airvisual_pipeline_by_lat_long",
      "business_domain": "Environmental Monitoring",
      "domain_category": "Air Quality Data Integration",
      "primary_objective": "Extract air quality data from AirVisual API and load into PostgreSQL data warehouse",
      "enrichment_objective": "Transform raw API data into dimensional model with pollution and weather mappings",
      "topology": {
        "pattern": "linear_sequential",
        "description": "Strict three-stage pipeline: API extraction → data validation → database loading with no branching or parallelism",
        "has_sensors": false,
        "has_branches": false,
        "has_fan_out": false,
        "has_fan_in": false,
        "parallelization_level": "none",
        "max_parallel_width": 1,
        "branch_depth": 0
      },
      "processing": {
        "total_tasks": 3,
        "task_types": [
          "PythonOperator",
          "PythonOperator",
          "PythonOperator"
        ],
        "processing_stages": [
          "API Extraction",
          "Data Validation",
          "Database Loading"
        ],
        "etl_pattern": "extract_transform_load",
        "data_flow_description": "AirVisual API → JSON file → validation → dimensional transformation → PostgreSQL tables"
      },
      "external_services": {
        "primary_services": [
          "AirVisual API"
        ],
        "apis_used": [
          "AirVisual API nearest_city endpoint"
        ],
        "databases": [
          "PostgreSQL"
        ],
        "service_integration_pattern": "sequential_api_calls"
      },
      "infrastructure": {
        "connections": [
          "postgres_conn"
        ],
        "file_paths": [
          "/opt/airflow/data/tmp_airvisual.json",
          "/opt/airflow/config/mapping_main_pollution.json",
          "/opt/airflow/config/mapping_weather_code.json"
        ],
        "external_systems": [
          "AirVisual API",
          "PostgreSQL database"
        ]
      },
      "scheduling": {
        "schedule_type": "manual",
        "has_catchup": false,
        "retry_policy": "2 retries with 3-minute delay (DAG level), custom 2 retries with 5-minute delay for extraction task"
      },
      "complexity": {
        "complexity_score": 3,
        "complexity_factors": [
          "External API integration",
          "Database dimensional modeling",
          "File-based intermediate storage",
          "Duplicate detection logic"
        ],
        "use_case_fit": "Standard ETL pipeline for environmental data warehousing",
        "standard_complexity_score": 2
      },
      "failure_handling": {
        "retry_mechanism": "Airflow retry policy with exponential backoff",
        "alert_mechanism": "Not specified in description",
        "transaction_safety": "Explicit commit/rollback with connection management"
      },
      "unique_features": [
        "Atomic file write pattern",
        "Duplicate detection with AirflowSkipException",
        "Bangkok timezone conversion",
        "Dimensional modeling with mapping files"
      ],
      "source_file": "6587093polakornming__RiskAlertPM25__airvisual_pipeline_by_lat_long_review.py_description.txt"
    },
    {
      "pipeline_name": "6587093polakorn.../mahidol_aqi_report_pipeline_al",
      "business_domain": "Environmental Monitoring",
      "domain_category": "Air Quality Monitoring and Alerting",
      "primary_objective": "Monitor PM2.5 air quality levels and provide alerts when thresholds are exceeded",
      "enrichment_objective": "Transform raw HTML AQI data into structured dimensional data for analysis and alerting",
      "topology": {
        "pattern": "linear_sequential",
        "description": "Strict sequential pipeline with four stages: web scraping → data parsing → database loading → conditional alerting",
        "has_sensors": false,
        "has_branches": false,
        "has_fan_out": false,
        "has_fan_in": false,
        "parallelization_level": "none",
        "max_parallel_width": 1,
        "branch_depth": 0
      },
      "processing": {
        "total_tasks": 4,
        "task_types": [
          "PythonOperator",
          "PythonOperator",
          "PythonOperator",
          "PythonOperator"
        ],
        "processing_stages": [
          "Data Extraction",
          "Data Transformation & Validation",
          "Data Loading",
          "Alerting"
        ],
        "etl_pattern": "extract_transform_load",
        "data_flow_description": "HTML web scraping → HTML parsing to JSON → PostgreSQL dimensional loading → conditional email alerts based on AQI thresholds"
      },
      "external_services": {
        "primary_services": [
          "HTTP Web Service",
          "PostgreSQL Database",
          "SMTP Email Service"
        ],
        "apis_used": [
          "Mahidol University AQI Website API"
        ],
        "databases": [
          "PostgreSQL"
        ],
        "service_integration_pattern": "sequential_api_calls"
      },
      "infrastructure": {
        "connections": [
          "postgres_conn",
          "SMTP connection"
        ],
        "file_paths": [
          "data/mahidol_aqi.html",
          "data/tmp_mahidol.json",
          "/opt/airflow/config/mapping_main_pollution.json",
          "BASE_DIR/config",
          "BASE_DIR/data",
          "pm25_alert_emails.txt"
        ],
        "external_systems": [
          "Mahidol University website (https://mahidol.ac.th/aqireport/)",
          "Gmail SMTP server"
        ]
      },
      "scheduling": {
        "schedule_type": "batch",
        "has_catchup": false,
        "retry_policy": "not_specified"
      },
      "complexity": {
        "complexity_score": 4,
        "complexity_factors": [
          "Multiple external service integrations",
          "Data validation and duplicate detection",
          "Conditional execution logic",
          "Dimensional database modeling",
          "File-based data persistence"
        ],
        "use_case_fit": "Well-structured environmental monitoring pipeline with appropriate complexity for air quality alerting",
        "standard_complexity_score": 3
      },
      "failure_handling": {
        "retry_mechanism": "not_specified",
        "alert_mechanism": "Email notifications via SMTP",
        "transaction_safety": "Database commit/rollback for data consistency"
      },
      "unique_features": [
        "HTML web scraping with BeautifulSoup",
        "AirflowSkipException for conditional workflow control",
        "Dimensional data modeling in PostgreSQL",
        "Atomic file write pattern",
        "Configurable AQI threshold-based alerting"
      ],
      "source_file": "6587093polakornming__RiskAlertPM25__mahidol_aqi_report_pipeline_alert.py_description.txt"
    },
    {
      "pipeline_name": "Environmental_Monitoring_Network.txt",
      "business_domain": "Environmental Monitoring",
      "domain_category": "Environmental Risk Analysis",
      "primary_objective": "Create comprehensive dataset for environmental risk analysis",
      "enrichment_objective": "Integrate location data with geocoding, weather history, land use, and demographic information",
      "topology": {
        "pattern": "linear_sequential",
        "description": "Strict sequential flow where each task depends on the output of the previous task",
        "has_sensors": false,
        "has_branches": false,
        "has_fan_out": false,
        "has_fan_in": false,
        "parallelization_level": "none",
        "max_parallel_width": 1,
        "branch_depth": 0
      },
      "processing": {
        "total_tasks": 7,
        "task_types": [
          "load_and_modify",
          "reconciliation",
          "data_extension",
          "data_extension",
          "data_extension",
          "column_extension",
          "save"
        ],
        "processing_stages": [
          "Data Ingestion & Standardization",
          "Geocoding",
          "Weather Data Enrichment",
          "Land Use Enrichment",
          "Demographic Enrichment",
          "Risk Calculation",
          "Export"
        ],
        "etl_pattern": "extract_transform_load",
        "data_flow_description": "CSV ingestion → JSON conversion → geocoding → weather data addition → land use classification → population density addition → risk score calculation → CSV export"
      },
      "external_services": {
        "primary_services": [
          "load-and-modify service",
          "reconciliation service",
          "OpenMeteo API",
          "GIS Land Use API",
          "Demographic Data Service",
          "column extension service",
          "save service"
        ],
        "apis_used": [
          "HERE API",
          "OpenMeteo API",
          "Geoapify API",
          "WorldPop API"
        ],
        "databases": [
          "MongoDB"
        ],
        "service_integration_pattern": "sequential_api_calls"
      },
      "infrastructure": {
        "connections": [
          "app_network Docker network",
          "DATA_DIR shared volume"
        ],
        "file_paths": [
          "stations.csv",
          "table_data_2.json",
          "reconciled_table_2.json",
          "open_meteo_2.json",
          "land_use_2.json",
          "pop_density_2.json",
          "column_extended_2.json",
          "enriched_data_2.csv"
        ],
        "external_systems": [
          "Intertwino API (port 5005)",
          "MongoDB (port 27017)"
        ]
      },
      "scheduling": {
        "schedule_type": "batch",
        "has_catchup": false,
        "retry_policy": "default retry (1 attempt)"
      },
      "complexity": {
        "complexity_score": 4,
        "complexity_factors": [
          "Multiple external API integrations",
          "Data transformation across formats",
          "Sequential dependency chain",
          "Docker container orchestration"
        ],
        "use_case_fit": "Well-structured data enrichment pipeline for environmental monitoring",
        "standard_complexity_score": 4
      },
      "failure_handling": {
        "retry_mechanism": "Airflow task retries",
        "alert_mechanism": "Not specified",
        "transaction_safety": "Intermediate file-based with cleanup"
      },
      "unique_features": [
        "Environmental risk score calculation",
        "Multi-source geospatial data integration",
        "Dockerized microservices architecture",
        "Shared volume for data persistence between tasks"
      ],
      "source_file": "Environmental_Monitoring_Network.txt"
    },
    {
      "pipeline_name": "Ferlab-Ste-Just.../etl_import_ensembl.py",
      "business_domain": "Healthcare",
      "domain_category": "Genomic Data Processing",
      "primary_objective": "Import genomic mapping data from Ensembl's FTP server to an S3 data lake and process it with Spark",
      "enrichment_objective": "Create structured tables from raw genomic mapping files for downstream analysis",
      "topology": {
        "pattern": "linear_sequential",
        "description": "Strict sequential two-stage pipeline: file download/version checking followed by Spark table processing",
        "has_sensors": false,
        "has_branches": false,
        "has_fan_out": false,
        "has_fan_in": false,
        "parallelization_level": "none",
        "max_parallel_width": 1,
        "branch_depth": 0
      },
      "processing": {
        "total_tasks": 2,
        "task_types": [
          "PythonOperator",
          "SparkOperator"
        ],
        "processing_stages": [
          "File Download & Version Check",
          "Spark Table Processing"
        ],
        "etl_pattern": "extract_transform_load",
        "data_flow_description": "Extract TSV.GZ files from Ensembl FTP, load to S3 landing zone, transform with Spark to create structured tables"
      },
      "external_services": {
        "primary_services": [
          "Ensembl FTP Server",
          "AWS S3",
          "Apache Spark on Kubernetes"
        ],
        "apis_used": [
          "HTTP FTP"
        ],
        "databases": [],
        "service_integration_pattern": "sequential_api_calls"
      },
      "infrastructure": {
        "connections": [
          "config.s3_conn_id"
        ],
        "file_paths": [
          "raw/landing/ensembl/"
        ],
        "external_systems": [
          "ftp.ensembl.org/pub/current_tsv/homo_sapiens",
          "cqgc-{env}-app-datalake S3 bucket"
        ]
      },
      "scheduling": {
        "schedule_type": "manual",
        "has_catchup": false,
        "retry_policy": "unspecified"
      },
      "complexity": {
        "complexity_score": 4,
        "complexity_factors": [
          "External FTP integration",
          "S3 file management",
          "Spark processing on Kubernetes",
          "Version-aware file checking",
          "Multiple notification callbacks"
        ],
        "use_case_fit": "Genomic data ingestion and transformation pipeline",
        "standard_complexity_score": 2
      },
      "failure_handling": {
        "retry_mechanism": "unspecified",
        "alert_mechanism": "Slack notifications on task failure, DAG start, and DAG completion",
        "transaction_safety": "partial (skip on no new versions, alerts on failures)"
      },
      "unique_features": [
        "Version-aware file downloading with AirflowSkipException",
        "Genomic mapping data processing (canonical, ena, entrez, refseq, uniprot)",
        "Kubernetes execution context for Spark jobs",
        "Environment-specific S3 bucket naming (cqgc-{env}-app-datalake)"
      ],
      "source_file": "Ferlab-Ste-Justine__clin-pipelines-dags__etl_import_ensembl.py_description.txt"
    },
    {
      "pipeline_name": "Ferlab-Ste-Just.../etl_import_mondo.py",
      "business_domain": "Healthcare",
      "domain_category": "Data Warehousing",
      "primary_objective": "Import and process Mondo ontology data for search and query capabilities",
      "enrichment_objective": "Normalize ontology terms and index into Elasticsearch for enhanced accessibility",
      "topology": {
        "pattern": "linear_sequential",
        "description": "Strict sequential execution with no parallel tasks; each task depends on the previous one's success",
        "has_sensors": false,
        "has_branches": false,
        "has_fan_out": false,
        "has_fan_in": false,
        "parallelization_level": "none",
        "max_parallel_width": 1,
        "branch_depth": 0
      },
      "processing": {
        "total_tasks": 6,
        "task_types": [
          "PythonOperator",
          "SparkOperator",
          "PipelineOperator",
          "EmptyOperator"
        ],
        "processing_stages": [
          "Parameter Validation",
          "Data Extraction",
          "Data Transformation",
          "Data Indexing",
          "Data Publishing",
          "Notification"
        ],
        "etl_pattern": "extract_transform_load",
        "data_flow_description": "Download Mondo OBO file from GitHub → Upload to S3 → Normalize terms with Spark → Index to Elasticsearch → Publish results → Send Slack notification"
      },
      "external_services": {
        "primary_services": [
          "GitHub Releases",
          "S3",
          "Elasticsearch",
          "Slack"
        ],
        "apis_used": [
          "GitHub Releases API",
          "Slack Webhook API"
        ],
        "databases": [
          "Elasticsearch"
        ],
        "service_integration_pattern": "sequential_api_calls"
      },
      "infrastructure": {
        "connections": [
          "s3_conn_id",
          "es_url"
        ],
        "file_paths": [
          "cqgc-{env}-app-datalake/raw/landing/mondo/",
          "public/mondo_terms"
        ],
        "external_systems": [
          "GitHub",
          "S3",
          "Elasticsearch",
          "Slack",
          "Kubernetes"
        ]
      },
      "scheduling": {
        "schedule_type": "manual",
        "has_catchup": false,
        "retry_policy": "not_specified"
      },
      "complexity": {
        "complexity_score": 4,
        "complexity_factors": [
          "Multiple external service integrations",
          "Spark-based transformations",
          "Version-aware skip logic",
          "Environment-specific configuration"
        ],
        "use_case_fit": "Well-suited for batch ontology data processing with linear dependencies",
        "standard_complexity_score": 4
      },
      "failure_handling": {
        "retry_mechanism": "not_specified",
        "alert_mechanism": "Slack notifications for task failures and DAG completion",
        "transaction_safety": "partial (skip logic for unchanged versions, trigger rule ensures predecessor success)"
      },
      "unique_features": [
        "Version-aware downloading with skip logic",
        "Environment targeting via color parameter",
        "Spark on Kubernetes for transformation and indexing",
        "Integrated Slack notifications for success/failure"
      ],
      "source_file": "Ferlab-Ste-Justine__clin-pipelines-dags__etl_import_mondo.py_description.txt"
    },
    {
      "pipeline_name": "HaydarovAkbar.../main.py",
      "business_domain": "Data Warehousing",
      "domain_category": "Data Warehousing",
      "primary_objective": "Orchestrate dataset loading from DWH L2 to L2 with segmentation processing and SAP integration",
      "enrichment_objective": "Client segmentation processing and SAP notification",
      "topology": {
        "pattern": "fan_out_fan_in",
        "description": "Sequential pre-processing with sensor gates, fan-out parallel execution (width: 2), and fan-in synchronization with mixed patterns",
        "has_sensors": true,
        "has_branches": false,
        "has_fan_out": true,
        "has_fan_in": true,
        "parallelization_level": "mixed",
        "max_parallel_width": 2,
        "branch_depth": 0
      },
      "processing": {
        "total_tasks": 9,
        "task_types": [
          "SqlSensor",
          "PythonOperator",
          "ExternalTaskSensor",
          "TriggerDagRunOperator",
          "TaskGroup",
          "EmailOperator"
        ],
        "processing_stages": [
          "Dependency Validation",
          "Session Initialization",
          "Parallel Processing",
          "Synchronization",
          "Completion Handling"
        ],
        "etl_pattern": "batch",
        "data_flow_description": "Validates dependencies via sensors, initializes workflow metadata, executes parallel data preparation and segmentation tasks, synchronizes results, and updates completion status"
      },
      "external_services": {
        "primary_services": [
          "PostgreSQL DWH",
          "SAP System",
          "SMTP Email"
        ],
        "apis_used": [
          "HTTP POST to SAP"
        ],
        "databases": [
          "PostgreSQL DWH"
        ],
        "service_integration_pattern": "orchestrated_services"
      },
      "infrastructure": {
        "connections": [
          "dwh",
          "sap_conn"
        ],
        "file_paths": [],
        "external_systems": [
          "PostgreSQL DWH",
          "SAP",
          "External DAGs: sys_kill_all_session_pg, wf_data_preparation_for_reports, l1_to_l2_p_load_data_ds_client_segmentation_full"
        ]
      },
      "scheduling": {
        "schedule_type": "manual",
        "has_catchup": false,
        "retry_policy": "no_retries"
      },
      "complexity": {
        "complexity_score": 7,
        "complexity_factors": [
          "Mixed topology patterns",
          "Multiple sensor gates",
          "Parallel execution with synchronization",
          "External DAG dependencies",
          "TaskGroup usage",
          "Multiple service integrations",
          "Metadata tracking"
        ],
        "use_case_fit": "Complex data warehousing workflow with dependency management and parallel processing",
        "standard_complexity_score": 8
      },
      "failure_handling": {
        "retry_mechanism": "none",
        "alert_mechanism": "email_notification",
        "transaction_safety": "partial"
      },
      "unique_features": [
        "SqlSensor for DWH flag monitoring",
        "ExternalTaskSensor for previous day completion",
        "TriggerDagRunOperator with pool constraints",
        "TaskGroup for segmentation workflow",
        "Manual trigger with 20 concurrent run support",
        "Session cleanup before parallel execution"
      ],
      "source_file": "HaydarovAkbar__airflow_dag__main.py_description.txt"
    },
    {
      "pipeline_name": "M4TTRX.../global_dag.py",
      "business_domain": "Environmental Monitoring",
      "domain_category": "Government Data Integration",
      "primary_objective": "Process French government death records and power plant data through staged ETL",
      "enrichment_objective": "Enhance death records with geographic coordinates and standardize power plant data",
      "topology": {
        "pattern": "branch_merge",
        "description": "Combines fan-out/fan-in for parallel data extraction, staged ETL for processing, and branch-merge for conditional loading",
        "has_sensors": false,
        "has_branches": true,
        "has_fan_out": true,
        "has_fan_in": true,
        "parallelization_level": "mixed",
        "max_parallel_width": 4,
        "branch_depth": 1
      },
      "processing": {
        "total_tasks": 16,
        "task_types": [
          "BashOperator",
          "PythonOperator",
          "PostgresOperator",
          "BranchPythonOperator",
          "DummyOperator"
        ],
        "processing_stages": [
          "Data Ingestion",
          "Data Cleansing",
          "Data Transformation",
          "Data Loading",
          "Cleanup"
        ],
        "etl_pattern": "extract_transform_load",
        "data_flow_description": "Extract from multiple government APIs in parallel → Stage raw files → Clean and transform with geographic mapping → Conditionally load to PostgreSQL → Cleanup intermediate storage"
      },
      "external_services": {
        "primary_services": [
          "data.gouv.fr APIs",
          "PostgreSQL",
          "Redis"
        ],
        "apis_used": [
          "data.gouv.fr death records API",
          "data.gouv.fr thermal plants API",
          "data.gouv.fr nuclear plants API",
          "static.data.gouv.fr city geo API"
        ],
        "databases": [
          "PostgreSQL",
          "Redis"
        ],
        "service_integration_pattern": "api_enrichment"
      },
      "infrastructure": {
        "connections": [
          "postgres_default",
          "Redis (host='redis', port=6379, db=0)"
        ],
        "file_paths": [
          "/opt/airflow/dags/data/ingestion/",
          "/opt/airflow/dags/data/staging/",
          "/opt/airflow/dags/sql/tmp/"
        ],
        "external_systems": [
          "data.gouv.fr government data platform",
          "PostgreSQL database",
          "Redis cache"
        ]
      },
      "scheduling": {
        "schedule_type": "manual",
        "has_catchup": false,
        "retry_policy": "1 retry with 10-second delay"
      },
      "complexity": {
        "complexity_score": 7,
        "complexity_factors": [
          "Multiple data sources with parallel extraction",
          "Conditional branching based on data availability",
          "Intermediate Redis storage for data deduplication",
          "Geographic coordinate mapping",
          "TaskGroup organization",
          "Mixed operator types"
        ],
        "use_case_fit": "Government data integration with data quality checks",
        "standard_complexity_score": 8
      },
      "failure_handling": {
        "retry_mechanism": "Basic retry with delay",
        "alert_mechanism": "None specified",
        "transaction_safety": "Intermediate with Redis tracking and file-based SQL queries"
      },
      "unique_features": [
        "Parallel extraction of 4 distinct government datasets",
        "Redis-based deduplication and intermediate storage",
        "Conditional branching for empty data handling",
        "Geographic enrichment of death records",
        "TaskGroup organization for pipeline stages"
      ],
      "source_file": "M4TTRX__data-eng-project__global_dag.py_description.txt"
    },
    {
      "pipeline_name": "Medical_Facility_Accessibility.txt",
      "business_domain": "Healthcare",
      "domain_category": "Healthcare Infrastructure Analytics",
      "primary_objective": "Assess medical facility accessibility by geocoding locations and calculating distances to key infrastructure",
      "enrichment_objective": "Enhance facility data with geospatial coordinates and proximity metrics to public transport and residential areas",
      "topology": {
        "pattern": "linear_sequential",
        "description": "Strict sequential flow where each task depends on the output of the previous one",
        "has_sensors": false,
        "has_branches": false,
        "has_fan_out": false,
        "has_fan_in": false,
        "parallelization_level": "none",
        "max_parallel_width": 1,
        "branch_depth": 0
      },
      "processing": {
        "total_tasks": 5,
        "task_types": [
          "load_and_modify",
          "reconciliation",
          "column_extension",
          "column_extension",
          "save"
        ],
        "processing_stages": [
          "Data Ingestion & Cleaning",
          "Geocoding",
          "Spatial Analysis (Public Transport)",
          "Spatial Analysis (Residential Areas)",
          "Data Export"
        ],
        "etl_pattern": "extract_transform_load",
        "data_flow_description": "CSV input → JSON transformation → geocoding via HERE API → distance calculations to transport → distance calculations to residential → CSV export"
      },
      "external_services": {
        "primary_services": [
          "HERE Geocoding API",
          "Intertwino API"
        ],
        "apis_used": [
          "HERE API",
          "Intertwino services (load-and-modify, reconciliation, column-extension, save)"
        ],
        "databases": [
          "MongoDB"
        ],
        "service_integration_pattern": "sequential_api_calls"
      },
      "infrastructure": {
        "connections": [
          "Docker network (app_network)",
          "shared volume mount (DATA_DIR)"
        ],
        "file_paths": [
          "/app/data/facilities.csv",
          "/app/data/transport_stops.geojson",
          "/app/data/residential_areas.geojson"
        ],
        "external_systems": [
          "HERE API",
          "MongoDB",
          "Intertwino services"
        ]
      },
      "scheduling": {
        "schedule_type": "batch",
        "has_catchup": false,
        "retry_policy": "default_retry"
      },
      "complexity": {
        "complexity_score": 4,
        "complexity_factors": [
          "multiple external API integrations",
          "geospatial calculations",
          "data format transformations",
          "Docker container orchestration"
        ],
        "use_case_fit": "Well-structured batch ETL for healthcare spatial analytics",
        "standard_complexity_score": 4
      },
      "failure_handling": {
        "retry_mechanism": "Airflow task retries (default: 1)",
        "alert_mechanism": "Not specified",
        "transaction_safety": "Intermediate file-based with cleanup"
      },
      "unique_features": [
        "Geospatial distance calculations to multiple infrastructure layers",
        "HERE API integration for geocoding",
        "Dockerized microservices architecture",
        "Healthcare accessibility analytics"
      ],
      "source_file": "Medical_Facility_Accessibility.txt"
    },
    {
      "pipeline_name": "Multilingual_Product_Review.txt",
      "business_domain": "E-commerce",
      "domain_category": "Customer Analytics",
      "primary_objective": "Enrich product reviews with language verification, sentiment analysis, and feature extraction",
      "enrichment_objective": "Provide deeper customer insights through LLM-powered analysis of multilingual reviews",
      "topology": {
        "pattern": "linear_sequential",
        "description": "Strict sequential flow where each task depends on the output of the previous task",
        "has_sensors": false,
        "has_branches": false,
        "has_fan_out": false,
        "has_fan_in": false,
        "parallelization_level": "none",
        "max_parallel_width": 1,
        "branch_depth": 0
      },
      "processing": {
        "total_tasks": 5,
        "task_types": [
          "DockerOperator",
          "DockerOperator",
          "DockerOperator",
          "DockerOperator",
          "DockerOperator"
        ],
        "processing_stages": [
          "Data Ingestion & Formatting",
          "Language Detection",
          "Sentiment Analysis",
          "Feature Extraction",
          "Data Export"
        ],
        "etl_pattern": "extract_transform_load",
        "data_flow_description": "CSV ingestion → JSON conversion → language detection → sentiment analysis → feature extraction → CSV export"
      },
      "external_services": {
        "primary_services": [
          "Docker",
          "Hugging Face Transformers",
          "Language Detection Service"
        ],
        "apis_used": [],
        "databases": [],
        "service_integration_pattern": "sequential_api_calls"
      },
      "infrastructure": {
        "connections": [
          "Docker network: app_network"
        ],
        "file_paths": [
          "DATA_DIR/reviews.csv",
          "DATA_DIR/table_data_2.json",
          "DATA_DIR/lang_detected_2.json",
          "DATA_DIR/sentiment_analyzed_2.json",
          "DATA_DIR/column_extended_2.json",
          "DATA_DIR/enriched_data_2.csv"
        ],
        "external_systems": [
          "Docker Registry",
          "Hugging Face Model Hub"
        ]
      },
      "scheduling": {
        "schedule_type": "batch",
        "has_catchup": false,
        "retry_policy": "default_retry"
      },
      "complexity": {
        "complexity_score": 3,
        "complexity_factors": [
          "Multiple Docker containers",
          "LLM integration",
          "Language detection service",
          "Shared volume mounting"
        ],
        "use_case_fit": "Well-structured for multilingual text analysis with clear sequential dependencies",
        "standard_complexity_score": 4
      },
      "failure_handling": {
        "retry_mechanism": "Airflow task retries (default: 1)",
        "alert_mechanism": "Not specified",
        "transaction_safety": "File-based with intermediate JSON outputs"
      },
      "unique_features": [
        "Multilingual language detection",
        "LLM-powered sentiment analysis",
        "Product feature extraction from reviews",
        "Docker container orchestration",
        "Shared volume data persistence"
      ],
      "source_file": "Multilingual_Product_Review.txt"
    },
    {
      "pipeline_name": "Procurement_Supplier_Validation.txt",
      "business_domain": "Procurement",
      "domain_category": "Data Quality",
      "primary_objective": "Validate and standardize supplier data for procurement systems",
      "enrichment_objective": "Reconcile supplier names against Wikidata to find canonical entities and add Wikidata IDs",
      "topology": {
        "pattern": "linear_sequential",
        "description": "Strict sequential flow: Load & Modify Data >> Entity Reconciliation >> Save Final Data",
        "has_sensors": false,
        "has_branches": false,
        "has_fan_out": false,
        "has_fan_in": false,
        "parallelization_level": "none",
        "max_parallel_width": 1,
        "branch_depth": 0
      },
      "processing": {
        "total_tasks": 3,
        "task_types": [
          "load_and_modify",
          "reconciliation",
          "save"
        ],
        "processing_stages": [
          "Data Ingestion & Standardization",
          "Entity Reconciliation",
          "Data Export"
        ],
        "etl_pattern": "extract_transform_load",
        "data_flow_description": "CSV input -> JSON transformation -> Wikidata API enrichment -> CSV output"
      },
      "external_services": {
        "primary_services": [
          "load-and-modify service",
          "reconciliation service",
          "save service"
        ],
        "apis_used": [
          "Wikidata API",
          "Intertwino API"
        ],
        "databases": [
          "MongoDB"
        ],
        "service_integration_pattern": "sequential_api_calls"
      },
      "infrastructure": {
        "connections": [
          "app_network Docker network"
        ],
        "file_paths": [
          "DATA_DIR/suppliers.csv",
          "DATA_DIR/table_data_2.json",
          "DATA_DIR/reconciled_table_2.json",
          "DATA_DIR/enriched_data_2.csv"
        ],
        "external_systems": [
          "Wikidata",
          "Intertwino API",
          "MongoDB"
        ]
      },
      "scheduling": {
        "schedule_type": "batch",
        "has_catchup": false,
        "retry_policy": "default retry (1 attempt)"
      },
      "complexity": {
        "complexity_score": 3,
        "complexity_factors": [
          "External API integration (Wikidata)",
          "Docker container orchestration",
          "Data format conversion (CSV/JSON)"
        ],
        "use_case_fit": "Data quality enrichment for procurement systems",
        "standard_complexity_score": 3
      },
      "failure_handling": {
        "retry_mechanism": "Airflow task retries",
        "alert_mechanism": "Not specified",
        "transaction_safety": "Basic retry with container cleanup"
      },
      "unique_features": [
        "Wikidata entity reconciliation for supplier disambiguation",
        "Docker-based microservices architecture with shared volumes",
        "Integration with Intertwino API for data processing"
      ],
      "source_file": "Procurement_Supplier_Validation.txt"
    },
    {
      "pipeline_name": "Sequential_Pipeline_Prompt.txt",
      "business_domain": "Data Warehousing",
      "domain_category": "ETL/ELT",
      "primary_objective": "Transform raw CSV data through sequential enrichment steps and export as processed CSV.",
      "enrichment_objective": "Enrich raw data with geocoding, weather information, and additional column properties.",
      "topology": {
        "pattern": "linear_sequential",
        "description": "Strictly sequential execution of Docker-containerized tasks with no parallelism or branching.",
        "has_sensors": false,
        "has_branches": false,
        "has_fan_out": false,
        "has_fan_in": false,
        "parallelization_level": "none",
        "max_parallel_width": 1,
        "branch_depth": 0
      },
      "processing": {
        "total_tasks": 5,
        "task_types": [
          "DockerOperator",
          "DockerOperator",
          "DockerOperator",
          "DockerOperator",
          "DockerOperator"
        ],
        "processing_stages": [
          "Load & Modify",
          "Data Reconciliation",
          "Weather Extension",
          "Column Extension",
          "Save Final Data"
        ],
        "etl_pattern": "extract_transform_load",
        "data_flow_description": "CSV files -> JSON conversion -> City reconciliation via HERE API -> Weather enrichment via OpenMeteo -> Column extension -> Final CSV export."
      },
      "external_services": {
        "primary_services": [
          "load-and-modify service",
          "reconciliation service",
          "OpenMeteo API",
          "Intertwino API"
        ],
        "apis_used": [
          "HERE Geocoding API",
          "OpenMeteo API"
        ],
        "databases": [
          "MongoDB"
        ],
        "service_integration_pattern": "sequential_api_calls"
      },
      "infrastructure": {
        "connections": [
          "app_network Docker network"
        ],
        "file_paths": [
          "/app/data (DATA_DIR)"
        ],
        "external_systems": [
          "Docker containers",
          "MongoDB",
          "Intertwino API"
        ]
      },
      "scheduling": {
        "schedule_type": "batch",
        "has_catchup": false,
        "retry_policy": "default_airflow_retry"
      },
      "complexity": {
        "complexity_score": 4,
        "complexity_factors": [
          "Multiple external API integrations",
          "Docker container orchestration",
          "Data format transformations",
          "Shared volume and network dependencies"
        ],
        "use_case_fit": "Batch data enrichment pipeline for structured CSV data with geospatial and meteorological augmentation.",
        "standard_complexity_score": 4
      },
      "failure_handling": {
        "retry_mechanism": "Airflow task retries (default: 1)",
        "alert_mechanism": "Airflow default alerting",
        "transaction_safety": "Intermediate file-based state; container auto-cleanup."
      },
      "unique_features": [
        "Docker-containerized tasks with custom images",
        "Custom Docker network (app_network) for service isolation",
        "Environment variable configuration for all tasks",
        "Sequential enrichment with geocoding and weather data"
      ],
      "source_file": "Sequential_Pipeline_Prompt.txt"
    },
    {
      "pipeline_name": "Tezz1999.../hive_connection.py",
      "business_domain": "Healthcare",
      "domain_category": "Data Warehousing",
      "primary_objective": "Execute Hive database operations for COVID-19 realtime streaming data",
      "enrichment_objective": "Create database and table, then insert test data for COVID-19 data storage",
      "topology": {
        "pattern": "linear_sequential",
        "description": "Two tasks execute in strict sequential order: system check followed by Hive database operations",
        "has_sensors": false,
        "has_branches": false,
        "has_fan_out": false,
        "has_fan_in": false,
        "parallelization_level": "none",
        "max_parallel_width": 1,
        "branch_depth": 0
      },
      "processing": {
        "total_tasks": 2,
        "task_types": [
          "BashOperator",
          "HiveOperator"
        ],
        "processing_stages": [
          "System Check",
          "Database Operations"
        ],
        "etl_pattern": "batch",
        "data_flow_description": "System command execution to identify user context, then HiveQL script execution to create database 'mydb', table 'mydb.test_af', and insert test value (2)"
      },
      "external_services": {
        "primary_services": [
          "Hive"
        ],
        "apis_used": [],
        "databases": [
          "Hive"
        ],
        "service_integration_pattern": "database_centric"
      },
      "infrastructure": {
        "connections": [
          "hive_local"
        ],
        "file_paths": [],
        "external_systems": [
          "Hive database"
        ]
      },
      "scheduling": {
        "schedule_type": "cron",
        "has_catchup": true,
        "retry_policy": "not specified"
      },
      "complexity": {
        "complexity_score": 2,
        "complexity_factors": [
          "linear sequential pattern",
          "few tasks (2)",
          "simple database operations",
          "no branching or parallelism"
        ],
        "use_case_fit": "Simple scheduled batch database setup for COVID-19 data storage",
        "standard_complexity_score": 1
      },
      "failure_handling": {
        "retry_mechanism": "not explicitly defined",
        "alert_mechanism": "none",
        "transaction_safety": "not specified"
      },
      "unique_features": [
        "Hive database operations for COVID-19 data",
        "System user context check before database operations",
        "Multi-statement HiveQL script execution"
      ],
      "source_file": "Tezz1999__Covid-19-Realtime-Stream__hive_connection.py_description.txt"
    },
    {
      "pipeline_name": "bcgov.../PCD-ETL.py",
      "business_domain": "Healthcare",
      "domain_category": "Primary Care Data Processing",
      "primary_objective": "Extract, transform, and load primary care data from multiple healthcare system APIs",
      "enrichment_objective": "Aggregate and process financial, patient service, HR, and administrative data from various healthcare endpoints",
      "topology": {
        "pattern": "fan_out_fan_in",
        "description": "Sequential folder validation → Parallel API extraction (fan-out) → Sequential processing and notification (fan-in)",
        "has_sensors": false,
        "has_branches": false,
        "has_fan_out": true,
        "has_fan_in": true,
        "parallelization_level": "mixed",
        "max_parallel_width": 18,
        "branch_depth": 0
      },
      "processing": {
        "total_tasks": 23,
        "task_types": [
          "KubernetesJobOperator",
          "HttpOperator",
          "EmptyOperator",
          "PythonOperator"
        ],
        "processing_stages": [
          "Folder Validation",
          "Parallel API Extraction",
          "Data Processing",
          "Notification"
        ],
        "etl_pattern": "batch",
        "data_flow_description": "SFTP and shared folder validation → Parallel HTTP API extraction from 18 healthcare endpoints → Kubernetes job processing and upload → Email notification with success/failure reporting"
      },
      "external_services": {
        "primary_services": [
          "HTTP APIs",
          "Kubernetes",
          "SFTP",
          "Email"
        ],
        "apis_used": [
          "Financial Expense",
          "UPCC Financial Reporting",
          "CHC Financial Reporting",
          "PCN Financial Reporting",
          "NPPCC Financial Reporting",
          "Fiscal Year Reporting Dates",
          "UPCC Primary Care Patient Services",
          "CHC Primary Care Patient Services",
          "Practitioner Role Mapping",
          "Status Tracker",
          "HR Records",
          "Provincial Risk Tracking",
          "Decision Log",
          "HA Hierarchy",
          "UPPC Budget",
          "CHC Budget",
          "PCN Budget",
          "NPPCC Budget"
        ],
        "databases": [],
        "service_integration_pattern": "parallel_api_fanout"
      },
      "infrastructure": {
        "connections": [
          "Kubernetes",
          "HTTP APIs",
          "SFTP",
          "Email"
        ],
        "file_paths": [
          "SFTP folder",
          "Shared folder"
        ],
        "external_systems": [
          "Healthcare APIs",
          "Kubernetes cluster",
          "Email server",
          "SFTP server"
        ]
      },
      "scheduling": {
        "schedule_type": "dynamic_variable",
        "has_catchup": false,
        "retry_policy": "not_specified"
      },
      "complexity": {
        "complexity_score": 7,
        "complexity_factors": [
          "18 parallel HTTP API calls",
          "Kubernetes job orchestration",
          "Dynamic scheduling via variables",
          "Comprehensive email notification system",
          "Multiple integration points",
          "Staged processing with synchronization"
        ],
        "use_case_fit": "Healthcare data aggregation and reporting",
        "standard_complexity_score": 7
      },
      "failure_handling": {
        "retry_mechanism": "not_specified",
        "alert_mechanism": "email_notification",
        "transaction_safety": "partial_with_notification"
      },
      "unique_features": [
        "Dynamic scheduling via Airflow Variables",
        "KubernetesJobOperator for containerized workloads",
        "Comprehensive email notification with success/failure differentiation",
        "Parallel extraction from 18 healthcare APIs",
        "Standardized JSON payload validation",
        "Environment-aware email distribution lists"
      ],
      "source_file": "bcgov__medis-scheduler__PCD-ETL.py_description.txt"
    },
    {
      "pipeline_name": "hejnal.../dataform_dag_dynamic_vars.py",
      "business_domain": "Data Warehousing",
      "domain_category": "Data Transformation",
      "primary_objective": "Orchestrate parameterized SQL workflow executions using Google Cloud Dataform",
      "enrichment_objective": "Transform and process data through compiled SQL workflows with dynamic parameterization",
      "topology": {
        "pattern": "sensor_gated",
        "description": "Sequential linear pipeline with a sensor that gates execution until external workflow completes",
        "has_sensors": true,
        "has_branches": false,
        "has_fan_out": false,
        "has_fan_in": false,
        "parallelization_level": "none",
        "max_parallel_width": 1,
        "branch_depth": 0
      },
      "processing": {
        "total_tasks": 6,
        "task_types": [
          "DummyOperator",
          "PythonOperator",
          "DataformCreateCompilationResultOperator",
          "DataformCreateWorkflowInvocationOperator",
          "DataformWorkflowInvocationStateSensor"
        ],
        "processing_stages": [
          "Initialization",
          "Parameter Parsing",
          "Compilation Creation",
          "Workflow Invocation",
          "State Monitoring",
          "Completion"
        ],
        "etl_pattern": "batch",
        "data_flow_description": "Dataset trigger initiates pipeline → parameters parsed and passed via XCom → Dataform compilation created → workflow invoked → sensor monitors completion → pipeline ends"
      },
      "external_services": {
        "primary_services": [
          "Google Cloud Dataform"
        ],
        "apis_used": [
          "Google Cloud Dataform API"
        ],
        "databases": [],
        "service_integration_pattern": "orchestrated_services"
      },
      "infrastructure": {
        "connections": [
          "modelling_cloud_default"
        ],
        "file_paths": [],
        "external_systems": [
          "Google Cloud Dataform (whejna-modelling-sandbox project, training-repo repository, europe-west3 region)"
        ]
      },
      "scheduling": {
        "schedule_type": "dataset_triggered",
        "has_catchup": false,
        "retry_policy": "none"
      },
      "complexity": {
        "complexity_score": 4,
        "complexity_factors": [
          "External service integration",
          "Parameter passing via XCom",
          "Sensor-gated execution",
          "Dataset-based triggering"
        ],
        "use_case_fit": "Medium complexity ETL orchestration with external workflow management",
        "standard_complexity_score": 5
      },
      "failure_handling": {
        "retry_mechanism": "none",
        "alert_mechanism": "none",
        "transaction_safety": "partial"
      },
      "unique_features": [
        "Dataset-triggered execution",
        "Dataform workflow orchestration",
        "Parameter-driven SQL compilation",
        "State sensor for external workflow monitoring"
      ],
      "source_file": "hejnal__dataform-training__dataform_dag_dynamic_vars.py_description.txt"
    },
    {
      "pipeline_name": "keeyong.../run_elt_error.py",
      "business_domain": "Data Warehousing",
      "domain_category": "ELT/ELT",
      "primary_objective": "Build analytics tables in Snowflake using CTAS operations with atomic swaps",
      "enrichment_objective": "Validate data quality and implement atomic table swaps for zero-downtime updates",
      "topology": {
        "pattern": "linear_sequential",
        "description": "Tables are processed one by one in a strict linear order, with each table task depending on the previous completion",
        "has_sensors": false,
        "has_branches": false,
        "has_fan_out": false,
        "has_fan_in": false,
        "parallelization_level": "none",
        "max_parallel_width": 1,
        "branch_depth": 0
      },
      "processing": {
        "total_tasks": 1,
        "task_types": [
          "PythonOperator"
        ],
        "processing_stages": [
          "Extract",
          "Transform",
          "Load"
        ],
        "etl_pattern": "extract_transform_load",
        "data_flow_description": "SQL SELECT from raw tables (Extract), aggregation/formatting in CTAS (Transform), atomic table swap using ALTER TABLE SWAP (Load)"
      },
      "external_services": {
        "primary_services": [
          "Snowflake",
          "Slack"
        ],
        "apis_used": [],
        "databases": [
          "Snowflake"
        ],
        "service_integration_pattern": "database_centric"
      },
      "infrastructure": {
        "connections": [
          "snowflake_conn"
        ],
        "file_paths": [],
        "external_systems": [
          "Snowflake data warehouse",
          "Slack"
        ]
      },
      "scheduling": {
        "schedule_type": "batch",
        "has_catchup": false,
        "retry_policy": "Not specified in description"
      },
      "complexity": {
        "complexity_score": 3,
        "complexity_factors": [
          "Multiple external dependencies (Snowflake, Slack)",
          "Data validation logic",
          "Atomic deployment pattern"
        ],
        "use_case_fit": "Scheduled batch ELT for analytics table creation with quality checks",
        "standard_complexity_score": 2
      },
      "failure_handling": {
        "retry_mechanism": "Not specified in description",
        "alert_mechanism": "Slack notifications via on_failure_callback",
        "transaction_safety": "Atomic table swaps using temporary tables and SWAP operation"
      },
      "unique_features": [
        "Zero-row validation prevents empty table deployment",
        "Atomic table swap for zero-downtime updates",
        "CTAS pattern with data validation"
      ],
      "source_file": "keeyong__airflow-bootcamp__run_elt_error.py_description.txt"
    },
    {
      "pipeline_name": "nazzang49.../airflow_db_cleanup.py",
      "business_domain": "System Operations",
      "domain_category": "Data Quality",
      "primary_objective": "Periodically clean old metadata entries from Airflow's MetaStore database tables to prevent excessive data accumulation",
      "enrichment_objective": "Configuration-driven cleanup with email alerting and safe execution modes",
      "topology": {
        "pattern": "linear_sequential",
        "description": "Two tasks execute in strict sequential order with data passing via XCom",
        "has_sensors": false,
        "has_branches": false,
        "has_fan_out": false,
        "has_fan_in": false,
        "parallelization_level": "none",
        "max_parallel_width": 1,
        "branch_depth": 0
      },
      "processing": {
        "total_tasks": 2,
        "task_types": [
          "PythonOperator",
          "PythonOperator"
        ],
        "processing_stages": [
          "Configuration Loading & Validation",
          "Database Cleanup Execution"
        ],
        "etl_pattern": "batch",
        "data_flow_description": "Load configuration parameters, calculate max_date, then delete old entries from multiple Airflow metadata tables based on retention rules"
      },
      "external_services": {
        "primary_services": [
          "Airflow MetaStore Database"
        ],
        "apis_used": [],
        "databases": [
          "Airflow MetaStore (SQLAlchemy)"
        ],
        "service_integration_pattern": "database_centric"
      },
      "infrastructure": {
        "connections": [
          "Airflow default database session via settings.Session()"
        ],
        "file_paths": [],
        "external_systems": [
          "Airflow Variables system",
          "Email alert system"
        ]
      },
      "scheduling": {
        "schedule_type": "@daily",
        "has_catchup": false,
        "retry_policy": "1 retry with 1-minute delay"
      },
      "complexity": {
        "complexity_score": 3,
        "complexity_factors": [
          "Multiple database table operations",
          "Configuration-driven execution",
          "Version-aware model handling",
          "Safe execution mode with dry-run capability"
        ],
        "use_case_fit": "Maintenance workflow for database hygiene with configurable retention and alerting",
        "standard_complexity_score": 1
      },
      "failure_handling": {
        "retry_mechanism": "Automatic retry with 1-minute delay",
        "alert_mechanism": "Email alerts to configurable addresses on task failures",
        "transaction_safety": "Graceful error handling with try/except blocks and ENABLE_DELETE flag for dry-run operations"
      },
      "unique_features": [
        "Configuration-driven cleanup with DATABASE_OBJECTS list",
        "Version-aware handling of Airflow model differences",
        "Safe execution mode with ENABLE_DELETE flag",
        "Integration with Airflow Variables for retention settings",
        "Handles multiple Airflow metadata tables with configurable retention rules"
      ],
      "source_file": "nazzang49__gcp-tutorials__airflow_db_cleanup.py_description.txt"
    },
    {
      "pipeline_name": "peterbull.../log_cleanup.py",
      "business_domain": "System Operations",
      "domain_category": "Infrastructure Maintenance",
      "primary_objective": "Periodically clean up old Airflow log files to prevent disk space issues",
      "enrichment_objective": "N/A",
      "topology": {
        "pattern": "fan_out_fan_in",
        "description": "Single start task fans out to multiple parallel BashOperator workers for log cleanup across directories, with no explicit synchronization join",
        "has_sensors": false,
        "has_branches": false,
        "has_fan_out": true,
        "has_fan_in": false,
        "parallelization_level": "task_level",
        "max_parallel_width": 0,
        "branch_depth": 0
      },
      "processing": {
        "total_tasks": 0,
        "task_types": [
          "EmptyOperator",
          "BashOperator"
        ],
        "processing_stages": [
          "Initialization",
          "Parallel Log Cleanup"
        ],
        "etl_pattern": "batch",
        "data_flow_description": "No data transformation; file system operations to delete old log files and empty directories"
      },
      "external_services": {
        "primary_services": [],
        "apis_used": [],
        "databases": [],
        "service_integration_pattern": "file_processing"
      },
      "infrastructure": {
        "connections": [],
        "file_paths": [
          "/tmp/airflow_log_cleanup_worker.lock",
          "BASE_LOG_FOLDER",
          "CHILD_PROCESS_LOG_DIRECTORY"
        ],
        "external_systems": [
          "Airflow File System"
        ]
      },
      "scheduling": {
        "schedule_type": "cron",
        "has_catchup": false,
        "retry_policy": "single_retry_with_delay"
      },
      "complexity": {
        "complexity_score": 4,
        "complexity_factors": [
          "Parallel execution coordination",
          "File-based locking mechanism",
          "Configurable worker count",
          "Multiple directory targets"
        ],
        "use_case_fit": "Maintenance automation with parallel file system operations",
        "standard_complexity_score": 2
      },
      "failure_handling": {
        "retry_mechanism": "Automatic retry with 1-minute delay",
        "alert_mechanism": "Email alerts to configured addresses",
        "transaction_safety": "Lock file cleanup on failure to allow subsequent executions"
      },
      "unique_features": [
        "Worker coordination via lock files",
        "Configurable log age thresholds",
        "Optional child process log directory cleanup",
        "Staggered execution with worker-specific delays"
      ],
      "source_file": "peterbull__bodhi-cast__log_cleanup.py_description.txt"
    },
    {
      "pipeline_name": "sivajik34.../magento_customer_graphql.py",
      "business_domain": "E-commerce",
      "domain_category": "Customer Management",
      "primary_objective": "Perform sequential Magento customer management operations using GraphQL API",
      "enrichment_objective": "Create customer, generate authentication token, and retrieve comprehensive customer information",
      "topology": {
        "pattern": "linear_sequential",
        "description": "Strict linear dependency chain: create_customer → generate_customer_token → get_customer_info with data passing between tasks",
        "has_sensors": false,
        "has_branches": false,
        "has_fan_out": false,
        "has_fan_in": false,
        "parallelization_level": "none",
        "max_parallel_width": 1,
        "branch_depth": 0
      },
      "processing": {
        "total_tasks": 3,
        "task_types": [
          "MagentoGraphQLOperator",
          "PythonOperator"
        ],
        "processing_stages": [
          "Customer Creation",
          "Authentication Token Generation",
          "Customer Information Retrieval"
        ],
        "etl_pattern": "batch",
        "data_flow_description": "Hardcoded customer data → customer email → authentication token → complete customer profile with addresses"
      },
      "external_services": {
        "primary_services": [
          "Magento GraphQL API"
        ],
        "apis_used": [
          "Magento GraphQL API"
        ],
        "databases": [],
        "service_integration_pattern": "sequential_api_calls"
      },
      "infrastructure": {
        "connections": [
          "magento_default"
        ],
        "file_paths": [],
        "external_systems": [
          "Magento E-commerce Platform"
        ]
      },
      "scheduling": {
        "schedule_type": "manual",
        "has_catchup": false,
        "retry_policy": "1 retry with 5-minute delay"
      },
      "complexity": {
        "complexity_score": 3,
        "complexity_factors": [
          "Linear sequential pattern",
          "Single external service integration",
          "Simple data flow",
          "No branching or parallelism"
        ],
        "use_case_fit": "Demonstration of complete customer lifecycle management in Magento via GraphQL API",
        "standard_complexity_score": 2
      },
      "failure_handling": {
        "retry_mechanism": "Basic retry with fixed delay",
        "alert_mechanism": "None (email notifications disabled)",
        "transaction_safety": "Partial (GraphQL error validation in tasks, but no rollback mechanism)"
      },
      "unique_features": [
        "Custom MagentoGraphQLOperator",
        "TaskFlow API decorators",
        "Complete customer authentication flow demonstration",
        "GraphQL error validation in tasks"
      ],
      "source_file": "sivajik34__magento-airflow__magento_customer_graphql.py_description.txt"
    },
    {
      "pipeline_name": "stikkireddy.../example_dag.py",
      "business_domain": "System Operations",
      "domain_category": "Data Processing Orchestration",
      "primary_objective": "Orchestrate Databricks notebook executions with conditional branching and cluster reuse",
      "enrichment_objective": "Execute secondary Databricks notebook based on conditional logic",
      "topology": {
        "pattern": "branch_merge",
        "description": "Conditional branching with BranchPythonOperator where one path terminates immediately and another continues with additional notebook execution",
        "has_sensors": false,
        "has_branches": true,
        "has_fan_out": false,
        "has_fan_in": false,
        "parallelization_level": "none",
        "max_parallel_width": 1,
        "branch_depth": 1
      },
      "processing": {
        "total_tasks": 8,
        "task_types": [
          "DummyOperator",
          "DatabricksSubmitRunOperator",
          "BranchPythonOperator"
        ],
        "processing_stages": [
          "Initialization",
          "Primary Notebook Execution",
          "Branch Decision",
          "Secondary Notebook Execution",
          "Completion"
        ],
        "etl_pattern": "batch",
        "data_flow_description": "Manual trigger starts pipeline, executes primary Databricks notebook, evaluates conditional branch function, then either terminates or executes secondary Databricks notebook before completion"
      },
      "external_services": {
        "primary_services": [
          "Databricks"
        ],
        "apis_used": [
          "Databricks REST API"
        ],
        "databases": [],
        "service_integration_pattern": "orchestrated_services"
      },
      "infrastructure": {
        "connections": [
          "databricks_default"
        ],
        "file_paths": [
          "/Users/sri.tikkireddy@databricks.com/workflow-mirroring/helloworld"
        ],
        "external_systems": [
          "Databricks"
        ]
      },
      "scheduling": {
        "schedule_type": "manual",
        "has_catchup": false,
        "retry_policy": "1 retry with 5-minute delay"
      },
      "complexity": {
        "complexity_score": 6,
        "complexity_factors": [
          "Conditional branching logic",
          "External Databricks integration",
          "Cluster reuse configuration",
          "Secret management",
          "Multiple notebook executions"
        ],
        "use_case_fit": "Databricks job orchestration with conditional execution paths",
        "standard_complexity_score": 5
      },
      "failure_handling": {
        "retry_mechanism": "Built-in Airflow retry with 5-minute delay",
        "alert_mechanism": "None configured",
        "transaction_safety": "Basic retry mechanism only"
      },
      "unique_features": [
        "AirflowDBXClusterReuseBuilder for cluster management",
        "Conditional branching with non-converging paths",
        "Databricks secret scope integration for authentication"
      ],
      "source_file": "stikkireddy__databricks-reusable-job-clusters__example_dag.py_description.txt"
    },
    {
      "pipeline_name": "synthetic-gener.../synthetic_branch_merge_00_data",
      "business_domain": "Data Quality",
      "domain_category": "Data Quality",
      "primary_objective": "Implement a data quality gate for customer CSV data with conditional routing based on quality scores",
      "enrichment_objective": "Quality assessment and conditional routing to production or quarantine",
      "topology": {
        "pattern": "branch_merge",
        "description": "Single branching gateway with two conditional paths that converge to a final cleanup task",
        "has_sensors": false,
        "has_branches": true,
        "has_fan_out": true,
        "has_fan_in": true,
        "parallelization_level": "task_level",
        "max_parallel_width": 2,
        "branch_depth": 1
      },
      "processing": {
        "total_tasks": 6,
        "task_types": [
          "PythonOperator",
          "BranchPythonOperator",
          "EmailOperator"
        ],
        "processing_stages": [
          "ingest",
          "quality_assessment",
          "conditional_routing",
          "load_or_quarantine",
          "alerting",
          "cleanup"
        ],
        "etl_pattern": "batch",
        "data_flow_description": "CSV ingestion → quality scoring → conditional branch → production load OR quarantine with alert → cleanup"
      },
      "external_services": {
        "primary_services": [
          "File System",
          "Email Service",
          "Database"
        ],
        "apis_used": [],
        "databases": [
          "Production database"
        ],
        "service_integration_pattern": "database_centric"
      },
      "infrastructure": {
        "connections": [
          "SMTP email connection",
          "Database connection"
        ],
        "file_paths": [
          "/data/raw/"
        ],
        "external_systems": [
          "File storage system",
          "Email system",
          "Production database",
          "Quarantine storage system"
        ]
      },
      "scheduling": {
        "schedule_type": "daily",
        "has_catchup": false,
        "retry_policy": "1 retry with 5-minute delay"
      },
      "complexity": {
        "complexity_score": 4,
        "complexity_factors": [
          "conditional branching",
          "parallel execution paths",
          "external service integration",
          "quality scoring logic"
        ],
        "use_case_fit": "Data quality assessment and conditional routing pipeline",
        "standard_complexity_score": 6
      },
      "failure_handling": {
        "retry_mechanism": "Task-level retries with delay",
        "alert_mechanism": "Email alerts to data stewards for quality failures",
        "transaction_safety": "Partial - cleanup task handles temporary resources"
      },
      "unique_features": [
        "Quality threshold-based branching",
        "Email alerts for quality violations",
        "Branch-merge pattern with convergence",
        "Completeness and validity scoring metrics"
      ],
      "source_file": "synthetic-generator-v2__synthetic_branch_merge_00_data_quality_gate.py_description.txt"
    },
    {
      "pipeline_name": "synthetic-gener.../synthetic_branch_merge_01_frau",
      "business_domain": "Financial Services",
      "domain_category": "Fraud Detection",
      "primary_objective": "Process daily transaction batches for fraud detection triage",
      "enrichment_objective": "Apply risk scoring and conditionally route transactions based on risk thresholds",
      "topology": {
        "pattern": "branch_merge",
        "description": "Conditional routing with two parallel execution paths that converge at a merge point",
        "has_sensors": false,
        "has_branches": true,
        "has_fan_out": true,
        "has_fan_in": true,
        "parallelization_level": "task_level",
        "max_parallel_width": 2,
        "branch_depth": 1
      },
      "processing": {
        "total_tasks": 5,
        "task_types": [
          "PythonOperator",
          "BranchPythonOperator"
        ],
        "processing_stages": [
          "extract_analyze",
          "risk_routing",
          "manual_review",
          "auto_approval",
          "notification"
        ],
        "etl_pattern": "batch",
        "data_flow_description": "Extract transaction CSV → Calculate risk score → Branch based on threshold → Parallel processing of high/low risk → Merge and notify"
      },
      "external_services": {
        "primary_services": [
          "File system",
          "Manual review queue",
          "Payment processing",
          "Notification system"
        ],
        "apis_used": [],
        "databases": [],
        "service_integration_pattern": "orchestrated_services"
      },
      "infrastructure": {
        "connections": [],
        "file_paths": [
          "Daily transaction CSV files"
        ],
        "external_systems": [
          "File system",
          "Manual review queue system",
          "Payment processing system",
          "Notification system"
        ]
      },
      "scheduling": {
        "schedule_type": "daily",
        "has_catchup": false,
        "retry_policy": "2 retries with 5-minute delay"
      },
      "complexity": {
        "complexity_score": 4,
        "complexity_factors": [
          "Conditional branching",
          "Parallel execution paths",
          "XCom data passing",
          "Merge synchronization"
        ],
        "use_case_fit": "Well-suited for fraud detection workflows requiring conditional routing based on risk thresholds",
        "standard_complexity_score": 7
      },
      "failure_handling": {
        "retry_mechanism": "Automatic retries with delay",
        "alert_mechanism": "Email notifications on failure",
        "transaction_safety": "Basic retry with notification"
      },
      "unique_features": [
        "BranchPythonOperator for conditional routing",
        "trigger_rule='none_failed' for merge synchronization",
        "Risk threshold-based branching (0.8 threshold)"
      ],
      "source_file": "synthetic-generator-v2__synthetic_branch_merge_01_fraud_detection_triage.py_description.txt"
    },
    {
      "pipeline_name": "synthetic-gener.../synthetic_branch_merge_02_back",
      "business_domain": "System Operations",
      "domain_category": "Database Operations",
      "primary_objective": "Automated database backup strategy selection based on day of week",
      "enrichment_objective": "Backup verification and integrity validation",
      "topology": {
        "pattern": "branch_merge",
        "description": "Conditional routing to either full backup (Saturday) or incremental backup (weekdays) with convergence for verification",
        "has_sensors": false,
        "has_branches": true,
        "has_fan_out": false,
        "has_fan_in": true,
        "parallelization_level": "none",
        "max_parallel_width": 1,
        "branch_depth": 1
      },
      "processing": {
        "total_tasks": 6,
        "task_types": [
          "EmptyOperator",
          "BranchPythonOperator",
          "BashOperator"
        ],
        "processing_stages": [
          "Initialization",
          "Condition Check",
          "Backup Execution",
          "Verification",
          "Completion"
        ],
        "etl_pattern": "batch",
        "data_flow_description": "Start → Day check → Conditional backup execution → Verification → Completion"
      },
      "external_services": {
        "primary_services": [
          "Database Backup System"
        ],
        "apis_used": [],
        "databases": [],
        "service_integration_pattern": "database_centric"
      },
      "infrastructure": {
        "connections": [],
        "file_paths": [],
        "external_systems": [
          "Database System"
        ]
      },
      "scheduling": {
        "schedule_type": "daily",
        "has_catchup": false,
        "retry_policy": "2 retries with 5-minute delay"
      },
      "complexity": {
        "complexity_score": 4,
        "complexity_factors": [
          "Conditional branching",
          "Merge synchronization",
          "Day-based routing logic",
          "Trigger rule configuration"
        ],
        "use_case_fit": "Well-suited for scheduled backup operations with conditional execution paths",
        "standard_complexity_score": 5
      },
      "failure_handling": {
        "retry_mechanism": "Task-level retries with delay",
        "alert_mechanism": "None (email notifications disabled)",
        "transaction_safety": "Merge task uses 'none_failed_min_one_success' trigger rule for branch convergence"
      },
      "unique_features": [
        "Day-of-week based backup strategy selection",
        "Mutually exclusive branch execution",
        "Special trigger rule for branch merging",
        "Simulated backup operations"
      ],
      "source_file": "synthetic-generator-v2__synthetic_branch_merge_02_backup_strategy_selector.py_description.txt"
    },
    {
      "pipeline_name": "synthetic-gener.../synthetic_branch_merge_03_regu",
      "business_domain": "Financial Services",
      "domain_category": "Regulatory Compliance",
      "primary_objective": "Process financial transaction data and route it to appropriate regulatory systems based on account type",
      "enrichment_objective": "Generate regulatory reports (FATCA XML, IRS Form 1099) and archive for compliance retention",
      "topology": {
        "pattern": "branch_merge",
        "description": "Sequential extract, conditional branch based on account type, parallel reporting workflows, and synchronized merge for archival",
        "has_sensors": false,
        "has_branches": true,
        "has_fan_out": true,
        "has_fan_in": true,
        "parallelization_level": "task_level",
        "max_parallel_width": 2,
        "branch_depth": 1
      },
      "processing": {
        "total_tasks": 5,
        "task_types": [
          "PythonOperator",
          "BranchPythonOperator"
        ],
        "processing_stages": [
          "Extract",
          "Branch Decision",
          "Parallel Transform",
          "Load"
        ],
        "etl_pattern": "extract_transform_load",
        "data_flow_description": "Extract CSV data → Analyze account type → Branch to FATCA or IRS reporting → Generate reports → Archive merged results"
      },
      "external_services": {
        "primary_services": [
          "FATCA regulatory compliance system",
          "IRS regulatory compliance system",
          "Secure archive storage system"
        ],
        "apis_used": [],
        "databases": [],
        "service_integration_pattern": "orchestrated_services"
      },
      "infrastructure": {
        "connections": [],
        "file_paths": [
          "Local CSV source files"
        ],
        "external_systems": [
          "FATCA system",
          "IRS system",
          "Secure archive storage"
        ]
      },
      "scheduling": {
        "schedule_type": "daily",
        "has_catchup": false,
        "retry_policy": "2 retries with 5-minute delay"
      },
      "complexity": {
        "complexity_score": 4,
        "complexity_factors": [
          "Conditional branching",
          "Parallel task execution",
          "Multiple external regulatory systems",
          "XCom-based data sharing"
        ],
        "use_case_fit": "Regulatory reporting with account-type-based routing",
        "standard_complexity_score": 7
      },
      "failure_handling": {
        "retry_mechanism": "Task-level retries with delay",
        "alert_mechanism": "None (email notifications disabled)",
        "transaction_safety": "Basic retry without explicit rollback"
      },
      "unique_features": [
        "BranchPythonOperator for regulatory routing",
        "XCom for cross-branch data sharing",
        "Parallel FATCA and IRS reporting workflows",
        "All-done merge for archival"
      ],
      "source_file": "synthetic-generator-v2__synthetic_branch_merge_03_regulatory_report_router.py_description.txt"
    },
    {
      "pipeline_name": "synthetic-gener.../synthetic_branch_merge_05_cont",
      "business_domain": "Content Moderation",
      "domain_category": "System Operations",
      "primary_objective": "Scan user-generated content for toxicity and route processing based on toxicity threshold",
      "enrichment_objective": "Audit logging of moderation outcomes",
      "topology": {
        "pattern": "branch_merge",
        "description": "Conditional routing based on toxicity threshold with convergence at audit logging",
        "has_sensors": false,
        "has_branches": true,
        "has_fan_out": false,
        "has_fan_in": false,
        "parallelization_level": "none",
        "max_parallel_width": 1,
        "branch_depth": 1
      },
      "processing": {
        "total_tasks": 5,
        "task_types": [
          "PythonOperator",
          "BranchPythonOperator"
        ],
        "processing_stages": [
          "Extract",
          "Score & Branch",
          "Process Toxic Content",
          "Process Safe Content",
          "Audit"
        ],
        "etl_pattern": "batch",
        "data_flow_description": "Extract content from CSV → Score toxicity → Branch based on threshold → Process content accordingly → Merge for audit logging"
      },
      "external_services": {
        "primary_services": [
          "Platform Content Management System",
          "Platform Publishing System",
          "Audit Logging System"
        ],
        "apis_used": [],
        "databases": [],
        "service_integration_pattern": "orchestrated_services"
      },
      "infrastructure": {
        "connections": [],
        "file_paths": [
          "/data/user_content.csv"
        ],
        "external_systems": [
          "Local file system",
          "Mock toxicity scoring system"
        ]
      },
      "scheduling": {
        "schedule_type": "daily",
        "has_catchup": false,
        "retry_policy": "2 retries with 5-minute delay"
      },
      "complexity": {
        "complexity_score": 4,
        "complexity_factors": [
          "Conditional branching",
          "XCom data passing between tasks",
          "Multiple external system integrations",
          "Branch-merge pattern"
        ],
        "use_case_fit": "Content moderation workflow with conditional processing paths",
        "standard_complexity_score": 6
      },
      "failure_handling": {
        "retry_mechanism": "Automatic retry with delay",
        "alert_mechanism": "Email notifications on failure",
        "transaction_safety": "Basic retry with no rollback mechanism"
      },
      "unique_features": [
        "Toxicity threshold-based branching",
        "Mutually exclusive branch paths",
        "Consolidated audit logging from both branches",
        "Mock toxicity scoring simulation"
      ],
      "source_file": "synthetic-generator-v2__synthetic_branch_merge_05_content_moderation_pipeline.py_description.txt"
    },
    {
      "pipeline_name": "synthetic-gener.../synthetic_fan_out_fan_in_00_mu",
      "business_domain": "E-commerce",
      "domain_category": "E-commerce Analytics",
      "primary_objective": "Perform multi-region ecommerce analytics by ingesting sales data from four geographic regions",
      "enrichment_objective": "Convert regional currencies to USD and aggregate results into a global revenue report",
      "topology": {
        "pattern": "fan_out_fan_in",
        "description": "Parallel ingestion and currency conversion for four regions followed by single aggregation step",
        "has_sensors": false,
        "has_branches": false,
        "has_fan_out": true,
        "has_fan_in": true,
        "parallelization_level": "task_level",
        "max_parallel_width": 4,
        "branch_depth": 2
      },
      "processing": {
        "total_tasks": 11,
        "task_types": [
          "EmptyOperator",
          "PythonOperator"
        ],
        "processing_stages": [
          "Pre-processing",
          "Parallel Ingestion",
          "Currency Conversion",
          "Aggregation",
          "Completion"
        ],
        "etl_pattern": "batch",
        "data_flow_description": "Regional extraction → Currency normalization → Global aggregation"
      },
      "external_services": {
        "primary_services": [],
        "apis_used": [],
        "databases": [],
        "service_integration_pattern": "file_processing"
      },
      "infrastructure": {
        "connections": [],
        "file_paths": [
          "implied CSV files for regional data"
        ],
        "external_systems": [
          "Regional data sources (implied)"
        ]
      },
      "scheduling": {
        "schedule_type": "daily",
        "has_catchup": false,
        "retry_policy": "2 retries with 5-minute delay"
      },
      "complexity": {
        "complexity_score": 6,
        "complexity_factors": [
          "Parallel processing across 4 regions",
          "XCom-based data passing",
          "Currency conversion logic",
          "Multi-stage data flow"
        ],
        "use_case_fit": "Multi-region data aggregation with currency normalization",
        "standard_complexity_score": 5
      },
      "failure_handling": {
        "retry_mechanism": "Automatic retry with 2 attempts",
        "alert_mechanism": "Email notifications on task failures",
        "transaction_safety": "Basic retry with email alerts"
      },
      "unique_features": [
        "Multi-region parallel processing",
        "Currency conversion to USD",
        "XCom-based inter-task data passing",
        "Two-stage parallelism (ingestion → conversion → aggregation)"
      ],
      "source_file": "synthetic-generator-v2__synthetic_fan_out_fan_in_00_multi_region_e_commerce_analytics.py_description.txt"
    },
    {
      "pipeline_name": "synthetic-gener.../synthetic_fan_out_fan_in_01_cl",
      "business_domain": "Environmental Monitoring",
      "domain_category": "Climate Data Processing",
      "primary_objective": "Download weather station data from five meteorological agencies and merge into unified dataset",
      "enrichment_objective": "Normalize disparate agency data formats to standardized schema (ISO timestamp, Celsius temperature, meters elevation)",
      "topology": {
        "pattern": "fan_out_fan_in",
        "description": "Five parallel download streams → Five parallel normalization streams → Single merge operation",
        "has_sensors": false,
        "has_branches": false,
        "has_fan_out": true,
        "has_fan_in": true,
        "parallelization_level": "task_level",
        "max_parallel_width": 5,
        "branch_depth": 0
      },
      "processing": {
        "total_tasks": 11,
        "task_types": [
          "PythonOperator"
        ],
        "processing_stages": [
          "Data Extraction",
          "Data Normalization",
          "Data Merging"
        ],
        "etl_pattern": "batch",
        "data_flow_description": "Parallel download from five agency endpoints → Parallel normalization to standard format → Single merge into unified Parquet dataset"
      },
      "external_services": {
        "primary_services": [
          "NOAA FTP",
          "ECMWF HTTPS",
          "JMA HTTPS",
          "MetOffice HTTPS",
          "BOM HTTPS"
        ],
        "apis_used": [],
        "databases": [],
        "service_integration_pattern": "parallel_api_fanout"
      },
      "infrastructure": {
        "connections": [
          "FTP connection to noaa.gov",
          "HTTPS connections to ecmwf.int, jma.go.jp, metoffice.gov.uk, bom.gov.au"
        ],
        "file_paths": [
          "ftp://noaa.gov/weather/stations.csv",
          "https://ecmwf.int/data/stations.csv",
          "https://jma.go.jp/weather/stations.csv",
          "https://metoffice.gov.uk/data/stations.csv",
          "https://bom.gov.au/observations/stations.csv"
        ],
        "external_systems": [
          "NOAA",
          "ECMWF",
          "JMA",
          "MetOffice",
          "BOM"
        ]
      },
      "scheduling": {
        "schedule_type": "daily",
        "has_catchup": false,
        "retry_policy": "3 retries with 5-minute delay"
      },
      "complexity": {
        "complexity_score": 6,
        "complexity_factors": [
          "Parallel processing across 5 agencies",
          "Multiple external service integrations",
          "Data normalization requirements",
          "XCom-based data passing between tasks"
        ],
        "use_case_fit": "Well-suited for multi-source data aggregation with parallel processing",
        "standard_complexity_score": 7
      },
      "failure_handling": {
        "retry_mechanism": "Automatic retries with delay",
        "alert_mechanism": "Email notifications on failure",
        "transaction_safety": "Partial - individual agency streams can fail independently"
      },
      "unique_features": [
        "Multi-agency climate data fusion",
        "Parallel width of 5 for both download and normalization",
        "Standardized schema conversion across disparate sources",
        "XCom-based data flow between parallel tasks"
      ],
      "source_file": "synthetic-generator-v2__synthetic_fan_out_fan_in_01_climate_data_fusion.py_description.txt"
    },
    {
      "pipeline_name": "synthetic-gener.../synthetic_fan_out_fan_in_03_re",
      "business_domain": "Retail",
      "domain_category": "Supply Chain",
      "primary_objective": "Reconcile inventory discrepancies across multiple warehouse systems",
      "enrichment_objective": "Standardize SKU formats and generate consolidated reconciliation reports",
      "topology": {
        "pattern": "fan_out_fan_in",
        "description": "Parallel CSV fetching from 4 warehouses → parallel SKU normalization → synchronization → final reporting",
        "has_sensors": false,
        "has_branches": false,
        "has_fan_out": true,
        "has_fan_in": true,
        "parallelization_level": "task_level",
        "max_parallel_width": 4,
        "branch_depth": 0
      },
      "processing": {
        "total_tasks": 10,
        "task_types": [
          "PythonOperator"
        ],
        "processing_stages": [
          "CSV extraction",
          "SKU normalization",
          "Inventory reconciliation",
          "Report generation"
        ],
        "etl_pattern": "batch",
        "data_flow_description": "Fetch CSV from 4 warehouse systems → normalize SKU formats in parallel → reconcile discrepancies → generate final PDF report"
      },
      "external_services": {
        "primary_services": [
          "Warehouse management systems"
        ],
        "apis_used": [],
        "databases": [],
        "service_integration_pattern": "file_processing"
      },
      "infrastructure": {
        "connections": [],
        "file_paths": [
          "warehouse_north_inventory.csv",
          "warehouse_south_inventory.csv",
          "warehouse_east_inventory.csv",
          "warehouse_west_inventory.csv",
          "normalized_north_inventory.csv",
          "normalized_south_inventory.csv",
          "normalized_east_inventory.csv",
          "normalized_west_inventory.csv",
          "inventory_discrepancies_report.csv",
          "retail_inventory_reconciliation_final.pdf"
        ],
        "external_systems": [
          "Warehouse management system (north region)",
          "Warehouse management system (south region)",
          "Warehouse management system (east region)",
          "Warehouse management system (west region)"
        ]
      },
      "scheduling": {
        "schedule_type": "daily",
        "has_catchup": false,
        "retry_policy": "2 retries with 5-minute delay"
      },
      "complexity": {
        "complexity_score": 6,
        "complexity_factors": [
          "Parallel processing across 4 warehouses",
          "Fan-out fan-in topology",
          "Heavy XCom usage for data passing",
          "Multiple data transformation stages"
        ],
        "use_case_fit": "Well-suited for distributed inventory reconciliation across multiple locations",
        "standard_complexity_score": 5
      },
      "failure_handling": {
        "retry_mechanism": "Built-in Airflow retry with exponential backoff",
        "alert_mechanism": "None (email notifications disabled)",
        "transaction_safety": "Basic (file-based processing with retries)"
      },
      "unique_features": [
        "Four-way parallel warehouse data processing",
        "SKU format standardization across regions",
        "Inventory discrepancy detection and reporting",
        "Heavy reliance on XCom for inter-task communication"
      ],
      "source_file": "synthetic-generator-v2__synthetic_fan_out_fan_in_03_retail_inventory_reconciliation.py_description.txt"
    },
    {
      "pipeline_name": "synthetic-gener.../synthetic_fan_out_fan_in_06_fi",
      "business_domain": "Financial Services",
      "domain_category": "Portfolio Management",
      "primary_objective": "Automated portfolio rebalancing across multiple brokerage accounts",
      "enrichment_objective": "Aggregate holdings data, calculate portfolio metrics, and generate rebalancing trade orders",
      "topology": {
        "pattern": "fan_out_fan_in",
        "description": "Parallel processing of 5 brokerage accounts followed by aggregation and trade order generation",
        "has_sensors": false,
        "has_branches": false,
        "has_fan_out": true,
        "has_fan_in": true,
        "parallelization_level": "task_level",
        "max_parallel_width": 5,
        "branch_depth": 1
      },
      "processing": {
        "total_tasks": 8,
        "task_types": [
          "PythonOperator"
        ],
        "processing_stages": [
          "Data Fetching",
          "Portfolio Analysis",
          "Aggregation & Rebalancing",
          "Trade Order Generation"
        ],
        "etl_pattern": "batch",
        "data_flow_description": "Parallel CSV data fetching from 5 brokerages → Individual portfolio analysis → Aggregation of results → Rebalancing calculation → Trade order CSV generation"
      },
      "external_services": {
        "primary_services": [
          "Brokerage APIs"
        ],
        "apis_used": [
          "Simulated brokerage APIs"
        ],
        "databases": [],
        "service_integration_pattern": "parallel_api_fanout"
      },
      "infrastructure": {
        "connections": [],
        "file_paths": [
          "trade_orders_YYYYMMDD.csv"
        ],
        "external_systems": [
          "Brokerage systems"
        ]
      },
      "scheduling": {
        "schedule_type": "daily",
        "has_catchup": false,
        "retry_policy": "2 retries with 5-minute delay"
      },
      "complexity": {
        "complexity_score": 6,
        "complexity_factors": [
          "Parallel processing across 5 accounts",
          "Fan-out fan-in orchestration",
          "Financial calculations",
          "Multiple data aggregation points"
        ],
        "use_case_fit": "Well-suited for financial portfolio management with parallel account processing",
        "standard_complexity_score": 5
      },
      "failure_handling": {
        "retry_mechanism": "Basic retry with delay",
        "alert_mechanism": "Email alerts disabled",
        "transaction_safety": "No explicit transaction handling"
      },
      "unique_features": [
        "Explicit 5-way parallel width",
        "Mock brokerage API implementations",
        "Portfolio metrics calculation",
        "Rebalancing trade generation"
      ],
      "source_file": "synthetic-generator-v2__synthetic_fan_out_fan_in_06_financial_portfolio_rebalancing.py_description.txt"
    },
    {
      "pipeline_name": "synthetic-gener.../synthetic_fan_out_only_00_mult",
      "business_domain": "Marketing Analytics",
      "domain_category": "Multi-channel Marketing Campaign Execution",
      "primary_objective": "Execute parallel marketing campaigns across email, SMS, and push notification channels",
      "enrichment_objective": "Target customer segments with channel-specific marketing messages",
      "topology": {
        "pattern": "fan_out_fan_in",
        "description": "Single data loading task triggers three independent parallel marketing channel executions without synchronization or fan-in",
        "has_sensors": false,
        "has_branches": false,
        "has_fan_out": true,
        "has_fan_in": false,
        "parallelization_level": "task_level",
        "max_parallel_width": 3,
        "branch_depth": 0
      },
      "processing": {
        "total_tasks": 4,
        "task_types": [
          "PythonOperator",
          "PythonOperator",
          "PythonOperator",
          "PythonOperator"
        ],
        "processing_stages": [
          "Data Loading",
          "Email Campaign",
          "SMS Campaign",
          "Push Notification Campaign"
        ],
        "etl_pattern": "batch",
        "data_flow_description": "Load customer segment data from CSV, then process three independent marketing channels in parallel using the same source data"
      },
      "external_services": {
        "primary_services": [
          "Email delivery system",
          "SMS delivery gateway",
          "Mobile push notification service"
        ],
        "apis_used": [],
        "databases": [],
        "service_integration_pattern": "parallel_api_fanout"
      },
      "infrastructure": {
        "connections": [],
        "file_paths": [
          "Customer segment CSV file"
        ],
        "external_systems": [
          "Local file system",
          "Email system",
          "SMS gateway",
          "Push notification service"
        ]
      },
      "scheduling": {
        "schedule_type": "daily",
        "has_catchup": false,
        "retry_policy": "2 retries with 5-minute delay"
      },
      "complexity": {
        "complexity_score": 3,
        "complexity_factors": [
          "Parallel execution",
          "Multiple external service integrations",
          "Independent channel processing"
        ],
        "use_case_fit": "Marketing campaign automation with parallel channel execution",
        "standard_complexity_score": 5
      },
      "failure_handling": {
        "retry_mechanism": "Basic retry with fixed delay",
        "alert_mechanism": "No email alerts configured",
        "transaction_safety": "Basic retry without transaction rollback"
      },
      "unique_features": [
        "Independent parallel marketing channels without synchronization",
        "Customer segmentation-based targeting",
        "Multi-channel campaign orchestration"
      ],
      "source_file": "synthetic-generator-v2__synthetic_fan_out_only_00_multi_channel_marketing_campaign.py_description.txt"
    },
    {
      "pipeline_name": "synthetic-gener.../synthetic_fan_out_only_01_data",
      "business_domain": "System Operations",
      "domain_category": "Data Replication",
      "primary_objective": "Daily database replication from production to multiple environments",
      "enrichment_objective": "None - pure data replication without transformation",
      "topology": {
        "pattern": "fan_out_fan_in",
        "description": "One initial task splits into three parallel tasks with no synchronization or merge point after parallel execution",
        "has_sensors": false,
        "has_branches": false,
        "has_fan_out": true,
        "has_fan_in": false,
        "parallelization_level": "task_level",
        "max_parallel_width": 3,
        "branch_depth": 0
      },
      "processing": {
        "total_tasks": 4,
        "task_types": [
          "BashOperator"
        ],
        "processing_stages": [
          "Extract",
          "Load"
        ],
        "etl_pattern": "batch",
        "data_flow_description": "Production database → CSV snapshot → parallel loading to Dev, Staging, QA environments"
      },
      "external_services": {
        "primary_services": [
          "Database systems"
        ],
        "apis_used": [],
        "databases": [
          "Production_DB",
          "Dev_DB",
          "Staging_DB",
          "QA_DB"
        ],
        "service_integration_pattern": "database_centric"
      },
      "infrastructure": {
        "connections": [],
        "file_paths": [
          "/tmp/prod_snapshot_$(date +%Y%m%d).csv"
        ],
        "external_systems": [
          "Production database",
          "Development database",
          "Staging database",
          "QA database"
        ]
      },
      "scheduling": {
        "schedule_type": "@daily",
        "has_catchup": false,
        "retry_policy": "2 retries with 5-minute delay"
      },
      "complexity": {
        "complexity_score": 3,
        "complexity_factors": [
          "Parallel execution",
          "Multiple target systems",
          "File-based data transfer"
        ],
        "use_case_fit": "Simple data replication across environments",
        "standard_complexity_score": 3
      },
      "failure_handling": {
        "retry_mechanism": "Basic retry with delay",
        "alert_mechanism": "None configured",
        "transaction_safety": "Basic file-based transfer with no rollback"
      },
      "unique_features": [
        "Fan-out only pattern without synchronization",
        "Date-based filename templating",
        "Parallel environment replication"
      ],
      "source_file": "synthetic-generator-v2__synthetic_fan_out_only_01_data_replication_to_environments.py_description.txt"
    },
    {
      "pipeline_name": "synthetic-gener.../synthetic_linear_01_report_gen",
      "business_domain": "Financial Services",
      "domain_category": "Sales Analytics",
      "primary_objective": "Generate and deliver daily sales reports to management",
      "enrichment_objective": "Transform raw sales data into formatted CSV and visual PDF chart for reporting",
      "topology": {
        "pattern": "linear_sequential",
        "description": "Strict four-stage sequential execution chain: query → transform_csv → generate_pdf → email",
        "has_sensors": false,
        "has_branches": false,
        "has_fan_out": false,
        "has_fan_in": false,
        "parallelization_level": "none",
        "max_parallel_width": 1,
        "branch_depth": 0
      },
      "processing": {
        "total_tasks": 4,
        "task_types": [
          "PostgresOperator",
          "PythonOperator",
          "PythonOperator",
          "EmailOperator"
        ],
        "processing_stages": [
          "Extract",
          "Transform",
          "Visualize",
          "Deliver"
        ],
        "etl_pattern": "extract_transform_load",
        "data_flow_description": "Extract sales data from PostgreSQL, transform to CSV, generate PDF chart, email both attachments"
      },
      "external_services": {
        "primary_services": [
          "PostgreSQL",
          "Email Service"
        ],
        "apis_used": [],
        "databases": [
          "PostgreSQL"
        ],
        "service_integration_pattern": "sequential_api_calls"
      },
      "infrastructure": {
        "connections": [
          "postgres_default"
        ],
        "file_paths": [
          "/tmp/sales_report.csv",
          "/tmp/sales_chart.pdf"
        ],
        "external_systems": [
          "PostgreSQL database",
          "Email system"
        ]
      },
      "scheduling": {
        "schedule_type": "daily",
        "has_catchup": false,
        "retry_policy": "2 retries with 5-minute delay"
      },
      "complexity": {
        "complexity_score": 3,
        "complexity_factors": [
          "Linear sequential pattern",
          "Multiple external dependencies (database, email)",
          "File-based data passing between stages",
          "Multiple transformation steps"
        ],
        "use_case_fit": "Well-suited for scheduled batch reporting with clear sequential stages",
        "standard_complexity_score": 3
      },
      "failure_handling": {
        "retry_mechanism": "Automatic retry with 2 attempts and 5-minute delays",
        "alert_mechanism": "Email notifications on task failures",
        "transaction_safety": "Basic retry mechanism with file-based intermediate storage"
      },
      "unique_features": [
        "Combined CSV and PDF report generation",
        "File-based data passing between PythonOperator tasks",
        "Date-parameterized SQL queries and email subjects",
        "Visual chart generation integrated into reporting pipeline"
      ],
      "source_file": "synthetic-generator-v2__synthetic_linear_01_report_generation.py_description.txt"
    },
    {
      "pipeline_name": "synthetic-gener.../synthetic_sensor_gated_00_upst",
      "business_domain": "Financial Services",
      "domain_category": "Sales Analytics",
      "primary_objective": "Generate executive dashboard with sales metrics and visualizations",
      "enrichment_objective": "Load and validate aggregated sales data from upstream DAG",
      "topology": {
        "pattern": "sensor_gated",
        "description": "Linear sequential pattern with initial sensor gate that blocks execution until upstream dependencies are satisfied",
        "has_sensors": true,
        "has_branches": false,
        "has_fan_out": false,
        "has_fan_in": false,
        "parallelization_level": "none",
        "max_parallel_width": 1,
        "branch_depth": 0
      },
      "processing": {
        "total_tasks": 3,
        "task_types": [
          "ExternalTaskSensor",
          "PythonOperator",
          "PythonOperator"
        ],
        "processing_stages": [
          "Sensor wait",
          "Data loading",
          "Dashboard generation"
        ],
        "etl_pattern": "batch",
        "data_flow_description": "Wait for external DAG completion → Load aggregated sales CSV → Generate executive dashboard"
      },
      "external_services": {
        "primary_services": [
          "External DAG monitoring"
        ],
        "apis_used": [],
        "databases": [],
        "service_integration_pattern": "sequential_api_calls"
      },
      "infrastructure": {
        "connections": [
          "External DAG dependency"
        ],
        "file_paths": [
          "CSV data files from daily_sales_aggregation DAG output"
        ],
        "external_systems": [
          "daily_sales_aggregation DAG"
        ]
      },
      "scheduling": {
        "schedule_type": "daily",
        "has_catchup": false,
        "retry_policy": "2 retries with 5-minute delay"
      },
      "complexity": {
        "complexity_score": 3,
        "complexity_factors": [
          "External DAG dependency",
          "Sensor-based gating",
          "Linear sequential flow"
        ],
        "use_case_fit": "Well-suited for dependent batch processing workflows with external dependencies",
        "standard_complexity_score": 3
      },
      "failure_handling": {
        "retry_mechanism": "Automatic retry with 2 attempts",
        "alert_mechanism": "Email notifications on task failures",
        "transaction_safety": "Basic retry with email alerts"
      },
      "unique_features": [
        "ExternalTaskSensor with reschedule mode",
        "Cross-DAG dependency management",
        "Executive dashboard generation"
      ],
      "source_file": "synthetic-generator-v2__synthetic_sensor_gated_00_upstream_etl_dependency.py_description.txt"
    },
    {
      "pipeline_name": "synthetic-gener.../synthetic_sensor_gated_01_file",
      "business_domain": "Financial Services",
      "domain_category": "Data Quality",
      "primary_objective": "Monitor for daily transaction file arrivals and load data to PostgreSQL",
      "enrichment_objective": "Validate incoming transaction file schema before loading",
      "topology": {
        "pattern": "sensor_gated",
        "description": "FileSensor gates entire processing workflow, ensuring files exist before validation and loading proceed in strict sequential order",
        "has_sensors": true,
        "has_branches": false,
        "has_fan_out": false,
        "has_fan_in": false,
        "parallelization_level": "none",
        "max_parallel_width": 1,
        "branch_depth": 0
      },
      "processing": {
        "total_tasks": 3,
        "task_types": [
          "FileSensor",
          "PythonOperator",
          "PythonOperator"
        ],
        "processing_stages": [
          "File Monitoring",
          "Schema Validation",
          "Database Loading"
        ],
        "etl_pattern": "batch",
        "data_flow_description": "File detection → Schema validation → Database loading in strict sequential order"
      },
      "external_services": {
        "primary_services": [
          "PostgreSQL"
        ],
        "apis_used": [],
        "databases": [
          "PostgreSQL"
        ],
        "service_integration_pattern": "database_centric"
      },
      "infrastructure": {
        "connections": [
          "PostgreSQL localhost:5432"
        ],
        "file_paths": [
          "/data/incoming/transactions_YYYYMMDD.csv"
        ],
        "external_systems": [
          "Local filesystem",
          "PostgreSQL database"
        ]
      },
      "scheduling": {
        "schedule_type": "@daily",
        "has_catchup": false,
        "retry_policy": "2 retries with 5-minute delays"
      },
      "complexity": {
        "complexity_score": 3,
        "complexity_factors": [
          "Sensor-gated execution",
          "External database integration",
          "File system monitoring"
        ],
        "use_case_fit": "Simple daily file processing with validation",
        "standard_complexity_score": 3
      },
      "failure_handling": {
        "retry_mechanism": "Task-level retries with exponential backoff",
        "alert_mechanism": "None (email alerts disabled)",
        "transaction_safety": "Basic retry mechanism without explicit transaction rollback"
      },
      "unique_features": [
        "FileSensor with 24-hour timeout and 30-second polling",
        "Python-based schema validation without external dependencies",
        "Daily file pattern matching with date substitution"
      ],
      "source_file": "synthetic-generator-v2__synthetic_sensor_gated_01_file_arrival_watcher.py_description.txt"
    },
    {
      "pipeline_name": "synthetic-gener.../synthetic_sensor_gated_02_data",
      "business_domain": "Data Warehousing",
      "domain_category": "ETL/ELT",
      "primary_objective": "Extract, transform, and load incremental orders data to a data warehouse",
      "enrichment_objective": "Clean and validate extracted orders data before loading",
      "topology": {
        "pattern": "sensor_gated",
        "description": "SqlSensor gates the entire ETL workflow, waiting for a daily database partition to exist before unlocking sequential extract, transform, and load tasks",
        "has_sensors": true,
        "has_branches": false,
        "has_fan_out": false,
        "has_fan_in": false,
        "parallelization_level": "none",
        "max_parallel_width": 1,
        "branch_depth": 0
      },
      "processing": {
        "total_tasks": 4,
        "task_types": [
          "SqlSensor",
          "PythonOperator",
          "PythonOperator",
          "PythonOperator"
        ],
        "processing_stages": [
          "Sensor Wait",
          "Extract",
          "Transform",
          "Load"
        ],
        "etl_pattern": "extract_transform_load",
        "data_flow_description": "SqlSensor checks for daily partition -> PythonOperator extracts incremental orders -> PythonOperator transforms/validates data -> PythonOperator loads to fact_orders table"
      },
      "external_services": {
        "primary_services": [
          "Database"
        ],
        "apis_used": [],
        "databases": [
          "Source database (orders table)",
          "Target data warehouse (fact_orders table)"
        ],
        "service_integration_pattern": "database_centric"
      },
      "infrastructure": {
        "connections": [
          "database_conn"
        ],
        "file_paths": [],
        "external_systems": [
          "Database with information_schema.partitions",
          "Data warehouse target system"
        ]
      },
      "scheduling": {
        "schedule_type": "batch",
        "has_catchup": false,
        "retry_policy": "2 retries with 5-minute delays"
      },
      "complexity": {
        "complexity_score": 3,
        "complexity_factors": [
          "Sensor-gated execution",
          "Database partition dependency",
          "Sequential ETL stages"
        ],
        "use_case_fit": "Daily incremental data loading with partition availability validation",
        "standard_complexity_score": 3
      },
      "failure_handling": {
        "retry_mechanism": "Operator-level retries (2 attempts)",
        "alert_mechanism": "None (email notifications disabled)",
        "transaction_safety": "Basic retry logic without explicit transaction management"
      },
      "unique_features": [
        "SqlSensor with reschedule mode to free worker slots",
        "Partition-based incremental data filtering",
        "Clear staged ETL with sensor gating"
      ],
      "source_file": "synthetic-generator-v2__synthetic_sensor_gated_02_database_partition_check.py_description.txt"
    },
    {
      "pipeline_name": "synthetic-gener.../synthetic_sensor_gated_03_ftp_",
      "business_domain": "Supply Chain",
      "domain_category": "Inventory Management",
      "primary_objective": "Monitor FTP server for vendor inventory files, download detected files, perform data cleansing, and merge with internal inventory systems",
      "enrichment_objective": "Data cleansing and merging with internal inventory to update stock levels and pricing",
      "topology": {
        "pattern": "sensor_gated",
        "description": "Custom FTP sensor gates all downstream processing tasks; strict linear sequential flow after sensor detection",
        "has_sensors": true,
        "has_branches": false,
        "has_fan_out": false,
        "has_fan_in": false,
        "parallelization_level": "none",
        "max_parallel_width": 1,
        "branch_depth": 0
      },
      "processing": {
        "total_tasks": 4,
        "task_types": [
          "FTPFileSensor",
          "PythonOperator",
          "PythonOperator",
          "PythonOperator"
        ],
        "processing_stages": [
          "File Detection",
          "File Download",
          "Data Cleansing",
          "Data Merge"
        ],
        "etl_pattern": "batch",
        "data_flow_description": "FTP file detection → download to local filesystem → cleanse data (remove nulls from product_id, quantity, price) → merge with internal inventory system"
      },
      "external_services": {
        "primary_services": [
          "FTP Server"
        ],
        "apis_used": [],
        "databases": [],
        "service_integration_pattern": "file_processing"
      },
      "infrastructure": {
        "connections": [
          "FTP server connection (implied)"
        ],
        "file_paths": [
          "/tmp/vendor_inventory.csv"
        ],
        "external_systems": [
          "FTP server",
          "Internal inventory system (implied)"
        ]
      },
      "scheduling": {
        "schedule_type": "@daily",
        "has_catchup": false,
        "retry_policy": "2 retries with 5-minute delay"
      },
      "complexity": {
        "complexity_score": 3,
        "complexity_factors": [
          "Sensor-gated execution",
          "External FTP dependency",
          "Sequential linear flow"
        ],
        "use_case_fit": "Well-suited for scheduled batch inventory updates from vendor FTP sources",
        "standard_complexity_score": 3
      },
      "failure_handling": {
        "retry_mechanism": "Task-level retries (2 attempts with 5-minute delay)",
        "alert_mechanism": "None (email notifications disabled)",
        "transaction_safety": "Basic retry without explicit rollback"
      },
      "unique_features": [
        "Custom FTPFileSensor with configurable polling (poke_interval=30s, timeout=300s)",
        "Sensor-gated sequential pattern",
        "File monitoring for specific vendor_inventory.csv"
      ],
      "source_file": "synthetic-generator-v2__synthetic_sensor_gated_03_ftp_drop_zone_monitor.py_description.txt"
    },
    {
      "pipeline_name": "synthetic-gener.../synthetic_staged_etl_00_health",
      "business_domain": "Healthcare",
      "domain_category": "Healthcare Claims Processing",
      "primary_objective": "Extract, transform, and load healthcare claims data for analytics",
      "enrichment_objective": "Join claims with provider data, anonymize PII, calculate risk scores, and refresh BI tools",
      "topology": {
        "pattern": "fan_out_fan_in",
        "description": "Clear three-stage ETL pattern with parallel extraction, sequential transformation, and parallel loading stages",
        "has_sensors": false,
        "has_branches": false,
        "has_fan_out": true,
        "has_fan_in": true,
        "parallelization_level": "task_level",
        "max_parallel_width": 2,
        "branch_depth": 0
      },
      "processing": {
        "total_tasks": 5,
        "task_types": [
          "PythonOperator"
        ],
        "processing_stages": [
          "Extract",
          "Transform",
          "Load"
        ],
        "etl_pattern": "extract_transform_load",
        "data_flow_description": "CSV extraction (parallel) → join/transform with PII anonymization → warehouse loading + BI refresh (parallel)"
      },
      "external_services": {
        "primary_services": [
          "Data Warehouse",
          "BI Tools"
        ],
        "apis_used": [],
        "databases": [
          "Postgres"
        ],
        "service_integration_pattern": "database_centric"
      },
      "infrastructure": {
        "connections": [],
        "file_paths": [
          "claims.csv",
          "providers.csv"
        ],
        "external_systems": [
          "Postgres",
          "Power BI",
          "Tableau"
        ]
      },
      "scheduling": {
        "schedule_type": "daily",
        "has_catchup": false,
        "retry_policy": "2 retries with 5-minute delay"
      },
      "complexity": {
        "complexity_score": 4,
        "complexity_factors": [
          "parallel execution",
          "multiple data sources",
          "PII handling",
          "BI tool integration"
        ],
        "use_case_fit": "Healthcare claims analytics with staged ETL processing",
        "standard_complexity_score": 5
      },
      "failure_handling": {
        "retry_mechanism": "Basic retry with fixed delay",
        "alert_mechanism": "None (email alerts disabled)",
        "transaction_safety": "Basic retry without explicit rollback"
      },
      "unique_features": [
        "TaskGroup organization for stages",
        "Parallel extraction and loading",
        "PII anonymization in transformation",
        "BI tool refresh integration"
      ],
      "source_file": "synthetic-generator-v2__synthetic_staged_etl_00_healthcare_claims_processing.py_description.txt"
    },
    {
      "pipeline_name": "synthetic-gener.../synthetic_staged_etl_01_supply",
      "business_domain": "Supply Chain",
      "domain_category": "Data Warehousing",
      "primary_objective": "Extract, transform, and load shipment data from multiple vendors into an inventory database",
      "enrichment_objective": "Normalize SKU formats, validate dates, filter invalid records, and enrich with location data",
      "topology": {
        "pattern": "fan_out_fan_in",
        "description": "Parallel extraction from three vendors (fan-out) followed by synchronized transformation and loading (fan-in)",
        "has_sensors": false,
        "has_branches": false,
        "has_fan_out": true,
        "has_fan_in": true,
        "parallelization_level": "task_level",
        "max_parallel_width": 3,
        "branch_depth": 0
      },
      "processing": {
        "total_tasks": 6,
        "task_types": [
          "PythonOperator",
          "EmailOperator"
        ],
        "processing_stages": [
          "Extract",
          "Transform",
          "Load"
        ],
        "etl_pattern": "extract_transform_load",
        "data_flow_description": "Raw vendor CSV files → Parallel extraction → Combined cleansing and normalization → Database load → Email notification"
      },
      "external_services": {
        "primary_services": [
          "File System",
          "PostgreSQL",
          "Email"
        ],
        "apis_used": [],
        "databases": [
          "PostgreSQL"
        ],
        "service_integration_pattern": "file_processing"
      },
      "infrastructure": {
        "connections": [
          "PostgreSQL inventory_db",
          "Email system"
        ],
        "file_paths": [
          "vendor_a_shipments_20240115.csv",
          "vendor_b_shipments_20240115.csv",
          "vendor_c_shipments_20240115.csv"
        ],
        "external_systems": [
          "Vendor A data source system",
          "Vendor B data source system",
          "Vendor C data source system"
        ]
      },
      "scheduling": {
        "schedule_type": "batch",
        "has_catchup": false,
        "retry_policy": "2 retries with 5-minute delay"
      },
      "complexity": {
        "complexity_score": 4,
        "complexity_factors": [
          "Parallel extraction from multiple sources",
          "Data transformation and enrichment",
          "Database integration",
          "Email notification"
        ],
        "use_case_fit": "Staged ETL with parallel ingestion and sequential processing",
        "standard_complexity_score": 5
      },
      "failure_handling": {
        "retry_mechanism": "Automatic retry with delay",
        "alert_mechanism": "Email on completion only (no failure alerts)",
        "transaction_safety": "Sequential dependency ensures failure stops downstream processing"
      },
      "unique_features": [
        "TaskGroup organization by ETL stages",
        "Heavy XCom usage for data passing between stages",
        "Parallel vendor data extraction",
        "Location data enrichment during transformation"
      ],
      "source_file": "synthetic-generator-v2__synthetic_staged_etl_01_supply_chain_etl.py_description.txt"
    },
    {
      "pipeline_name": "technoavengers-.../assignment_template.py",
      "business_domain": "Data Integration",
      "domain_category": "API Integration",
      "primary_objective": "Fetch user data from external API and store in PostgreSQL database",
      "enrichment_objective": "Transform raw API response into structured database records",
      "topology": {
        "pattern": "linear_sequential",
        "description": "Strict linear dependency chain: get_user → process_user → create_table → insert_data",
        "has_sensors": false,
        "has_branches": false,
        "has_fan_out": false,
        "has_fan_in": false,
        "parallelization_level": "none",
        "max_parallel_width": 1,
        "branch_depth": 0
      },
      "processing": {
        "total_tasks": 4,
        "task_types": [
          "SimpleHttpOperator",
          "PythonOperator",
          "PostgresOperator",
          "CustomPostgresOperator"
        ],
        "processing_stages": [
          "API Extraction",
          "Data Transformation",
          "Table Creation",
          "Data Insertion"
        ],
        "etl_pattern": "extract_transform_load",
        "data_flow_description": "Fetch JSON from API → Parse and transform → Create table → Insert transformed data"
      },
      "external_services": {
        "primary_services": [
          "HTTP API",
          "PostgreSQL"
        ],
        "apis_used": [
          "reqres.in API"
        ],
        "databases": [
          "PostgreSQL"
        ],
        "service_integration_pattern": "sequential_api_calls"
      },
      "infrastructure": {
        "connections": [
          "reqres",
          "postgres"
        ],
        "file_paths": [],
        "external_systems": [
          "HTTP API endpoint",
          "PostgreSQL database"
        ]
      },
      "scheduling": {
        "schedule_type": "batch",
        "has_catchup": true,
        "retry_policy": "standard_airflow_default"
      },
      "complexity": {
        "complexity_score": 3,
        "complexity_factors": [
          "XCom data passing",
          "Custom operator usage",
          "External API integration",
          "Database operations"
        ],
        "use_case_fit": "Simple data ingestion from API to database",
        "standard_complexity_score": 3
      },
      "failure_handling": {
        "retry_mechanism": "Standard Airflow task retry",
        "alert_mechanism": "None configured",
        "transaction_safety": "Basic task-level retry"
      },
      "unique_features": [
        "CustomPostgresOperator with enhanced templating",
        "XCom-based data passing between tasks",
        "Dynamic SQL generation via Airflow Variables"
      ],
      "source_file": "technoavengers-com__airflow-training-private__assignment_template.py_description.txt"
    }
  ],
  "metadata": {
    "total_files": 38,
    "successful_analyses": 38,
    "failed_analyses": 0
  }
}