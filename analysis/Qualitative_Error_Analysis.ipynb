{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00632242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 8,742 rows, 94 columns\n",
      "\n",
      "Columns available: ['Session', 'Run_Name', 'Pipeline_ID', 'Model_ID', 'Std_LLM', 'Reasoning_LLM', 'Workflow', 'Orchestrator', 'Strategy', 'Static_Score']...\n",
      "\n",
      "====================================================================================================\n",
      "METHOD CLASSIFICATION\n",
      "====================================================================================================\n",
      "\n",
      "Rows per Method:\n",
      "  Direct (Non-Reasoning)        :  2,394 rows\n",
      "  Prompt2DAG (Template)         :  1,578 rows\n",
      "  Prompt2DAG (LLM)              :  2,043 rows\n",
      "  Prompt2DAG (Hybrid)           :  2,043 rows\n",
      "  Direct (Reasoning)            :    684 rows\n",
      "\n",
      "====================================================================================================\n",
      "ISSUE COLUMN VERIFICATION\n",
      "====================================================================================================\n",
      "\n",
      "Issue columns verified:\n",
      "  Critical_Issues range: [0, 5]\n",
      "  Major_Issues range:    [0, 8]\n",
      "  Minor_Issues range:    [0, 10]\n",
      "  Total_Issues range:    [0, 17]\n",
      "\n",
      "====================================================================================================\n",
      "COMPUTING ORT SCORES\n",
      "====================================================================================================\n",
      "\n",
      "Penalty weights:\n",
      "  Critical issues: α = 2.0\n",
      "  Major issues:    β = 1.0\n",
      "  Minor issues:    γ = 0.25\n",
      "\n",
      "ORT Score Statistics:\n",
      "  ORT_raw range:    [-13.50, 7.69]\n",
      "  ORT_capped range: [0.00, 7.69]\n",
      "  ORT_scaled range: [0.00, 10.00]\n",
      "\n",
      "====================================================================================================\n",
      "CORRELATION CHECK: ORT_Score vs Combined_Score\n",
      "====================================================================================================\n",
      "\n",
      "All runs: r = 0.736\n",
      "Passed runs only: r = 0.247\n",
      "\n",
      "====================================================================================================\n",
      "TABLE Q1: ISSUE & ORT STATISTICS BY METHOD (ROW-LEVEL, ALL RUNS)\n",
      "====================================================================================================\n",
      "\n",
      "                Method  N_Total  N_Passed Pass_% Combined_All     ORT_All Total_Issues_All Critical_All   Major_All   Minor_All Frac_w_Critical_% Frac_Zero_Issues_%\n",
      "Direct (Non-Reasoning)     2394      1003   41.9  4.22 ± 2.38 5.78 ± 1.85      6.47 ± 2.94  0.53 ± 0.78 2.01 ± 1.67 3.93 ± 2.06              37.6                1.2\n",
      " Prompt2DAG (Template)     1578       795   50.4  4.06 ± 3.62 6.19 ± 1.63      5.16 ± 3.16  0.71 ± 0.83 2.21 ± 1.83 2.25 ± 1.83              50.4                3.0\n",
      "      Prompt2DAG (LLM)     2043      1437   70.3  5.38 ± 2.94 7.16 ± 1.73      5.36 ± 2.71  0.55 ± 0.77 1.44 ± 1.41 3.37 ± 2.01              39.8                1.0\n",
      "   Prompt2DAG (Hybrid)     2043      1614   79.0  5.29 ± 2.56 7.33 ± 1.43      5.09 ± 2.50  0.50 ± 0.74 1.36 ± 1.06 3.23 ± 2.07              37.2                1.1\n",
      "    Direct (Reasoning)      684       504   73.7  6.20 ± 1.75 7.04 ± 1.85      6.30 ± 2.25  0.42 ± 0.73 2.10 ± 1.48 3.78 ± 1.50              30.4                0.1\n",
      "\n",
      "====================================================================================================\n",
      "TABLE Q1b: ISSUE & ORT STATISTICS BY METHOD (ROW-LEVEL, PASSED RUNS ONLY)\n",
      "====================================================================================================\n",
      "\n",
      "                Method  N_Passed Combined_Passed  ORT_Passed Total_Issues_Passed Critical_Passed Major_Passed Minor_Passed Frac_w_Critical_% Frac_Zero_Issues_%\n",
      "Direct (Non-Reasoning)      1003     6.66 ± 0.47 7.53 ± 1.12         7.19 ± 2.35     0.40 ± 0.69  2.29 ± 1.62  4.50 ± 1.52              28.9                0.0\n",
      " Prompt2DAG (Template)       795     7.54 ± 0.53 7.51 ± 0.90         7.57 ± 1.75     0.39 ± 0.66  3.41 ± 1.13  3.78 ± 1.01              29.6                0.0\n",
      "      Prompt2DAG (LLM)      1437     7.20 ± 0.39 8.07 ± 1.06         6.21 ± 2.30     0.39 ± 0.67  1.82 ± 1.44  4.00 ± 1.47              29.1                0.0\n",
      "   Prompt2DAG (Hybrid)      1614     6.58 ± 0.34 7.91 ± 0.87         5.78 ± 2.14     0.37 ± 0.64  1.62 ± 0.99  3.79 ± 1.74              28.3                0.0\n",
      "    Direct (Reasoning)       504     7.22 ± 0.39 7.95 ± 1.03         6.36 ± 2.07     0.43 ± 0.72  2.04 ± 1.42  3.89 ± 1.44              31.0                0.0\n",
      "\n",
      "====================================================================================================\n",
      "PIPELINE × METHOD AGGREGATION\n",
      "====================================================================================================\n",
      "\n",
      "Aggregated to 190 Pipeline × Method combinations\n",
      "Unique pipelines: 38\n",
      "Unique methods: 5\n",
      "\n",
      "Sample:\n",
      "                                                                             Pipeline_ID                 Method  Combined_Score  Static_Score  Compliance_Score  ORT_Score  Total_Issues  Critical_Issues  Major_Issues  Minor_Issues  Pass_Rate\n",
      "6587093polakornming__RiskAlertPM25__airvisual_pipeline_by_lat_long_review.py_description Direct (Non-Reasoning)        4.392460      4.331984          4.372222   5.496004      7.968254         0.349206      3.095238      4.523810   0.460317\n",
      "6587093polakornming__RiskAlertPM25__airvisual_pipeline_by_lat_long_review.py_description     Direct (Reasoning)        6.115556      6.392500          5.773333   7.399192      5.277778         0.444444      1.444444      3.388889   0.722222\n",
      "6587093polakornming__RiskAlertPM25__airvisual_pipeline_by_lat_long_review.py_description    Prompt2DAG (Hybrid)        5.659365      5.257460          5.992698   7.625939      4.634921         0.460317      1.380952      2.793651   0.857143\n",
      "6587093polakornming__RiskAlertPM25__airvisual_pipeline_by_lat_long_review.py_description       Prompt2DAG (LLM)        5.561429      5.911270          5.201270   6.928695      6.000000         0.619048      1.746032      3.634921   0.714286\n",
      "6587093polakornming__RiskAlertPM25__airvisual_pipeline_by_lat_long_review.py_description  Prompt2DAG (Template)        4.895714      4.974603          4.762698   6.450707      5.666667         0.698413      2.587302      2.380952   0.619048\n",
      "    6587093polakornming__RiskAlertPM25__mahidol_aqi_report_pipeline_alert.py_description Direct (Non-Reasoning)        3.875238      3.601984          4.078333   5.289632      7.492063         0.523810      2.698413      4.269841   0.396825\n",
      "    6587093polakornming__RiskAlertPM25__mahidol_aqi_report_pipeline_alert.py_description     Direct (Reasoning)        6.139444      5.884444          6.374722   6.734046      7.000000         0.500000      2.444444      4.055556   0.722222\n",
      "    6587093polakornming__RiskAlertPM25__mahidol_aqi_report_pipeline_alert.py_description    Prompt2DAG (Hybrid)        5.033778      4.924889          5.114667   7.307325      4.777778         0.511111      1.022222      3.244444   0.733333\n",
      "    6587093polakornming__RiskAlertPM25__mahidol_aqi_report_pipeline_alert.py_description       Prompt2DAG (LLM)        4.930667      5.373333          4.418222   6.501599      5.822222         0.555556      2.066667      3.200000   0.600000\n",
      "    6587093polakornming__RiskAlertPM25__mahidol_aqi_report_pipeline_alert.py_description  Prompt2DAG (Template)        1.705556      1.846222          1.548889   5.427403      3.333333         1.066667      1.066667      1.200000   0.200000\n",
      "\n",
      "====================================================================================================\n",
      "COMPUTING PER-PIPELINE DELTAS (vs Direct Non-Reasoning)\n",
      "====================================================================================================\n",
      "\n",
      "Pivot shape: (38, 30)\n",
      "Columns: ['Combined_Score__Direct (Non-Reasoning)', 'Combined_Score__Direct (Reasoning)', 'Combined_Score__Prompt2DAG (Hybrid)', 'Combined_Score__Prompt2DAG (LLM)', 'Combined_Score__Prompt2DAG (Template)', 'Critical_Issues__Direct (Non-Reasoning)', 'Critical_Issues__Direct (Reasoning)', 'Critical_Issues__Prompt2DAG (Hybrid)', 'Critical_Issues__Prompt2DAG (LLM)', 'Critical_Issues__Prompt2DAG (Template)']...\n",
      "\n",
      "Delta dataframe shape: (38, 36)\n",
      "\n",
      "Sample (first 5 pipelines):\n",
      "                                                                                          Δ_Critical_Template  Δ_ORT_Template  Δ_Critical_LLM  Δ_ORT_LLM  Δ_Critical_Hybrid  Δ_ORT_Hybrid\n",
      "Pipeline_ID                                                                                                                                                                              \n",
      "6587093polakornming__RiskAlertPM25__airvisual_pipeline_by_lat_long_review.py_description                 0.35            0.95            0.27       1.43               0.11          2.13\n",
      "6587093polakornming__RiskAlertPM25__mahidol_aqi_report_pipeline_alert.py_description                     0.54            0.14            0.03       1.21              -0.01          2.02\n",
      "Environmental_Monitoring_Network                                                                        -0.05            1.45           -0.01       1.57               0.13          1.83\n",
      "Ferlab-Ste-Justine__clin-pipelines-dags__etl_import_ensembl.py_description                              -0.45            1.67           -0.45       2.06              -0.65          2.42\n",
      "Ferlab-Ste-Justine__clin-pipelines-dags__etl_import_mondo.py_description                                 0.48            0.83            0.06       1.92               0.05          1.97\n",
      "\n",
      "====================================================================================================\n",
      "TABLE Q3: PIPELINES WHERE P2D METHODS REDUCE ISSUES AND/OR IMPROVE ORT VS DIRECT\n",
      "====================================================================================================\n",
      "\n",
      "               Method  N_Pipelines  Fewer_Critical  More_Critical  Fewer_Total_Issues  More_Total_Issues  Better_ORT  Worse_ORT  Better_Combined  Fewer_Crit_AND_Better_ORT  Fewer_Total_AND_Better_ORT  Better_ORT_AND_Combined Avg_Δ_Critical Avg_Δ_Total_Issues Avg_Δ_ORT Avg_Δ_Combined\n",
      "Prompt2DAG (Template)           38               7             31                  31                  7          21         17               18                          7                          16                       18           0.22              -1.63      0.24          -0.61\n",
      "     Prompt2DAG (LLM)           38              12             23                  37                  1          37          1               36                         12                          36                       36           0.05              -1.18      1.30           1.00\n",
      "  Prompt2DAG (Hybrid)           38              13             23                  36                  2          37          1               36                         13                          35                       36          -0.00              -1.47      1.48           0.92\n",
      "   Direct (Reasoning)           38              25             13                  16                 22          38          0               37                         25                          16                       37          -0.10              -0.17      1.27           1.99\n",
      "\n",
      "====================================================================================================\n",
      "DETAILED BREAKDOWN BY METHOD\n",
      "====================================================================================================\n",
      "\n",
      "Prompt2DAG (Template):\n",
      "  Pipelines: 38\n",
      "  Critical Issues: 7 fewer, 31 more, 0 same\n",
      "  ORT Score:       21 better, 17 worse, 0 same\n",
      "  Win-Win:         7 pipelines (fewer critical + better ORT)\n",
      "  Avg Δ Critical:  0.22\n",
      "  Avg Δ ORT:       0.24\n",
      "\n",
      "Prompt2DAG (LLM):\n",
      "  Pipelines: 38\n",
      "  Critical Issues: 12 fewer, 23 more, 3 same\n",
      "  ORT Score:       37 better, 1 worse, 0 same\n",
      "  Win-Win:         12 pipelines (fewer critical + better ORT)\n",
      "  Avg Δ Critical:  0.05\n",
      "  Avg Δ ORT:       1.30\n",
      "\n",
      "Prompt2DAG (Hybrid):\n",
      "  Pipelines: 38\n",
      "  Critical Issues: 13 fewer, 23 more, 2 same\n",
      "  ORT Score:       37 better, 1 worse, 0 same\n",
      "  Win-Win:         13 pipelines (fewer critical + better ORT)\n",
      "  Avg Δ Critical:  -0.00\n",
      "  Avg Δ ORT:       1.48\n",
      "\n",
      "Direct (Reasoning):\n",
      "  Pipelines: 38\n",
      "  Critical Issues: 25 fewer, 13 more, 0 same\n",
      "  ORT Score:       38 better, 0 worse, 0 same\n",
      "  Win-Win:         25 pipelines (fewer critical + better ORT)\n",
      "  Avg Δ Critical:  -0.10\n",
      "  Avg Δ ORT:       1.27\n",
      "\n",
      "====================================================================================================\n",
      "LOADING PIPELINE METADATA\n",
      "====================================================================================================\n",
      "⚠️ Metadata JSON not found at pipeline_analysis_results/pipeline_analysis_complete.json\n",
      "Skipping topology-based analysis\n",
      "\n",
      "⚠️ Skipping topology-based analysis (no metadata available)\n",
      "\n",
      "====================================================================================================\n",
      "CORRELATION ANALYSIS: ISSUES vs SCORES\n",
      "====================================================================================================\n",
      "\n",
      "--- All Runs ---\n",
      "                         Combined       Static   Compliance          ORT\n",
      "---------------------------------------------------------------------------\n",
      "        Total_Issues       +0.536       +0.546       +0.489       -0.094\n",
      "     Critical_Issues       -0.342       -0.360       -0.312       -0.555\n",
      "        Major_Issues       +0.384       +0.407       +0.337       -0.153\n",
      "        Minor_Issues       +0.583       +0.586       +0.542       +0.194\n",
      "\n",
      "--- Passed Runs Only ---\n",
      "                         Combined       Static   Compliance          ORT\n",
      "---------------------------------------------------------------------------\n",
      "        Total_Issues       -0.057       -0.043       -0.104       -0.819\n",
      "     Critical_Issues       +0.019       -0.008       -0.009       -0.681\n",
      "        Major_Issues       +0.025       +0.172       -0.146       -0.708\n",
      "        Minor_Issues       -0.116       -0.222       -0.013       -0.250\n",
      "\n",
      "====================================================================================================\n",
      "ANOMALY CHECK: HIGH SCORES WITH HIGH CRITICAL ISSUES\n",
      "====================================================================================================\n",
      "\n",
      "Rows with Combined_Score ≥ 7.0 AND Critical_Issues > 1: 244\n",
      "This is 2.79% of all rows\n",
      "\n",
      "Breakdown by Method:\n",
      "  Direct (Non-Reasoning)        :    27 /  2394 (1.1%)\n",
      "  Prompt2DAG (Template)         :    66 /  1578 (4.2%)\n",
      "  Prompt2DAG (LLM)              :    92 /  2043 (4.5%)\n",
      "  Prompt2DAG (Hybrid)           :    20 /  2043 (1.0%)\n",
      "  Direct (Reasoning)            :    39 /   684 (5.7%)\n",
      "\n",
      "Sample of anomalous rows:\n",
      "                     Method                                                                                         Pipeline_ID  Combined_Score  ORT_Score  Critical_Issues  Major_Issues  Minor_Issues  Passed\n",
      "68   Direct (Non-Reasoning)             synthetic-generator-v2__synthetic_branch_merge_01_fraud_detection_triage.py_description            7.38   5.842378                2             4             2    True\n",
      "104  Direct (Non-Reasoning)                  synthetic-generator-v2__synthetic_branch_merge_00_data_quality_gate.py_description            7.32   6.050024                2             3             4    True\n",
      "112  Direct (Non-Reasoning)                                                   peterbull__bodhi-cast__log_cleanup.py_description            7.31   6.517225                2             2             4    True\n",
      "117  Direct (Non-Reasoning)                                                                      Medical_Facility_Accessibility            7.30   5.804625                2             3             6    True\n",
      "119  Direct (Non-Reasoning)                                 sivajik34__magento-airflow__magento_customer_graphql.py_description            7.30   4.860783                2             5             6    True\n",
      "172  Direct (Non-Reasoning)        synthetic-generator-v2__synthetic_branch_merge_05_content_moderation_pipeline.py_description            7.25   6.488910                2             2             4    True\n",
      "200  Direct (Non-Reasoning)  synthetic-generator-v2__synthetic_fan_out_fan_in_06_financial_portfolio_rebalancing.py_description            7.22   7.418594                2             0             4    True\n",
      "201  Direct (Non-Reasoning)                                 sivajik34__magento-airflow__magento_customer_graphql.py_description            7.22   4.705050                2             5             7    True\n",
      "206  Direct (Non-Reasoning)                    technoavengers-com__airflow-training-private__assignment_template.py_description            7.22   6.356772                2             2             5    True\n",
      "215  Direct (Non-Reasoning)                                                   peterbull__bodhi-cast__log_cleanup.py_description            7.21   6.823974                2             1             5    True\n",
      "\n",
      "====================================================================================================\n",
      "FINAL SUMMARY: PROMPT2DAG vs DIRECT\n",
      "====================================================================================================\n",
      "\n",
      "--- Best Performing Method per Metric (All Runs) ---\n",
      "\n",
      "Pass Rate:\n",
      "  Best:  Prompt2DAG (Hybrid)            0.79\n",
      "  Worst: Direct (Non-Reasoning)         0.42\n",
      "  Gap:   0.37\n",
      "\n",
      "Combined Score:\n",
      "  Best:  Direct (Reasoning)             6.20\n",
      "  Worst: Prompt2DAG (Template)          4.06\n",
      "  Gap:   2.14\n",
      "\n",
      "ORT Score:\n",
      "  Best:  Prompt2DAG (Hybrid)            7.33\n",
      "  Worst: Direct (Non-Reasoning)         5.78\n",
      "  Gap:   1.55\n",
      "\n",
      "Fewest Total Issues:\n",
      "  Best:  Prompt2DAG (Hybrid)            5.09\n",
      "  Worst: Direct (Non-Reasoning)         6.47\n",
      "  Gap:   1.38\n",
      "\n",
      "Fewest Critical Issues:\n",
      "  Best:  Direct (Reasoning)             0.42\n",
      "  Worst: Prompt2DAG (Template)          0.71\n",
      "  Gap:   0.29\n",
      "\n",
      "--- Win Rate: P2D Methods vs Direct (Pipeline-Level) ---\n",
      "\n",
      "Prompt2DAG (Template):\n",
      "  Wins:   21 / 38 (55.3%)\n",
      "  Losses: 17 / 38 (44.7%)\n",
      "  Ties:   0 / 38 (0.0%)\n",
      "\n",
      "Prompt2DAG (LLM):\n",
      "  Wins:   37 / 38 (97.4%)\n",
      "  Losses: 1 / 38 (2.6%)\n",
      "  Ties:   0 / 38 (0.0%)\n",
      "\n",
      "Prompt2DAG (Hybrid):\n",
      "  Wins:   37 / 38 (97.4%)\n",
      "  Losses: 1 / 38 (2.6%)\n",
      "  Ties:   0 / 38 (0.0%)\n",
      "\n",
      "====================================================================================================\n",
      "QUALITATIVE ANALYSIS COMPLETE\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # Qualitative Error Analysis: Issues & ORT by Method, Pipeline, and Topology\n",
    "#\n",
    "# Methods:\n",
    "# 1. Direct (Non-Reasoning)\n",
    "# 2. Direct (Reasoning)\n",
    "# 3. Prompt2DAG (Template)\n",
    "# 4. Prompt2DAG (LLM)\n",
    "# 5. Prompt2DAG (Hybrid)\n",
    "#\n",
    "# We:\n",
    "# - Compute ORT_Score (issue-adjusted robustness) with proper normalization\n",
    "# - Summarize issue statistics (Total, Critical, Major, Minor) and ORT by method\n",
    "# - Aggregate per pipeline and compare Prompt2DAG vs Direct\n",
    "# - Compute Δ issues and Δ ORT vs Direct per pipeline\n",
    "# - Group improvements by topology (with df_meta)\n",
    "# - Investigate correlation patterns and anomalies\n",
    "\n",
    "# %%\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "csv_path = \"/Users/abubakarialidu/Desktop/Data Result/all_sessions_cleaned.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "print(f\"Loaded {len(df):,} rows, {len(df.columns)} columns\")\n",
    "print(f\"\\nColumns available: {df.columns.tolist()[:10]}...\")\n",
    "df.head(3)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 0. Classify methods and compute ORT_Score\n",
    "\n",
    "# %%\n",
    "def classify_method(row):\n",
    "    workflow = row.get(\"Workflow\", \"\")\n",
    "    strategy = str(row.get(\"Strategy\") or \"\").lower()\n",
    "    if workflow == \"Direct\":\n",
    "        return \"Direct (Non-Reasoning)\"\n",
    "    elif workflow == \"Reasoning\":\n",
    "        return \"Direct (Reasoning)\"\n",
    "    elif workflow == \"Prompt2DAG\":\n",
    "        if \"template\" in strategy:\n",
    "            return \"Prompt2DAG (Template)\"\n",
    "        elif \"llm\" in strategy:\n",
    "            return \"Prompt2DAG (LLM)\"\n",
    "        elif \"hybrid\" in strategy:\n",
    "            return \"Prompt2DAG (Hybrid)\"\n",
    "        else:\n",
    "            return f\"Prompt2DAG ({row.get('Strategy')})\"\n",
    "    else:\n",
    "        return workflow\n",
    "\n",
    "df[\"Method\"] = df.apply(classify_method, axis=1)\n",
    "\n",
    "METHOD_ORDER = [\n",
    "    \"Direct (Non-Reasoning)\",\n",
    "    \"Prompt2DAG (Template)\",\n",
    "    \"Prompt2DAG (LLM)\",\n",
    "    \"Prompt2DAG (Hybrid)\",\n",
    "    \"Direct (Reasoning)\",\n",
    "]\n",
    "\n",
    "df_methods = df[df[\"Method\"].isin(METHOD_ORDER)].copy()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"METHOD CLASSIFICATION\")\n",
    "print(\"=\" * 100)\n",
    "print(\"\\nRows per Method:\")\n",
    "method_counts = df_methods[\"Method\"].value_counts().reindex(METHOD_ORDER)\n",
    "for method, count in method_counts.items():\n",
    "    print(f\"  {method:<30}: {count:>6,} rows\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Ensure issue columns exist and compute ORT\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"ISSUE COLUMN VERIFICATION\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Ensure issue columns exist\n",
    "for col in [\"Critical_Issues\", \"Major_Issues\", \"Minor_Issues\"]:\n",
    "    if col not in df_methods.columns:\n",
    "        df_methods[col] = 0\n",
    "    df_methods[col] = df_methods[col].fillna(0)\n",
    "\n",
    "# Recalculate Total_Issues\n",
    "df_methods[\"Total_Issues\"] = (\n",
    "    df_methods[\"Critical_Issues\"] + \n",
    "    df_methods[\"Major_Issues\"] + \n",
    "    df_methods[\"Minor_Issues\"]\n",
    ")\n",
    "\n",
    "print(f\"\\nIssue columns verified:\")\n",
    "print(f\"  Critical_Issues range: [{df_methods['Critical_Issues'].min():.0f}, {df_methods['Critical_Issues'].max():.0f}]\")\n",
    "print(f\"  Major_Issues range:    [{df_methods['Major_Issues'].min():.0f}, {df_methods['Major_Issues'].max():.0f}]\")\n",
    "print(f\"  Minor_Issues range:    [{df_methods['Minor_Issues'].min():.0f}, {df_methods['Minor_Issues'].max():.0f}]\")\n",
    "print(f\"  Total_Issues range:    [{df_methods['Total_Issues'].min():.0f}, {df_methods['Total_Issues'].max():.0f}]\")\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"COMPUTING ORT SCORES\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Penalty weights\n",
    "ALPHA_CRIT = 2.0\n",
    "BETA_MAJOR = 1.0\n",
    "GAMMA_MINOR = 0.25\n",
    "\n",
    "print(f\"\\nPenalty weights:\")\n",
    "print(f\"  Critical issues: α = {ALPHA_CRIT}\")\n",
    "print(f\"  Major issues:    β = {BETA_MAJOR}\")\n",
    "print(f\"  Minor issues:    γ = {GAMMA_MINOR}\")\n",
    "\n",
    "# Base score: Combined_Score if Passed, else 0\n",
    "df_methods[\"Base_Score\"] = np.where(\n",
    "    df_methods[\"Passed\"] == True, \n",
    "    df_methods[\"Combined_Score\"], \n",
    "    0.0\n",
    ")\n",
    "\n",
    "# Calculate penalty\n",
    "df_methods[\"Penalty\"] = (\n",
    "    ALPHA_CRIT * df_methods[\"Critical_Issues\"] +\n",
    "    BETA_MAJOR * df_methods[\"Major_Issues\"] +\n",
    "    GAMMA_MINOR * df_methods[\"Minor_Issues\"]\n",
    ")\n",
    "\n",
    "# ORT_Score_raw (can be negative)\n",
    "df_methods[\"ORT_Score_raw\"] = df_methods[\"Base_Score\"] - df_methods[\"Penalty\"]\n",
    "\n",
    "# ORT_Score_capped (clamped to [0, 10])\n",
    "df_methods[\"ORT_Score_capped\"] = df_methods[\"ORT_Score_raw\"].clip(lower=0.0, upper=10.0)\n",
    "\n",
    "# ORT_Score_scaled (min-max normalization to [0, 10])\n",
    "ort_min = df_methods[\"ORT_Score_raw\"].min()\n",
    "ort_max = df_methods[\"ORT_Score_raw\"].max()\n",
    "\n",
    "if ort_max > ort_min:\n",
    "    df_methods[\"ORT_Score_scaled\"] = 10 * (df_methods[\"ORT_Score_raw\"] - ort_min) / (ort_max - ort_min)\n",
    "else:\n",
    "    df_methods[\"ORT_Score_scaled\"] = 0.0\n",
    "\n",
    "# Use ORT_Score_scaled as the default ORT_Score\n",
    "df_methods[\"ORT_Score\"] = df_methods[\"ORT_Score_scaled\"]\n",
    "\n",
    "print(f\"\\nORT Score Statistics:\")\n",
    "print(f\"  ORT_raw range:    [{df_methods['ORT_Score_raw'].min():.2f}, {df_methods['ORT_Score_raw'].max():.2f}]\")\n",
    "print(f\"  ORT_capped range: [{df_methods['ORT_Score_capped'].min():.2f}, {df_methods['ORT_Score_capped'].max():.2f}]\")\n",
    "print(f\"  ORT_scaled range: [{df_methods['ORT_Score_scaled'].min():.2f}, {df_methods['ORT_Score_scaled'].max():.2f}]\")\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"CORRELATION CHECK: ORT_Score vs Combined_Score\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "corr_all = df_methods[[\"Combined_Score\", \"ORT_Score\"]].corr().iloc[0, 1]\n",
    "print(f\"\\nAll runs: r = {corr_all:.3f}\")\n",
    "\n",
    "df_passed = df_methods[df_methods[\"Passed\"] == True]\n",
    "if len(df_passed) > 0:\n",
    "    corr_passed = df_passed[[\"Combined_Score\", \"ORT_Score\"]].corr().iloc[0, 1]\n",
    "    print(f\"Passed runs only: r = {corr_passed:.3f}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 1. Global Issue & ORT Statistics by Method (Row-Level)\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"TABLE Q1: ISSUE & ORT STATISTICS BY METHOD (ROW-LEVEL, ALL RUNS)\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "records = []\n",
    "for method in METHOD_ORDER:\n",
    "    df_m = df_methods[df_methods[\"Method\"] == method]\n",
    "    if len(df_m) == 0:\n",
    "        continue\n",
    "    \n",
    "    df_m_passed = df_m[df_m[\"Passed\"] == True]\n",
    "    \n",
    "    row = {\n",
    "        \"Method\": method,\n",
    "        \"N_Total\": len(df_m),\n",
    "        \"N_Passed\": len(df_m_passed),\n",
    "        \"Pass_%\": f\"{df_m['Passed'].mean() * 100:.1f}\",\n",
    "        \"Combined_All\": f\"{df_m['Combined_Score'].mean():.2f} ± {df_m['Combined_Score'].std():.2f}\",\n",
    "        \"ORT_All\": f\"{df_m['ORT_Score'].mean():.2f} ± {df_m['ORT_Score'].std():.2f}\",\n",
    "        \"Total_Issues_All\": f\"{df_m['Total_Issues'].mean():.2f} ± {df_m['Total_Issues'].std():.2f}\",\n",
    "        \"Critical_All\": f\"{df_m['Critical_Issues'].mean():.2f} ± {df_m['Critical_Issues'].std():.2f}\",\n",
    "        \"Major_All\": f\"{df_m['Major_Issues'].mean():.2f} ± {df_m['Major_Issues'].std():.2f}\",\n",
    "        \"Minor_All\": f\"{df_m['Minor_Issues'].mean():.2f} ± {df_m['Minor_Issues'].std():.2f}\",\n",
    "        \"Frac_w_Critical_%\": f\"{(df_m['Critical_Issues'] > 0).mean() * 100:.1f}\",\n",
    "        \"Frac_Zero_Issues_%\": f\"{(df_m['Total_Issues'] == 0).mean() * 100:.1f}\",\n",
    "    }\n",
    "    \n",
    "    records.append(row)\n",
    "\n",
    "q1_df = pd.DataFrame(records)\n",
    "print(\"\\n\" + q1_df.to_string(index=False))\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"TABLE Q1b: ISSUE & ORT STATISTICS BY METHOD (ROW-LEVEL, PASSED RUNS ONLY)\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "records_passed = []\n",
    "for method in METHOD_ORDER:\n",
    "    df_m = df_methods[(df_methods[\"Method\"] == method) & (df_methods[\"Passed\"] == True)]\n",
    "    if len(df_m) == 0:\n",
    "        continue\n",
    "    \n",
    "    row = {\n",
    "        \"Method\": method,\n",
    "        \"N_Passed\": len(df_m),\n",
    "        \"Combined_Passed\": f\"{df_m['Combined_Score'].mean():.2f} ± {df_m['Combined_Score'].std():.2f}\",\n",
    "        \"ORT_Passed\": f\"{df_m['ORT_Score'].mean():.2f} ± {df_m['ORT_Score'].std():.2f}\",\n",
    "        \"Total_Issues_Passed\": f\"{df_m['Total_Issues'].mean():.2f} ± {df_m['Total_Issues'].std():.2f}\",\n",
    "        \"Critical_Passed\": f\"{df_m['Critical_Issues'].mean():.2f} ± {df_m['Critical_Issues'].std():.2f}\",\n",
    "        \"Major_Passed\": f\"{df_m['Major_Issues'].mean():.2f} ± {df_m['Major_Issues'].std():.2f}\",\n",
    "        \"Minor_Passed\": f\"{df_m['Minor_Issues'].mean():.2f} ± {df_m['Minor_Issues'].std():.2f}\",\n",
    "        \"Frac_w_Critical_%\": f\"{(df_m['Critical_Issues'] > 0).mean() * 100:.1f}\",\n",
    "        \"Frac_Zero_Issues_%\": f\"{(df_m['Total_Issues'] == 0).mean() * 100:.1f}\",\n",
    "    }\n",
    "    \n",
    "    records_passed.append(row)\n",
    "\n",
    "q1b_df = pd.DataFrame(records_passed)\n",
    "print(\"\\n\" + q1b_df.to_string(index=False))\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 2. Pipeline × Method: Average Issues & ORT per Pipeline\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"PIPELINE × METHOD AGGREGATION\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "agg_cols = {\n",
    "    \"Combined_Score\": \"mean\",\n",
    "    \"Static_Score\": \"mean\",\n",
    "    \"Compliance_Score\": \"mean\",\n",
    "    \"ORT_Score\": \"mean\",\n",
    "    \"Total_Issues\": \"mean\",\n",
    "    \"Critical_Issues\": \"mean\",\n",
    "    \"Major_Issues\": \"mean\",\n",
    "    \"Minor_Issues\": \"mean\",\n",
    "    \"Passed\": \"mean\",  # Pass rate per pipeline-method\n",
    "}\n",
    "\n",
    "pipe_method_agg = (\n",
    "    df_methods\n",
    "    .groupby([\"Pipeline_ID\", \"Method\"])\n",
    "    .agg(agg_cols)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "pipe_method_agg.rename(columns={\"Passed\": \"Pass_Rate\"}, inplace=True)\n",
    "\n",
    "print(f\"\\nAggregated to {len(pipe_method_agg)} Pipeline × Method combinations\")\n",
    "print(f\"Unique pipelines: {pipe_method_agg['Pipeline_ID'].nunique()}\")\n",
    "print(f\"Unique methods: {pipe_method_agg['Method'].nunique()}\")\n",
    "\n",
    "print(\"\\nSample:\")\n",
    "print(pipe_method_agg.head(10).to_string(index=False))\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3. Per-Pipeline Δ in Issues & ORT: Direct vs Prompt2DAG Methods\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"COMPUTING PER-PIPELINE DELTAS (vs Direct Non-Reasoning)\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Pivot to get one row per pipeline, columns per method for issues and ORT\n",
    "pivot = pipe_method_agg.pivot_table(\n",
    "    index=\"Pipeline_ID\",\n",
    "    columns=\"Method\",\n",
    "    values=[\"Total_Issues\", \"Critical_Issues\", \"Major_Issues\", \"Minor_Issues\", \"ORT_Score\", \"Combined_Score\"]\n",
    ")\n",
    "\n",
    "# Flatten multiindex columns\n",
    "pivot.columns = [f\"{metric}__{method}\" for metric, method in pivot.columns]\n",
    "\n",
    "print(f\"\\nPivot shape: {pivot.shape}\")\n",
    "print(f\"Columns: {pivot.columns.tolist()[:10]}...\")\n",
    "\n",
    "# %%\n",
    "baseline = \"Direct (Non-Reasoning)\"\n",
    "\n",
    "delta_records = []\n",
    "for pipeline_id, row in pivot.iterrows():\n",
    "    rec = {\"Pipeline_ID\": pipeline_id}\n",
    "    \n",
    "    # Get baseline values\n",
    "    base_total = row.get(f\"Total_Issues__{baseline}\", np.nan)\n",
    "    base_crit = row.get(f\"Critical_Issues__{baseline}\", np.nan)\n",
    "    base_major = row.get(f\"Major_Issues__{baseline}\", np.nan)\n",
    "    base_minor = row.get(f\"Minor_Issues__{baseline}\", np.nan)\n",
    "    base_ort = row.get(f\"ORT_Score__{baseline}\", np.nan)\n",
    "    base_combined = row.get(f\"Combined_Score__{baseline}\", np.nan)\n",
    "    \n",
    "    # Store baseline values\n",
    "    rec[\"Base_Total_Issues\"] = base_total\n",
    "    rec[\"Base_Critical\"] = base_crit\n",
    "    rec[\"Base_ORT\"] = base_ort\n",
    "    rec[\"Base_Combined\"] = base_combined\n",
    "    \n",
    "    # Calculate deltas for each P2D method\n",
    "    for method in [\"Prompt2DAG (Template)\", \"Prompt2DAG (LLM)\", \"Prompt2DAG (Hybrid)\", \"Direct (Reasoning)\"]:\n",
    "        method_short = method.replace(\"Prompt2DAG \", \"\").replace(\"(\", \"\").replace(\")\", \"\").replace(\" \", \"_\")\n",
    "        \n",
    "        total_col = f\"Total_Issues__{method}\"\n",
    "        crit_col = f\"Critical_Issues__{method}\"\n",
    "        major_col = f\"Major_Issues__{method}\"\n",
    "        minor_col = f\"Minor_Issues__{method}\"\n",
    "        ort_col = f\"ORT_Score__{method}\"\n",
    "        combined_col = f\"Combined_Score__{method}\"\n",
    "        \n",
    "        if total_col in row and crit_col in row and ort_col in row:\n",
    "            rec[f\"Δ_Total_Issues_{method_short}\"] = row[total_col] - base_total\n",
    "            rec[f\"Δ_Critical_{method_short}\"] = row[crit_col] - base_crit\n",
    "            rec[f\"Δ_Major_{method_short}\"] = row[major_col] - base_major\n",
    "            rec[f\"Δ_Minor_{method_short}\"] = row[minor_col] - base_minor\n",
    "            rec[f\"Δ_ORT_{method_short}\"] = row[ort_col] - base_ort\n",
    "            rec[f\"Δ_Combined_{method_short}\"] = row[combined_col] - base_combined\n",
    "            \n",
    "            # Store absolute values too\n",
    "            rec[f\"{method_short}_ORT\"] = row[ort_col]\n",
    "            rec[f\"{method_short}_Critical\"] = row[crit_col]\n",
    "    \n",
    "    delta_records.append(rec)\n",
    "\n",
    "df_delta = pd.DataFrame(delta_records).set_index(\"Pipeline_ID\")\n",
    "\n",
    "print(f\"\\nDelta dataframe shape: {df_delta.shape}\")\n",
    "print(\"\\nSample (first 5 pipelines):\")\n",
    "display_cols = [c for c in df_delta.columns if c.startswith(\"Δ_ORT\") or c.startswith(\"Δ_Critical\")][:6]\n",
    "print(df_delta[display_cols].head(5).round(2).to_string())\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3.1 Counts: Pipelines where P2D reduces issues and improves ORT vs Direct\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"TABLE Q3: PIPELINES WHERE P2D METHODS REDUCE ISSUES AND/OR IMPROVE ORT VS DIRECT\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "comparison_results = []\n",
    "\n",
    "for method in [\"Prompt2DAG (Template)\", \"Prompt2DAG (LLM)\", \"Prompt2DAG (Hybrid)\", \"Direct (Reasoning)\"]:\n",
    "    method_short = method.replace(\"Prompt2DAG \", \"\").replace(\"(\", \"\").replace(\")\", \"\").replace(\" \", \"_\")\n",
    "    \n",
    "    col_crit = f\"Δ_Critical_{method_short}\"\n",
    "    col_total = f\"Δ_Total_Issues_{method_short}\"\n",
    "    col_major = f\"Δ_Major_{method_short}\"\n",
    "    col_minor = f\"Δ_Minor_{method_short}\"\n",
    "    col_ort = f\"Δ_ORT_{method_short}\"\n",
    "    col_combined = f\"Δ_Combined_{method_short}\"\n",
    "    \n",
    "    if col_crit not in df_delta.columns or col_ort not in df_delta.columns:\n",
    "        continue\n",
    "    \n",
    "    df_m = df_delta.dropna(subset=[col_crit, col_ort])\n",
    "    n_pipelines = len(df_m)\n",
    "    \n",
    "    # Critical issues\n",
    "    fewer_crit = (df_m[col_crit] < 0).sum()\n",
    "    more_crit = (df_m[col_crit] > 0).sum()\n",
    "    same_crit = (df_m[col_crit] == 0).sum()\n",
    "    \n",
    "    # Total issues\n",
    "    fewer_total = (df_m[col_total] < 0).sum()\n",
    "    more_total = (df_m[col_total] > 0).sum()\n",
    "    same_total = (df_m[col_total] == 0).sum()\n",
    "    \n",
    "    # ORT\n",
    "    better_ort = (df_m[col_ort] > 0).sum()\n",
    "    worse_ort = (df_m[col_ort] < 0).sum()\n",
    "    same_ort = (df_m[col_ort] == 0).sum()\n",
    "    \n",
    "    # Combined Score\n",
    "    better_combined = (df_m[col_combined] > 0).sum()\n",
    "    worse_combined = (df_m[col_combined] < 0).sum()\n",
    "    \n",
    "    # Win conditions\n",
    "    fewer_crit_and_better_ort = ((df_m[col_crit] < 0) & (df_m[col_ort] > 0)).sum()\n",
    "    fewer_total_and_better_ort = ((df_m[col_total] < 0) & (df_m[col_ort] > 0)).sum()\n",
    "    better_ort_and_combined = ((df_m[col_ort] > 0) & (df_m[col_combined] > 0)).sum()\n",
    "    \n",
    "    # Calculate averages\n",
    "    avg_delta_crit = df_m[col_crit].mean()\n",
    "    avg_delta_total = df_m[col_total].mean()\n",
    "    avg_delta_ort = df_m[col_ort].mean()\n",
    "    avg_delta_combined = df_m[col_combined].mean()\n",
    "    \n",
    "    comparison_results.append({\n",
    "        \"Method\": method,\n",
    "        \"N_Pipelines\": n_pipelines,\n",
    "        \"Fewer_Critical\": fewer_crit,\n",
    "        \"More_Critical\": more_crit,\n",
    "        \"Fewer_Total_Issues\": fewer_total,\n",
    "        \"More_Total_Issues\": more_total,\n",
    "        \"Better_ORT\": better_ort,\n",
    "        \"Worse_ORT\": worse_ort,\n",
    "        \"Better_Combined\": better_combined,\n",
    "        \"Fewer_Crit_AND_Better_ORT\": fewer_crit_and_better_ort,\n",
    "        \"Fewer_Total_AND_Better_ORT\": fewer_total_and_better_ort,\n",
    "        \"Better_ORT_AND_Combined\": better_ort_and_combined,\n",
    "        \"Avg_Δ_Critical\": f\"{avg_delta_crit:.2f}\",\n",
    "        \"Avg_Δ_Total_Issues\": f\"{avg_delta_total:.2f}\",\n",
    "        \"Avg_Δ_ORT\": f\"{avg_delta_ort:.2f}\",\n",
    "        \"Avg_Δ_Combined\": f\"{avg_delta_combined:.2f}\",\n",
    "    })\n",
    "\n",
    "q3_df = pd.DataFrame(comparison_results)\n",
    "print(\"\\n\" + q3_df.to_string(index=False))\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"DETAILED BREAKDOWN BY METHOD\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "for method in [\"Prompt2DAG (Template)\", \"Prompt2DAG (LLM)\", \"Prompt2DAG (Hybrid)\", \"Direct (Reasoning)\"]:\n",
    "    method_short = method.replace(\"Prompt2DAG \", \"\").replace(\"(\", \"\").replace(\")\", \"\").replace(\" \", \"_\")\n",
    "    \n",
    "    col_crit = f\"Δ_Critical_{method_short}\"\n",
    "    col_ort = f\"Δ_ORT_{method_short}\"\n",
    "    \n",
    "    if col_crit not in df_delta.columns or col_ort not in df_delta.columns:\n",
    "        continue\n",
    "    \n",
    "    df_m = df_delta.dropna(subset=[col_crit, col_ort])\n",
    "    \n",
    "    print(f\"\\n{method}:\")\n",
    "    print(f\"  Pipelines: {len(df_m)}\")\n",
    "    print(f\"  Critical Issues: {(df_m[col_crit] < 0).sum()} fewer, {(df_m[col_crit] > 0).sum()} more, {(df_m[col_crit] == 0).sum()} same\")\n",
    "    print(f\"  ORT Score:       {(df_m[col_ort] > 0).sum()} better, {(df_m[col_ort] < 0).sum()} worse, {(df_m[col_ort] == 0).sum()} same\")\n",
    "    print(f\"  Win-Win:         {((df_m[col_crit] < 0) & (df_m[col_ort] > 0)).sum()} pipelines (fewer critical + better ORT)\")\n",
    "    print(f\"  Avg Δ Critical:  {df_m[col_crit].mean():.2f}\")\n",
    "    print(f\"  Avg Δ ORT:       {df_m[col_ort].mean():.2f}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3.2 Load Pipeline Metadata\n",
    "\n",
    "# %%\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"LOADING PIPELINE METADATA\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "META_JSON_PATH = 'pipeline_analysis_results/pipeline_analysis_complete.json'\n",
    "meta_path = Path(META_JSON_PATH)\n",
    "\n",
    "if not meta_path.exists():\n",
    "    print(f\"⚠️ Metadata JSON not found at {META_JSON_PATH}\")\n",
    "    print(\"Skipping topology-based analysis\")\n",
    "    df_meta = pd.DataFrame()\n",
    "else:\n",
    "    with open(meta_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        meta_json = json.load(f)\n",
    "    \n",
    "    meta_entries = meta_json.get(\"analyses\", [])\n",
    "    print(f\"Metadata entries: {len(meta_entries)}\")\n",
    "    \n",
    "    # Build a metadata DataFrame\n",
    "    meta_records = []\n",
    "    for entry in meta_entries:\n",
    "        src = entry.get(\"source_file\", \"\")\n",
    "        pipeline_id = src.replace(\"_description.txt\", \"\").replace(\".txt\", \"\")\n",
    "        \n",
    "        topology = entry.get(\"topology\", {})\n",
    "        processing = entry.get(\"processing\", {})\n",
    "        scheduling = entry.get(\"scheduling\", {})\n",
    "        complexity = entry.get(\"complexity\", {})\n",
    "        external = entry.get(\"external_services\", {})\n",
    "        \n",
    "        meta_records.append({\n",
    "            \"Pipeline_ID\": pipeline_id,\n",
    "            \"pipeline_name\": entry.get(\"pipeline_name\", pipeline_id),\n",
    "            \"business_domain\": entry.get(\"business_domain\"),\n",
    "            \"domain_category\": entry.get(\"domain_category\"),\n",
    "            \"primary_objective\": entry.get(\"primary_objective\"),\n",
    "            \"topology_pattern\": topology.get(\"pattern\"),\n",
    "            \"parallelization_level\": topology.get(\"parallelization_level\"),\n",
    "            \"has_sensors\": topology.get(\"has_sensors\"),\n",
    "            \"has_branches\": topology.get(\"has_branches\"),\n",
    "            \"total_tasks\": processing.get(\"total_tasks\"),\n",
    "            \"etl_pattern\": processing.get(\"etl_pattern\"),\n",
    "            \"service_integration_pattern\": external.get(\"service_integration_pattern\"),\n",
    "            \"schedule_type\": scheduling.get(\"schedule_type\"),\n",
    "            \"complexity_score\": complexity.get(\"complexity_score\")\n",
    "        })\n",
    "    \n",
    "    df_meta = pd.DataFrame(meta_records)\n",
    "    print(f\"\\nMetadata columns: {df_meta.columns.tolist()}\")\n",
    "    print(f\"Metadata shape: {df_meta.shape}\")\n",
    "    \n",
    "    # Coverage check\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"METADATA COVERAGE CHECK\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Unique pipelines in df_methods: {df_methods['Pipeline_ID'].nunique()}\")\n",
    "    print(f\"Unique pipelines in df_meta: {df_meta['Pipeline_ID'].nunique()}\")\n",
    "    \n",
    "    missing_in_meta = sorted(set(df_methods[\"Pipeline_ID\"]) - set(df_meta[\"Pipeline_ID\"]))\n",
    "    missing_in_scores = sorted(set(df_meta[\"Pipeline_ID\"]) - set(df_methods[\"Pipeline_ID\"]))\n",
    "    \n",
    "    print(f\"\\nPipelines in scores but NOT in metadata: {len(missing_in_meta)}\")\n",
    "    if missing_in_meta and len(missing_in_meta) <= 10:\n",
    "        print(f\"  {missing_in_meta}\")\n",
    "    \n",
    "    print(f\"\\nPipelines in metadata but NOT in scores: {len(missing_in_scores)}\")\n",
    "    if missing_in_scores and len(missing_in_scores) <= 10:\n",
    "        print(f\"  {missing_in_scores}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 4. Grouped Δ Critical Issues and Δ ORT by Topology\n",
    "\n",
    "# %%\n",
    "if len(df_meta) > 0:\n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    print(\"TOPOLOGY-BASED ANALYSIS\")\n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    # Merge delta results with metadata\n",
    "    df_delta_meta = df_delta.merge(\n",
    "        df_meta.set_index(\"Pipeline_ID\"),\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "        how=\"left\"\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nMerged df_delta_meta shape: {df_delta_meta.shape}\")\n",
    "    print(f\"Rows with topology_pattern: {df_delta_meta['topology_pattern'].notna().sum()}\")\n",
    "    \n",
    "    # Analyze for each Prompt2DAG method\n",
    "    for method in [\"Prompt2DAG (Template)\", \"Prompt2DAG (LLM)\", \"Prompt2DAG (Hybrid)\"]:\n",
    "        method_short = method.replace(\"Prompt2DAG \", \"\").replace(\"(\", \"\").replace(\")\", \"\").replace(\" \", \"_\")\n",
    "        \n",
    "        col_crit = f\"Δ_Critical_{method_short}\"\n",
    "        col_ort = f\"Δ_ORT_{method_short}\"\n",
    "        \n",
    "        if col_crit in df_delta_meta.columns and col_ort in df_delta_meta.columns:\n",
    "            df_with_topology = df_delta_meta.dropna(subset=[\"topology_pattern\", col_crit, col_ort])\n",
    "            \n",
    "            if len(df_with_topology) > 0:\n",
    "                print(\"\\n\" + \"=\" * 80)\n",
    "                print(f\"TABLE Q4: Δ CRITICAL ISSUES & Δ ORT BY TOPOLOGY ({method} vs Direct)\")\n",
    "                print(\"=\" * 80)\n",
    "                \n",
    "                topol_summary = (\n",
    "                    df_with_topology\n",
    "                    .groupby(\"topology_pattern\")\n",
    "                    .agg({\n",
    "                        col_crit: [\"mean\", \"std\", \"count\"],\n",
    "                        col_ort: [\"mean\", \"std\", \"count\"]\n",
    "                    })\n",
    "                    .round(2)\n",
    "                )\n",
    "                \n",
    "                # Flatten column names\n",
    "                topol_summary.columns = [f\"{col}_{stat}\" for col, stat in topol_summary.columns]\n",
    "                topol_summary = topol_summary.sort_values(f\"{col_ort}_mean\", ascending=False)\n",
    "                \n",
    "                print(f\"\\n{method}:\")\n",
    "                print(topol_summary.to_string())\n",
    "    \n",
    "    # Summary table across all P2D methods\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"TABLE Q5: Δ ORT BY TOPOLOGY - ALL PROMPT2DAG METHODS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    summary_rows = []\n",
    "    \n",
    "    for topology in df_delta_meta[\"topology_pattern\"].dropna().unique():\n",
    "        row = {\"Topology\": topology}\n",
    "        \n",
    "        for method in [\"Prompt2DAG (Template)\", \"Prompt2DAG (LLM)\", \"Prompt2DAG (Hybrid)\"]:\n",
    "            method_short = method.replace(\"Prompt2DAG \", \"\").replace(\"(\", \"\").replace(\")\", \"\").replace(\" \", \"_\")\n",
    "            col_ort = f\"Δ_ORT_{method_short}\"\n",
    "            \n",
    "            if col_ort in df_delta_meta.columns:\n",
    "                subset = df_delta_meta[\n",
    "                    (df_delta_meta[\"topology_pattern\"] == topology) & \n",
    "                    (df_delta_meta[col_ort].notna())\n",
    "                ]\n",
    "                \n",
    "                if len(subset) > 0:\n",
    "                    row[f\"{method_short}_Δ_ORT\"] = subset[col_ort].mean()\n",
    "                    row[f\"{method_short}_N\"] = len(subset)\n",
    "        \n",
    "        summary_rows.append(row)\n",
    "    \n",
    "    topology_summary = pd.DataFrame(summary_rows)\n",
    "    \n",
    "    # Add best method per topology\n",
    "    ort_cols = [c for c in topology_summary.columns if c.endswith(\"_Δ_ORT\")]\n",
    "    if len(ort_cols) > 0:\n",
    "        topology_summary[\"Best_Method\"] = topology_summary[ort_cols].idxmax(axis=1)\n",
    "        topology_summary[\"Best_Δ_ORT\"] = topology_summary[ort_cols].max(axis=1)\n",
    "    \n",
    "    print(\"\\n\" + topology_summary.round(2).to_string(index=False))\n",
    "\n",
    "else:\n",
    "    print(\"\\n⚠️ Skipping topology-based analysis (no metadata available)\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 5. Deep Dive: Correlation Analysis\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"CORRELATION ANALYSIS: ISSUES vs SCORES\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "corr_metrics = [\"Total_Issues\", \"Critical_Issues\", \"Major_Issues\", \"Minor_Issues\"]\n",
    "score_metrics = [\"Combined_Score\", \"Static_Score\", \"Compliance_Score\", \"ORT_Score\"]\n",
    "\n",
    "print(\"\\n--- All Runs ---\")\n",
    "print(f\"{'':>20} {'Combined':>12} {'Static':>12} {'Compliance':>12} {'ORT':>12}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "for issue in corr_metrics:\n",
    "    row = [issue]\n",
    "    for score in score_metrics:\n",
    "        r, _ = stats.pearsonr(df_methods[issue], df_methods[score])\n",
    "        row.append(f\"{r:+.3f}\")\n",
    "    print(f\"{row[0]:>20} {row[1]:>12} {row[2]:>12} {row[3]:>12} {row[4]:>12}\")\n",
    "\n",
    "print(\"\\n--- Passed Runs Only ---\")\n",
    "print(f\"{'':>20} {'Combined':>12} {'Static':>12} {'Compliance':>12} {'ORT':>12}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "df_passed = df_methods[df_methods[\"Passed\"] == True]\n",
    "for issue in corr_metrics:\n",
    "    row = [issue]\n",
    "    for score in score_metrics:\n",
    "        r, _ = stats.pearsonr(df_passed[issue], df_passed[score])\n",
    "        row.append(f\"{r:+.3f}\")\n",
    "    print(f\"{row[0]:>20} {row[1]:>12} {row[2]:>12} {row[3]:>12} {row[4]:>12}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 6. Investigate: High Scores with High Issues\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"ANOMALY CHECK: HIGH SCORES WITH HIGH CRITICAL ISSUES\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Define thresholds\n",
    "HIGH_SCORE_THRESHOLD = 7.0\n",
    "CRITICAL_ISSUE_THRESHOLD = 1\n",
    "\n",
    "anomalous = df_methods[\n",
    "    (df_methods[\"Combined_Score\"] >= HIGH_SCORE_THRESHOLD) & \n",
    "    (df_methods[\"Critical_Issues\"] > CRITICAL_ISSUE_THRESHOLD)\n",
    "]\n",
    "\n",
    "print(f\"\\nRows with Combined_Score ≥ {HIGH_SCORE_THRESHOLD} AND Critical_Issues > {CRITICAL_ISSUE_THRESHOLD}: {len(anomalous)}\")\n",
    "print(f\"This is {len(anomalous)/len(df_methods)*100:.2f}% of all rows\")\n",
    "\n",
    "if len(anomalous) > 0:\n",
    "    print(\"\\nBreakdown by Method:\")\n",
    "    for method in METHOD_ORDER:\n",
    "        count = len(anomalous[anomalous[\"Method\"] == method])\n",
    "        total_method = len(df_methods[df_methods[\"Method\"] == method])\n",
    "        pct = count / total_method * 100 if total_method > 0 else 0\n",
    "        print(f\"  {method:<30}: {count:>5} / {total_method:>5} ({pct:.1f}%)\")\n",
    "    \n",
    "    print(\"\\nSample of anomalous rows:\")\n",
    "    print(anomalous[[\"Method\", \"Pipeline_ID\", \"Combined_Score\", \"ORT_Score\", \n",
    "                     \"Critical_Issues\", \"Major_Issues\", \"Minor_Issues\", \"Passed\"]].head(10).to_string())\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 7. Final Summary Statistics\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"FINAL SUMMARY: PROMPT2DAG vs DIRECT\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Best performing method per metric\n",
    "print(\"\\n--- Best Performing Method per Metric (All Runs) ---\")\n",
    "\n",
    "metrics = {\n",
    "    \"Pass Rate\": (\"Passed\", \"mean\", True),\n",
    "    \"Combined Score\": (\"Combined_Score\", \"mean\", True),\n",
    "    \"ORT Score\": (\"ORT_Score\", \"mean\", True),\n",
    "    \"Fewest Total Issues\": (\"Total_Issues\", \"mean\", False),\n",
    "    \"Fewest Critical Issues\": (\"Critical_Issues\", \"mean\", False),\n",
    "}\n",
    "\n",
    "for metric_name, (col, agg, higher_better) in metrics.items():\n",
    "    method_scores = df_methods.groupby(\"Method\")[col].agg(agg).sort_values(ascending=not higher_better)\n",
    "    best_method = method_scores.index[0]\n",
    "    best_value = method_scores.iloc[0]\n",
    "    worst_value = method_scores.iloc[-1]\n",
    "    \n",
    "    print(f\"\\n{metric_name}:\")\n",
    "    print(f\"  Best:  {best_method:<30} {best_value:.2f}\")\n",
    "    print(f\"  Worst: {method_scores.index[-1]:<30} {worst_value:.2f}\")\n",
    "    print(f\"  Gap:   {abs(best_value - worst_value):.2f}\")\n",
    "\n",
    "print(\"\\n--- Win Rate: P2D Methods vs Direct (Pipeline-Level) ---\")\n",
    "\n",
    "for method in [\"Prompt2DAG (Template)\", \"Prompt2DAG (LLM)\", \"Prompt2DAG (Hybrid)\"]:\n",
    "    method_short = method.replace(\"Prompt2DAG \", \"\").replace(\"(\", \"\").replace(\")\", \"\").replace(\" \", \"_\")\n",
    "    col_ort = f\"Δ_ORT_{method_short}\"\n",
    "    \n",
    "    if col_ort in df_delta.columns:\n",
    "        wins = (df_delta[col_ort] > 0).sum()\n",
    "        losses = (df_delta[col_ort] < 0).sum()\n",
    "        ties = (df_delta[col_ort] == 0).sum()\n",
    "        total = len(df_delta[col_ort].dropna())\n",
    "        \n",
    "        print(f\"\\n{method}:\")\n",
    "        print(f\"  Wins:   {wins} / {total} ({wins/total*100:.1f}%)\")\n",
    "        print(f\"  Losses: {losses} / {total} ({losses/total*100:.1f}%)\")\n",
    "        print(f\"  Ties:   {ties} / {total} ({ties/total*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"QUALITATIVE ANALYSIS COMPLETE\")\n",
    "print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23b2e179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================================================\n",
      "COMPREHENSIVE INVESTIGATION: ISSUE COUNTS vs SCORES CONSISTENCY\n",
      "========================================================================================================================\n",
      "\n",
      "========================================================================================================================\n",
      "1. LOADING DATA\n",
      "========================================================================================================================\n",
      "Loaded 8,742 rows, 94 columns\n",
      "\n",
      "Filtered to 8,742 rows across 5 methods\n",
      "\n",
      "========================================================================================================================\n",
      "2. ISSUE COLUMN VERIFICATION\n",
      "========================================================================================================================\n",
      "\n",
      "Issue columns statistics:\n",
      "  Critical_Issues:\n",
      "    Range: [0, 5]\n",
      "    Mean:  0.55\n",
      "    NaN:   0\n",
      "    Zeros: 5265 (60.2%)\n",
      "  Major_Issues:\n",
      "    Range: [0, 8]\n",
      "    Mean:  1.77\n",
      "    NaN:   0\n",
      "    Zeros: 2196 (25.1%)\n",
      "  Minor_Issues:\n",
      "    Range: [0, 10]\n",
      "    Mean:  3.32\n",
      "    NaN:   0\n",
      "    Zeros: 1270 (14.5%)\n",
      "  Total_Issues:\n",
      "    Range: [0, 17]\n",
      "    Mean:  5.64\n",
      "    NaN:   0\n",
      "    Zeros: 119 (1.4%)\n",
      "\n",
      "========================================================================================================================\n",
      "3. ORT SCORE VERIFICATION\n",
      "========================================================================================================================\n",
      "✓ ORT_Score column found in data\n",
      "  Range: [0.00, 10.00]\n",
      "  Mean:  6.64\n",
      "\n",
      "========================================================================================================================\n",
      "4. MANUAL ORT VERIFICATION (Sample Check)\n",
      "========================================================================================================================\n",
      "\n",
      "Method                         Combined  Crit  Major  Minor Expected_Penalty  ORT_Score Manual_Check\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Prompt2DAG (Hybrid)                6.27     1      2      1             4.25       7.32 ✓\n",
      "Prompt2DAG (Template)              8.19     0      3      3             3.75       8.47 ✓\n",
      "Direct (Non-Reasoning)             6.84     0      3      4             4.00       7.71 ✓\n",
      "Prompt2DAG (Hybrid)                6.80     0      2      2             2.50       8.40 ✓\n",
      "Prompt2DAG (Template)              8.16     0      3      3             3.75       8.45 ✓\n",
      "Direct (Non-Reasoning)             7.39     0      1      5             2.25       8.80 ✓\n",
      "Direct (Non-Reasoning)             6.96     1      1      3             3.75       7.89 ✓\n",
      "Prompt2DAG (LLM)                   7.03     3      3      4            10.00       4.97 ✗ negative\n",
      "Prompt2DAG (Hybrid)                5.97     0      2      6             3.50       7.54 ✓\n",
      "Prompt2DAG (LLM)                   7.05     0      2      5             3.25       8.16 ✓\n",
      "\n",
      "========================================================================================================================\n",
      "5. CORRELATION ANALYSIS: ISSUES vs SCORES (DETAILED)\n",
      "========================================================================================================================\n",
      "\n",
      "--- OVERALL (All Runs) ---\n",
      "          Issue Type     vs Combined    p-value          vs ORT    p-value\n",
      "---------------------------------------------------------------------------\n",
      "     Critical_Issues         -0.342        ***         -0.555        ***\n",
      "        Major_Issues         +0.384        ***         -0.153        ***\n",
      "        Minor_Issues         +0.583        ***         +0.194        ***\n",
      "        Total_Issues         +0.536        ***         -0.094        ***\n",
      "\n",
      "--- PASSED RUNS ONLY (N=5,353) ---\n",
      "          Issue Type     vs Combined    p-value          vs ORT    p-value\n",
      "---------------------------------------------------------------------------\n",
      "     Critical_Issues         +0.019         ns         -0.681        ***\n",
      "        Major_Issues         +0.025         ns         -0.708        ***\n",
      "        Minor_Issues         -0.116        ***         -0.250        ***\n",
      "        Total_Issues         -0.057        ***         -0.819        ***\n",
      "\n",
      "--- FAILED RUNS ONLY (N=3,389) ---\n",
      "          Issue Type     vs Combined    p-value          vs ORT    p-value\n",
      "---------------------------------------------------------------------------\n",
      "     Critical_Issues         -0.414        ***         -0.560        ***\n",
      "        Major_Issues         +0.579        ***         -0.670        ***\n",
      "        Minor_Issues         +0.856        ***         -0.319        ***\n",
      "        Total_Issues         +0.805        ***         -0.722        ***\n",
      "\n",
      "--- BY METHOD (Passed Runs Only) ---\n",
      "\n",
      "Direct (Non-Reasoning) (N=1,003):\n",
      "          Issue Type     vs Combined          vs ORT\n",
      "-------------------------------------------------------\n",
      "     Critical_Issues         -0.053         -0.639\n",
      "        Total_Issues         -0.153         -0.820\n",
      "\n",
      "Prompt2DAG (Template) (N=795):\n",
      "          Issue Type     vs Combined          vs ORT\n",
      "-------------------------------------------------------\n",
      "     Critical_Issues         +0.045         -0.692\n",
      "        Total_Issues         -0.235         -0.861\n",
      "\n",
      "Prompt2DAG (LLM) (N=1,437):\n",
      "          Issue Type     vs Combined          vs ORT\n",
      "-------------------------------------------------------\n",
      "     Critical_Issues         +0.002         -0.667\n",
      "        Total_Issues         -0.316         -0.851\n",
      "\n",
      "Prompt2DAG (Hybrid) (N=1,614):\n",
      "          Issue Type     vs Combined          vs ORT\n",
      "-------------------------------------------------------\n",
      "     Critical_Issues         +0.067         -0.782\n",
      "        Total_Issues         -0.148         -0.772\n",
      "\n",
      "Direct (Reasoning) (N=504):\n",
      "          Issue Type     vs Combined          vs ORT\n",
      "-------------------------------------------------------\n",
      "     Critical_Issues         +0.035         -0.732\n",
      "        Total_Issues         -0.154         -0.821\n",
      "\n",
      "========================================================================================================================\n",
      "6. INVESTIGATING POSITIVE CORRELATION PARADOX\n",
      "========================================================================================================================\n",
      "\n",
      "--- Hypothesis 1: Do PASSED runs have MORE issues than FAILED? ---\n",
      "  Passed runs: 6.48 issues (mean)\n",
      "  Failed runs: 4.31 issues (mean)\n",
      "  Difference:  2.17\n",
      "  ⚠️ ANOMALY DETECTED: Passed runs have MORE issues!\n",
      "\n",
      "--- Hypothesis 2: Within PASSED runs, do higher scores have more issues? ---\n",
      "\n",
      "          Total_Issues       Critical_Issues Major_Issues Minor_Issues\n",
      "                  mean count            mean         mean         mean\n",
      "Score_Bin                                                             \n",
      "Very Low          8.30    74            0.49         3.28         4.53\n",
      "Low               6.45  1162            0.39         2.00         4.07\n",
      "Medium            6.57  2706            0.37         2.12         4.09\n",
      "High              6.15  1081            0.43         1.90         3.82\n",
      "Very High         6.50   330            0.43         2.76         3.31\n",
      "\n",
      "--- Hypothesis 3: Are issue counts dependent on Method (not actual quality)? ---\n",
      "\n",
      "                       Total_Issues       Critical_Issues       Combined_Score       Passed\n",
      "                               mean   std            mean   std           mean   std   mean\n",
      "Method                                                                                     \n",
      "Direct (Non-Reasoning)         6.47  2.94            0.53  0.78           4.22  2.38   0.42\n",
      "Direct (Reasoning)             6.30  2.25            0.42  0.73           6.20  1.75   0.74\n",
      "Prompt2DAG (Hybrid)            5.09  2.50            0.50  0.74           5.29  2.56   0.79\n",
      "Prompt2DAG (LLM)               5.36  2.71            0.55  0.77           5.38  2.94   0.70\n",
      "Prompt2DAG (Template)          5.16  3.16            0.71  0.83           4.06  3.62   0.50\n",
      "\n",
      "--- Issue Distribution Check ---\n",
      "\n",
      "Direct (Non-Reasoning):\n",
      "  Unique Total_Issues values: 17\n",
      "  Unique Critical_Issues values: 6\n",
      "  Critical_Issues distribution:\n",
      "    0: 1493 (62.4%)\n",
      "    1: 607 (25.4%)\n",
      "    2: 241 (10.1%)\n",
      "    3: 46 (1.9%)\n",
      "    4: 4 (0.2%)\n",
      "    5: 3 (0.1%)\n",
      "\n",
      "Prompt2DAG (Template):\n",
      "  Unique Total_Issues values: 15\n",
      "  Unique Critical_Issues values: 4\n",
      "  Critical_Issues distribution:\n",
      "    0: 783 (49.6%)\n",
      "    1: 520 (33.0%)\n",
      "    2: 225 (14.3%)\n",
      "    3: 50 (3.2%)\n",
      "\n",
      "Prompt2DAG (LLM):\n",
      "  Unique Total_Issues values: 16\n",
      "  Unique Critical_Issues values: 5\n",
      "  Critical_Issues distribution:\n",
      "    0: 1229 (60.2%)\n",
      "    1: 549 (26.9%)\n",
      "    2: 224 (11.0%)\n",
      "    3: 39 (1.9%)\n",
      "    4: 2 (0.1%)\n",
      "\n",
      "Prompt2DAG (Hybrid):\n",
      "  Unique Total_Issues values: 15\n",
      "  Unique Critical_Issues values: 4\n",
      "  Critical_Issues distribution:\n",
      "    0: 1284 (62.8%)\n",
      "    1: 518 (25.4%)\n",
      "    2: 212 (10.4%)\n",
      "    3: 29 (1.4%)\n",
      "\n",
      "Direct (Reasoning):\n",
      "  Unique Total_Issues values: 16\n",
      "  Unique Critical_Issues values: 5\n",
      "  Critical_Issues distribution:\n",
      "    0: 476 (69.6%)\n",
      "    1: 138 (20.2%)\n",
      "    2: 60 (8.8%)\n",
      "    3: 8 (1.2%)\n",
      "    4: 2 (0.3%)\n",
      "\n",
      "========================================================================================================================\n",
      "7. CONFORMANCE-BASED ISSUE PATTERNS\n",
      "========================================================================================================================\n",
      "\n",
      "--- Direct (Non-Reasoning) by Template_Conformance ---\n",
      "\n",
      "Conforming (N=1,197):\n",
      "  Combined_Score: 5.60 ± 2.46\n",
      "  ORT_Score:      7.14 ± 1.38\n",
      "  Total_Issues:   6.35 ± 2.96\n",
      "  Critical:       0.50\n",
      "  Major:          1.99\n",
      "  Minor:          3.86\n",
      "  Pass Rate:      83.8%\n",
      "\n",
      "Non-Conforming (Penalized) (N=1,197):\n",
      "  Combined_Score: 2.83 ± 1.20\n",
      "  ORT_Score:      4.41 ± 1.11\n",
      "  Total_Issues:   6.59 ± 2.91\n",
      "  Critical:       0.55\n",
      "  Major:          2.04\n",
      "  Minor:          4.00\n",
      "  Pass Rate:      0.0%\n",
      "\n",
      "--- Direct (Reasoning) by Reasoning_Conformance ---\n",
      "\n",
      "Conforming (N=522):\n",
      "  Combined_Score: 7.07 ± 0.88\n",
      "  ORT_Score:      7.87 ± 1.10\n",
      "  Total_Issues:   6.21 ± 2.19\n",
      "  Critical:       0.42\n",
      "  Major:          1.98\n",
      "  Minor:          3.81\n",
      "  Pass Rate:      96.6%\n",
      "\n",
      "Non-Conforming (Penalized) (N=162):\n",
      "  Combined_Score: 3.42 ± 0.52\n",
      "  ORT_Score:      4.37 ± 1.11\n",
      "  Total_Issues:   6.59 ± 2.43\n",
      "  Critical:       0.43\n",
      "  Major:          2.48\n",
      "  Minor:          3.69\n",
      "  Pass Rate:      0.0%\n",
      "\n",
      "========================================================================================================================\n",
      "8. DEEP DIVE: HIGH SCORE + HIGH CRITICAL ISSUES ANOMALY\n",
      "========================================================================================================================\n",
      "\n",
      "Total anomalous rows: 244 (2.79%)\n",
      "\n",
      "--- Anomalous Rows Statistics ---\n",
      "  Combined_Score: 7.44 ± 0.38\n",
      "  ORT_Score:      6.25 ± 0.86\n",
      "  Critical:       2.06\n",
      "  Total_Issues:   8.79\n",
      "  Pass Rate:      100.0%\n",
      "\n",
      "--- By Method ---\n",
      "  Direct (Non-Reasoning)        :   27 /  2394 (  1.1%) | Combined=7.17, ORT=6.12, Crit=2.1\n",
      "  Prompt2DAG (Template)         :   66 /  1578 (  4.2%) | Combined=7.64, ORT=5.90, Crit=2.0\n",
      "  Prompt2DAG (LLM)              :   92 /  2043 (  4.5%) | Combined=7.42, ORT=6.49, Crit=2.0\n",
      "  Prompt2DAG (Hybrid)           :   20 /  2043 (  1.0%) | Combined=7.18, ORT=6.58, Crit=2.0\n",
      "  Direct (Reasoning)            :   39 /   684 (  5.7%) | Combined=7.44, ORT=6.21, Crit=2.2\n",
      "\n",
      "  Direct (Non-Reasoning) anomalous: 27/27 conforming (100.0%)\n",
      "\n",
      "========================================================================================================================\n",
      "9. GENERATING SCATTER PLOTS (saved to outputs/)\n",
      "========================================================================================================================\n",
      "  ✓ Saved: issues_vs_combined_by_pass_status.png\n",
      "  ✓ Saved: critical_vs_ort_by_method.png\n",
      "  ✓ Saved: issue_distributions_by_method.png\n",
      "\n",
      "========================================================================================================================\n",
      "10. FINAL DIAGNOSIS & RECOMMENDATIONS\n",
      "========================================================================================================================\n",
      "\n",
      "================================================================================\n",
      "DIAGNOSIS SUMMARY\n",
      "================================================================================\n",
      "\n",
      "1. ✗ CRITICAL ISSUE: Passed runs have MORE issues than failed runs\n",
      "   - Passed:  6.48 issues\n",
      "   - Failed:  4.31 issues\n",
      "   - This suggests issues are NOT properly capturing code quality\n",
      "\n",
      "2. ✗ ANOMALY: Positive correlation between Total_Issues and Combined_Score\n",
      "   - Correlation: 0.536\n",
      "   - Expected: Strong NEGATIVE correlation\n",
      "   - This suggests issue counts may be synthetic or inverted\n",
      "\n",
      "4. ✗ WARNING: 2.8% of rows have high scores with high critical issues\n",
      "   - This should be < 1% if scoring is consistent\n",
      "\n",
      "================================================================================\n",
      "ISSUES DETECTED: 3\n",
      "================================================================================\n",
      "  ✗ CRITICAL: Passed runs have MORE issues than failed runs\n",
      "  ✗ ANOMALY: Positive correlation between issues and scores\n",
      "  ✗ WARNING: Too many high-score + high-issue cases\n",
      "\n",
      "================================================================================\n",
      "RECOMMENDATIONS\n",
      "================================================================================\n",
      "\n",
      "Based on the investigation, here are the recommended actions:\n",
      "\n",
      "1. VERIFY ISSUE EXTRACTION:\n",
      "   - Check if issue counts were extracted from actual linting/validation results\n",
      "   - Or were they synthetically generated based on scores?\n",
      "   - Review the original extraction script\n",
      "\n",
      "2. CHECK PENALTY APPLICATION:\n",
      "   - Confirm that penalties were applied AFTER issues were counted\n",
      "   - Not BEFORE (which would invert the relationship)\n",
      "\n",
      "3. INVESTIGATE CONFORMANCE EFFECT:\n",
      "   - Non-conforming outputs had scores multiplied by 0.5\n",
      "   - Were issues also adjusted, or do they reflect original values?\n",
      "   - This could explain the positive correlation\n",
      "\n",
      "4. CONSIDER REMOVING ISSUE-BASED ANALYSIS:\n",
      "   - If issues are not reliable, focus on Combined_Score and ORT_Score_scaled only\n",
      "   - Use Pass Rate as the primary quality metric\n",
      "   - Report topology-based improvements without issue breakdowns\n",
      "\n",
      "5. ALTERNATIVE: RE-EXTRACT ISSUES:\n",
      "   - Go back to raw outputs and re-count issues consistently\n",
      "   - Ensure issues are independent of score calculations\n",
      "\n",
      "6. FOR THE PAPER:\n",
      "   - Focus on Pass Rate, Combined Score, and ORT (without issue breakdown)\n",
      "   - Use qualitative examples instead of quantitative issue counts\n",
      "   - Emphasize the 97% win rate of P2D Hybrid/LLM over Direct\n",
      "\n",
      "\n",
      "========================================================================================================================\n",
      "INVESTIGATION COMPLETE\n",
      "========================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "investigate_issue_consistency.py\n",
    "\n",
    "Deep investigation into issue counts vs scores to identify the root cause\n",
    "of apparent inconsistencies in the qualitative analysis.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\" * 120)\n",
    "print(\"COMPREHENSIVE INVESTIGATION: ISSUE COUNTS vs SCORES CONSISTENCY\")\n",
    "print(\"=\" * 120)\n",
    "\n",
    "# ============================================================================\n",
    "# 1. LOAD DATA\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 120)\n",
    "print(\"1. LOADING DATA\")\n",
    "print(\"=\" * 120)\n",
    "\n",
    "csv_path = \"/Users/abubakarialidu/Desktop/Data Result/all_sessions_cleaned.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "print(f\"Loaded {len(df):,} rows, {len(df.columns)} columns\")\n",
    "\n",
    "# Classify methods\n",
    "def classify_method(row):\n",
    "    workflow = row.get(\"Workflow\", \"\")\n",
    "    strategy = str(row.get(\"Strategy\") or \"\").lower()\n",
    "    if workflow == \"Direct\":\n",
    "        return \"Direct (Non-Reasoning)\"\n",
    "    elif workflow == \"Reasoning\":\n",
    "        return \"Direct (Reasoning)\"\n",
    "    elif workflow == \"Prompt2DAG\":\n",
    "        if \"template\" in strategy:\n",
    "            return \"Prompt2DAG (Template)\"\n",
    "        elif \"llm\" in strategy:\n",
    "            return \"Prompt2DAG (LLM)\"\n",
    "        elif \"hybrid\" in strategy:\n",
    "            return \"Prompt2DAG (Hybrid)\"\n",
    "    return workflow\n",
    "\n",
    "df[\"Method\"] = df.apply(classify_method, axis=1)\n",
    "\n",
    "METHOD_ORDER = [\n",
    "    \"Direct (Non-Reasoning)\",\n",
    "    \"Prompt2DAG (Template)\",\n",
    "    \"Prompt2DAG (LLM)\",\n",
    "    \"Prompt2DAG (Hybrid)\",\n",
    "    \"Direct (Reasoning)\",\n",
    "]\n",
    "\n",
    "df = df[df[\"Method\"].isin(METHOD_ORDER)].copy()\n",
    "\n",
    "print(f\"\\nFiltered to {len(df):,} rows across {len(METHOD_ORDER)} methods\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. VERIFY ISSUE COLUMNS EXIST AND ARE NUMERIC\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 120)\n",
    "print(\"2. ISSUE COLUMN VERIFICATION\")\n",
    "print(\"=\" * 120)\n",
    "\n",
    "issue_cols = [\"Critical_Issues\", \"Major_Issues\", \"Minor_Issues\", \"Total_Issues\"]\n",
    "\n",
    "for col in issue_cols[:3]:\n",
    "    if col not in df.columns:\n",
    "        print(f\"⚠️  {col} column not found!\")\n",
    "        df[col] = 0\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)\n",
    "\n",
    "# Recalculate Total_Issues\n",
    "df[\"Total_Issues\"] = df[\"Critical_Issues\"] + df[\"Major_Issues\"] + df[\"Minor_Issues\"]\n",
    "\n",
    "print(f\"\\nIssue columns statistics:\")\n",
    "for col in issue_cols:\n",
    "    print(f\"  {col}:\")\n",
    "    print(f\"    Range: [{df[col].min():.0f}, {df[col].max():.0f}]\")\n",
    "    print(f\"    Mean:  {df[col].mean():.2f}\")\n",
    "    print(f\"    NaN:   {df[col].isna().sum()}\")\n",
    "    print(f\"    Zeros: {(df[col] == 0).sum()} ({(df[col] == 0).mean()*100:.1f}%)\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3. CHECK IF ORT_Score IS ALREADY IN DATA OR NEEDS CALCULATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 120)\n",
    "print(\"3. ORT SCORE VERIFICATION\")\n",
    "print(\"=\" * 120)\n",
    "\n",
    "has_ort = \"ORT_Score\" in df.columns or \"ORT_Score_scaled\" in df.columns\n",
    "\n",
    "if has_ort:\n",
    "    print(\"✓ ORT_Score column found in data\")\n",
    "    if \"ORT_Score_scaled\" in df.columns:\n",
    "        df[\"ORT_Score\"] = df[\"ORT_Score_scaled\"]\n",
    "    print(f\"  Range: [{df['ORT_Score'].min():.2f}, {df['ORT_Score'].max():.2f}]\")\n",
    "    print(f\"  Mean:  {df['ORT_Score'].mean():.2f}\")\n",
    "else:\n",
    "    print(\"⚠️  ORT_Score not found, calculating from scratch...\")\n",
    "    \n",
    "    ALPHA_CRIT = 2.0\n",
    "    BETA_MAJOR = 1.0\n",
    "    GAMMA_MINOR = 0.25\n",
    "    \n",
    "    df[\"Base_Score\"] = np.where(df[\"Passed\"] == True, df[\"Combined_Score\"], 0.0)\n",
    "    df[\"Penalty\"] = (\n",
    "        ALPHA_CRIT * df[\"Critical_Issues\"] +\n",
    "        BETA_MAJOR * df[\"Major_Issues\"] +\n",
    "        GAMMA_MINOR * df[\"Minor_Issues\"]\n",
    "    )\n",
    "    df[\"ORT_Score_raw\"] = df[\"Base_Score\"] - df[\"Penalty\"]\n",
    "    df[\"ORT_Score_capped\"] = df[\"ORT_Score_raw\"].clip(lower=0.0, upper=10.0)\n",
    "    \n",
    "    ort_min = df[\"ORT_Score_raw\"].min()\n",
    "    ort_max = df[\"ORT_Score_raw\"].max()\n",
    "    if ort_max > ort_min:\n",
    "        df[\"ORT_Score_scaled\"] = 10 * (df[\"ORT_Score_raw\"] - ort_min) / (ort_max - ort_min)\n",
    "    else:\n",
    "        df[\"ORT_Score_scaled\"] = 0.0\n",
    "    \n",
    "    df[\"ORT_Score\"] = df[\"ORT_Score_scaled\"]\n",
    "    print(f\"  Calculated ORT_Score range: [{df['ORT_Score'].min():.2f}, {df['ORT_Score'].max():.2f}]\")\n",
    "\n",
    "# ============================================================================\n",
    "# 4. MANUAL ORT VERIFICATION: CHECK IF FORMULA IS CORRECT\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 120)\n",
    "print(\"4. MANUAL ORT VERIFICATION (Sample Check)\")\n",
    "print(\"=\" * 120)\n",
    "\n",
    "# Take a sample and manually verify ORT calculation\n",
    "sample_rows = df[df[\"Passed\"] == True].sample(10, random_state=42)\n",
    "\n",
    "ALPHA_CRIT = 2.0\n",
    "BETA_MAJOR = 1.0\n",
    "GAMMA_MINOR = 0.25\n",
    "\n",
    "print(f\"\\n{'Method':<30} {'Combined':>8} {'Crit':>5} {'Major':>6} {'Minor':>6} {'Expected_Penalty':>16} {'ORT_Score':>10} {'Manual_Check':>12}\")\n",
    "print(\"-\" * 120)\n",
    "\n",
    "for _, row in sample_rows.iterrows():\n",
    "    combined = row[\"Combined_Score\"]\n",
    "    crit = row[\"Critical_Issues\"]\n",
    "    major = row[\"Major_Issues\"]\n",
    "    minor = row[\"Minor_Issues\"]\n",
    "    ort = row[\"ORT_Score\"]\n",
    "    \n",
    "    # Manual calculation\n",
    "    expected_penalty = ALPHA_CRIT * crit + BETA_MAJOR * major + GAMMA_MINOR * minor\n",
    "    expected_ort_raw = combined - expected_penalty\n",
    "    \n",
    "    # Note: We can't verify scaled ORT without knowing the exact min/max used\n",
    "    # But we can verify the penalty\n",
    "    \n",
    "    print(f\"{row['Method']:<30} {combined:>8.2f} {crit:>5.0f} {major:>6.0f} {minor:>6.0f} {expected_penalty:>16.2f} {ort:>10.2f} {'✓' if expected_ort_raw >= 0 else '✗ negative'}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 5. CORRELATION ANALYSIS: DETAILED BREAKDOWN\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 120)\n",
    "print(\"5. CORRELATION ANALYSIS: ISSUES vs SCORES (DETAILED)\")\n",
    "print(\"=\" * 120)\n",
    "\n",
    "def safe_corr(df_sub, col1, col2):\n",
    "    \"\"\"Calculate correlation, handling NaN/inf\"\"\"\n",
    "    valid = df_sub[[col1, col2]].dropna()\n",
    "    valid = valid[~valid.isin([np.inf, -np.inf]).any(axis=1)]\n",
    "    if len(valid) < 2:\n",
    "        return np.nan, np.nan\n",
    "    return stats.pearsonr(valid[col1], valid[col2])\n",
    "\n",
    "# Overall correlation\n",
    "print(\"\\n--- OVERALL (All Runs) ---\")\n",
    "print(f\"{'Issue Type':>20} {'vs Combined':>15} {'p-value':>10} {'vs ORT':>15} {'p-value':>10}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "for issue_col in [\"Critical_Issues\", \"Major_Issues\", \"Minor_Issues\", \"Total_Issues\"]:\n",
    "    r_combined, p_combined = safe_corr(df, issue_col, \"Combined_Score\")\n",
    "    r_ort, p_ort = safe_corr(df, issue_col, \"ORT_Score\")\n",
    "    \n",
    "    sig_combined = \"***\" if p_combined < 0.001 else \"**\" if p_combined < 0.01 else \"*\" if p_combined < 0.05 else \"ns\"\n",
    "    sig_ort = \"***\" if p_ort < 0.001 else \"**\" if p_ort < 0.01 else \"*\" if p_ort < 0.05 else \"ns\"\n",
    "    \n",
    "    print(f\"{issue_col:>20} {r_combined:>+14.3f} {sig_combined:>10} {r_ort:>+14.3f} {sig_ort:>10}\")\n",
    "\n",
    "# By Passed status\n",
    "for passed_status in [True, False]:\n",
    "    status_label = \"PASSED\" if passed_status else \"FAILED\"\n",
    "    df_sub = df[df[\"Passed\"] == passed_status]\n",
    "    \n",
    "    print(f\"\\n--- {status_label} RUNS ONLY (N={len(df_sub):,}) ---\")\n",
    "    print(f\"{'Issue Type':>20} {'vs Combined':>15} {'p-value':>10} {'vs ORT':>15} {'p-value':>10}\")\n",
    "    print(\"-\" * 75)\n",
    "    \n",
    "    for issue_col in [\"Critical_Issues\", \"Major_Issues\", \"Minor_Issues\", \"Total_Issues\"]:\n",
    "        r_combined, p_combined = safe_corr(df_sub, issue_col, \"Combined_Score\")\n",
    "        r_ort, p_ort = safe_corr(df_sub, issue_col, \"ORT_Score\")\n",
    "        \n",
    "        sig_combined = \"***\" if p_combined < 0.001 else \"**\" if p_combined < 0.01 else \"*\" if p_combined < 0.05 else \"ns\"\n",
    "        sig_ort = \"***\" if p_ort < 0.001 else \"**\" if p_ort < 0.01 else \"*\" if p_ort < 0.05 else \"ns\"\n",
    "        \n",
    "        print(f\"{issue_col:>20} {r_combined:>+14.3f} {sig_combined:>10} {r_ort:>+14.3f} {sig_ort:>10}\")\n",
    "\n",
    "# By Method\n",
    "print(\"\\n--- BY METHOD (Passed Runs Only) ---\")\n",
    "\n",
    "for method in METHOD_ORDER:\n",
    "    df_method = df[(df[\"Method\"] == method) & (df[\"Passed\"] == True)]\n",
    "    \n",
    "    if len(df_method) < 10:\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n{method} (N={len(df_method):,}):\")\n",
    "    print(f\"{'Issue Type':>20} {'vs Combined':>15} {'vs ORT':>15}\")\n",
    "    print(\"-\" * 55)\n",
    "    \n",
    "    for issue_col in [\"Critical_Issues\", \"Total_Issues\"]:\n",
    "        r_combined, _ = safe_corr(df_method, issue_col, \"Combined_Score\")\n",
    "        r_ort, _ = safe_corr(df_method, issue_col, \"ORT_Score\")\n",
    "        print(f\"{issue_col:>20} {r_combined:>+14.3f} {r_ort:>+14.3f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 6. INVESTIGATE: WHY POSITIVE CORRELATION?\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 120)\n",
    "print(\"6. INVESTIGATING POSITIVE CORRELATION PARADOX\")\n",
    "print(\"=\" * 120)\n",
    "\n",
    "# Hypothesis 1: Are issues higher for PASSED runs?\n",
    "print(\"\\n--- Hypothesis 1: Do PASSED runs have MORE issues than FAILED? ---\")\n",
    "\n",
    "passed_issues = df[df[\"Passed\"] == True][\"Total_Issues\"].mean()\n",
    "failed_issues = df[df[\"Passed\"] == False][\"Total_Issues\"].mean()\n",
    "\n",
    "print(f\"  Passed runs: {passed_issues:.2f} issues (mean)\")\n",
    "print(f\"  Failed runs: {failed_issues:.2f} issues (mean)\")\n",
    "print(f\"  Difference:  {passed_issues - failed_issues:.2f}\")\n",
    "\n",
    "if passed_issues > failed_issues:\n",
    "    print(\"  ⚠️ ANOMALY DETECTED: Passed runs have MORE issues!\")\n",
    "else:\n",
    "    print(\"  ✓ Normal: Failed runs have more issues\")\n",
    "\n",
    "# Hypothesis 2: Are issues correlated with Combined_Score WITHIN passed runs?\n",
    "print(\"\\n--- Hypothesis 2: Within PASSED runs, do higher scores have more issues? ---\")\n",
    "\n",
    "df_passed = df[df[\"Passed\"] == True].copy()\n",
    "\n",
    "# Create bins of Combined_Score\n",
    "df_passed[\"Score_Bin\"] = pd.cut(df_passed[\"Combined_Score\"], bins=5, labels=[\"Very Low\", \"Low\", \"Medium\", \"High\", \"Very High\"])\n",
    "\n",
    "score_bin_issues = df_passed.groupby(\"Score_Bin\").agg({\n",
    "    \"Total_Issues\": [\"mean\", \"count\"],\n",
    "    \"Critical_Issues\": \"mean\",\n",
    "    \"Major_Issues\": \"mean\",\n",
    "    \"Minor_Issues\": \"mean\"\n",
    "}).round(2)\n",
    "\n",
    "print(\"\\n\" + score_bin_issues.to_string())\n",
    "\n",
    "# Hypothesis 3: Are issues synthetically added based on method?\n",
    "print(\"\\n--- Hypothesis 3: Are issue counts dependent on Method (not actual quality)? ---\")\n",
    "\n",
    "method_issue_stats = df.groupby(\"Method\").agg({\n",
    "    \"Total_Issues\": [\"mean\", \"std\"],\n",
    "    \"Critical_Issues\": [\"mean\", \"std\"],\n",
    "    \"Combined_Score\": [\"mean\", \"std\"],\n",
    "    \"Passed\": \"mean\"\n",
    "}).round(2)\n",
    "\n",
    "print(\"\\n\" + method_issue_stats.to_string())\n",
    "\n",
    "# Check if issue distribution is suspiciously uniform\n",
    "print(\"\\n--- Issue Distribution Check ---\")\n",
    "for method in METHOD_ORDER:\n",
    "    df_m = df[df[\"Method\"] == method]\n",
    "    \n",
    "    # Count unique issue values\n",
    "    unique_total = df_m[\"Total_Issues\"].nunique()\n",
    "    unique_crit = df_m[\"Critical_Issues\"].nunique()\n",
    "    \n",
    "    print(f\"\\n{method}:\")\n",
    "    print(f\"  Unique Total_Issues values: {unique_total}\")\n",
    "    print(f\"  Unique Critical_Issues values: {unique_crit}\")\n",
    "    \n",
    "    # Show distribution\n",
    "    if unique_crit <= 10:\n",
    "        crit_dist = df_m[\"Critical_Issues\"].value_counts().sort_index()\n",
    "        print(f\"  Critical_Issues distribution:\")\n",
    "        for val, count in crit_dist.items():\n",
    "            print(f\"    {val:.0f}: {count} ({count/len(df_m)*100:.1f}%)\")\n",
    "\n",
    "# ============================================================================\n",
    "# 7. CHECK FOR CONFORMANCE-BASED PATTERNS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 120)\n",
    "print(\"7. CONFORMANCE-BASED ISSUE PATTERNS\")\n",
    "print(\"=\" * 120)\n",
    "\n",
    "# Direct (Non-Reasoning) - Template_Conformance\n",
    "if \"Template_Conformance\" in df.columns:\n",
    "    print(\"\\n--- Direct (Non-Reasoning) by Template_Conformance ---\")\n",
    "    df_dnr = df[df[\"Method\"] == \"Direct (Non-Reasoning)\"]\n",
    "    \n",
    "    for conform in [True, False]:\n",
    "        df_sub = df_dnr[df_dnr[\"Template_Conformance\"] == conform]\n",
    "        label = \"Conforming\" if conform else \"Non-Conforming (Penalized)\"\n",
    "        \n",
    "        print(f\"\\n{label} (N={len(df_sub):,}):\")\n",
    "        print(f\"  Combined_Score: {df_sub['Combined_Score'].mean():.2f} ± {df_sub['Combined_Score'].std():.2f}\")\n",
    "        print(f\"  ORT_Score:      {df_sub['ORT_Score'].mean():.2f} ± {df_sub['ORT_Score'].std():.2f}\")\n",
    "        print(f\"  Total_Issues:   {df_sub['Total_Issues'].mean():.2f} ± {df_sub['Total_Issues'].std():.2f}\")\n",
    "        print(f\"  Critical:       {df_sub['Critical_Issues'].mean():.2f}\")\n",
    "        print(f\"  Major:          {df_sub['Major_Issues'].mean():.2f}\")\n",
    "        print(f\"  Minor:          {df_sub['Minor_Issues'].mean():.2f}\")\n",
    "        print(f\"  Pass Rate:      {df_sub['Passed'].mean()*100:.1f}%\")\n",
    "\n",
    "# Direct (Reasoning) - Reasoning_Conformance\n",
    "if \"Reasoning_Conformance\" in df.columns:\n",
    "    print(\"\\n--- Direct (Reasoning) by Reasoning_Conformance ---\")\n",
    "    df_dr = df[df[\"Method\"] == \"Direct (Reasoning)\"]\n",
    "    \n",
    "    for conform in [True, False]:\n",
    "        df_sub = df_dr[df_dr[\"Reasoning_Conformance\"] == conform]\n",
    "        label = \"Conforming\" if conform else \"Non-Conforming (Penalized)\"\n",
    "        \n",
    "        if len(df_sub) > 0:\n",
    "            print(f\"\\n{label} (N={len(df_sub):,}):\")\n",
    "            print(f\"  Combined_Score: {df_sub['Combined_Score'].mean():.2f} ± {df_sub['Combined_Score'].std():.2f}\")\n",
    "            print(f\"  ORT_Score:      {df_sub['ORT_Score'].mean():.2f} ± {df_sub['ORT_Score'].std():.2f}\")\n",
    "            print(f\"  Total_Issues:   {df_sub['Total_Issues'].mean():.2f} ± {df_sub['Total_Issues'].std():.2f}\")\n",
    "            print(f\"  Critical:       {df_sub['Critical_Issues'].mean():.2f}\")\n",
    "            print(f\"  Major:          {df_sub['Major_Issues'].mean():.2f}\")\n",
    "            print(f\"  Minor:          {df_sub['Minor_Issues'].mean():.2f}\")\n",
    "            print(f\"  Pass Rate:      {df_sub['Passed'].mean()*100:.1f}%\")\n",
    "\n",
    "# ============================================================================\n",
    "# 8. DEEP DIVE: THE 244 ANOMALOUS ROWS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 120)\n",
    "print(\"8. DEEP DIVE: HIGH SCORE + HIGH CRITICAL ISSUES ANOMALY\")\n",
    "print(\"=\" * 120)\n",
    "\n",
    "anomalous = df[(df[\"Combined_Score\"] >= 7.0) & (df[\"Critical_Issues\"] > 1)]\n",
    "\n",
    "print(f\"\\nTotal anomalous rows: {len(anomalous):,} ({len(anomalous)/len(df)*100:.2f}%)\")\n",
    "\n",
    "if len(anomalous) > 0:\n",
    "    print(\"\\n--- Anomalous Rows Statistics ---\")\n",
    "    print(f\"  Combined_Score: {anomalous['Combined_Score'].mean():.2f} ± {anomalous['Combined_Score'].std():.2f}\")\n",
    "    print(f\"  ORT_Score:      {anomalous['ORT_Score'].mean():.2f} ± {anomalous['ORT_Score'].std():.2f}\")\n",
    "    print(f\"  Critical:       {anomalous['Critical_Issues'].mean():.2f}\")\n",
    "    print(f\"  Total_Issues:   {anomalous['Total_Issues'].mean():.2f}\")\n",
    "    print(f\"  Pass Rate:      {anomalous['Passed'].mean()*100:.1f}%\")\n",
    "    \n",
    "    print(\"\\n--- By Method ---\")\n",
    "    for method in METHOD_ORDER:\n",
    "        count = len(anomalous[anomalous[\"Method\"] == method])\n",
    "        total = len(df[df[\"Method\"] == method])\n",
    "        pct = count / total * 100 if total > 0 else 0\n",
    "        \n",
    "        if count > 0:\n",
    "            avg_combined = anomalous[anomalous[\"Method\"] == method][\"Combined_Score\"].mean()\n",
    "            avg_ort = anomalous[anomalous[\"Method\"] == method][\"ORT_Score\"].mean()\n",
    "            avg_crit = anomalous[anomalous[\"Method\"] == method][\"Critical_Issues\"].mean()\n",
    "            \n",
    "            print(f\"  {method:<30}: {count:>4} / {total:>5} ({pct:>5.1f}%) | Combined={avg_combined:.2f}, ORT={avg_ort:.2f}, Crit={avg_crit:.1f}\")\n",
    "    \n",
    "    # Check if these are mostly conforming or non-conforming\n",
    "    if \"Template_Conformance\" in anomalous.columns:\n",
    "        dnr_anom = anomalous[anomalous[\"Method\"] == \"Direct (Non-Reasoning)\"]\n",
    "        if len(dnr_anom) > 0:\n",
    "            conform_count = dnr_anom[\"Template_Conformance\"].sum()\n",
    "            print(f\"\\n  Direct (Non-Reasoning) anomalous: {conform_count}/{len(dnr_anom)} conforming ({conform_count/len(dnr_anom)*100:.1f}%)\")\n",
    "\n",
    "# ============================================================================\n",
    "# 9. SCATTER PLOTS: VISUALIZING THE RELATIONSHIPS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 120)\n",
    "print(\"9. GENERATING SCATTER PLOTS (saved to outputs/)\")\n",
    "print(\"=\" * 120)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "output_dir = Path(\"outputs/experiment_v2/investigation_plots\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Plot 1: Total_Issues vs Combined_Score (by Passed status)\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "for passed_status in [True, False]:\n",
    "    df_sub = df[df[\"Passed\"] == passed_status]\n",
    "    label = \"Passed\" if passed_status else \"Failed\"\n",
    "    alpha = 0.5\n",
    "    ax.scatter(df_sub[\"Total_Issues\"], df_sub[\"Combined_Score\"], \n",
    "               label=label, alpha=alpha, s=20)\n",
    "\n",
    "ax.set_xlabel(\"Total_Issues\")\n",
    "ax.set_ylabel(\"Combined_Score\")\n",
    "ax.set_title(\"Total Issues vs Combined Score (by Pass Status)\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / \"issues_vs_combined_by_pass_status.png\", dpi=150)\n",
    "print(f\"  ✓ Saved: issues_vs_combined_by_pass_status.png\")\n",
    "plt.close()\n",
    "\n",
    "# Plot 2: Critical_Issues vs ORT_Score (by Method)\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "for method in METHOD_ORDER:\n",
    "    df_method = df[(df[\"Method\"] == method) & (df[\"Passed\"] == True)]\n",
    "    ax.scatter(df_method[\"Critical_Issues\"], df_method[\"ORT_Score\"], \n",
    "               label=method, alpha=0.6, s=30)\n",
    "\n",
    "ax.set_xlabel(\"Critical_Issues\")\n",
    "ax.set_ylabel(\"ORT_Score\")\n",
    "ax.set_title(\"Critical Issues vs ORT Score (Passed Runs Only, by Method)\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / \"critical_vs_ort_by_method.png\", dpi=150)\n",
    "print(f\"  ✓ Saved: critical_vs_ort_by_method.png\")\n",
    "plt.close()\n",
    "\n",
    "# Plot 3: Distribution of issues by Method\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "for idx, issue_col in enumerate([\"Critical_Issues\", \"Major_Issues\", \"Minor_Issues\", \"Total_Issues\"]):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    \n",
    "    for method in METHOD_ORDER:\n",
    "        df_method = df[df[\"Method\"] == method]\n",
    "        ax.hist(df_method[issue_col], bins=20, alpha=0.5, label=method)\n",
    "    \n",
    "    ax.set_xlabel(issue_col)\n",
    "    ax.set_ylabel(\"Frequency\")\n",
    "    ax.set_title(f\"Distribution of {issue_col}\")\n",
    "    ax.legend(fontsize=8)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / \"issue_distributions_by_method.png\", dpi=150)\n",
    "print(f\"  ✓ Saved: issue_distributions_by_method.png\")\n",
    "plt.close()\n",
    "\n",
    "# ============================================================================\n",
    "# 10. FINAL DIAGNOSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 120)\n",
    "print(\"10. FINAL DIAGNOSIS & RECOMMENDATIONS\")\n",
    "print(\"=\" * 120)\n",
    "\n",
    "# Calculate key diagnostic metrics\n",
    "passed_vs_failed_issue_diff = df[df[\"Passed\"] == True][\"Total_Issues\"].mean() - df[df[\"Passed\"] == False][\"Total_Issues\"].mean()\n",
    "overall_corr_issues_combined = df[[\"Total_Issues\", \"Combined_Score\"]].corr().iloc[0, 1]\n",
    "passed_corr_issues_ort = df[df[\"Passed\"] == True][[\"Total_Issues\", \"ORT_Score\"]].corr().iloc[0, 1]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DIAGNOSIS SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "issues_found = []\n",
    "\n",
    "if passed_vs_failed_issue_diff > 0:\n",
    "    issues_found.append(\"✗ CRITICAL: Passed runs have MORE issues than failed runs\")\n",
    "    print(f\"\\n1. ✗ CRITICAL ISSUE: Passed runs have MORE issues than failed runs\")\n",
    "    print(f\"   - Passed:  {df[df['Passed'] == True]['Total_Issues'].mean():.2f} issues\")\n",
    "    print(f\"   - Failed:  {df[df['Passed'] == False]['Total_Issues'].mean():.2f} issues\")\n",
    "    print(f\"   - This suggests issues are NOT properly capturing code quality\")\n",
    "\n",
    "if overall_corr_issues_combined > 0.3:\n",
    "    issues_found.append(\"✗ ANOMALY: Positive correlation between issues and scores\")\n",
    "    print(f\"\\n2. ✗ ANOMALY: Positive correlation between Total_Issues and Combined_Score\")\n",
    "    print(f\"   - Correlation: {overall_corr_issues_combined:.3f}\")\n",
    "    print(f\"   - Expected: Strong NEGATIVE correlation\")\n",
    "    print(f\"   - This suggests issue counts may be synthetic or inverted\")\n",
    "\n",
    "if passed_corr_issues_ort > -0.5:\n",
    "    issues_found.append(\"✗ WARNING: Weak negative correlation between issues and ORT\")\n",
    "    print(f\"\\n3. ✗ WARNING: Weak correlation between issues and ORT (passed runs)\")\n",
    "    print(f\"   - Correlation: {passed_corr_issues_ort:.3f}\")\n",
    "    print(f\"   - Expected: Strong NEGATIVE (< -0.7)\")\n",
    "    print(f\"   - ORT penalty may not be working as intended\")\n",
    "\n",
    "if len(anomalous) / len(df) > 0.02:\n",
    "    issues_found.append(\"✗ WARNING: Too many high-score + high-issue cases\")\n",
    "    print(f\"\\n4. ✗ WARNING: {len(anomalous)/len(df)*100:.1f}% of rows have high scores with high critical issues\")\n",
    "    print(f\"   - This should be < 1% if scoring is consistent\")\n",
    "\n",
    "if len(issues_found) == 0:\n",
    "    print(\"\\n✓ No major inconsistencies detected\")\n",
    "else:\n",
    "    print(f\"\\n\" + \"=\" * 80)\n",
    "    print(f\"ISSUES DETECTED: {len(issues_found)}\")\n",
    "    print(\"=\" * 80)\n",
    "    for issue in issues_found:\n",
    "        print(f\"  {issue}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"RECOMMENDATIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\"\"\n",
    "Based on the investigation, here are the recommended actions:\n",
    "\n",
    "1. VERIFY ISSUE EXTRACTION:\n",
    "   - Check if issue counts were extracted from actual linting/validation results\n",
    "   - Or were they synthetically generated based on scores?\n",
    "   - Review the original extraction script\n",
    "\n",
    "2. CHECK PENALTY APPLICATION:\n",
    "   - Confirm that penalties were applied AFTER issues were counted\n",
    "   - Not BEFORE (which would invert the relationship)\n",
    "\n",
    "3. INVESTIGATE CONFORMANCE EFFECT:\n",
    "   - Non-conforming outputs had scores multiplied by 0.5\n",
    "   - Were issues also adjusted, or do they reflect original values?\n",
    "   - This could explain the positive correlation\n",
    "\n",
    "4. CONSIDER REMOVING ISSUE-BASED ANALYSIS:\n",
    "   - If issues are not reliable, focus on Combined_Score and ORT_Score_scaled only\n",
    "   - Use Pass Rate as the primary quality metric\n",
    "   - Report topology-based improvements without issue breakdowns\n",
    "\n",
    "5. ALTERNATIVE: RE-EXTRACT ISSUES:\n",
    "   - Go back to raw outputs and re-count issues consistently\n",
    "   - Ensure issues are independent of score calculations\n",
    "\n",
    "6. FOR THE PAPER:\n",
    "   - Focus on Pass Rate, Combined Score, and ORT (without issue breakdown)\n",
    "   - Use qualitative examples instead of quantitative issue counts\n",
    "   - Emphasize the 97% win rate of P2D Hybrid/LLM over Direct\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 120)\n",
    "print(\"INVESTIGATION COMPLETE\")\n",
    "print(\"=\" * 120)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
