{# dag_parallel_template.py.j2 #}
# Generated by: {{ script_name }}
# Source YAML: {{ source_yaml_filename }}
# TEMPLATE: dag_parallel_template.py.j2
# Generated at: {{ generation_timestamp }}
# WARNING: Review carefully before execution.
# ==============================================================================

from __future__ import annotations

import os
import glob
import time
import pandas as pd
from datetime import timedelta, datetime

from airflow import DAG
from airflow.providers.docker.operators.docker import DockerOperator
from airflow.operators.python import PythonOperator
from airflow.exceptions import AirflowException
from docker.types import Mount

# ------------------------------------------------------------------
# Global Defaults
# ------------------------------------------------------------------
default_args = {
    "owner": "airflow", 
    "depends_on_past": False,
    "email_on_failure": False,
    "email_on_retry": False,
    "retries": {{ default_retries }},
    "retry_delay": timedelta(minutes={{ default_retry_delay }}),
}

NUM_PARALLEL_BRANCHES = int(os.getenv("NUM_PARALLEL_PROCESSES", {{ num_parallel_branches }}))

# Host directory (bind-mounted) â€“ can be overridden at runtime
HOST_DATA_DIR = os.getenv("DATA_DIR", {{ host_data_dir | repr }})
CONTAINER_DATA_DIR = "/app/data"

# ------------------------------------------------------------------
# Helper Python callables
# ------------------------------------------------------------------
def split_dataset(**context):
    """
    Split the main CSV into NUM_PARALLEL_BRANCHES parts so each branch
    processes its own subset.
    """
    in_file = os.path.join(HOST_DATA_DIR, {{ split_input_file | repr }})
    
    if not os.path.exists(in_file):
        raise AirflowException(f"Input file {in_file} not found")
    
    df = pd.read_csv(in_file)
    total = len(df)
    chunk_size = max(1, total // NUM_PARALLEL_BRANCHES)
    
    for idx in range(NUM_PARALLEL_BRANCHES):
        start = idx * chunk_size
        end = None if idx == NUM_PARALLEL_BRANCHES - 1 else (idx + 1) * chunk_size
        output_file = os.path.join(HOST_DATA_DIR, f"{{ split_output_prefix }}_{idx + 1}.csv")
        df.iloc[start:end].to_csv(output_file, index=False)
        print(f"Created chunk file: {output_file}")

def wait_for_files(**context):
    """
    Block until every branch produced its final CSV.
    """
    patterns = [
        os.path.join(HOST_DATA_DIR, f"{{ final_branch_output_prefix }}_{idx + 1}.csv")
        for idx in range(NUM_PARALLEL_BRANCHES)
    ]
    
    start_time = time.time()
    timeout = 3600  # 1 hour timeout
    
    while True:
        missing_files = [p for p in patterns if not glob.glob(p)]
        if not missing_files:
            print("All branch output files found")
            return
            
        if time.time() - start_time > timeout:
            raise AirflowException(f"Timeout waiting for files: {missing_files}")
        
        print(f"Waiting for {len(missing_files)} files...")
        time.sleep(30)

def concatenate_results(**context):
    """
    Concatenate all branch CSVs into a single file.
    """
    files = []
    for idx in range(NUM_PARALLEL_BRANCHES):
        pattern = os.path.join(HOST_DATA_DIR, f"{{ final_branch_output_prefix }}_{idx + 1}.csv")
        matching_files = glob.glob(pattern)
        if matching_files:
            files.extend(matching_files)
    
    files = sorted(files)
    
    if len(files) < NUM_PARALLEL_BRANCHES:
        raise AirflowException(f"Missing branch output files. Found {len(files)}, expected {NUM_PARALLEL_BRANCHES}")
    
    dfs = [pd.read_csv(f) for f in files]
    result_df = pd.concat(dfs, ignore_index=True)
    
    output_file = os.path.join(HOST_DATA_DIR, {{ concatenated_output_file | repr }})
    result_df.to_csv(output_file, index=False)
    print(f"Concatenated {len(files)} files into {output_file}")

# ------------------------------------------------------------------
# DAG Definition
# ------------------------------------------------------------------
with DAG(
    dag_id={{ dag_id | repr }},
    default_args=default_args,
    description={{ description | repr }},
    schedule_interval=None,
    start_date=datetime({{ start_date.year }}, {{ start_date.month }}, {{ start_date.day }}),
    catchup=False,
    tags={{ tags | list | repr }},
) as dag:

    # -- Step 1: Split main dataset into chunks ------------------------------
    split_task = PythonOperator(
        task_id="split_dataset", 
        python_callable=split_dataset
    )

    # -- Step 2: Create parallel branches ------------------------------------
    {% for idx in range(1, num_parallel_branches + 1) %}
    # -------------------- BRANCH {{ idx }} --------------------
    {% for stage in branch_stages %}
    {{ stage.id }}_{{ idx }} = DockerOperator(
        task_id="{{ stage.id }}_{{ idx }}",
        image={{ stage.image | repr }},
        command={{ stage.command | format_command_list }},
        environment={{ stage.environment | format_environment_dict }} | {"BRANCH_IDX": "{{ idx }}"},
        network_mode={{ network_mode | repr }},
        mounts=[Mount(source=HOST_DATA_DIR, target=CONTAINER_DATA_DIR, type="bind")],
        docker_url="unix://var/run/docker.sock",
        auto_remove=True,
        mount_tmp_dir=False,
        force_pull=False,
        tty=True,
    )
    {% endfor %}

    # Chain tasks within branch {{ idx }}
    {% if branch_stages|length > 1 %}
    {% for i in range(branch_stages|length - 1) %}
    {{ branch_stages[i].id }}_{{ idx }} >> {{ branch_stages[i + 1].id }}_{{ idx }}
    {% endfor %}
    {% endif %}

    # Connect split to first task of branch {{ idx }}
    {% if branch_stages %}
    split_task >> {{ branch_stages[0].id }}_{{ idx }}
    {% endif %}

    {% endfor %}

    # -- Step 3: Wait for all branches --------------------------------------
    wait_for_files_task = PythonOperator(
        task_id="wait_for_files", 
        python_callable=wait_for_files
    )

    # -- Step 4: Concatenate outputs ----------------------------------------
    concatenate_task = PythonOperator(
        task_id="concatenate_results", 
        python_callable=concatenate_results
    )

    # Connect last task of each branch to wait task
    {% for idx in range(1, num_parallel_branches + 1) %}
    {% if branch_stages %}
    {{ branch_stages[-1].id }}_{{ idx }} >> wait_for_files_task
    {% endif %}
    {% endfor %}

    # Final dependency
    wait_for_files_task >> concatenate_task