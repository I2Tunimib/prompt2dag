{# scripts/utils/dag_internal_parallel_template.py.j2 #}
# Generated by: {{ script_name }}
# Source YAML: {{ source_yaml_filename }}
# TEMPLATE: dag_internal_parallel_template.py.j2
# Generated at: {{ generation_timestamp }}
# WARNING: Review carefully before execution.
# ==============================================================================

from __future__ import annotations

import os
import glob
import time
import pandas as pd
import json
from datetime import timedelta, datetime

from airflow import DAG
from airflow.providers.docker.operators.docker import DockerOperator
from airflow.operators.python import PythonOperator
from airflow.exceptions import AirflowException
from airflow.utils.helpers import chain
from docker.types import Mount

# ------------------------------------------------------------------
# Global Defaults
# ------------------------------------------------------------------
default_args = {
    "owner": "airflow", 
    "depends_on_past": False,
    "email_on_failure": False,
    "email_on_retry": False,
    "retries": {{ default_retries }},
    "retry_delay": timedelta(minutes={{ default_retry_delay }}),
}

# Host directory (bind-mounted) â€“ can be overridden at runtime
HOST_DATA_DIR = os.getenv("DATA_DIR", {{ host_data_dir | repr }})
CONTAINER_DATA_DIR = "/app/data"

# Internal parallelism configuration
{% for task_id, task_info in internal_parallel_tasks.items() %}
{{ task_id.upper() }}_PARALLEL_CHUNKS = int(os.getenv("{{ task_id.upper() }}_PARALLEL_CHUNKS", {{ task_info.num_chunks }}))
{% endfor %}

# ------------------------------------------------------------------
# Helper Python callables for internal parallelism
# ------------------------------------------------------------------
def split_for_internal_parallel(**context):
    """
    Split input JSON files into chunks for internal parallel processing.
    """
    task_id = context['task_id'].replace('_split', '')
    input_pattern = context.get('input_pattern', task_id + '_input_*.json')
    num_chunks = context.get('num_chunks', 2)
    output_prefix = context.get('output_prefix', task_id + '_chunk')
    
    # Find input files
    input_files = glob.glob(os.path.join(HOST_DATA_DIR, input_pattern))
    if not input_files:
        raise AirflowException(f"No input files found matching pattern: {input_pattern}")
    
    for input_file in input_files:
        print(f"Processing file for splitting: {input_file}")
        
        with open(input_file, 'r') as f:
            data = json.load(f)
        
        # Extract rows for splitting
        rows = data.get('rows', {})
        if not rows:
            print(f"No rows found in {input_file}, skipping...")
            continue
        
        # Split rows into chunks
        row_items = list(rows.items())
        chunk_size = max(1, len(row_items) // num_chunks)
        
        base_filename = os.path.splitext(os.path.basename(input_file))[0]
        
        for chunk_idx in range(num_chunks):
            start_idx = chunk_idx * chunk_size
            end_idx = len(row_items) if chunk_idx == num_chunks - 1 else (chunk_idx + 1) * chunk_size
            
            chunk_rows = dict(row_items[start_idx:end_idx])
            
            # Create chunk data with same structure
            chunk_data = {
                'table': data.get('table', {}),
                'columns': data.get('columns', {}),
                'rows': chunk_rows
            }
            
            # Save chunk
            chunk_filename = f"{output_prefix}_{base_filename}_{chunk_idx + 1}.json"
            chunk_path = os.path.join(HOST_DATA_DIR, chunk_filename)
            
            with open(chunk_path, 'w') as f:
                json.dump(chunk_data, f, indent=2)
            
            print(f"Created chunk: {chunk_path} with {len(chunk_rows)} rows")

def merge_internal_parallel_results(**context):
    """
    Merge results from internal parallel processing back into single files.
    """
    task_id = context['task_id'].replace('_merge', '')
    input_pattern = context.get('input_pattern', task_id + '_result_*_*.json')
    output_pattern = context.get('output_pattern', task_id + '_output_{}.json')
    
    # Find all result files
    result_files = glob.glob(os.path.join(HOST_DATA_DIR, input_pattern))
    if not result_files:
        raise AirflowException(f"No result files found matching pattern: {input_pattern}")
    
    # Group files by original file (before chunking)
    file_groups = {}
    for result_file in result_files:
        # Extract original filename from chunk filename
        # Pattern: {task_id}_result_{original_name}_{chunk_num}.json
        basename = os.path.basename(result_file)
        parts = basename.replace('.json', '').split('_')
        
        # Find the chunk number (last part that's a digit)
        chunk_num = None
        original_parts = []
        for i, part in enumerate(parts):
            if part.isdigit() and i == len(parts) - 1:
                chunk_num = int(part)
                original_parts = parts[:i]
                break
        
        if chunk_num is not None:
            original_name = '_'.join(original_parts[2:])  # Skip task_id and 'result'
            if original_name not in file_groups:
                file_groups[original_name] = []
            file_groups[original_name].append((chunk_num, result_file))
    
    # Merge each group
    for original_name, chunk_files in file_groups.items():
        # Sort by chunk number
        chunk_files.sort(key=lambda x: x[0])
        
        merged_data = None
        merged_rows = {}
        
        for chunk_num, chunk_file in chunk_files:
            print(f"Merging chunk {chunk_num}: {chunk_file}")
            
            with open(chunk_file, 'r') as f:
                chunk_data = json.load(f)
            
            if merged_data is None:
                merged_data = {
                    'table': chunk_data.get('table', {}),
                    'columns': chunk_data.get('columns', {}),
                    'rows': {}
                }
            
            # Merge rows
            chunk_rows = chunk_data.get('rows', {})
            merged_rows.update(chunk_rows)
        
        # Save merged result
        merged_data['rows'] = merged_rows
        output_filename = output_pattern.format(original_name)
        output_path = os.path.join(HOST_DATA_DIR, output_filename)
        
        with open(output_path, 'w') as f:
            json.dump(merged_data, f, indent=2)
        
        print(f"Merged {len(chunk_files)} chunks into: {output_path} with {len(merged_rows)} rows")

# ------------------------------------------------------------------
# DAG Definition
# ------------------------------------------------------------------
with DAG(
    dag_id={{ dag_id | repr }},
    default_args=default_args,
    description={{ description | repr }},
    schedule_interval=None,
    start_date=datetime({{ start_date.year }}, {{ start_date.month }}, {{ start_date.day }}),
    catchup=False,
    tags={{ tags | list | repr }},
) as dag:

    # ==========================================================================
    # Regular Sequential Tasks
    # ==========================================================================
    {% for task in sequential_tasks %}
    # Task: {{ task.task_id }} (Type: {{ task.component_type_id }})
    {{ task.task_id }} = DockerOperator(
        task_id={{ task.task_id | repr }},
        image={{ task.image | repr }},
        command={{ task.command | format_command_list }},
        environment={{ task.environment | format_environment_dict }},
        network_mode={{ task.network_mode | repr }},
        mounts=[Mount(source=HOST_DATA_DIR, target=CONTAINER_DATA_DIR, type='bind')],
        mount_tmp_dir={{ task.mount_tmp_dir | default(False) }},
        auto_remove={{ task.auto_remove | default(True) }},
        docker_url={{ task.docker_url | default('unix://var/run/docker.sock') | repr }},
        force_pull={{ task.force_pull | default(False) }},
        tty={{ task.tty | default(True) }},
    )
    {% endfor %}

    # ==========================================================================
    # Internal Parallel Task Blocks
    # ==========================================================================
    {% for task_id, task_info in internal_parallel_tasks.items() %}
    
    # -------------------- Internal Parallel Block: {{ task_id }} --------------------
    
    # Split task for {{ task_id }}
    {{ task_id }}_split = PythonOperator(
        task_id="{{ task_id }}_split",
        python_callable=split_for_internal_parallel,
        op_kwargs={
            'input_pattern': '{{ task_info.input_pattern }}',
            'num_chunks': {{ task_id.upper() }}_PARALLEL_CHUNKS,
            'output_prefix': '{{ task_id }}_chunk'
        }
    )
    
    # Parallel processing tasks for {{ task_id }}
    {% for chunk_idx in range(1, task_info.num_chunks + 1) %}
    {{ task_id }}_process_{{ chunk_idx }} = DockerOperator(
        task_id="{{ task_id }}_process_{{ chunk_idx }}",
        image={{ task_info.image | repr }},
        command={{ task_info.command | format_command_list }},
        environment={{ task_info.environment | format_environment_dict }} | {
            "CHUNK_INDEX": "{{ chunk_idx }}",
            "TOTAL_CHUNKS": str({{ task_id.upper() }}_PARALLEL_CHUNKS),
            "INPUT_PATTERN": "{{ task_id }}_chunk_*_{{ chunk_idx }}.json",
            "OUTPUT_PATTERN": "{{ task_id }}_result_*_{{ chunk_idx }}.json"
        },
        network_mode={{ task_info.network_mode | repr }},
        mounts=[Mount(source=HOST_DATA_DIR, target=CONTAINER_DATA_DIR, type='bind')],
        mount_tmp_dir=False,
        auto_remove=True,
        docker_url="unix://var/run/docker.sock",
        force_pull=False,
        tty=True,
    )
    {% endfor %}
    
    # Merge task for {{ task_id }}
    {{ task_id }}_merge = PythonOperator(
        task_id="{{ task_id }}_merge",
        python_callable=merge_internal_parallel_results,
        op_kwargs={
            'input_pattern': '{{ task_id }}_result_*_*.json',
            'output_pattern': '{{ task_info.output_pattern }}'
        }
    )
    
    # Dependencies within {{ task_id }} internal parallel block
    {% set process_tasks = [] %}
    {% for chunk_idx in range(1, task_info.num_chunks + 1) %}
    {% set _ = process_tasks.append(task_id + '_process_' + chunk_idx|string) %}
    {% endfor %}
    {{ task_id }}_split >> [{{ process_tasks | join(', ') }}]
    [{{ process_tasks | join(', ') }}] >> {{ task_id }}_merge
    
    {% endfor %}

    # ==========================================================================
    # Task Dependencies
    # ==========================================================================
    {% if dependencies %}
    # Define execution order between tasks and internal parallel blocks
    {% for src, dst in dependencies %}
    {% if src in internal_parallel_tasks and dst in internal_parallel_tasks %}
    {{ src }}_merge >> {{ dst }}_split
    {% elif src in internal_parallel_tasks %}
    {{ src }}_merge >> {{ dst }}
    {% elif dst in internal_parallel_tasks %}
    {{ src }} >> {{ dst }}_split
    {% else %}
    {{ src }} >> {{ dst }}
    {% endif %}
    {% endfor %}
    {% else %}
    # No explicit dependencies defined.
    {% endif %}