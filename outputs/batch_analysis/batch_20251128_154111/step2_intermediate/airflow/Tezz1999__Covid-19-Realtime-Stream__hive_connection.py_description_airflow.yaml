metadata:
  target_orchestrator: airflow
  generated_at: 2025-11-28 15:44:21.058490
  source_analysis_file: 
    Pipeline_Description_Dataset/Tezz1999__Covid-19-Realtime-Stream__hive_connection.py_description.txt
  pipeline_name: Comprehensive Pipeline Description
  pipeline_description: This is a simple linear data pipeline that executes Hive database operations for COVID-19 
    realtime streaming data. The pipeline follows a sequential topology pattern with two tasks executing in strict 
    order.
  orchestrator_specific: {}
schedule:
  enabled: true
  schedule_expression: 00 1 * * *
  start_date:
  end_date:
  timezone: UTC
  catchup: true
connections:
  - conn_id: hive_local_connection
    conn_type: generic
    description: Hive Local Database Connection
    config:
      host:
      port:
      protocol: jdbc
      database: mydb
      schema:
      base_path:
      base_url:
      bucket:
      queue_name:
tasks:
  - task_id: identify_user_context
    task_name: Identify User Context
    operator_class: BashOperator
    operator_module: airflow.operators.bash
    component_ref: identify_user_context
    config:
      bash_command:
        - echo
        - '`whoami`'
    upstream_task_ids: []
    trigger_rule: all_success
    retries: 0
    retry_delay_seconds: 0
    validation_warnings: []
  - task_id: execute_hive_script
    task_name: Execute Hive Script
    operator_class: DockerOperator
    operator_module: airflow.providers.docker.operators.docker
    component_ref: execute_hive_script
    config:
      image: apache/hive:latest
      environment: {}
      cpus: '1'
      mem_limit: 2Gi
      auto_remove: true
      docker_url: unix://var/run/docker.sock
    upstream_task_ids:
      - identify_user_context
    trigger_rule: all_success
    retries: 0
    retry_delay_seconds: 0
    validation_warnings:
      - Uses image default command - ensure Dockerfile has ENTRYPOINT/CMD
