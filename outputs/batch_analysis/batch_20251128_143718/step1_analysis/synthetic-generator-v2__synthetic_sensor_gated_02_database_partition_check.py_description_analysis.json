{
  "metadata": {
    "schema_version": "1.0",
    "analysis_timestamp": "2025-11-28T14:45:08.408755",
    "source_file": "Pipeline_Description_Dataset/synthetic-generator-v2__synthetic_sensor_gated_02_database_partition_check.py_description.txt",
    "llm_provider": "deepinfra",
    "llm_model": "Qwen3-Coder",
    "analysis_results": {
      "detected_patterns": [
        "sequential",
        "sensor_driven"
      ],
      "task_executors_used": [
        "python",
        "sql"
      ],
      "has_branching": false,
      "has_parallelism": false,
      "has_sensors": true,
      "total_components": 4,
      "complexity_score": "low"
    },
    "orchestrator_compatibility": {
      "airflow": {
        "supported": true,
        "confidence": "high",
        "notes": [
          "Sequential pattern fully supported",
          "Native sensor support"
        ]
      },
      "prefect": {
        "supported": true,
        "confidence": "high",
        "notes": [
          "Sequential flow with task dependencies",
          "wait_for() or custom sensors"
        ]
      },
      "dagster": {
        "supported": true,
        "confidence": "high",
        "notes": [
          "Op graph with dependencies",
          "Sensors via run_status_sensor"
        ]
      }
    },
    "validation_warnings": []
  },
  "pipeline_summary": {
    "name": "Database Partition Check ETL",
    "description": "A sensor-gated daily ETL pipeline that waits for database partition availability before extracting, transforming, and loading incremental orders data.",
    "flow_patterns": [
      "sequential",
      "sensor_driven"
    ],
    "task_executors": [
      "python",
      "sql"
    ],
    "complexity": "low"
  },
  "components": [
    {
      "id": "wait_partition_sensor",
      "name": "Wait for Partition Availability",
      "category": "Sensor",
      "description": "Waits for the daily partition in the orders table to exist before allowing downstream tasks to proceed. Uses reschedule mode to free up worker slots during wait periods.",
      "inputs": [
        "information_schema.partitions"
      ],
      "outputs": [
        "sensor_success_signal"
      ],
      "executor_type": "python",
      "executor_config": {
        "image": null,
        "command": null,
        "script_path": null,
        "entry_point": null,
        "environment": {},
        "resources": {},
        "network": null
      },
      "io_spec": [
        {
          "name": "partition_metadata",
          "direction": "input",
          "kind": "table",
          "format": "sql",
          "path_pattern": "information_schema.partitions",
          "connection_id": "database_conn"
        },
        {
          "name": "sensor_status",
          "direction": "output",
          "kind": "object",
          "format": "binary",
          "path_pattern": null,
          "connection_id": null
        }
      ],
      "upstream_policy": {
        "type": "none_failed",
        "description": "Proceeds immediately as this is the initial step in the pipeline",
        "timeout_seconds": null
      },
      "retry_policy": {
        "max_attempts": 2,
        "delay_seconds": 300,
        "exponential_backoff": false,
        "retry_on": [
          "timeout",
          "network_error"
        ]
      },
      "concurrency": {
        "supports_parallelism": false,
        "supports_dynamic_mapping": false,
        "map_over_param": null,
        "max_parallel_instances": null
      },
      "connections": [
        {
          "id": "database_conn",
          "type": "database",
          "purpose": "Access to information_schema and orders table for partition checking"
        }
      ],
      "datasets": {
        "consumes": [
          "orders_partition_metadata"
        ],
        "produces": []
      }
    },
    {
      "id": "extract_incremental_orders",
      "name": "Extract Incremental Orders",
      "category": "Extractor",
      "description": "Extracts new orders data from the daily partition of the orders table based on current date filter.",
      "inputs": [
        "orders_table"
      ],
      "outputs": [
        "raw_orders_data"
      ],
      "executor_type": "python",
      "executor_config": {
        "image": null,
        "command": null,
        "script_path": null,
        "entry_point": null,
        "environment": {},
        "resources": {},
        "network": null
      },
      "io_spec": [
        {
          "name": "source_orders",
          "direction": "input",
          "kind": "table",
          "format": "sql",
          "path_pattern": "orders",
          "connection_id": "database_conn"
        },
        {
          "name": "extracted_data",
          "direction": "output",
          "kind": "object",
          "format": "json",
          "path_pattern": null,
          "connection_id": null
        }
      ],
      "upstream_policy": {
        "type": "all_success",
        "description": "Requires successful completion of wait_partition_sensor to ensure partition exists",
        "timeout_seconds": null
      },
      "retry_policy": {
        "max_attempts": 2,
        "delay_seconds": 300,
        "exponential_backoff": false,
        "retry_on": [
          "timeout",
          "network_error"
        ]
      },
      "concurrency": {
        "supports_parallelism": false,
        "supports_dynamic_mapping": false,
        "map_over_param": null,
        "max_parallel_instances": null
      },
      "connections": [
        {
          "id": "database_conn",
          "type": "database",
          "purpose": "Access to source orders table"
        }
      ],
      "datasets": {
        "consumes": [
          "orders_partition"
        ],
        "produces": [
          "raw_orders_dataset"
        ]
      }
    },
    {
      "id": "transform_orders_data",
      "name": "Transform Orders Data",
      "category": "Transformer",
      "description": "Cleans and validates extracted orders data including customer names, addresses, amounts, and timestamps.",
      "inputs": [
        "raw_orders_data"
      ],
      "outputs": [
        "cleaned_validated_orders"
      ],
      "executor_type": "python",
      "executor_config": {
        "image": null,
        "command": null,
        "script_path": null,
        "entry_point": null,
        "environment": {},
        "resources": {},
        "network": null
      },
      "io_spec": [
        {
          "name": "raw_orders_input",
          "direction": "input",
          "kind": "object",
          "format": "json",
          "path_pattern": null,
          "connection_id": null
        },
        {
          "name": "transformed_output",
          "direction": "output",
          "kind": "object",
          "format": "json",
          "path_pattern": null,
          "connection_id": null
        }
      ],
      "upstream_policy": {
        "type": "all_success",
        "description": "Requires successful completion of extract_incremental_orders",
        "timeout_seconds": null
      },
      "retry_policy": {
        "max_attempts": 2,
        "delay_seconds": 300,
        "exponential_backoff": false,
        "retry_on": [
          "timeout",
          "network_error"
        ]
      },
      "concurrency": {
        "supports_parallelism": false,
        "supports_dynamic_mapping": false,
        "map_over_param": null,
        "max_parallel_instances": null
      },
      "connections": [],
      "datasets": {
        "consumes": [
          "raw_orders_dataset"
        ],
        "produces": [
          "cleaned_orders_dataset"
        ]
      }
    },
    {
      "id": "load_orders_warehouse",
      "name": "Load Orders to Warehouse",
      "category": "Loader",
      "description": "Loads transformed orders data into the fact_orders table in the target data warehouse.",
      "inputs": [
        "cleaned_validated_orders"
      ],
      "outputs": [
        "loaded_orders_records"
      ],
      "executor_type": "python",
      "executor_config": {
        "image": null,
        "command": null,
        "script_path": null,
        "entry_point": null,
        "environment": {},
        "resources": {},
        "network": null
      },
      "io_spec": [
        {
          "name": "transformed_orders",
          "direction": "input",
          "kind": "object",
          "format": "json",
          "path_pattern": null,
          "connection_id": null
        },
        {
          "name": "warehouse_target",
          "direction": "output",
          "kind": "table",
          "format": "sql",
          "path_pattern": "fact_orders",
          "connection_id": "warehouse_conn"
        }
      ],
      "upstream_policy": {
        "type": "all_success",
        "description": "Requires successful completion of transform_orders_data",
        "timeout_seconds": null
      },
      "retry_policy": {
        "max_attempts": 2,
        "delay_seconds": 300,
        "exponential_backoff": false,
        "retry_on": [
          "timeout",
          "network_error"
        ]
      },
      "concurrency": {
        "supports_parallelism": false,
        "supports_dynamic_mapping": false,
        "map_over_param": null,
        "max_parallel_instances": null
      },
      "connections": [
        {
          "id": "warehouse_conn",
          "type": "database",
          "purpose": "Connection to target data warehouse for loading fact_orders table"
        }
      ],
      "datasets": {
        "consumes": [
          "cleaned_orders_dataset"
        ],
        "produces": [
          "fact_orders_table"
        ]
      }
    }
  ],
  "flow_structure": {
    "pattern": "sensor_driven",
    "entry_points": [
      "wait_partition_sensor"
    ],
    "nodes": {
      "wait_partition_sensor": {
        "kind": "Sensor",
        "component_type_id": "wait_partition_sensor",
        "upstream_policy": {
          "type": "all_success",
          "timeout_seconds": null
        },
        "next_nodes": [
          "extract_incremental_orders"
        ],
        "sensor_config": {
          "sensor_type": "custom",
          "target": "information_schema.partitions for orders table daily partition",
          "poke_interval_seconds": 300,
          "timeout_seconds": 3600,
          "mode": "reschedule"
        },
        "branch_config": null,
        "parallel_config": null
      },
      "extract_incremental_orders": {
        "kind": "Task",
        "component_type_id": "extract_incremental_orders",
        "upstream_policy": {
          "type": "all_success",
          "timeout_seconds": null
        },
        "next_nodes": [
          "transform_orders_data"
        ],
        "branch_config": null,
        "sensor_config": null,
        "parallel_config": null
      },
      "transform_orders_data": {
        "kind": "Task",
        "component_type_id": "transform_orders_data",
        "upstream_policy": {
          "type": "all_success",
          "timeout_seconds": null
        },
        "next_nodes": [
          "load_orders_warehouse"
        ],
        "branch_config": null,
        "sensor_config": null,
        "parallel_config": null
      },
      "load_orders_warehouse": {
        "kind": "Task",
        "component_type_id": "load_orders_warehouse",
        "upstream_policy": {
          "type": "all_success",
          "timeout_seconds": null
        },
        "next_nodes": [],
        "branch_config": null,
        "sensor_config": null,
        "parallel_config": null
      }
    },
    "edges": [
      {
        "from": "wait_partition_sensor",
        "to": "extract_incremental_orders",
        "edge_type": "success"
      },
      {
        "from": "extract_incremental_orders",
        "to": "transform_orders_data",
        "edge_type": "success"
      },
      {
        "from": "transform_orders_data",
        "to": "load_orders_warehouse",
        "edge_type": "success"
      }
    ]
  },
  "parameters": {
    "pipeline": {
      "name": {
        "description": "Pipeline identifier",
        "type": "string",
        "default": "Database Partition Check ETL",
        "required": false,
        "constraints": null
      },
      "description": {
        "description": "Pipeline description",
        "type": "string",
        "default": "A sensor-gated daily ETL pipeline that waits for database partition availability before extracting, transforming, and loading incremental orders data.",
        "required": false,
        "constraints": null
      },
      "tags": {
        "description": "Classification tags",
        "type": "array",
        "default": [
          "sensor_gated",
          "daily",
          "etl",
          "database_partition"
        ],
        "required": false
      }
    },
    "schedule": {
      "enabled": {
        "description": "Whether pipeline runs on schedule",
        "type": "boolean",
        "default": true,
        "required": false
      },
      "cron_expression": {
        "description": "Cron or preset (e.g., @daily, 0 0 * * *)",
        "type": "string",
        "default": "@daily",
        "required": false
      },
      "start_date": {
        "description": "When to start scheduling",
        "type": "datetime",
        "default": "2024-01-01T00:00:00",
        "required": false,
        "format": "ISO8601"
      },
      "end_date": {
        "description": "When to stop scheduling",
        "type": "datetime",
        "default": null,
        "required": false
      },
      "timezone": {
        "description": "Schedule timezone",
        "type": "string",
        "default": null,
        "required": false
      },
      "catchup": {
        "description": "Run missed intervals",
        "type": "boolean",
        "default": false,
        "required": false
      },
      "batch_window": {
        "description": "Batch window parameter name (e.g., ds, execution_date)",
        "type": "string",
        "default": "ds",
        "required": false
      },
      "partitioning": {
        "description": "Data partitioning strategy (e.g., daily, hourly, monthly)",
        "type": "string",
        "default": "daily",
        "required": false
      }
    },
    "execution": {
      "max_active_runs": {
        "description": "Max concurrent pipeline runs",
        "type": "integer",
        "default": 1,
        "required": false
      },
      "timeout_seconds": {
        "description": "Pipeline execution timeout",
        "type": "integer",
        "default": 3600,
        "required": false
      },
      "retry_policy": {
        "description": "Pipeline-level retry behavior",
        "type": "object",
        "default": {
          "retries": 2,
          "retry_delay_minutes": 5
        },
        "required": false
      },
      "depends_on_past": {
        "description": "Whether execution depends on previous run success",
        "type": "boolean",
        "default": false,
        "required": false
      }
    },
    "components": {
      "wait_partition_sensor": {
        "conn_id": {
          "description": "Database connection identifier for SqlSensor",
          "type": "string",
          "default": "database_conn",
          "required": true,
          "constraints": "Must match an existing database connection"
        },
        "mode": {
          "description": "Sensor execution mode",
          "type": "string",
          "default": "reschedule",
          "required": false,
          "constraints": "Allowed values: reschedule, poke"
        },
        "timeout": {
          "description": "Sensor timeout in seconds",
          "type": "integer",
          "default": 3600,
          "required": false,
          "constraints": "Maximum time to wait for partition availability"
        },
        "poke_interval": {
          "description": "Interval between sensor checks in seconds",
          "type": "integer",
          "default": 300,
          "required": false,
          "constraints": "Time between consecutive sensor evaluations"
        }
      },
      "extract_incremental_orders": {
        "source_table": {
          "description": "Source table for incremental data extraction",
          "type": "string",
          "default": "orders",
          "required": false,
          "constraints": null
        },
        "partition_date_column": {
          "description": "Column used for partition date filtering",
          "type": "string",
          "default": "partition_date",
          "required": false,
          "constraints": null
        }
      },
      "transform_orders_data": {
        "validation_rules": {
          "description": "Data validation rules for transformation",
          "type": "object",
          "default": {
            "customer_name_required": true,
            "amount_positive": true,
            "timestamp_format": "ISO8601"
          },
          "required": false,
          "constraints": null
        }
      },
      "load_orders_warehouse": {
        "target_table": {
          "description": "Target table in data warehouse",
          "type": "string",
          "default": "fact_orders",
          "required": false,
          "constraints": null
        },
        "write_mode": {
          "description": "Data loading strategy",
          "type": "string",
          "default": "append",
          "required": false,
          "constraints": "Allowed values: append, overwrite, upsert"
        }
      }
    },
    "environment": {
      "DATABASE_CONN": {
        "description": "Database connection string for source system",
        "type": "string",
        "default": null,
        "required": true,
        "associated_component_id": "wait_partition_sensor"
      },
      "WAREHOUSE_CONN": {
        "description": "Connection string for target data warehouse",
        "type": "string",
        "default": null,
        "required": true,
        "associated_component_id": "load_orders_warehouse"
      }
    }
  },
  "integrations": {
    "connections": [
      {
        "id": "database_conn",
        "name": "Database Connection",
        "type": "database",
        "config": {
          "host": null,
          "port": null,
          "database": null,
          "schema": null,
          "base_path": null,
          "base_url": null,
          "protocol": "jdbc",
          "bucket": null,
          "queue_name": null
        },
        "authentication": {
          "type": "none",
          "token_env_var": null,
          "username_env_var": null,
          "password_env_var": null,
          "credentials_path": null
        },
        "used_by_components": [
          "wait_partition_sensor",
          "extract_incremental_orders"
        ],
        "direction": "input",
        "rate_limit": {
          "requests_per_second": null,
          "burst": null
        },
        "datasets": {
          "produces": [],
          "consumes": [
            "information_schema.partitions",
            "orders"
          ]
        }
      },
      {
        "id": "data_warehouse_target",
        "name": "Data Warehouse Target System",
        "type": "database",
        "config": {
          "host": null,
          "port": null,
          "database": null,
          "schema": null,
          "base_path": null,
          "base_url": null,
          "protocol": "jdbc",
          "bucket": null,
          "queue_name": null
        },
        "authentication": {
          "type": "none",
          "token_env_var": null,
          "username_env_var": null,
          "password_env_var": null,
          "credentials_path": null
        },
        "used_by_components": [
          "load_orders_warehouse"
        ],
        "direction": "output",
        "rate_limit": {
          "requests_per_second": null,
          "burst": null
        },
        "datasets": {
          "produces": [
            "fact_orders"
          ],
          "consumes": []
        }
      }
    ],
    "data_lineage": {
      "sources": [
        "Database orders table with daily partitioning strategy",
        "information_schema.partitions system table for metadata checks"
      ],
      "sinks": [
        "fact_orders table in target data warehouse"
      ],
      "intermediate_datasets": [
        "extracted_incremental_orders",
        "transformed_validated_orders"
      ]
    }
  }
}