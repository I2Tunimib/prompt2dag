{
  "metadata": {
    "schema_version": "1.0",
    "analysis_timestamp": "2025-11-28T14:43:52.577894",
    "source_file": "Pipeline_Description_Dataset/synthetic-generator-v2__synthetic_fan_out_only_01_data_replication_to_environments.py_description.txt",
    "llm_provider": "deepinfra",
    "llm_model": "Qwen3-Coder",
    "analysis_results": {
      "detected_patterns": [
        "sequential",
        "parallel"
      ],
      "task_executors_used": [
        "bash"
      ],
      "has_branching": false,
      "has_parallelism": true,
      "has_sensors": false,
      "total_components": 4,
      "complexity_score": "low"
    },
    "orchestrator_compatibility": {
      "airflow": {
        "supported": true,
        "confidence": "high",
        "notes": [
          "Sequential pattern fully supported",
          "Parallelism via TaskFlow API expand()"
        ]
      },
      "prefect": {
        "supported": true,
        "confidence": "high",
        "notes": [
          "Sequential flow with task dependencies",
          "map() for parallel execution"
        ]
      },
      "dagster": {
        "supported": true,
        "confidence": "high",
        "notes": [
          "Op graph with dependencies",
          "DynamicOutput for fan-out"
        ]
      }
    },
    "validation_warnings": []
  },
  "pipeline_summary": {
    "name": "Database Replication Fanout",
    "description": "Performs daily database replication from production to multiple environments by first dumping the production database to a CSV snapshot, then copying that snapshot to three target environments (Development, Staging, QA) in parallel.",
    "flow_patterns": [
      "sequential",
      "parallel"
    ],
    "task_executors": [
      "bash"
    ],
    "complexity": "low"
  },
  "components": [
    {
      "id": "dump_prod_csv",
      "name": "Dump Production Database to CSV",
      "category": "Extractor",
      "description": "Creates a CSV snapshot of the production database for replication to downstream environments. Uses image default entrypoint/command",
      "inputs": [
        "Production database"
      ],
      "outputs": [
        "/tmp/prod_snapshot_$(date +%Y%m%d).csv"
      ],
      "executor_type": "docker",
      "executor_config": {
        "image": "bash:latest",
        "command": null,
        "script_path": null,
        "entry_point": null,
        "environment": {},
        "resources": {
          "cpu": "1",
          "memory": "512Mi",
          "gpu": null
        },
        "network": null
      },
      "io_spec": [
        {
          "name": "production_db",
          "direction": "input",
          "kind": "table",
          "format": "other",
          "path_pattern": null,
          "connection_id": null
        },
        {
          "name": "csv_snapshot",
          "direction": "output",
          "kind": "file",
          "format": "csv",
          "path_pattern": "/tmp/prod_snapshot_$(date +%Y%m%d).csv",
          "connection_id": null
        }
      ],
      "upstream_policy": {
        "type": "none_failed",
        "description": "Runs immediately as the initial task in the pipeline",
        "timeout_seconds": null
      },
      "retry_policy": {
        "max_attempts": 2,
        "delay_seconds": 300,
        "exponential_backoff": false,
        "retry_on": [
          "timeout",
          "system_error"
        ]
      },
      "concurrency": {
        "supports_parallelism": false,
        "supports_dynamic_mapping": false,
        "map_over_param": null,
        "max_parallel_instances": null
      },
      "connections": [],
      "datasets": {
        "consumes": [
          "production_database"
        ],
        "produces": [
          "prod_csv_snapshot"
        ]
      }
    },
    {
      "id": "copy_dev",
      "name": "Copy CSV to Development Environment",
      "category": "Loader",
      "description": "Loads the production CSV snapshot into the Development environment database. Uses image default entrypoint/command",
      "inputs": [
        "/tmp/prod_snapshot_$(date +%Y%m%d).csv"
      ],
      "outputs": [
        "Dev_DB"
      ],
      "executor_type": "docker",
      "executor_config": {
        "image": "bash:latest",
        "command": null,
        "script_path": null,
        "entry_point": null,
        "environment": {},
        "resources": {
          "cpu": "1",
          "memory": "512Mi",
          "gpu": null
        },
        "network": null
      },
      "io_spec": [
        {
          "name": "csv_input",
          "direction": "input",
          "kind": "file",
          "format": "csv",
          "path_pattern": "/tmp/prod_snapshot_$(date +%Y%m%d).csv",
          "connection_id": null
        },
        {
          "name": "dev_database",
          "direction": "output",
          "kind": "table",
          "format": "other",
          "path_pattern": null,
          "connection_id": null
        }
      ],
      "upstream_policy": {
        "type": "all_success",
        "description": "Requires successful completion of dump_prod_csv",
        "timeout_seconds": null
      },
      "retry_policy": {
        "max_attempts": 2,
        "delay_seconds": 300,
        "exponential_backoff": false,
        "retry_on": [
          "timeout",
          "system_error"
        ]
      },
      "concurrency": {
        "supports_parallelism": false,
        "supports_dynamic_mapping": false,
        "map_over_param": null,
        "max_parallel_instances": null
      },
      "connections": [],
      "datasets": {
        "consumes": [
          "prod_csv_snapshot"
        ],
        "produces": [
          "development_database"
        ]
      }
    },
    {
      "id": "copy_staging",
      "name": "Copy CSV to Staging Environment",
      "category": "Loader",
      "description": "Loads the production CSV snapshot into the Staging environment database. Uses image default entrypoint/command",
      "inputs": [
        "/tmp/prod_snapshot_$(date +%Y%m%d).csv"
      ],
      "outputs": [
        "Staging_DB"
      ],
      "executor_type": "docker",
      "executor_config": {
        "image": "bash:latest",
        "command": null,
        "script_path": null,
        "entry_point": null,
        "environment": {},
        "resources": {
          "cpu": "1",
          "memory": "512Mi",
          "gpu": null
        },
        "network": null
      },
      "io_spec": [
        {
          "name": "csv_input",
          "direction": "input",
          "kind": "file",
          "format": "csv",
          "path_pattern": "/tmp/prod_snapshot_$(date +%Y%m%d).csv",
          "connection_id": null
        },
        {
          "name": "staging_database",
          "direction": "output",
          "kind": "table",
          "format": "other",
          "path_pattern": null,
          "connection_id": null
        }
      ],
      "upstream_policy": {
        "type": "all_success",
        "description": "Requires successful completion of dump_prod_csv",
        "timeout_seconds": null
      },
      "retry_policy": {
        "max_attempts": 2,
        "delay_seconds": 300,
        "exponential_backoff": false,
        "retry_on": [
          "timeout",
          "system_error"
        ]
      },
      "concurrency": {
        "supports_parallelism": false,
        "supports_dynamic_mapping": false,
        "map_over_param": null,
        "max_parallel_instances": null
      },
      "connections": [],
      "datasets": {
        "consumes": [
          "prod_csv_snapshot"
        ],
        "produces": [
          "staging_database"
        ]
      }
    },
    {
      "id": "copy_qa",
      "name": "Copy CSV to QA Environment",
      "category": "Loader",
      "description": "Loads the production CSV snapshot into the QA environment database. Uses image default entrypoint/command",
      "inputs": [
        "/tmp/prod_snapshot_$(date +%Y%m%d).csv"
      ],
      "outputs": [
        "QA_DB"
      ],
      "executor_type": "docker",
      "executor_config": {
        "image": "bash:latest",
        "command": null,
        "script_path": null,
        "entry_point": null,
        "environment": {},
        "resources": {
          "cpu": "1",
          "memory": "512Mi",
          "gpu": null
        },
        "network": null
      },
      "io_spec": [
        {
          "name": "csv_input",
          "direction": "input",
          "kind": "file",
          "format": "csv",
          "path_pattern": "/tmp/prod_snapshot_$(date +%Y%m%d).csv",
          "connection_id": null
        },
        {
          "name": "qa_database",
          "direction": "output",
          "kind": "table",
          "format": "other",
          "path_pattern": null,
          "connection_id": null
        }
      ],
      "upstream_policy": {
        "type": "all_success",
        "description": "Requires successful completion of dump_prod_csv",
        "timeout_seconds": null
      },
      "retry_policy": {
        "max_attempts": 2,
        "delay_seconds": 300,
        "exponential_backoff": false,
        "retry_on": [
          "timeout",
          "system_error"
        ]
      },
      "concurrency": {
        "supports_parallelism": false,
        "supports_dynamic_mapping": false,
        "map_over_param": null,
        "max_parallel_instances": null
      },
      "connections": [],
      "datasets": {
        "consumes": [
          "prod_csv_snapshot"
        ],
        "produces": [
          "qa_database"
        ]
      }
    }
  ],
  "flow_structure": {
    "pattern": "parallel",
    "entry_points": [
      "dump_prod_csv"
    ],
    "nodes": {
      "dump_prod_csv": {
        "kind": "Task",
        "component_type_id": "dump_prod_csv",
        "upstream_policy": {
          "type": "all_success",
          "timeout_seconds": null
        },
        "next_nodes": [
          "copy_dev",
          "copy_staging",
          "copy_qa"
        ],
        "branch_config": null,
        "sensor_config": null,
        "parallel_config": null
      },
      "copy_dev": {
        "kind": "Task",
        "component_type_id": "copy_dev",
        "upstream_policy": {
          "type": "all_success",
          "timeout_seconds": null
        },
        "next_nodes": [],
        "branch_config": null,
        "sensor_config": null,
        "parallel_config": null
      },
      "copy_staging": {
        "kind": "Task",
        "component_type_id": "copy_staging",
        "upstream_policy": {
          "type": "all_success",
          "timeout_seconds": null
        },
        "next_nodes": [],
        "branch_config": null,
        "sensor_config": null,
        "parallel_config": null
      },
      "copy_qa": {
        "kind": "Task",
        "component_type_id": "copy_qa",
        "upstream_policy": {
          "type": "all_success",
          "timeout_seconds": null
        },
        "next_nodes": [],
        "branch_config": null,
        "sensor_config": null,
        "parallel_config": null
      }
    },
    "edges": [
      {
        "from": "dump_prod_csv",
        "to": "copy_dev",
        "edge_type": "success"
      },
      {
        "from": "dump_prod_csv",
        "to": "copy_staging",
        "edge_type": "success"
      },
      {
        "from": "dump_prod_csv",
        "to": "copy_qa",
        "edge_type": "success"
      }
    ]
  },
  "parameters": {
    "pipeline": {
      "name": {
        "description": "Pipeline identifier",
        "type": "string",
        "default": "Database Replication Fanout",
        "required": false,
        "constraints": null
      },
      "description": {
        "description": "Pipeline description",
        "type": "string",
        "default": "Performs daily database replication from production to multiple environments by first dumping the production database to a CSV snapshot, then copying that snapshot to three target environments (Development, Staging, QA) in parallel.",
        "required": false,
        "constraints": null
      },
      "tags": {
        "description": "Classification tags",
        "type": "array",
        "default": [
          "database",
          "replication",
          "fan-out",
          "daily"
        ],
        "required": false
      }
    },
    "schedule": {
      "enabled": {
        "description": "Whether pipeline runs on schedule",
        "type": "boolean",
        "default": true,
        "required": false
      },
      "cron_expression": {
        "description": "Cron or preset (e.g., @daily, 0 0 * * *)",
        "type": "string",
        "default": "@daily",
        "required": false
      },
      "start_date": {
        "description": "When to start scheduling",
        "type": "datetime",
        "default": "2024-01-01T00:00:00Z",
        "required": false,
        "format": "ISO8601"
      },
      "end_date": {
        "description": "When to stop scheduling",
        "type": "datetime",
        "default": null,
        "required": false
      },
      "timezone": {
        "description": "Schedule timezone",
        "type": "string",
        "default": null,
        "required": false
      },
      "catchup": {
        "description": "Run missed intervals",
        "type": "boolean",
        "default": false,
        "required": false
      },
      "batch_window": {
        "description": "Batch window parameter name (e.g., ds, execution_date)",
        "type": "string",
        "default": "ds",
        "required": false
      },
      "partitioning": {
        "description": "Data partitioning strategy (e.g., daily, hourly, monthly)",
        "type": "string",
        "default": "daily",
        "required": false
      }
    },
    "execution": {
      "max_active_runs": {
        "description": "Max concurrent pipeline runs",
        "type": "integer",
        "default": 1,
        "required": false
      },
      "timeout_seconds": {
        "description": "Pipeline execution timeout",
        "type": "integer",
        "default": 3600,
        "required": false
      },
      "retry_policy": {
        "description": "Pipeline-level retry behavior",
        "type": "object",
        "default": {
          "retries": 2,
          "retry_delay": 300
        },
        "required": false
      },
      "depends_on_past": {
        "description": "Whether execution depends on previous run success",
        "type": "boolean",
        "default": false,
        "required": false
      }
    },
    "components": {
      "dump_prod_csv": {
        "output_file_path": {
          "description": "Path to the CSV output file with date-based templating",
          "type": "string",
          "default": "/tmp/prod_snapshot_$(date +%Y%m%d).csv",
          "required": true,
          "constraints": "Must be a valid file path with date template"
        }
      },
      "copy_dev": {
        "input_file_path": {
          "description": "Path to the input CSV file from dump_prod_csv",
          "type": "string",
          "default": "/tmp/prod_snapshot_$(date +%Y%m%d).csv",
          "required": true,
          "constraints": "Must match the output path from dump_prod_csv"
        },
        "target_database": {
          "description": "Target database for the Dev environment",
          "type": "string",
          "default": "Dev_DB",
          "required": true,
          "constraints": null
        }
      },
      "copy_staging": {
        "input_file_path": {
          "description": "Path to the input CSV file from dump_prod_csv",
          "type": "string",
          "default": "/tmp/prod_snapshot_$(date +%Y%m%d).csv",
          "required": true,
          "constraints": "Must match the output path from dump_prod_csv"
        },
        "target_database": {
          "description": "Target database for the Staging environment",
          "type": "string",
          "default": "Staging_DB",
          "required": true,
          "constraints": null
        }
      },
      "copy_qa": {
        "input_file_path": {
          "description": "Path to the input CSV file from dump_prod_csv",
          "type": "string",
          "default": "/tmp/prod_snapshot_$(date +%Y%m%d).csv",
          "required": true,
          "constraints": "Must match the output path from dump_prod_csv"
        },
        "target_database": {
          "description": "Target database for the QA environment",
          "type": "string",
          "default": "QA_DB",
          "required": true,
          "constraints": null
        }
      }
    },
    "environment": {
      "PROD_DB_CONNECTION": {
        "description": "Connection string for the production database",
        "type": "string",
        "default": null,
        "required": false,
        "associated_component_id": "dump_prod_csv"
      },
      "DEV_DB_CONNECTION": {
        "description": "Connection string for the development database",
        "type": "string",
        "default": null,
        "required": false,
        "associated_component_id": "copy_dev"
      },
      "STAGING_DB_CONNECTION": {
        "description": "Connection string for the staging database",
        "type": "string",
        "default": null,
        "required": false,
        "associated_component_id": "copy_staging"
      },
      "QA_DB_CONNECTION": {
        "description": "Connection string for the QA database",
        "type": "string",
        "default": null,
        "required": false,
        "associated_component_id": "copy_qa"
      }
    }
  },
  "integrations": {
    "connections": [
      {
        "id": "local_filesystem",
        "name": "Local Filesystem Storage",
        "type": "filesystem",
        "config": {
          "base_path": "/tmp",
          "protocol": "file"
        },
        "authentication": {
          "type": "none"
        },
        "used_by_components": [
          "dump_prod_csv",
          "copy_dev",
          "copy_staging",
          "copy_qa"
        ],
        "direction": "both",
        "datasets": {
          "produces": [
            "prod_snapshot_*.csv"
          ],
          "consumes": [
            "prod_snapshot_*.csv"
          ]
        }
      },
      {
        "id": "dev_database",
        "name": "Development Environment Database",
        "type": "database",
        "config": {
          "host": "dev-db-host",
          "port": 5432,
          "database": "Dev_DB",
          "schema": "public"
        },
        "authentication": {
          "type": "basic",
          "username_env_var": "DEV_DB_USER",
          "password_env_var": "DEV_DB_PASSWORD"
        },
        "used_by_components": [
          "copy_dev"
        ],
        "direction": "output",
        "datasets": {
          "produces": [
            "dev_tables"
          ]
        }
      },
      {
        "id": "staging_database",
        "name": "Staging Environment Database",
        "type": "database",
        "config": {
          "host": "staging-db-host",
          "port": 5432,
          "database": "Staging_DB",
          "schema": "public"
        },
        "authentication": {
          "type": "basic",
          "username_env_var": "STAGING_DB_USER",
          "password_env_var": "STAGING_DB_PASSWORD"
        },
        "used_by_components": [
          "copy_staging"
        ],
        "direction": "output",
        "datasets": {
          "produces": [
            "staging_tables"
          ]
        }
      },
      {
        "id": "qa_database",
        "name": "QA Environment Database",
        "type": "database",
        "config": {
          "host": "qa-db-host",
          "port": 5432,
          "database": "QA_DB",
          "schema": "public"
        },
        "authentication": {
          "type": "basic",
          "username_env_var": "QA_DB_USER",
          "password_env_var": "QA_DB_PASSWORD"
        },
        "used_by_components": [
          "copy_qa"
        ],
        "direction": "output",
        "datasets": {
          "produces": [
            "qa_tables"
          ]
        }
      },
      {
        "id": "production_database",
        "name": "Production Database",
        "type": "database",
        "config": {
          "host": "prod-db-host",
          "port": 5432,
          "database": "Production_DB",
          "schema": "public"
        },
        "authentication": {
          "type": "basic",
          "username_env_var": "PROD_DB_USER",
          "password_env_var": "PROD_DB_PASSWORD"
        },
        "used_by_components": [
          "dump_prod_csv"
        ],
        "direction": "input",
        "datasets": {
          "consumes": [
            "production_tables"
          ]
        }
      }
    ],
    "data_lineage": {
      "sources": [
        "Production database containing current operational data"
      ],
      "sinks": [
        "Development environment database (Dev_DB)",
        "Staging environment database (Staging_DB)",
        "QA environment database (QA_DB)"
      ],
      "intermediate_datasets": [
        "/tmp/prod_snapshot_*.csv"
      ]
    }
  }
}