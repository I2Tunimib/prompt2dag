metadata:
  target_orchestrator: airflow
  generated_at: 2025-11-28 12:31:09.826972
  source_analysis_file: 
    Pipeline_Description_Dataset/Ferlab-Ste-Justine__clin-pipelines-dags__etl_import_ensembl.py_description.txt
  pipeline_name: etl_import_ensembl
  pipeline_description: Comprehensive ETL pipeline to import genomic mapping data from Ensembl's FTP server to an S3 
    data lake and process it with Spark
  orchestrator_specific: {}
schedule:
  enabled: false
  schedule_expression:
  start_date: '2022-01-01T00:00:00Z'
  end_date:
  timezone: UTC
  catchup: false
connections:
  - conn_id: ensembl_ftp_server
    conn_type: fs
    description: Ensembl FTP Server
    config:
      base_path: /pub/current_tsv/homo_sapiens
      base_url: ftp://ftp.ensembl.org/pub/current_tsv/homo_sapiens
      host: ftp.ensembl.org
      port: 21
      protocol: ftp
      database:
      schema:
      bucket:
      queue_name:
  - conn_id: s3_datalake_raw
    conn_type: s3
    description: S3 Data Lake Raw Landing Zone
    config:
      base_path:
      base_url:
      host:
      port:
      protocol: s3
      database:
      schema:
      bucket: cqgc-{env}-app-datalake
      queue_name:
  - conn_id: spark_kubernetes_etl
    conn_type: generic
    description: Spark on Kubernetes ETL Context
    config:
      base_path:
      base_url:
      host:
      port:
      protocol:
      database:
      schema:
      bucket:
      queue_name:
  - conn_id: slack_notifications
    conn_type: http
    description: Slack Notifications
    config:
      base_path:
      base_url: https://hooks.slack.com
      host: hooks.slack.com
      port: 443
      protocol: https
      database:
      schema:
      bucket:
      queue_name:
      extra:
        token_env_var: SLACK_WEBHOOK_TOKEN
tasks:
  - task_id: extract_ensembl_files
    task_name: Extract Ensembl Files
    operator_class: DockerOperator
    operator_module: airflow.providers.docker.operators.docker
    component_ref: extract_ensembl_files
    config:
      image: bio.ferlab.datalake.spark3.publictables.ImportPublicTable
      environment: {}
      cpus: '1'
      mem_limit: 2Gi
      auto_remove: true
      docker_url: unix://var/run/docker.sock
      retry_delay: timedelta(seconds=60)
    upstream_task_ids: []
    trigger_rule: all_success
    retries: 3
    retry_delay_seconds: 60
    validation_warnings:
      - Uses image default command - ensure Dockerfile has ENTRYPOINT/CMD
  - task_id: process_ensembl_tables
    task_name: Process Ensembl Tables
    operator_class: SparkSubmitOperator
    operator_module: airflow.providers.apache.spark.operators.spark_submit
    component_ref: process_ensembl_tables
    config:
      retry_delay: timedelta(seconds=120)
    upstream_task_ids:
      - extract_ensembl_files
    trigger_rule: all_success
    retries: 2
    retry_delay_seconds: 120
    validation_warnings: []
