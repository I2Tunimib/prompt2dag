# Generated by Dagster code generator
# Date: 2024-06-12
# Pipeline: etl_import_mondo
# Description: Comprehensive Pipeline Description

from typing import Any, Dict, List, Optional

from dagster import (
    In,
    Out,
    OpExecutionContext,
    RetryPolicy,
    ResourceDefinition,
    String,
    job,
    op,
    fs_io_manager,
    in_process_executor,
    ConfigurableResource,
)


# ----------------------------------------------------------------------
# Resource Definitions
# ----------------------------------------------------------------------


class GithubMondoReleasesResource(ConfigurableResource):
    """Resource for interacting with the GitHub Mondo releases API."""

    token: str
    repo: str = "mondo/ontology"

    def download_obo(self, destination_path: str) -> str:
        """Download the latest Mondo OBO file to the given destination path.

        Returns the local file path.
        """
        # Placeholder implementation – replace with actual GitHub API calls.
        with open(destination_path, "w") as f:
            f.write("# Mock OBO content")
        return destination_path


class S3DataLakeResource(ConfigurableResource):
    """Resource for writing files to the S3 data lake."""

    bucket_name: str

    def upload_file(self, local_path: str, s3_key: str) -> str:
        """Upload a local file to S3 and return the S3 URI."""
        # Placeholder implementation – replace with boto3 logic.
        return f"s3://{self.bucket_name}/{s3_key}"


class ElasticsearchMondoResource(ConfigurableResource):
    """Resource for indexing documents into the Mondo Elasticsearch index."""

    es_url: str
    index_name: str = "mondo"

    def bulk_index(self, documents: List[Dict[str, Any]]) -> int:
        """Index a list of documents and return the number of successfully indexed items."""
        # Placeholder implementation – replace with Elasticsearch client logic.
        return len(documents)


class SlackWebhookResource(ConfigurableResource):
    """Resource for sending messages to a Slack webhook."""

    webhook_url: str

    def post_message(self, text: str) -> None:
        """Post a simple text message to Slack."""
        # Placeholder implementation – replace with requests.post logic.
        print(f"Slack message sent: {text}")


class InternalPublishingApiResource(ConfigurableResource):
    """Resource for publishing datasets to an internal system."""

    api_endpoint: str
    api_key: str

    def publish_dataset(self, dataset_id: str, metadata: Dict[str, Any]) -> bool:
        """Publish a dataset and return success status."""
        # Placeholder implementation – replace with actual HTTP request logic.
        print(f"Published dataset {dataset_id} with metadata {metadata}")
        return True


# ----------------------------------------------------------------------
# Op Definitions
# ----------------------------------------------------------------------


@op(
    name="validate_color_parameter",
    description="Validate the supplied color parameter before proceeding.",
    out=Out(String),
    retry_policy=RetryPolicy(max_retries=0),
    required_resource_keys=set(),
)
def validate_color_parameter(context: OpExecutionContext, config: Dict[str, Any]) -> str:
    """
    Simple validation op that checks a ``color`` key in the job config.

    Returns the validated color string.
    """
    color = config.get("color")
    if not color:
        raise ValueError("Missing required 'color' configuration.")
    if color not in {"red", "green", "blue"}:
        raise ValueError(f"Unsupported color '{color}'. Allowed values are red, green, blue.")
    context.log.info(f"Validated color parameter: {color}")
    return color


@op(
    name="download_mondo_terms",
    description="Download the latest Mondo OBO file from GitHub.",
    ins={"color": In(String)},
    out=Out(String),
    required_resource_keys={"github_mondo_releases"},
    retry_policy=RetryPolicy(max_retries=0),
)
def download_mondo_terms(context: OpExecutionContext, color: str) -> str:
    """
    Downloads the Mondo OBO file using the ``github_mondo_releases`` resource.

    Returns the local path to the downloaded file.
    """
    github: GithubMondoReleasesResource = context.resources.github_mondo_releases
    destination = f"/tmp/mondo_{color}.obo"
    local_path = github.download_obo(destination_path=destination)
    context.log.info(f"Downloaded OBO file to {local_path}")
    return local_path


@op(
    name="normalize_mondo_terms",
    description="Normalize the raw Mondo OBO file into a structured JSON representation.",
    ins={"obo_path": In(String)},
    out=Out(List[Dict[str, Any]]),
    required_resource_keys={"s3_data_lake"},
    retry_policy=RetryPolicy(max_retries=0),
)
def normalize_mondo_terms(context: OpExecutionContext, obo_path: str) -> List[Dict[str, Any]]:
    """
    Reads the OBO file, performs normalization, and stores the result in S3.

    Returns a list of normalized term dictionaries.
    """
    # Placeholder parsing – replace with real OBO parsing logic.
    normalized_terms = [
        {"id": "MONDO:0000001", "label": "example disease", "definition": "An example definition."}
    ]

    s3: S3DataLakeResource = context.resources.s3_data_lake
    s3_key = "mondo/normalized_terms.json"
    local_tmp = "/tmp/normalized_terms.json"
    import json

    with open(local_tmp, "w") as f:
        json.dump(normalized_terms, f)

    s3_uri = s3.upload_file(local_path=local_tmp, s3_key=s3_key)
    context.log.info(f"Uploaded normalized terms to {s3_uri}")
    return normalized_terms


@op(
    name="index_mondo_terms",
    description="Index normalized Mondo terms into Elasticsearch.",
    ins={"terms": In(List[Dict[str, Any]])},
    out=Out(int),
    required_resource_keys={"elasticsearch_mondo"},
    retry_policy=RetryPolicy(max_retries=0),
)
def index_mondo_terms(context: OpExecutionContext, terms: List[Dict[str, Any]]) -> int:
    """
    Bulk indexes the provided terms into the Mondo Elasticsearch index.

    Returns the number of documents indexed.
    """
    es: ElasticsearchMondoResource = context.resources.elasticsearch_mondo
    indexed_count = es.bulk_index(documents=terms)
    context.log.info(f"Indexed {indexed_count} Mondo terms into Elasticsearch.")
    return indexed_count


@op(
    name="publish_mondo_data",
    description="Publish the indexed Mondo dataset to the internal publishing system.",
    ins={"indexed_count": In(int)},
    out=Out(bool),
    required_resource_keys={"internal_publishing_api"},
    retry_policy=RetryPolicy(max_retries=0),
)
def publish_mondo_data(context: OpExecutionContext, indexed_count: int) -> bool:
    """
    Notifies the internal publishing API that a new dataset version is ready.

    Returns ``True`` on successful publication.
    """
    publishing_api: InternalPublishingApiResource = context.resources.internal_publishing_api
    dataset_id = "mondo_dataset"
    metadata = {"record_count": indexed_count, "source": "GitHub Mondo OBO"}
    success = publishing_api.publish_dataset(dataset_id=dataset_id, metadata=metadata)
    if not success:
        raise RuntimeError("Failed to publish Mondo dataset.")
    context.log.info("Mondo dataset published successfully.")
    return success


@op(
    name="send_slack_notification",
    description="Send a Slack notification after successful pipeline execution.",
    ins={"publish_success": In(bool)},
    out=Out(None),
    required_resource_keys={"slack_webhook"},
    retry_policy=RetryPolicy(max_retries=0),
)
def send_slack_notification(context: OpExecutionContext, publish_success: bool) -> None:
    """
    Posts a message to Slack indicating the pipeline outcome.
    """
    slack: SlackWebhookResource = context.resources.slack_webhook
    if publish_success:
        message = ":white_check_mark: Mondo ETL pipeline completed successfully."
    else:
        message = ":x: Mondo ETL pipeline failed during publishing."
    slack.post_message(text=message)
    context.log.info("Slack notification sent.")


# ----------------------------------------------------------------------
# Job Definition
# ----------------------------------------------------------------------


@job(
    name="etl_import_mondo",
    description="Comprehensive Pipeline Description",
    executor_def=in_process_executor,
    resource_defs={
        "github_mondo_releases": GithubMondoReleasesResource(
            token="YOUR_GITHUB_TOKEN"
        ),
        "s3_data_lake": S3DataLakeResource(bucket_name="cqgc-dev-app-datalake"),
        "elasticsearch_mondo": ElasticsearchMondoResource(es_url="http://localhost:9200"),
        "slack_webhook": SlackWebhookResource(webhook_url="https://hooks.slack.com/services/XXX/YYY/ZZZ"),
        "internal_publishing_api": InternalPublishingApiResource(
            api_endpoint="https://internal.api/publish",
            api_key="YOUR_API_KEY",
        ),
        "io_manager": fs_io_manager,
    },
    config={
        "ops": {
            "validate_color_parameter": {
                "config": {"color": "green"}  # Example default; override at run time as needed.
            }
        }
    },
)
def etl_import_mondo():
    """
    Sequential ETL pipeline that validates a color parameter, downloads the Mondo OBO file,
    normalizes its contents, indexes the terms into Elasticsearch, publishes the dataset,
    and finally sends a Slack notification.
    """
    color = validate_color_parameter()
    obo_path = download_mondo_terms(color)
    normalized = normalize_mondo_terms(obo_path)
    indexed_cnt = index_mondo_terms(normalized)
    publish_ok = publish_mondo_data(indexed_cnt)
    send_slack_notification(publish_ok)


# ----------------------------------------------------------------------
# Schedule (disabled)
# ----------------------------------------------------------------------


# The schedule is intentionally disabled per specification.
# To enable, uncomment and adjust the cron schedule as needed.

# from dagster import ScheduleDefinition, default_executors

# schedule = ScheduleDefinition(
#     job=etl_import_mondo,
#     cron_schedule="0 0 * * *",  # Example: daily at midnight UTC
#     execution_timezone="UTC",
#     name="etl_import_mondo_schedule",
#     description="Scheduled execution of the Mondo ETL pipeline.",
#     default_status="inactive",  # Disabled by default
# )