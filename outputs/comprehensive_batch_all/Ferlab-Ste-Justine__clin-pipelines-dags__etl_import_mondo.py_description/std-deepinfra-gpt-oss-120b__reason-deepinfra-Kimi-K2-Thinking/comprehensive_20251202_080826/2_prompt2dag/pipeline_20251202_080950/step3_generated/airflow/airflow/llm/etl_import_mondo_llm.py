# Generated by Airflow DAG generator on 2024-06-13
"""
DAG: etl_import_mondo
Description: Comprehensive Pipeline Description
"""

from datetime import timedelta

from airflow import DAG
from airflow.utils.dates import days_ago
from airflow.decorators import task
from airflow.providers.http.hooks.http import HttpHook
from airflow.providers.http.operators.http import SimpleHttpOperator
from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator
from airflow.operators.python import PythonOperator
from airflow.providers.amazon.aws.hooks.s3 import S3Hook

# ----------------------------------------------------------------------
# Default arguments
# ----------------------------------------------------------------------
default_args = {
    "owner": "airflow",
    "depends_on_past": False,
    "retries": 0,
    "retry_delay": timedelta(minutes=5),
    "email_on_failure": False,
    "email_on_retry": False,
}


def _failure_callback(context):
    """
    Sends a Slack notification when a task fails.
    """
    slack_hook = HttpHook(http_conn_id="slack_webhook", method="POST")
    message = {
        "text": f":x: Task *{context['task_instance'].task_id}* failed.\n"
        f"Dag: *{context['dag'].dag_id}*\n"
        f"Execution: {context['execution_date']}",
    }
    slack_hook.run(endpoint="", json=message)


# ----------------------------------------------------------------------
# DAG definition
# ----------------------------------------------------------------------
with DAG(
    dag_id="etl_import_mondo",
    description="Comprehensive Pipeline Description",
    schedule_interval=None,  # Disabled schedule
    start_date=days_ago(1),
    catchup=False,
    default_args=default_args,
    tags=["etl", "mondo"],
    on_failure_callback=_failure_callback,
) as dag:

    # ------------------------------------------------------------------
    # Task: Validate Color Parameter
    # ------------------------------------------------------------------
    @task(retries=0, on_failure_callback=_failure_callback)
    def validate_color_parameter(**kwargs):
        """
        Validates that the 'color' parameter is present in the DAG run configuration.
        Raises an exception if validation fails.
        """
        dag_run = kwargs.get("dag_run")
        if not dag_run or not dag_run.conf:
            raise ValueError("No DAG run configuration found.")
        color = dag_run.conf.get("color")
        if not color:
            raise ValueError("Missing required parameter: 'color'")
        # Additional validation logic can be added here
        return color

    # ------------------------------------------------------------------
    # Task: Download Mondo OBO File
    # ------------------------------------------------------------------
    @task(retries=0, on_failure_callback=_failure_callback)
    def download_mondo_terms(**kwargs):
        """
        Downloads the latest Mondo OBO file from the GitHub releases endpoint
        and stores it in the S3 data lake.
        """
        # Use HttpHook to call the GitHub releases API
        http = HttpHook(http_conn_id="github_mondo_releases", method="GET")
        # Assuming the endpoint returns a direct download URL for the OBO file
        response = http.run(endpoint="/latest")  # Adjust endpoint as needed
        if response.status_code != 200:
            raise RuntimeError(
                f"Failed to fetch Mondo release info: {response.status_code}"
            )
        release_info = response.json()
        obo_url = release_info.get("assets", [{}])[0].get("browser_download_url")
        if not obo_url:
            raise RuntimeError("OBO download URL not found in release info.")

        # Download the OBO file
        file_response = http.run(endpoint=obo_url, extra_options={"allow_redirects": True})
        if file_response.status_code != 200:
            raise RuntimeError(f"Failed to download OBO file: {file_response.status_code}")

        # Store the file in S3
        s3 = S3Hook(aws_conn_id="s3_data_lake")
        bucket_name = s3.get_bucket_name()
        key = "mondo/raw/mondo.obo"
        s3.load_string(
            string_data=file_response.text,
            key=key,
            bucket_name=bucket_name,
            replace=True,
        )
        return f"s3://{bucket_name}/{key}"

    # ------------------------------------------------------------------
    # Task: Normalize Mondo Terms (Spark)
    # ------------------------------------------------------------------
    normalize_mondo_terms = SparkSubmitOperator(
        task_id="normalize_mondo_terms",
        application="/opt/airflow/dags/spark/normalize_mondo_terms.py",
        name="normalize_mondo_terms",
        conn_id="spark_default",
        conf={"spark.master": "local[*]"},
        application_args=[
            "--input",
            "s3://{{ var.value.s3_data_lake_bucket }}/mondo/raw/mondo.obo",
            "--output",
            "s3://{{ var.value.s3_data_lake_bucket }}/mondo/normalized/",
        ],
        retries=0,
        on_failure_callback=_failure_callback,
    )

    # ------------------------------------------------------------------
    # Task: Index Mondo Terms (Spark)
    # ------------------------------------------------------------------
    index_mondo_terms = SparkSubmitOperator(
        task_id="index_mondo_terms",
        application="/opt/airflow/dags/spark/index_mondo_terms.py",
        name="index_mondo_terms",
        conn_id="spark_default",
        conf={"spark.master": "local[*]"},
        application_args=[
            "--input",
            "s3://{{ var.value.s3_data_lake_bucket }}/mondo/normalized/",
            "--es-index",
            "mondo",
        ],
        retries=0,
        on_failure_callback=_failure_callback,
    )

    # ------------------------------------------------------------------
    # Task: Publish Mondo Dataset
    # ------------------------------------------------------------------
    @task(retries=0, on_failure_callback=_failure_callback)
    def publish_mondo_data(**kwargs):
        """
        Publishes the processed Mondo dataset to the internal publishing system.
        """
        http = HttpHook(http_conn_id="internal_publishing_api", method="POST")
        payload = {
            "dataset": "mondo",
            "status": "ready",
            "location": "s3://{{ var.value.s3_data_lake_bucket }}/mondo/normalized/",
        }
        response = http.run(endpoint="/publish", json=payload)
        if response.status_code not in (200, 201):
            raise RuntimeError(
                f"Publishing failed with status {response.status_code}: {response.text}"
            )
        return response.json()

    # ------------------------------------------------------------------
    # Task: Send Slack Notification
    # ------------------------------------------------------------------
    send_slack_notification = SimpleHttpOperator(
        task_id="send_slack_notification",
        http_conn_id="slack_webhook",
        endpoint="",
        method="POST",
        data='{"text": ":white_check_mark: Mondo ETL pipeline completed successfully."}',
        headers={"Content-Type": "application/json"},
        retries=0,
        on_failure_callback=_failure_callback,
    )

    # ------------------------------------------------------------------
    # Define task dependencies (sequential pattern)
    # ------------------------------------------------------------------
    color = validate_color_parameter()
    download_path = download_mondo_terms()
    color >> download_path

    download_path >> normalize_mondo_terms
    normalize_mondo_terms >> index_mondo_terms
    index_mondo_terms >> publish_mondo_data()
    publish_mondo_data() >> send_slack_notification

# End of DAG definition.