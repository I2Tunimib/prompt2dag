# Generated by Prefect Pipeline Generator
# Date: 2024-06-13
# Pipeline: etl_import_mondo
# Description: Comprehensive Pipeline Description
# Prefect version: 2.14.0

from __future__ import annotations

import json
import subprocess
from pathlib import Path
from typing import Any

from prefect import flow, task, get_run_logger
from prefect.task_runners import SequentialTaskRunner
from prefect.blocks.system import Secret
from prefect_aws.s3 import S3Bucket
from prefect.blocks.core import Block

# -------------------------------------------------------------------------
# Helper functions to retrieve secret and block resources
# -------------------------------------------------------------------------

def _get_secret(name: str) -> str:
    """Retrieve a secret value from a Prefect Secret block."""
    logger = get_run_logger()
    try:
        secret_block = Secret.load(name)
        value = secret_block.get()
        logger.debug(f"Secret '{name}' retrieved successfully.")
        return value
    except Exception as exc:
        logger.error(f"Failed to load secret '{name}': {exc}")
        raise


def _get_s3_bucket(name: str) -> S3Bucket:
    """Retrieve an S3Bucket block."""
    logger = get_run_logger()
    try:
        bucket = S3Bucket.load(name)
        logger.debug(f"S3 bucket block '{name}' loaded successfully.")
        return bucket
    except Exception as exc:
        logger.error(f"Failed to load S3 bucket block '{name}': {exc}")
        raise


# -------------------------------------------------------------------------
# Tasks
# -------------------------------------------------------------------------

@task(retries=0, name="Validate Color Parameter")
def validate_color_parameter(color: str | None = None) -> str:
    """
    Validate the ``color`` parameter used downstream.

    Args:
        color: Optional color string supplied by the user.

    Returns:
        A validated color string.

    Raises:
        ValueError: If the color is not one of the allowed values.
    """
    logger = get_run_logger()
    allowed_colors = {"red", "green", "blue", "yellow"}
    if color is None:
        logger.info("No color supplied; defaulting to 'green'.")
        return "green"
    if color.lower() not in allowed_colors:
        logger.error(f"Invalid color '{color}'. Allowed values: {allowed_colors}")
        raise ValueError(f"Invalid color '{color}'.")
    logger.info(f"Color '{color}' validated.")
    return color.lower()


@task(retries=0, name="Download Mondo OBO File")
def download_mondo_terms(github_token: str) -> Path:
    """
    Download the latest Mondo OBO file from the GitHub releases.

    Args:
        github_token: Personal access token for GitHub authentication.

    Returns:
        Path to the downloaded OBO file.

    Raises:
        RuntimeError: If the download command fails.
    """
    logger = get_run_logger()
    download_url = "https://api.github.com/repos/monarch-initiative/mondo/releases/latest"
    output_path = Path("/tmp/mondo.obo")

    try:
        logger.info("Fetching latest release metadata from GitHub.")
        curl_cmd = [
            "curl",
            "-H",
            f"Authorization: token {github_token}",
            "-L",
            "-s",
            "-o",
            str(output_path),
            "--fail",
            f"{download_url}"
        ]
        # First get the asset download URL
        meta_cmd = [
            "curl",
            "-H",
            f"Authorization: token {github_token}",
            "-s",
            download_url
        ]
        meta_output = subprocess.check_output(meta_cmd, text=True)
        release_info = json.loads(meta_output)
        assets = release_info.get("assets", [])
        obo_asset = next((a for a in assets if a["name"].endswith(".obo")), None)
        if not obo_asset:
            raise RuntimeError("No OBO asset found in the latest release.")
        asset_url = obo_asset["browser_download_url"]
        logger.info(f"Downloading OBO file from {asset_url}")
        subprocess.run(
            ["curl", "-L", "-H", f"Authorization: token {github_token}", "-o", str(output_path), asset_url],
            check=True,
        )
        logger.info(f"Mondo OBO file downloaded to {output_path}")
        return output_path
    except subprocess.CalledProcessError as exc:
        logger.error(f"Failed to download Mondo OBO file: {exc}")
        raise RuntimeError("Download failed.") from exc
    except Exception as exc:
        logger.error(f"Unexpected error during download: {exc}")
        raise


@task(retries=0, name="Normalize Mondo Terms")
def normalize_mondo_terms(obo_path: Path) -> Path:
    """
    Normalize the downloaded Mondo OBO file.

    This placeholder implementation simply copies the file to a new location
    to simulate a transformation step.

    Args:
        obo_path: Path to the raw OBO file.

    Returns:
        Path to the normalized OBO file.
    """
    logger = get_run_logger()
    normalized_path = Path("/tmp/mondo_normalized.obo")
    try:
        logger.info(f"Normalizing OBO file {obo_path}")
        # Placeholder for real normalization logic
        normalized_path.write_text(obo_path.read_text())
        logger.info(f"Normalized OBO file written to {normalized_path}")
        return normalized_path
    except Exception as exc:
        logger.error(f"Normalization failed: {exc}")
        raise


@task(retries=0, name="Index Mondo Terms")
def index_mondo_terms(
    normalized_path: Path,
    es_secret: str,
    index_name: str = "mondo"
) -> dict[str, Any]:
    """
    Index the normalized Mondo terms into Elasticsearch.

    Args:
        normalized_path: Path to the normalized OBO file.
        es_secret: Elasticsearch connection string or credentials.
        index_name: Name of the Elasticsearch index.

    Returns:
        Summary information about the indexing operation.
    """
    logger = get_run_logger()
    try:
        logger.info(f"Indexing terms from {normalized_path} into Elasticsearch index '{index_name}'.")
        # Placeholder: In a real implementation, parse the OBO file and bulk index.
        # Here we just simulate success.
        indexed_count = 12345  # dummy count
        logger.info(f"Indexed {indexed_count} documents into '{index_name}'.")
        return {"index": index_name, "documents_indexed": indexed_count}
    except Exception as exc:
        logger.error(f"Elasticsearch indexing failed: {exc}")
        raise


@task(retries=0, name="Publish Mondo Dataset")
def publish_mondo_data(
    s3_bucket_name: str,
    normalized_path: Path,
    publishing_api_secret: str
) -> str:
    """
    Publish the normalized Mondo dataset to the internal publishing system
    and store a copy in the data lake S3 bucket.

    Args:
        s3_bucket_name: Name of the Prefect S3Bucket block.
        normalized_path: Path to the normalized OBO file.
        publishing_api_secret: Secret token for the internal publishing API.

    Returns:
        URL or identifier of the published dataset.
    """
    logger = get_run_logger()
    try:
        # Upload to S3 Data Lake
        bucket = _get_s3_bucket(s3_bucket_name)
        s3_key = f"mondo/{normalized_path.name}"
        logger.info(f"Uploading normalized file to S3 bucket '{s3_bucket_name}' at key '{s3_key}'.")
        bucket.upload_from_path(local_path=str(normalized_path), key=s3_key)

        # Simulate publishing via internal API
        logger.info("Publishing dataset via internal publishing API.")
        # Placeholder for real API call
        published_id = "mondo-dataset-2024-06-13"
        logger.info(f"Dataset published with identifier '{published_id}'.")
        return published_id
    except Exception as exc:
        logger.error(f"Publishing failed: {exc}")
        raise


@task(retries=0, name="Send Slack Notification")
def send_slack_notification(
    slack_webhook_secret: str,
    message: str,
    channel: str = "#data-pipelines"
) -> None:
    """
    Send a notification to Slack indicating pipeline completion.

    Args:
        slack_webhook_secret: Webhook URL stored in a Prefect Secret block.
        message: Message payload to send.
        channel: Slack channel to post to (used for formatting only).
    """
    logger = get_run_logger()
    try:
        webhook_url = slack_webhook_secret
        payload = {"text": f"{message}\nChannel: {channel}"}
        logger.info("Sending Slack notification.")
        subprocess.run(
            ["curl", "-X", "POST", "-H", "Content-Type: application/json", "-d", json.dumps(payload), webhook_url],
            check=True,
        )
        logger.info("Slack notification sent successfully.")
    except subprocess.CalledProcessError as exc:
        logger.error(f"Failed to send Slack notification: {exc}")
        raise
    except Exception as exc:
        logger.error(f"Unexpected error while sending Slack notification: {exc}")
        raise


# -------------------------------------------------------------------------
# Flow definition
# -------------------------------------------------------------------------

@flow(
    name="etl_import_mondo",
    task_runner=SequentialTaskRunner(),
    description="Comprehensive Pipeline Description",
)
def etl_import_mondo(
    color: str | None = None,
) -> None:
    """
    Orchestrates the ETL import of Mondo terminology.

    The flow follows a strict sequential pattern:
    1. Validate the color parameter.
    2. Download the latest Mondo OBO file from GitHub.
    3. Normalize the OBO file.
    4. Index the normalized terms into Elasticsearch.
    5. Publish the dataset to the internal publishing system and S3 data lake.
    6. Send a Slack notification upon successful completion.

    Args:
        color: Optional color parameter used for downstream processing.
    """
    logger = get_run_logger()

    # Load secrets / blocks
    github_token = _get_secret("github_mondo_releases")
    es_secret = _get_secret("elasticsearch_mondo")
    slack_webhook = _get_secret("slack_webhook")
    publishing_api_secret = _get_secret("internal_publishing_api")
    s3_bucket_name = "s3_data_lake"  # Name of the S3Bucket block registered in Prefect

    # Step 1: Validate color
    validated_color = validate_color_parameter(color)

    # Step 2: Download OBO
    obo_path = download_mondo_terms(github_token)

    # Step 3: Normalize
    normalized_path = normalize_mondo_terms(obo_path)

    # Step 4: Index
    index_summary = index_mondo_terms(normalized_path, es_secret)

    # Step 5: Publish
    published_id = publish_mondo_data(s3_bucket_name, normalized_path, publishing_api_secret)

    # Step 6: Notify
    notification_msg = (
        f"âœ… ETL Import Mondo completed successfully.\n"
        f"Color: {validated_color}\n"
        f"Indexed: {index_summary['documents_indexed']} documents.\n"
        f"Published ID: {published_id}"
    )
    send_slack_notification(slack_webhook, notification_msg)

    logger.info("ETL pipeline 'etl_import_mondo' finished.")


# -------------------------------------------------------------------------
# Entry point for local execution
# -------------------------------------------------------------------------

if __name__ == "__main__":
    # The schedule is disabled; run manually when needed.
    etl_import_mondo()