# Generated by Dagster Code Generator
# Date: 2024-06-13
# Pipeline: etl_import_mondo
# Description: Comprehensive Pipeline Description
# Executor: in_process_executor
# Dagster version: 1.5.0

from dagster import (
    op,
    job,
    In,
    Out,
    Nothing,
    RetryPolicy,
    ResourceDefinition,
    fs_io_manager,
    ConfigurableResource,
    InitResourceContext,
    get_dagster_logger,
    ScheduleDefinition,
    ScheduleStatus,
    Definitions,
)

# ----------------------------------------------------------------------
# Resource placeholders
# ----------------------------------------------------------------------


class ElasticsearchResource(ConfigurableResource):
    """Placeholder for an Elasticsearch connection."""

    es_conn_id: str

    def get_client(self):
        # Replace with actual Elasticsearch client initialization
        return f"Elasticsearch client for conn_id={self.es_conn_id}"


class S3Resource(ConfigurableResource):
    """Placeholder for an S3 connection."""

    s3_conn_id: str

    def get_client(self):
        # Replace with actual S3 client initialization
        return f"S3 client for conn_id={self.s3_conn_id}"


class SlackWebhookResource(ConfigurableResource):
    """Placeholder for a Slack webhook."""

    slack_webhook_conn: str

    def post_message(self, text: str):
        # Replace with actual HTTP POST to Slack webhook
        logger = get_dagster_logger()
        logger.info(f"Posting to Slack webhook {self.slack_webhook_conn}: {text}")


# ----------------------------------------------------------------------
# Ops
# ----------------------------------------------------------------------


@op(
    name="Validate Parameters",
    description="Validate input parameters before proceeding with the ETL.",
    out=Out(dict, description="Validated parameters."),
    retry_policy=RetryPolicy(max_retries=0),
)
def validate_params(context) -> dict:
    """Validate parameters required for downstream tasks."""
    logger = context.log
    logger.info("Validating parameters...")
    # Placeholder validation logic
    params = {"valid": True}
    logger.debug(f"Parameters validated: {params}")
    return params


@op(
    name="Download Mondo OBO File",
    description="Download the Mondo OBO file from a remote source.",
    ins={"params": In(dict)},
    out=Out(str, description="Path to the downloaded OBO file."),
    retry_policy=RetryPolicy(max_retries=0),
    required_resource_keys={"s3_conn_id"},
)
def download_mondo_terms(context, params: dict) -> str:
    """Download the Mondo OBO file using the S3 resource."""
    logger = context.log
    logger.info("Downloading Mondo OBO file...")
    s3_client = context.resources.s3_conn_id.get_client()
    logger.debug(f"Using S3 client: {s3_client}")
    # Placeholder download logic
    file_path = "/tmp/mondo.obo"
    logger.info(f"Mondo OBO file downloaded to {file_path}")
    return file_path


@op(
    name="Normalize Mondo Terms",
    description="Normalize the downloaded Mondo terms.",
    ins={"obo_path": In(str)},
    out=Out(list, description="List of normalized term dictionaries."),
    retry_policy=RetryPolicy(max_retries=0),
)
def normalize_mondo_terms(context, obo_path: str) -> list:
    """Normalize terms from the OBO file."""
    logger = context.log
    logger.info(f"Normalizing Mondo terms from {obo_path}...")
    # Placeholder normalization logic
    normalized_terms = [{"id": "MONDO:0000001", "label": "example term"}]
    logger.debug(f"Normalized terms: {normalized_terms}")
    return normalized_terms


@op(
    name="Index Mondo Terms",
    description="Index normalized Mondo terms into Elasticsearch.",
    ins={"terms": In(list)},
    out=Out(Nothing, description="Signal that indexing is complete."),
    retry_policy=RetryPolicy(max_retries=0),
    required_resource_keys={"es_conn_id"},
)
def index_mondo_terms(context, terms: list) -> Nothing:
    """Index the normalized terms into Elasticsearch."""
    logger = context.log
    logger.info("Indexing Mondo terms into Elasticsearch...")
    es_client = context.resources.es_conn_id.get_client()
    logger.debug(f"Using Elasticsearch client: {es_client}")
    # Placeholder indexing logic
    for term in terms:
        logger.debug(f"Indexing term: {term}")
    logger.info("Indexing complete.")
    return Nothing


@op(
    name="Publish Mondo Dataset",
    description="Publish the indexed Mondo dataset to downstream consumers.",
    ins={"_:": In(Nothing)},
    out=Out(Nothing, description="Signal that publishing is complete."),
    retry_policy=RetryPolicy(max_retries=0),
)
def publish_mondo(context) -> Nothing:
    """Publish the dataset after successful indexing."""
    logger = context.log
    logger.info("Publishing Mondo dataset...")
    # Placeholder publishing logic
    logger.info("Publish complete.")
    return Nothing


@op(
    name="Send Slack Notification",
    description="Send a Slack notification upon pipeline completion.",
    ins={"_:": In(Nothing)},
    out=Out(Nothing, description="Signal that notification was sent."),
    retry_policy=RetryPolicy(max_retries=0),
    required_resource_keys={"slack_webhook_conn"},
)
def slack_notification(context) -> Nothing:
    """Notify stakeholders via Slack that the pipeline has finished."""
    logger = context.log
    logger.info("Sending Slack notification...")
    slack = context.resources.slack_webhook_conn
    slack.post_message("ETL import Mondo pipeline has completed successfully.")
    logger.info("Slack notification sent.")
    return Nothing


# ----------------------------------------------------------------------
# Job definition
# ----------------------------------------------------------------------


@job(
    name="etl_import_mondo",
    description="Comprehensive Pipeline Description",
    executor_def="in_process_executor",
    resource_defs={
        "es_conn_id": ElasticsearchResource,
        "s3_conn_id": S3Resource,
        "slack_webhook_conn": SlackWebhookResource,
        "io_manager": fs_io_manager,
    },
)
def etl_import_mondo_job():
    """Sequential ETL job that validates parameters, downloads, normalizes,
    indexes, publishes Mondo terms, and sends a Slack notification.
    """
    params = validate_params()
    obo_path = download_mondo_terms(params)
    terms = normalize_mondo_terms(obo_path)
    index_mondo_terms(terms)
    publish_mondo()
    slack_notification()


# ----------------------------------------------------------------------
# Schedule (disabled)
# ----------------------------------------------------------------------


def disabled_schedule():
    """Creates a disabled schedule for the ETL job."""
    return ScheduleDefinition(
        job=etl_import_mondo_job,
        cron_schedule=None,  # No cron expression
        name="etl_import_mondo_schedule",
        default_status=ScheduleStatus.INACTIVE,
        execution_timezone="UTC",
    )


# ----------------------------------------------------------------------
# Definitions (entry point for Dagster UI)
# ----------------------------------------------------------------------


defs = Definitions(
    jobs=[etl_import_mondo_job],
    schedules=[disabled_schedule()],
    resources={
        "es_conn_id": ElasticsearchResource(es_conn_id="my_es_conn"),
        "s3_conn_id": S3Resource(s3_conn_id="my_s3_conn"),
        "slack_webhook_conn": SlackWebhookResource(slack_webhook_conn="my_slack_webhook"),
    },
)