# Generated by Prefect Pipeline Generator
# Date: 2024-06-28
# Pipeline: etl_import_mondo
# Description: Comprehensive Pipeline Description
# Prefect version: 2.14.0
# Task runner: SequentialTaskRunner

import os
import subprocess
import sys
from pathlib import Path
from typing import Optional

import requests
from prefect import flow, task, get_run_logger
from prefect.task_runners import SequentialTaskRunner


@task(name="Validate Parameters", retries=0)
def validate_params() -> bool:
    """
    Validate input parameters for the ETL pipeline.

    Returns
    -------
    bool
        ``True`` if validation succeeds, otherwise raises an exception.
    """
    logger = get_run_logger()
    logger.info("Validating parameters...")

    # Placeholder for real validation logic.
    # Raise an exception if validation fails.
    # Example:
    # if not os.getenv("DATA_DIR"):
    #     raise ValueError("DATA_DIR environment variable is required")

    logger.info("Parameter validation completed successfully.")
    return True


@task(name="Download Mondo OBO File", retries=0)
def download_mondo_terms(valid: bool) -> Path:
    """
    Download the Mondo OBO file from the official source.

    Parameters
    ----------
    valid : bool
        Result from the ``validate_params`` task; ensures validation passed.

    Returns
    -------
    pathlib.Path
        Path to the downloaded OBO file.
    """
    logger = get_run_logger()
    logger.info("Starting download of Mondo OBO file...")

    if not valid:
        raise RuntimeError("Parameter validation failed; aborting download.")

    url = "https://github.com/monarch-initiative/mondo/releases/latest/download/mondo.obo"
    download_dir = Path("data") / "raw"
    download_dir.mkdir(parents=True, exist_ok=True)
    file_path = download_dir / "mondo.obo"

    try:
        response = requests.get(url, timeout=60)
        response.raise_for_status()
        file_path.write_bytes(response.content)
        logger.info(f"Downloaded Mondo OBO file to {file_path}")
    except Exception as exc:
        logger.error(f"Failed to download Mondo OBO file: {exc}")
        raise

    return file_path


@task(name="Normalize Mondo Terms", retries=0)
def normalize_mondo_terms(obo_path: Path) -> Path:
    """
    Normalize the downloaded Mondo terms (e.g., clean, reformat).

    Parameters
    ----------
    obo_path : pathlib.Path
        Path to the raw OBO file.

    Returns
    -------
    pathlib.Path
        Path to the normalized OBO file.
    """
    logger = get_run_logger()
    logger.info(f"Normalizing Mondo terms from {obo_path}")

    normalized_dir = Path("data") / "normalized"
    normalized_dir.mkdir(parents=True, exist_ok=True)
    normalized_path = normalized_dir / "mondo_normalized.obo"

    # Placeholder normalization: copy file (replace with real logic)
    try:
        subprocess.run(
            ["cp", str(obo_path), str(normalized_path)],
            check=True,
            capture_output=True,
        )
        logger.info(f"Normalized file written to {normalized_path}")
    except subprocess.CalledProcessError as exc:
        logger.error(f"Normalization failed: {exc.stderr.decode()}")
        raise

    return normalized_path


@task(name="Index Mondo Terms", retries=0)
def index_mondo_terms(normalized_path: Path) -> Path:
    """
    Create an index (e.g., Elasticsearch, SQLite) for the normalized terms.

    Parameters
    ----------
    normalized_path : pathlib.Path
        Path to the normalized OBO file.

    Returns
    -------
    pathlib.Path
        Path to the generated index file or directory.
    """
    logger = get_run_logger()
    logger.info(f"Indexing Mondo terms from {normalized_path}")

    index_dir = Path("data") / "index"
    index_dir.mkdir(parents=True, exist_ok=True)
    index_path = index_dir / "mondo_index.sqlite"

    # Placeholder indexing: create empty SQLite DB (replace with real indexing)
    try:
        subprocess.run(
            ["sqlite3", str(index_path), ".tables"],
            check=True,
            capture_output=True,
        )
        logger.info(f"Index created at {index_path}")
    except subprocess.CalledProcessError as exc:
        logger.error(f"Indexing failed: {exc.stderr.decode()}")
        raise

    return index_path


@task(name="Publish Mondo Dataset", retries=0)
def publish_mondo(index_path: Path) -> str:
    """
    Publish the indexed dataset to a data portal or storage bucket.

    Parameters
    ----------
    index_path : pathlib.Path
        Path to the indexed dataset.

    Returns
    -------
    str
        URL or identifier of the published dataset.
    """
    logger = get_run_logger()
    logger.info(f"Publishing dataset from {index_path}")

    # Placeholder publishing: pretend we upload and return a URL.
    # Replace with actual upload logic (e.g., S3, GCS, HTTP API).
    published_url = f"https://example.com/datasets/{index_path.name}"
    logger.info(f"Dataset published at {published_url}")

    return published_url


@task(name="Send Slack Notification", retries=0)
def slack_notification(published_url: str) -> None:
    """
    Send a Slack notification indicating successful pipeline completion.

    Parameters
    ----------
    published_url : str
        URL of the published dataset to include in the message.
    """
    logger = get_run_logger()
    logger.info("Sending Slack notification...")

    webhook_url = os.getenv("SLACK_WEBHOOK_URL")
    if not webhook_url:
        logger.warning("SLACK_WEBHOOK_URL not set; skipping Slack notification.")
        return

    message = {
        "text": f":white_check_mark: Mondo dataset has been published successfully: {published_url}"
    }

    try:
        response = requests.post(webhook_url, json=message, timeout=10)
        response.raise_for_status()
        logger.info("Slack notification sent.")
    except Exception as exc:
        logger.error(f"Failed to send Slack notification: {exc}")
        raise


@flow(
    name="etl_import_mondo",
    task_runner=SequentialTaskRunner(),
)
def etl_import_mondo() -> None:
    """
    Orchestrates the Mondo ETL pipeline.

    The flow follows a strict sequential pattern:
    1. Validate parameters
    2. Download the Mondo OBO file
    3. Normalize the terms
    4. Index the normalized terms
    5. Publish the indexed dataset
    6. Send a Slack notification
    """
    logger = get_run_logger()
    logger.info("Starting ETL pipeline: etl_import_mondo")

    validation = validate_params()
    obo_path = download_mondo_terms(validation)
    normalized_path = normalize_mondo_terms(obo_path)
    index_path = index_mondo_terms(normalized_path)
    published_url = publish_mondo(index_path)
    slack_notification(published_url)

    logger.info("ETL pipeline completed successfully.")


if __name__ == "__main__":
    # Running the flow directly (useful for local testing)
    try:
        etl_import_mondo()
    except Exception as exc:
        print(f"Pipeline execution failed: {exc}", file=sys.stderr)
        sys.exit(1)