# Generated by Prefect Pipeline Generator
# Date: 2024-06-28
# Prefect version: 2.14.0
# Flow name: etl_import_mondo

import json
import os
import subprocess
import sys
from pathlib import Path
from typing import Any, Dict

import requests
from elasticsearch import Elasticsearch, helpers
from prefect import flow, task, get_run_logger
from prefect.blocks.system import Secret
from prefect.filesystems import S3Bucket
from prefect.task_runners import SequentialTaskRunner

# -------------------------------------------------------------------------
# Helper functions
# -------------------------------------------------------------------------

def _load_secret(block_name: str) -> str:
    """
    Retrieve a secret value from a Prefect Secret block.

    Args:
        block_name: Name of the Prefect Secret block.

    Returns:
        The secret string.

    Raises:
        RuntimeError: If the secret block cannot be loaded.
    """
    try:
        secret_block = Secret.load(block_name)
        return secret_block.get()
    except Exception as exc:
        raise RuntimeError(f"Failed to load secret block '{block_name}': {exc}") from exc


def _load_s3_bucket(block_name: str) -> S3Bucket:
    """
    Load an S3Bucket block.

    Args:
        block_name: Name of the Prefect S3Bucket block.

    Returns:
        An S3Bucket instance.

    Raises:
        RuntimeError: If the block cannot be loaded.
    """
    try:
        return S3Bucket.load(block_name)
    except Exception as exc:
        raise RuntimeError(f"Failed to load S3 bucket block '{block_name}': {exc}") from exc


# -------------------------------------------------------------------------
# Tasks
# -------------------------------------------------------------------------

@task(retries=0, retry_delay_seconds=0)
def validate_params() -> Dict[str, Any]:
    """
    Validate pipeline parameters and environment configuration.

    Returns:
        A dictionary of validated parameters.

    Raises:
        ValueError: If required parameters are missing or invalid.
    """
    logger = get_run_logger()
    logger.info("Validating pipeline parameters...")

    # Example validation: ensure required environment variables are set
    required_env = ["ENV"]
    params = {}
    for var in required_env:
        value = os.getenv(var)
        if not value:
            raise ValueError(f"Environment variable '{var}' is required but not set.")
        params[var] = value

    logger.info("All required parameters are present.")
    return params


@task(retries=0, retry_delay_seconds=0)
def download_mondo_terms(params: Dict[str, Any]) -> Path:
    """
    Download the latest Mondo OBO file from the GitHub releases.

    Args:
        params: Dictionary of validated parameters (unused in this stub).

    Returns:
        Path to the downloaded OBO file.

    Raises:
        RuntimeError: If the download fails.
    """
    logger = get_run_logger()
    logger.info("Downloading Mondo OBO file...")

    github_token = _load_secret("github_mondo_releases")
    headers = {"Authorization": f"token {github_token}"}
    api_url = "https://api.github.com/repos/monarch-initiative/mondo/releases/latest"

    try:
        response = requests.get(api_url, headers=headers, timeout=30)
        response.raise_for_status()
        release_data = response.json()
        # Find the OBO asset
        obo_asset = next(
            (a for a in release_data["assets"] if a["name"].endswith(".obo")),
            None,
        )
        if not obo_asset:
            raise RuntimeError("No OBO asset found in the latest release.")
        download_url = obo_asset["browser_download_url"]
        logger.info(f"Downloading from {download_url}")

        obo_response = requests.get(download_url, headers=headers, stream=True, timeout=60)
        obo_response.raise_for_status()

        output_path = Path("data") / "mondo.obo"
        output_path.parent.mkdir(parents=True, exist_ok=True)

        with open(output_path, "wb") as f:
            for chunk in obo_response.iter_content(chunk_size=8192):
                if chunk:
                    f.write(chunk)

        logger.info(f"Mondo OBO file saved to {output_path}")
        return output_path
    except Exception as exc:
        raise RuntimeError(f"Failed to download Mondo OBO file: {exc}") from exc


@task(retries=0, retry_delay_seconds=0)
def normalize_mondo_terms(obo_path: Path) -> Path:
    """
    Normalize the downloaded Mondo OBO file into a JSONL format suitable for indexing.

    Args:
        obo_path: Path to the downloaded OBO file.

    Returns:
        Path to the normalized JSONL file.

    Raises:
        RuntimeError: If normalization fails.
    """
    logger = get_run_logger()
    logger.info(f"Normalizing Mondo terms from {obo_path}")

    normalized_path = obo_path.with_suffix(".jsonl")
    try:
        # Example normalization using a subprocess call to a Docker container
        # that runs the `mondo` CLI tool. Replace with actual command as needed.
        cmd = [
            "docker",
            "run",
            "--rm",
            "-v",
            f"{obo_path.parent.resolve()}:/data",
            "mondo/cli:latest",
            "convert",
            f"/data/{obo_path.name}",
            f"/data/{normalized_path.name}",
        ]
        logger.debug(f"Running command: {' '.join(cmd)}")
        subprocess.run(cmd, check=True, capture_output=True, text=True)
        logger.info(f"Normalized file written to {normalized_path}")
        return normalized_path
    except subprocess.CalledProcessError as exc:
        raise RuntimeError(f"Normalization subprocess failed: {exc.stderr}") from exc
    except Exception as exc:
        raise RuntimeError(f"Failed to normalize Mondo terms: {exc}") from exc


@task(retries=0, retry_delay_seconds=0)
def index_mondo_terms(jsonl_path: Path) -> None:
    """
    Index the normalized Mondo terms into Elasticsearch.

    Args:
        jsonl_path: Path to the normalized JSONL file.

    Raises:
        RuntimeError: If indexing fails.
    """
    logger = get_run_logger()
    logger.info(f"Indexing Mondo terms from {jsonl_path}")

    es_secret = _load_secret("elasticsearch_mondo")
    es_config = json.loads(es_secret)  # Expect JSON with host, username, password
    es = Elasticsearch(
        hosts=[es_config["host"]],
        http_auth=(es_config["username"], es_config["password"]),
        timeout=30,
    )
    index_name = "mondo"

    def _generate_actions():
        with open(jsonl_path, "r", encoding="utf-8") as f:
            for line in f:
                doc = json.loads(line)
                yield {
                    "_index": index_name,
                    "_id": doc.get("id"),
                    "_source": doc,
                }

    try:
        helpers.bulk(es, _generate_actions())
        logger.info(f"Successfully indexed documents into '{index_name}'.")
    except Exception as exc:
        raise RuntimeError(f"Failed to index Mondo terms: {exc}") from exc


@task(retries=0, retry_delay_seconds=0)
def publish_mondo(params: Dict[str, Any]) -> None:
    """
    Publish the Mondo dataset to the S3 data lake.

    Args:
        params: Dictionary of validated parameters (must contain 'ENV').

    Raises:
        RuntimeError: If publishing fails.
    """
    logger = get_run_logger()
    logger.info("Publishing Mondo dataset to S3 data lake...")

    env = params.get("ENV")
    if not env:
        raise RuntimeError("Environment parameter 'ENV' is missing.")

    bucket_block_name = f"cqgc-{env}-app-datalake"
    s3_bucket = _load_s3_bucket(bucket_block_name)

    # Example: upload the normalized JSONL file (assumes it exists in ./data)
    source_path = Path("data") / "mondo.jsonl"
    if not source_path.is_file():
        raise RuntimeError(f"Source file {source_path} does not exist.")

    destination_path = f"mondo/{source_path.name}"
    try:
        s3_bucket.upload_from_path(
            from_path=str(source_path),
            to_path=destination_path,
        )
        logger.info(f"Uploaded {source_path} to s3://{s3_bucket.bucket_name}/{destination_path}")
    except Exception as exc:
        raise RuntimeError(f"Failed to upload to S3: {exc}") from exc


@task(retries=0, retry_delay_seconds=0)
def notify_slack() -> None:
    """
    Send a Slack notification indicating successful pipeline completion.

    Raises:
        RuntimeError: If the Slack webhook call fails.
    """
    logger = get_run_logger()
    logger.info("Sending Slack notification...")

    webhook_url = _load_secret("slack_webhook")
    payload = {
        "text": ":white_check_mark: Mondo ETL pipeline completed successfully."
    }

    try:
        response = requests.post(webhook_url, json=payload, timeout=10)
        response.raise_for_status()
        logger.info("Slack notification sent.")
    except Exception as exc:
        raise RuntimeError(f"Failed to send Slack notification: {exc}") from exc


# -------------------------------------------------------------------------
# Flow definition
# -------------------------------------------------------------------------

@flow(
    name="etl_import_mondo",
    task_runner=SequentialTaskRunner(),
    description="Comprehensive Pipeline Description",
)
def etl_import_mondo_flow() -> None:
    """
    Orchestrates the Mondo ETL pipeline in a sequential manner.
    """
    logger = get_run_logger()
    logger.info("Starting ETL Import Mondo pipeline...")

    # Step 1: Validate parameters
    params = validate_params()

    # Step 2: Download OBO file
    obo_path = download_mondo_terms(params)

    # Step 3: Normalize terms
    jsonl_path = normalize_mondo_terms(obo_path)

    # Step 4: Index into Elasticsearch
    index_mondo_terms(jsonl_path)

    # Step 5: Publish to S3 data lake
    publish_mondo(params)

    # Step 6: Notify Slack
    notify_slack()

    logger.info("ETL Import Mondo pipeline completed successfully.")


# -------------------------------------------------------------------------
# Entry point
# -------------------------------------------------------------------------

if __name__ == "__main__":
    # Execute the flow directly (useful for local testing)
    etl_import_mondo_flow()


# -------------------------------------------------------------------------
# Deployment specification (optional)
# -------------------------------------------------------------------------

# The following block can be used to create a Prefect deployment via code.
# It is not executed automatically; run `prefect deployment build` or
# import this module in a deployment script.

"""
from prefect.deployments import DeploymentSpec
from prefect.infrastructure import Process

DeploymentSpec(
    name="etl_import_mondo_deployment",
    flow=etl_import_mondo_flow,
    schedule=None,  # Schedule disabled per specification
    work_pool_name="default-agent-pool",
    task_runner=SequentialTaskRunner(),
    infrastructure=Process(),
    tags=["etl", "mondo"],
)
"""