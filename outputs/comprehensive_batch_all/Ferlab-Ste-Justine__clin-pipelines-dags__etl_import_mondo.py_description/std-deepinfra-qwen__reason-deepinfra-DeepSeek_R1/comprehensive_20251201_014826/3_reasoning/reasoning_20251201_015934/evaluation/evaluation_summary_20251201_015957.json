{
  "evaluation_timestamp": "2025-12-01T01:59:57.399502",
  "evaluator_version": "2.0",
  "total_files": 1,
  "passed": 0,
  "failed": 1,
  "average_scores": {
    "static": 0.0,
    "compliance": 0.0,
    "combined": 0.0
  },
  "results": [
    {
      "file_path": "/Users/abubakarialidu/Desktop/Prompt2DAG/New_Experiments_2/outputs/comprehensive_batch_all/Ferlab-Ste-Justine__clin-pipelines-dags__etl_import_mondo.py_description/std-deepinfra-qwen__reason-deepinfra-DeepSeek_R1/comprehensive_20251201_014826/3_reasoning/reasoning_20251201_015934/generated_code/prefect_flow_20251201_015955.py",
      "orchestrator": "prefect",
      "evaluation_timestamp": "2025-12-01T01:59:57.399031",
      "static_analysis": {
        "dimensions": {
          "correctness": {
            "raw_score": 0.0,
            "penalties": 0.0,
            "final_score": 0.0,
            "issue_count": 1,
            "details": {}
          },
          "code_quality": {
            "raw_score": 0.0,
            "penalties": 0.0,
            "final_score": 0.0,
            "issue_count": 0,
            "details": {}
          },
          "best_practices": {
            "raw_score": 0.0,
            "penalties": 0.0,
            "final_score": 0.0,
            "issue_count": 0,
            "details": {}
          },
          "maintainability": {
            "raw_score": 0.0,
            "penalties": 0.0,
            "final_score": 0.0,
            "issue_count": 0,
            "details": {}
          },
          "robustness": {
            "raw_score": 0.0,
            "penalties": 0.0,
            "final_score": 0.0,
            "issue_count": 0,
            "details": {}
          }
        },
        "gates_passed": true,
        "gate_checks": [],
        "overall_score": 0.0,
        "grade": "F",
        "total_penalties": 0.0,
        "issues": [
          {
            "severity": "critical",
            "category": "syntax",
            "message": "Syntax error: invalid decimal literal",
            "line": 2,
            "column": null,
            "tool": null,
            "code": null,
            "penalty": 3.0
          }
        ],
        "critical_issues": 1
      },
      "platform_compliance": {
        "dimensions": {
          "loadability": {
            "raw_score": 0.0,
            "penalties": 0.0,
            "final_score": 0.0,
            "issue_count": 0,
            "details": {}
          },
          "structure_validity": {
            "raw_score": 0.0,
            "penalties": 0.0,
            "final_score": 0.0,
            "issue_count": 0,
            "details": {}
          },
          "configuration_validity": {
            "raw_score": 0.0,
            "penalties": 0.0,
            "final_score": 0.0,
            "issue_count": 0,
            "details": {}
          },
          "task_validity": {
            "raw_score": 0.0,
            "penalties": 0.0,
            "final_score": 0.0,
            "issue_count": 0,
            "details": {}
          },
          "executability": {
            "raw_score": 0.0,
            "penalties": 0.0,
            "final_score": 0.0,
            "issue_count": 0,
            "details": {}
          }
        },
        "gates_passed": false,
        "gate_checks": [
          {
            "name": "syntax_valid",
            "passed": false,
            "message": "Syntax error at line 2: invalid decimal literal",
            "is_critical": true
          }
        ],
        "overall_score": 0.0,
        "grade": "F",
        "total_penalties": 0.0,
        "issues": [],
        "critical_issues": 0
      },
      "summary": {
        "static_score": 0.0,
        "static_grade": "F",
        "compliance_score": 0.0,
        "compliance_grade": "F",
        "combined_score": 0.0,
        "combined_grade": "F",
        "passed": false,
        "total_penalties": 0.0,
        "total_issues": 1,
        "critical_issues": 1,
        "major_issues": 0,
        "minor_issues": 0,
        "gates_passed": false,
        "static_dimensions": {
          "correctness": 0.0,
          "code_quality": 0.0,
          "best_practices": 0.0,
          "maintainability": 0.0,
          "robustness": 0.0
        },
        "compliance_dimensions": {
          "loadability": 0.0,
          "structure_validity": 0.0,
          "configuration_validity": 0.0,
          "task_validity": 0.0,
          "executability": 0.0
        }
      },
      "output_file": "/Users/abubakarialidu/Desktop/Prompt2DAG/New_Experiments_2/outputs/comprehensive_batch_all/Ferlab-Ste-Justine__clin-pipelines-dags__etl_import_mondo.py_description/std-deepinfra-qwen__reason-deepinfra-DeepSeek_R1/comprehensive_20251201_014826/3_reasoning/reasoning_20251201_015934/evaluation/evaluation_prefect_flow_20251201_015955_20251201_015957.json"
    }
  ]
}