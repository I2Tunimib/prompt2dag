metadata:
  target_orchestrator: airflow
  generated_at: 2025-12-01 12:00:23.211657
  source_analysis_file: 
    Pipeline_Description_Dataset/synthetic-generator-v2__synthetic_fan_out_only_01_data_replication_to_environments.py_description.txt
  pipeline_name: dump_production_csv_pipeline
  pipeline_description: No description provided.
  orchestrator_specific: {}
schedule:
  enabled: true
  schedule_expression: '@daily'
  start_date: '2024-01-01T00:00:00+00:00'
  end_date:
  timezone: UTC
  catchup: false
connections:
  - conn_id: prod_database
    conn_type: generic
    description: Production Database
    config:
      base_path:
      base_url:
      host:
      port:
      protocol: jdbc
      database: production_db
      schema:
      bucket:
      queue_name:
  - conn_id: dev_database
    conn_type: generic
    description: Development Environment Database
    config:
      base_path:
      base_url:
      host:
      port:
      protocol: jdbc
      database: Dev_DB
      schema:
      bucket:
      queue_name:
  - conn_id: staging_database
    conn_type: generic
    description: Staging Environment Database
    config:
      base_path:
      base_url:
      host:
      port:
      protocol: jdbc
      database: Staging_DB
      schema:
      bucket:
      queue_name:
  - conn_id: qa_database
    conn_type: generic
    description: QA Environment Database
    config:
      base_path:
      base_url:
      host:
      port:
      protocol: jdbc
      database: QA_DB
      schema:
      bucket:
      queue_name:
  - conn_id: local_tmp_filesystem
    conn_type: fs
    description: Local /tmp Filesystem
    config:
      base_path: /tmp
      base_url:
      host:
      port:
      protocol: file
      database:
      schema:
      bucket:
      queue_name:
tasks:
  - task_id: dump_production_csv
    task_name: Dump Production Database to CSV
    operator_class: BashOperator
    operator_module: airflow.operators.bash
    component_ref: dump_production_csv
    config:
      retry_delay: timedelta(seconds=300)
    upstream_task_ids: []
    trigger_rule: all_success
    retries: 2
    retry_delay_seconds: 300
    validation_warnings: []
  - task_id: load_dev_database
    task_name: Load CSV Snapshot into Development Database
    operator_class: BashOperator
    operator_module: airflow.operators.bash
    component_ref: load_dev_database
    config:
      retry_delay: timedelta(seconds=300)
    upstream_task_ids:
      - dump_production_csv
    trigger_rule: all_success
    retries: 2
    retry_delay_seconds: 300
    validation_warnings: []
  - task_id: load_staging_database
    task_name: Load CSV Snapshot into Staging Database
    operator_class: BashOperator
    operator_module: airflow.operators.bash
    component_ref: load_staging_database
    config:
      retry_delay: timedelta(seconds=300)
    upstream_task_ids:
      - dump_production_csv
    trigger_rule: all_success
    retries: 2
    retry_delay_seconds: 300
    validation_warnings: []
  - task_id: load_qa_database
    task_name: Load CSV Snapshot into QA Database
    operator_class: BashOperator
    operator_module: airflow.operators.bash
    component_ref: load_qa_database
    config:
      retry_delay: timedelta(seconds=300)
    upstream_task_ids:
      - dump_production_csv
    trigger_rule: all_success
    retries: 2
    retry_delay_seconds: 300
    validation_warnings: []
