# Generated by Dagster Pipeline Generator
# Pipeline: dump_production_csv_pipeline
# Description: No description provided.
# Generated on: 2024-06-12

from __future__ import annotations

from pathlib import Path
from typing import Any, Dict

from dagster import (
    In,
    Nothing,
    Out,
    RetryPolicy,
    String,
    ResourceDefinition,
    fs_io_manager,
    job,
    op,
    multiprocess_executor,
    schedule,
)


# -------------------------------------------------------------------------
# Resource definitions (placeholders – replace with real implementations)
# -------------------------------------------------------------------------

def prod_database_resource() -> Dict[str, Any]:
    """Placeholder for a production database connection."""
    # Replace with actual connection logic, e.g., SQLAlchemy engine
    return {"name": "prod_database", "connection": "prod_connection_string"}


def dev_database_resource() -> Dict[str, Any]:
    """Placeholder for a development database connection."""
    return {"name": "dev_database", "connection": "dev_connection_string"}


def staging_database_resource() -> Dict[str, Any]:
    """Placeholder for a staging database connection."""
    return {"name": "staging_database", "connection": "staging_connection_string"}


def qa_database_resource() -> Dict[str, Any]:
    """Placeholder for a QA database connection."""
    return {"name": "qa_database", "connection": "qa_connection_string"}


# -------------------------------------------------------------------------
# Ops
# -------------------------------------------------------------------------

@op(
    name="dump_production_csv",
    description="Dump the production database to a CSV file stored in the local /tmp filesystem.",
    required_resource_keys={"prod_database", "local_tmp_filesystem"},
    out=Out(String, description="Path to the generated CSV file."),
    retry_policy=RetryPolicy(max_retries=2),
)
def dump_production_csv(context) -> str:
    """Extract data from the production database and write it to a CSV file."""
    prod_db = context.resources.prod_database
    io_manager = context.resources.local_tmp_filesystem

    # Placeholder logic – replace with actual query & CSV generation
    csv_path = Path("/tmp/production_snapshot.csv")
    context.log.info(f"Dumping data from {prod_db['name']} to {csv_path}")

    # Simulate CSV creation
    csv_path.parent.mkdir(parents=True, exist_ok=True)
    csv_path.write_text("id,name,value\n1,example,42\n")

    # Register the file with the IO manager (optional, depending on usage)
    return str(csv_path)


@op(
    name="load_dev_database",
    description="Load the CSV snapshot into the development database.",
    required_resource_keys={"dev_database", "local_tmp_filesystem"},
    ins={"csv_path": In(String)},
    out=Out(Nothing),
    retry_policy=RetryPolicy(max_retries=2),
)
def load_dev_database(context, csv_path: str) -> Nothing:
    """Read the CSV file and load its contents into the development database."""
    dev_db = context.resources.dev_database
    context.log.info(f"Loading CSV {csv_path} into {dev_db['name']}")

    # Placeholder logic – replace with actual CSV parsing & DB insertion
    # For example, using pandas.read_csv and SQLAlchemy bulk insert
    return Nothing


@op(
    name="load_qa_database",
    description="Load the CSV snapshot into the QA database.",
    required_resource_keys={"qa_database", "local_tmp_filesystem"},
    ins={"csv_path": In(String)},
    out=Out(Nothing),
    retry_policy=RetryPolicy(max_retries=2),
)
def load_qa_database(context, csv_path: str) -> Nothing:
    """Read the CSV file and load its contents into the QA database."""
    qa_db = context.resources.qa_database
    context.log.info(f"Loading CSV {csv_path} into {qa_db['name']}")
    return Nothing


@op(
    name="load_staging_database",
    description="Load the CSV snapshot into the staging database.",
    required_resource_keys={"staging_database", "local_tmp_filesystem"},
    ins={"csv_path": In(String)},
    out=Out(Nothing),
    retry_policy=RetryPolicy(max_retries=2),
)
def load_staging_database(context, csv_path: str) -> Nothing:
    """Read the CSV file and load its contents into the staging database."""
    staging_db = context.resources.staging_database
    context.log.info(f"Loading CSV {csv_path} into {staging_db['name']}")
    return Nothing


# -------------------------------------------------------------------------
# Job definition
# -------------------------------------------------------------------------

@job(
    name="dump_production_csv_pipeline",
    description="No description provided.",
    resource_defs={
        "prod_database": ResourceDefinition.resource_fn(prod_database_resource),
        "dev_database": ResourceDefinition.resource_fn(dev_database_resource),
        "staging_database": ResourceDefinition.resource_fn(staging_database_resource),
        "qa_database": ResourceDefinition.resource_fn(qa_database_resource),
        "local_tmp_filesystem": fs_io_manager,
    },
    executor_def=multiprocess_executor,
)
def dump_production_csv_pipeline():
    """Orchestrates dumping the production DB to CSV and loading it into dev, QA, and staging."""
    csv_path = dump_production_csv()
    load_dev_database(csv_path)
    load_qa_database(csv_path)
    load_staging_database(csv_path)


# -------------------------------------------------------------------------
# Schedule
# -------------------------------------------------------------------------

@schedule(
    cron_schedule="@daily",
    job=dump_production_csv_pipeline,
    execution_timezone="UTC",
    name="dump_production_csv_daily_schedule",
    description="Daily execution of the dump_production_csv_pipeline.",
    default_status="RUNNING",  # Enabled
    catchup=False,
)
def dump_production_csv_daily_schedule():
    """Schedule that triggers the pipeline once per day at midnight UTC."""
    return {}


# -------------------------------------------------------------------------
# End of file
# -------------------------------------------------------------------------