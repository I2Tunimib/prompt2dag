# Generated by Airflow DAG generator
# Pipeline: dump_production_csv_pipeline
# Description: No description provided.
# Generated on: 2024-06-25

from datetime import datetime, timedelta

from airflow import DAG
from airflow.providers.cncf.kubernetes.operators.kubernetes_pod import KubernetesPodOperator
from airflow.providers.docker.operators.docker import DockerOperator
from airflow.operators.bash import BashOperator
from airflow.utils.task_group import TaskGroup
from airflow.utils.dates import days_ago

# Default arguments applied to all tasks
default_args = {
    "owner": "airflow",
    "depends_on_past": False,
    "email_on_failure": False,
    "email_on_retry": False,
    "retries": 2,
    "retry_delay": timedelta(minutes=5),
    "start_date": days_ago(1),
}

# DAG definition
with DAG(
    dag_id="dump_production_csv_pipeline",
    description="No description provided.",
    schedule_interval="@daily",
    default_args=default_args,
    catchup=False,
    tags=["fanout", "csv", "dump"],
    max_active_runs=1,
    timezone="UTC",
) as dag:

    # -------------------------------------------------------------------------
    # Task: Dump Production Database to CSV
    # -------------------------------------------------------------------------
    dump_production_csv = BashOperator(
        task_id="dump_production_csv",
        bash_command=(
            "python /opt/airflow/scripts/dump_production_to_csv.py "
            "--conn_id {{ conn.prod_database.conn_id }} "
            "--output_path /tmp/production_snapshot.csv"
        ),
        env={"TMP_DIR": "/tmp"},
        retries=2,
        retry_delay=timedelta(minutes=5),
    )

    # -------------------------------------------------------------------------
    # Fanâ€‘out loading tasks
    # -------------------------------------------------------------------------
    load_dev_database = BashOperator(
        task_id="load_dev_database",
        bash_command=(
            "python /opt/airflow/scripts/load_csv_to_db.py "
            "--conn_id {{ conn.dev_database.conn_id }} "
            "--input_path /tmp/production_snapshot.csv"
        ),
        retries=2,
        retry_delay=timedelta(minutes=5),
    )

    load_qa_database = BashOperator(
        task_id="load_qa_database",
        bash_command=(
            "python /opt/airflow/scripts/load_csv_to_db.py "
            "--conn_id {{ conn.qa_database.conn_id }} "
            "--input_path /tmp/production_snapshot.csv"
        ),
        retries=2,
        retry_delay=timedelta(minutes=5),
    )

    load_staging_database = BashOperator(
        task_id="load_staging_database",
        bash_command=(
            "python /opt/airflow/scripts/load_csv_to_db.py "
            "--conn_id {{ conn.staging_database.conn_id }} "
            "--input_path /tmp/production_snapshot.csv"
        ),
        retries=2,
        retry_delay=timedelta(minutes=5),
    )

    # -------------------------------------------------------------------------
    # Define dependencies (fanâ€‘out pattern)
    # -------------------------------------------------------------------------
    dump_production_csv >> [load_dev_database, load_qa_database, load_staging_database]