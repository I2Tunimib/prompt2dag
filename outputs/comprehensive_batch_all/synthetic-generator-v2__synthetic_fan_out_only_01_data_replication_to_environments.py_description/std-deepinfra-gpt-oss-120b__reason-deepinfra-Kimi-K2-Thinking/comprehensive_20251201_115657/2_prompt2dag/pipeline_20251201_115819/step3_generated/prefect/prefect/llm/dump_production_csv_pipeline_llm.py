# Generated by Prefect Pipeline Generator
# Date: 2024-06-28
# Pipeline: dump_production_csv_pipeline
# Description: No description provided.
# Pattern: fanout
# Prefect version: 2.14.0

import subprocess
from pathlib import Path
from typing import Any

from prefect import flow, task, get_run_logger
from prefect.task_runners import ConcurrentTaskRunner
from prefect.deployments import DeploymentSpec
from prefect.server.schemas.schedules import CronSchedule
from prefect.blocks.system import Secret
from prefect.filesystems import LocalFileSystem


def _get_secret(secret_name: str) -> str:
    """
    Retrieve the value of a Prefect Secret block.

    Args:
        secret_name: Name of the Secret block.

    Returns:
        The secret value as a string.

    Raises:
        RuntimeError: If the secret block cannot be loaded.
    """
    try:
        secret_block = Secret.load(secret_name)
        return secret_block.get()
    except Exception as exc:
        raise RuntimeError(f"Unable to load secret '{secret_name}': {exc}") from exc


def _get_local_filesystem(block_name: str) -> LocalFileSystem:
    """
    Load a Prefect LocalFileSystem block.

    Args:
        block_name: Name of the LocalFileSystem block.

    Returns:
        An instance of LocalFileSystem.

    Raises:
        RuntimeError: If the block cannot be loaded.
    """
    try:
        return LocalFileSystem.load(block_name)
    except Exception as exc:
        raise RuntimeError(f"Unable to load LocalFileSystem block '{block_name}': {exc}") from exc


@task(retries=2, retry_delay_seconds=60)
def dump_production_csv() -> Path:
    """
    Dump the production database to a CSV file stored in the local /tmp filesystem.

    Returns:
        Path to the generated CSV file.

    Raises:
        RuntimeError: If the dump command fails.
    """
    logger = get_run_logger()
    prod_secret_name = "prod_database"
    filesystem_block_name = "local_tmp_filesystem"

    logger.info("Fetching production database credentials.")
    prod_conn_str = _get_secret(prod_secret_name)

    logger.info("Loading local temporary filesystem block.")
    tmp_fs = _get_local_filesystem(filesystem_block_name)

    # Define the CSV file location within the block's base path
    csv_path = Path(tmp_fs.base_path) / "production_snapshot.csv"
    logger.info(f"CSV will be written to {csv_path}")

    # Example command: using psql to copy a table to CSV.
    # Adjust the command according to your actual DB engine and schema.
    dump_cmd = [
        "psql",
        prod_conn_str,
        "-c",
        f"COPY (SELECT * FROM public.my_table) TO STDOUT WITH CSV HEADER",
    ]

    logger.info(f"Running dump command: {' '.join(dump_cmd)}")
    try:
        with open(csv_path, "w", encoding="utf-8") as csv_file:
            subprocess.run(dump_cmd, check=True, stdout=csv_file, stderr=subprocess.PIPE)
        logger.info("Production database successfully dumped to CSV.")
    except subprocess.CalledProcessError as exc:
        logger.error(f"Dump command failed: {exc.stderr.decode()}")
        raise RuntimeError("Failed to dump production database to CSV.") from exc

    return csv_path


@task(retries=2, retry_delay_seconds=60)
def load_dev_database(csv_path: Path) -> None:
    """
    Load the CSV snapshot into the development database.

    Args:
        csv_path: Path to the CSV file generated by `dump_production_csv`.

    Raises:
        RuntimeError: If the load command fails.
    """
    logger = get_run_logger()
    dev_secret_name = "dev_database"

    logger.info("Fetching development database credentials.")
    dev_conn_str = _get_secret(dev_secret_name)

    load_cmd = [
        "psql",
        dev_conn_str,
        "-c",
        f"COPY public.my_table FROM STDIN WITH CSV HEADER",
    ]

    logger.info(f"Running load command for development DB: {' '.join(load_cmd)}")
    try:
        with open(csv_path, "r", encoding="utf-8") as csv_file:
            subprocess.run(load_cmd, check=True, stdin=csv_file, stderr=subprocess.PIPE)
        logger.info("CSV successfully loaded into development database.")
    except subprocess.CalledProcessError as exc:
        logger.error(f"Load command failed: {exc.stderr.decode()}")
        raise RuntimeError("Failed to load CSV into development database.") from exc


@task(retries=2, retry_delay_seconds=60)
def load_qa_database(csv_path: Path) -> None:
    """
    Load the CSV snapshot into the QA database.

    Args:
        csv_path: Path to the CSV file generated by `dump_production_csv`.

    Raises:
        RuntimeError: If the load command fails.
    """
    logger = get_run_logger()
    qa_secret_name = "qa_database"

    logger.info("Fetching QA database credentials.")
    qa_conn_str = _get_secret(qa_secret_name)

    load_cmd = [
        "psql",
        qa_conn_str,
        "-c",
        f"COPY public.my_table FROM STDIN WITH CSV HEADER",
    ]

    logger.info(f"Running load command for QA DB: {' '.join(load_cmd)}")
    try:
        with open(csv_path, "r", encoding="utf-8") as csv_file:
            subprocess.run(load_cmd, check=True, stdin=csv_file, stderr=subprocess.PIPE)
        logger.info("CSV successfully loaded into QA database.")
    except subprocess.CalledProcessError as exc:
        logger.error(f"Load command failed: {exc.stderr.decode()}")
        raise RuntimeError("Failed to load CSV into QA database.") from exc


@task(retries=2, retry_delay_seconds=60)
def load_staging_database(csv_path: Path) -> None:
    """
    Load the CSV snapshot into the staging database.

    Args:
        csv_path: Path to the CSV file generated by `dump_production_csv`.

    Raises:
        RuntimeError: If the load command fails.
    """
    logger = get_run_logger()
    staging_secret_name = "staging_database"

    logger.info("Fetching staging database credentials.")
    staging_conn_str = _get_secret(staging_secret_name)

    load_cmd = [
        "psql",
        staging_conn_str,
        "-c",
        f"COPY public.my_table FROM STDIN WITH CSV HEADER",
    ]

    logger.info(f"Running load command for staging DB: {' '.join(load_cmd)}")
    try:
        with open(csv_path, "r", encoding="utf-8") as csv_file:
            subprocess.run(load_cmd, check=True, stdin=csv_file, stderr=subprocess.PIPE)
        logger.info("CSV successfully loaded into staging database.")
    except subprocess.CalledProcessError as exc:
        logger.error(f"Load command failed: {exc.stderr.decode()}")
        raise RuntimeError("Failed to load CSV into staging database.") from exc


@flow(
    name="dump_production_csv_pipeline",
    task_runner=ConcurrentTaskRunner(),
)
def dump_production_csv_pipeline() -> None:
    """
    Orchestrates the dumping of the production database to CSV and loading
    that snapshot into development, QA, and staging environments.
    """
    logger = get_run_logger()
    logger.info("Starting pipeline: dump production CSV and distribute to environments.")

    csv_path = dump_production_csv()

    # Fanâ€‘out: load into each environment in parallel
    load_dev_database.submit(csv_path)
    load_qa_database.submit(csv_path)
    load_staging_database.submit(csv_path)

    logger.info("Pipeline execution submitted.")


# -------------------------------------------------------------------------
# Deployment configuration
# -------------------------------------------------------------------------

DeploymentSpec(
    name="dump_production_csv_pipeline_deployment",
    flow=dump_production_csv_pipeline,
    schedule=CronSchedule(cron="0 0 * * *", timezone="UTC"),
    tags=["default-agent-pool"],
    parameters={},
    enforce_parameter_schema=False,
    flow_name="dump_production_csv_pipeline",
    work_pool_name="default-agent-pool",
    # Prefect 2.x does not have a direct `catchup` flag on the schedule;
    # it is controlled via the deployment's `schedule` behavior.
    # Setting `catchup=False` is achieved by using `CronSchedule` with
    # `catchup=False` in the deployment API (if supported). Here we
    # explicitly set it via the `schedule` argument.
    schedule_options={"catchup": False},
)