# Generated by Prefect Pipeline Generator
# Date: 2024-06-28
# Pipeline: dump_prod_csv_pipeline
# Description: No description provided.
# Pattern: fanout
# Prefect version: 2.14.0

import subprocess
import tempfile
from pathlib import Path
from typing import Any

from prefect import flow, task, get_run_logger
from prefect.task_runners import ConcurrentTaskRunner
from prefect.deployments import DeploymentSpec
from prefect.orion.schemas.schedules import CronSchedule

from prefect.blocks.system import Secret
from prefect.filesystems import LocalFileSystem


def _load_secret(secret_name: str) -> str:
    """
    Load a secret block and return its stored value.

    Args:
        secret_name: Name of the Prefect Secret block.

    Returns:
        The secret value as a string.

    Raises:
        RuntimeError: If the secret cannot be loaded.
    """
    try:
        secret_block = Secret.load(secret_name)
        return secret_block.get()
    except Exception as exc:
        raise RuntimeError(f"Unable to load secret '{secret_name}': {exc}") from exc


def _get_temp_dir() -> Path:
    """
    Retrieve a temporary directory from the local filesystem block.

    Returns:
        Path object pointing to the temporary directory.
    """
    fs_block = LocalFileSystem.load("local_tmp_fs")
    # The block's basepath is used as the root for temporary files.
    return Path(fs_block.basepath)


@task(retries=2, retry_delay_seconds=10, log_prints=True)
def dump_prod_csv() -> Path:
    """
    Dump the production database into a CSV file stored on the local temporary filesystem.

    Returns:
        Path to the generated CSV file.

    Raises:
        RuntimeError: If the dump command fails.
    """
    logger = get_run_logger()
    prod_conn_str = _load_secret("prod_db")
    temp_dir = _get_temp_dir()
    csv_path = temp_dir / "prod_snapshot.csv"

    # Example command using psql to export a table to CSV.
    # Adjust the command according to the actual database type and requirements.
    dump_cmd = [
        "psql",
        prod_conn_str,
        "-c",
        f"COPY (SELECT * FROM public.my_table) TO STDOUT WITH CSV HEADER",
    ]

    logger.info("Running dump command: %s", " ".join(dump_cmd))
    try:
        with csv_path.open("w", encoding="utf-8") as csv_file:
            subprocess.run(
                dump_cmd,
                check=True,
                stdout=csv_file,
                stderr=subprocess.PIPE,
                text=True,
            )
        logger.info("Production dump completed: %s", csv_path)
        return csv_path
    except subprocess.CalledProcessError as exc:
        logger.error("Dump command failed: %s", exc.stderr)
        raise RuntimeError(f"Failed to dump production DB: {exc.stderr}") from exc


def _load_csv_into_db(csv_path: Path, secret_name: str, target_name: str) -> None:
    """
    Helper to load a CSV file into a target database.

    Args:
        csv_path: Path to the CSV file.
        secret_name: Name of the Secret block containing DB connection string.
        target_name: Human‑readable name of the target database (for logging).

    Raises:
        RuntimeError: If the load command fails.
    """
    logger = get_run_logger()
    conn_str = _load_secret(secret_name)

    # Example command using psql to import CSV into a table.
    load_cmd = [
        "psql",
        conn_str,
        "-c",
        f"COPY public.my_table FROM STDIN WITH CSV HEADER",
    ]

    logger.info("Loading CSV into %s using command: %s", target_name, " ".join(load_cmd))
    try:
        with csv_path.open("r", encoding="utf-8") as csv_file:
            subprocess.run(
                load_cmd,
                check=True,
                stdin=csv_file,
                stderr=subprocess.PIPE,
                text=True,
            )
        logger.info("CSV successfully loaded into %s", target_name)
    except subprocess.CalledProcessError as exc:
        logger.error("Load into %s failed: %s", target_name, exc.stderr)
        raise RuntimeError(f"Failed to load CSV into {target_name}: {exc.stderr}") from exc


@task(retries=2, retry_delay_seconds=10, log_prints=True)
def copy_dev(csv_path: Path) -> None:
    """
    Load the CSV snapshot into the Development database.

    Args:
        csv_path: Path to the CSV file generated by ``dump_prod_csv``.
    """
    _load_csv_into_db(csv_path, secret_name="dev_db", target_name="Development Database")


@task(retries=2, retry_delay_seconds=10, log_prints=True)
def copy_qa(csv_path: Path) -> None:
    """
    Load the CSV snapshot into the QA database.

    Args:
        csv_path: Path to the CSV file generated by ``dump_prod_csv``.
    """
    _load_csv_into_db(csv_path, secret_name="qa_db", target_name="QA Database")


@task(retries=2, retry_delay_seconds=10, log_prints=True)
def copy_staging(csv_path: Path) -> None:
    """
    Load the CSV snapshot into the Staging database.

    Args:
        csv_path: Path to the CSV file generated by ``dump_prod_csv``.
    """
    _load_csv_into_db(csv_path, secret_name="staging_db", target_name="Staging Database")


@flow(
    name="dump_prod_csv_pipeline",
    task_runner=ConcurrentTaskRunner(),
)
def dump_prod_csv_pipeline() -> None:
    """
    Orchestrates dumping the production database to CSV and fan‑out loading
    the snapshot into Development, QA, and Staging environments.
    """
    csv_path = dump_prod_csv()
    # Fan‑out: each copy task runs independently after the dump completes.
    copy_dev.submit(csv_path)
    copy_qa.submit(csv_path)
    copy_staging.submit(csv_path)


# -------------------------------------------------------------------------
# Deployment specification (disabled schedule)
# -------------------------------------------------------------------------

DeploymentSpec(
    name="dump_prod_csv_pipeline_deployment",
    flow=dump_prod_csv_pipeline,
    schedule=None,  # Disabled; set to a CronSchedule if you wish to enable.
    flow_runner="prefect.engine.flow_runner.FlowRunner",
    task_runner="prefect.task_runners.concurrent.ConcurrentTaskRunner",
    work_pool_name="default-agent-pool",
    tags=["dump_prod_csv_pipeline"],
    description="Deployment for dump_prod_csv_pipeline (schedule disabled).",
)


if __name__ == "__main__":
    # Running the flow locally for testing/debugging purposes.
    dump_prod_csv_pipeline()