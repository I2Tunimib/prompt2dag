# Generated by Dagster Code Generator
# Date: 2024-06-28
# Pipeline: dump_prod_csv_pipeline
# Description: No description provided.
# Pattern: fanout
# Executor: multiprocess_executor
# Dagster version: 1.5.0

from dagster import (
    op,
    job,
    RetryPolicy,
    ResourceDefinition,
    fs_io_manager,
    multiprocess_executor,
    ScheduleDefinition,
    ScheduleStatus,
    ConfigurableResource,
    InitResourceContext,
    get_dagster_logger,
)

# -------------------------------------------------------------------------
# Resource definitions
# -------------------------------------------------------------------------

class DatabaseResource(ConfigurableResource):
    """Placeholder resource for a database connection."""

    connection_string: str

    def get_connection(self):
        """Return a mock connection object."""
        logger = get_dagster_logger()
        logger.info(f"Connecting to database with: {self.connection_string}")
        # In a real implementation, return an actual DB connection (e.g., SQLAlchemy engine)
        return self.connection_string


prod_db = DatabaseResource(connection_string="postgresql://prod_user:prod_pass@prod_host/prod_db")
dev_db = DatabaseResource(connection_string="postgresql://dev_user:dev_pass@dev_host/dev_db")
staging_db = DatabaseResource(connection_string="postgresql://staging_user:staging_pass@staging_host/staging_db")
qa_db = DatabaseResource(connection_string="postgresql://qa_user:qa_pass@qa_host/qa_db")

# -------------------------------------------------------------------------
# Ops
# -------------------------------------------------------------------------

@op(
    required_resource_keys={"prod_db", "local_tmp_fs"},
    retry_policy=RetryPolicy(max_retries=2),
    description="Dump the production database to a CSV file stored on the local temporary filesystem.",
    out={"csv_path": None},
)
def dump_prod_csv(context) -> str:
    """Export data from the production database to a CSV file.

    Returns:
        str: The absolute path to the generated CSV file.
    """
    logger = context.log
    prod_conn = context.resources.prod_db.get_connection()
    # Placeholder logic â€“ replace with actual export code.
    csv_path = "/tmp/prod_snapshot.csv"
    logger.info(f"Exporting data from production DB ({prod_conn}) to {csv_path}")
    # Simulate file creation using the filesystem IO manager.
    with context.resources.local_tmp_fs.get_output_context().get_output_path() as _:
        pass  # In real code, write CSV content here.
    return csv_path


@op(
    required_resource_keys={"dev_db", "local_tmp_fs"},
    retry_policy=RetryPolicy(max_retries=2),
    description="Load the CSV snapshot into the development database.",
    ins={"csv_path": None},
)
def copy_dev(context, csv_path: str) -> None:
    """Load CSV data into the development database.

    Args:
        csv_path (str): Path to the CSV file generated by `dump_prod_csv`.
    """
    logger = context.log
    dev_conn = context.resources.dev_db.get_connection()
    logger.info(f"Loading CSV {csv_path} into development DB ({dev_conn})")
    # Placeholder: replace with actual loading logic (e.g., COPY command).


@op(
    required_resource_keys={"qa_db", "local_tmp_fs"},
    retry_policy=RetryPolicy(max_retries=2),
    description="Load the CSV snapshot into the QA database.",
    ins={"csv_path": None},
)
def copy_qa(context, csv_path: str) -> None:
    """Load CSV data into the QA database.

    Args:
        csv_path (str): Path to the CSV file generated by `dump_prod_csv`.
    """
    logger = context.log
    qa_conn = context.resources.qa_db.get_connection()
    logger.info(f"Loading CSV {csv_path} into QA DB ({qa_conn})")
    # Placeholder: replace with actual loading logic.


@op(
    required_resource_keys={"staging_db", "local_tmp_fs"},
    retry_policy=RetryPolicy(max_retries=2),
    description="Load the CSV snapshot into the staging database.",
    ins={"csv_path": None},
)
def copy_staging(context, csv_path: str) -> None:
    """Load CSV data into the staging database.

    Args:
        csv_path (str): Path to the CSV file generated by `dump_prod_csv`.
    """
    logger = context.log
    staging_conn = context.resources.staging_db.get_connection()
    logger.info(f"Loading CSV {csv_path} into staging DB ({staging_conn})")
    # Placeholder: replace with actual loading logic.


# -------------------------------------------------------------------------
# Job definition
# -------------------------------------------------------------------------

@job(
    executor_def=multiprocess_executor,
    resource_defs={
        "prod_db": prod_db,
        "dev_db": dev_db,
        "staging_db": staging_db,
        "qa_db": qa_db,
        "local_tmp_fs": fs_io_manager,
    },
    description="No description provided.",
)
def dump_prod_csv_pipeline():
    """Orchestrates dumping production data to CSV and loading it into dev, QA, and staging."""
    csv_path = dump_prod_csv()
    copy_dev(csv_path)
    copy_qa(csv_path)
    copy_staging(csv_path)


# -------------------------------------------------------------------------
# Schedule (disabled by default)
# -------------------------------------------------------------------------

daily_dump_schedule = ScheduleDefinition(
    job=dump_prod_csv_pipeline,
    cron_schedule="@daily",
    execution_timezone="UTC",
    default_status=ScheduleStatus.STOPPED,
    description="Daily schedule for dumping production CSV (disabled by default).",
)

# Export symbols for Dagster discovery
__all__ = [
    "dump_prod_csv_pipeline",
    "daily_dump_schedule",
    "prod_db",
    "dev_db",
    "staging_db",
    "qa_db",
    "local_tmp_fs",
]