# Generated by Dagster Code Generator
# Date: 2024-06-12
# Pipeline: dump_production_csv_pipeline
# Description: No description provided.
# Dagster version: 1.5.0

from dagster import (
    op,
    job,
    In,
    Out,
    RetryPolicy,
    ResourceDefinition,
    fs_io_manager,
    multiprocess_executor,
    ScheduleDefinition,
    DefaultScheduleStatus,
)


# ----------------------------------------------------------------------
# Resource definitions (placeholders for actual implementations)
# ----------------------------------------------------------------------


class ProdDatabaseResource:
    """Placeholder resource for the production database connection."""

    def dump_to_csv(self, file_path: str) -> None:
        # Implement actual dump logic here.
        pass


class DevDatabaseResource:
    """Placeholder resource for the development database connection."""

    def load_from_csv(self, file_path: str) -> None:
        # Implement actual load logic here.
        pass


class QaDatabaseResource:
    """Placeholder resource for the QA database connection."""

    def load_from_csv(self, file_path: str) -> None:
        # Implement actual load logic here.
        pass


class StagingDatabaseResource:
    """Placeholder resource for the staging database connection."""

    def load_from_csv(self, file_path: str) -> None:
        # Implement actual load logic here.
        pass


prod_database = ResourceDefinition.hardcoded_resource(ProdDatabaseResource())
dev_database = ResourceDefinition.hardcoded_resource(DevDatabaseResource())
qa_database = ResourceDefinition.hardcoded_resource(QaDatabaseResource())
staging_database = ResourceDefinition.hardcoded_resource(StagingDatabaseResource())
local_filesystem = fs_io_manager


# ----------------------------------------------------------------------
# Ops
# ----------------------------------------------------------------------


@op(
    required_resource_keys={"prod_database", "local_filesystem"},
    out=Out(str),
    retry_policy=RetryPolicy(max_retries=2),
    description="Dump the production database to a CSV file and return the file path.",
)
def dump_production_csv(context) -> str:
    """Dump production DB to CSV and return the temporary file path."""
    # In a real implementation, you would use the prod_database resource to
    # stream data and the local_filesystem IO manager to write the CSV.
    file_path = "/tmp/production_dump.csv"
    context.log.info(f"Dumping production database to CSV at {file_path}")

    # Simulated dump operation
    prod_db: ProdDatabaseResource = context.resources.prod_database
    prod_db.dump_to_csv(file_path)

    return file_path


@op(
    required_resource_keys={"dev_database", "local_filesystem"},
    ins={"csv_path": In(str)},
    retry_policy=RetryPolicy(max_retries=2),
    description="Load a CSV dump into the development database.",
)
def load_dev_database(context, csv_path: str) -> None:
    """Load CSV data into the development database."""
    context.log.info(f"Loading CSV from {csv_path} into development database")
    dev_db: DevDatabaseResource = context.resources.dev_database
    dev_db.load_from_csv(csv_path)


@op(
    required_resource_keys={"qa_database", "local_filesystem"},
    ins={"csv_path": In(str)},
    retry_policy=RetryPolicy(max_retries=2),
    description="Load a CSV dump into the QA database.",
)
def load_qa_database(context, csv_path: str) -> None:
    """Load CSV data into the QA database."""
    context.log.info(f"Loading CSV from {csv_path} into QA database")
    qa_db: QaDatabaseResource = context.resources.qa_database
    qa_db.load_from_csv(csv_path)


@op(
    required_resource_keys={"staging_database", "local_filesystem"},
    ins={"csv_path": In(str)},
    retry_policy=RetryPolicy(max_retries=2),
    description="Load a CSV dump into the staging database.",
)
def load_staging_database(context, csv_path: str) -> None:
    """Load CSV data into the staging database."""
    context.log.info(f"Loading CSV from {csv_path} into staging database")
    staging_db: StagingDatabaseResource = context.resources.staging_database
    staging_db.load_from_csv(csv_path)


# ----------------------------------------------------------------------
# Job definition
# ----------------------------------------------------------------------


@job(
    executor_def=multiprocess_executor,
    resource_defs={
        "prod_database": prod_database,
        "dev_database": dev_database,
        "qa_database": qa_database,
        "staging_database": staging_database,
        "local_filesystem": local_filesystem,
    },
    description="No description provided.",
)
def dump_production_csv_pipeline():
    """
    Orchestrates dumping the production database to a CSV file and loading that
    CSV into development, QA, and staging databases.
    """
    csv_path = dump_production_csv()
    load_dev_database(csv_path)
    load_qa_database(csv_path)
    load_staging_database(csv_path)


# ----------------------------------------------------------------------
# Schedule (disabled by default)
# ----------------------------------------------------------------------


daily_schedule = ScheduleDefinition(
    job=dump_production_csv_pipeline,
    cron_schedule="@daily",
    execution_timezone="UTC",
    default_status=DefaultScheduleStatus.INACTIVE,
    description="Daily schedule for dump_production_csv_pipeline (disabled by default).",
)


# ----------------------------------------------------------------------
# Exported symbols
# ----------------------------------------------------------------------


__all__ = [
    "dump_production_csv_pipeline",
    "daily_schedule",
    "dump_production_csv",
    "load_dev_database",
    "load_qa_database",
    "load_staging_database",
    "prod_database",
    "dev_database",
    "qa_database",
    "staging_database",
    "local_filesystem",
]