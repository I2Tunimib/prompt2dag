# Generated by Prefect Pipeline Generator
# Date: 2024-06-28
# Prefect version: 2.14.0
# Flow: load_and_modify_data_pipeline

from prefect import flow, task
from prefect.task_runners import SequentialTaskRunner
from prefect_docker import DockerContainer
from prefect.filesystems import LocalFileSystem
from prefect.blocks.system import Secret
from typing import Dict, Any


def _get_shared_volume() -> Dict[str, str]:
    """
    Retrieve the shared filesystem block and construct a Docker volume mapping.

    Returns
    -------
    Dict[str, str]
        Mapping of host path to container path for Docker volume mounting.
    """
    # The block name is defined in the pipeline metadata
    shared_fs: LocalFileSystem = LocalFileSystem.load("shared_filesystem")
    host_path = shared_fs.basepath  # type: ignore[attr-defined]
    return {host_path: "/data"}


def _load_secret(secret_name: str) -> str:
    """
    Load a secret value from a Prefect Secret block.

    Parameters
    ----------
    secret_name : str
        Name of the secret block.

    Returns
    -------
    str
        The secret value.
    """
    secret: Secret = Secret.load(secret_name)
    return secret.get()


def _run_docker_task(
    image: str,
    env: Dict[str, str],
    network: str = None,
    retries: int = 1,
) -> None:
    """
    Execute a Docker container as a Prefect task.

    Parameters
    ----------
    image : str
        Docker image to run.
    env : Dict[str, str]
        Environment variables passed to the container.
    network : str, optional
        Docker network to attach the container to.
    retries : int, default 1
        Number of retries on failure.
    """
    volumes = _get_shared_volume()
    docker_task = DockerContainer(
        image=image,
        env=env,
        volumes=volumes,
        network=network,
        stream_output=True,
        # Prefect's DockerContainer task uses the `retries` argument from the task decorator
    )
    docker_task.run()


@task(retries=1, retry_delay_seconds=30)
def load_and_modify_data() -> None:
    """
    Load & Modify Data task.
    """
    env = {
        "DATASET_ID": "2",
        "DATE_COLUMN": "installation_date",
        "TABLE_NAME_PREFIX": "JOT_",
        "DATA_DIR": "/data",
    }
    _run_docker_task(
        image="i2t-backendwithintertwino6-load-and-modify:latest",
        env=env,
    )


@task(retries=1, retry_delay_seconds=30)
def geocode_reconciliation() -> None:
    """
    Geocoding Reconciliation task.
    """
    env = {
        "PRIMARY_COLUMN": "location",
        "RECONCILIATOR_ID": "geocodingHere",
        "API_TOKEN": _load_secret("reconciliation_api"),
        "DATASET_ID": "2",
        "DATA_DIR": "/data",
    }
    _run_docker_task(
        image="i2t-backendwithintertwino6-reconciliation:latest",
        env=env,
    )


@task(retries=1, retry_delay_seconds=30)
def openmeteo_extension() -> None:
    """
    OpenMeteo Data Extension task.
    """
    env = {
        "LAT_COLUMN": "latitude",
        "LON_COLUMN": "longitude",
        "DATE_COLUMN": "installation_date",
        "WEATHER_VARIABLES": "apparent_temperature_max,apparent_temperature_min,precipitation_sum,precipitation_hours",
        "DATE_SEPARATOR_FORMAT": "YYYYMMDD",
        "DATA_DIR": "/data",
    }
    _run_docker_task(
        image="i2t-backendwithintertwino6-openmeteo-extension:latest",
        env=env,
        network=_load_secret("app_network"),
    )


@task(retries=1, retry_delay_seconds=30)
def land_use_extension() -> None:
    """
    Land Use Extension task.
    """
    env = {
        "LAT_COLUMN": "latitude",
        "LON_COLUMN": "longitude",
        "OUTPUT_COLUMN": "land_use_type",
        "API_KEY": _load_secret("geoapify_api"),
        "DATA_DIR": "/data",
    }
    _run_docker_task(
        image="geoapify-land-use:latest",
        env=env,
        network=_load_secret("app_network"),
    )


@task(retries=1, retry_delay_seconds=30)
def population_density_extension() -> None:
    """
    Population Density Extension task.
    """
    env = {
        "LAT_COLUMN": "latitude",
        "LON_COLUMN": "longitude",
        "OUTPUT_COLUMN": "population_density",
        "RADIUS": "5000",
        "DATA_DIR": "/data",
    }
    _run_docker_task(
        image="worldpop-density:latest",
        env=env,
        network=_load_secret("app_network"),
    )


@task(retries=1, retry_delay_seconds=30)
def calculate_environmental_risk() -> None:
    """
    Environmental Calculation (Column Extension) task.
    """
    env = {
        "EXTENDER_ID": "environmentalRiskCalculator",
        "INPUT_COLUMNS": "precipitation_sum,population_density,land_use_type",
        "OUTPUT_COLUMN": "risk_score",
        "CALCULATION_FORMULA": "[risk calculation parameters]",
        "DATA_DIR": "/data",
    }
    _run_docker_task(
        image="i2t-backendwithintertwino6-column-extension:latest",
        env=env,
        network=_load_secret("app_network"),
    )


@task(retries=1, retry_delay_seconds=30)
def save_final_data() -> None:
    """
    Save Final Data task.
    """
    env = {
        "DATASET_ID": "2",
        "DATA_DIR": "/data",
    }
    _run_docker_task(
        image="i2t-backendwithintertwino6-save:latest",
        env=env,
    )


@flow(
    name="load_and_modify_data_pipeline",
    task_runner=SequentialTaskRunner(),
)
def load_and_modify_data_pipeline() -> None:
    """
    Orchestrates the full data loading, enrichment, and saving pipeline.
    The execution order follows the defined task dependencies.
    """
    load_and_modify_data()
    geocode_reconciliation()
    openmeteo_extension()
    land_use_extension()
    population_density_extension()
    calculate_environmental_risk()
    save_final_data()


if __name__ == "__main__":
    # Running the flow locally (useful for testing)
    load_and_modify_data_pipeline()