# Generated by Prefect Pipeline Generator
# Pipeline: load_and_modify_data_pipeline
# Generation Timestamp: 2024-06-28T12:00:00Z
# Prefect version: 2.14.0

from __future__ import annotations

import os
from typing import Dict

from prefect import flow, task, get_run_logger
from prefect.task_runners import SequentialTaskRunner
from prefect_docker import DockerContainer
from prefect.blocks.system import Secret
from prefect.exceptions import PrefectException


def _load_secret(secret_name: str) -> str:
    """
    Load a secret value from a Prefect Secret block.

    Args:
        secret_name: The name of the secret block to load.

    Returns:
        The secret value as a string.

    Raises:
        PrefectException: If the secret cannot be loaded.
    """
    try:
        secret_block = Secret.load(secret_name)
        return secret_block.get()
    except Exception as exc:
        raise PrefectException(f"Unable to load secret '{secret_name}': {exc}") from exc


def _docker_task(
    image: str,
    env: Dict[str, str] | None = None,
    volumes: Dict[str, str] | None = None,
    network: str | None = None,
) -> None:
    """
    Execute a Docker container using the Prefect DockerContainer task.

    Args:
        image: Docker image to run.
        env: Environment variables to pass to the container.
        volumes: Mapping of host paths to container paths.
        network: Docker network name.

    Raises:
        PrefectException: If the Docker container fails.
    """
    logger = get_run_logger()
    logger.info("Running Docker image %s", image)

    container_task = DockerContainer(
        image=image,
        env=env or {},
        volumes=volumes or {},
        network=network,
        # Ensure the container runs to completion and returns its logs.
        stream_output=True,
        # Pull the latest image if not present.
        pull_image=True,
    )
    try:
        container_task.run()
        logger.info("Docker image %s completed successfully.", image)
    except Exception as exc:
        logger.error("Docker image %s failed: %s", image, exc)
        raise PrefectException(f"Docker task failed for image {image}") from exc


@task(retries=1, retry_delay_seconds=30, name="Load and Modify Data")
def load_and_modify_data() -> None:
    """
    Load and modify the base dataset using the dedicated Docker image.
    """
    env = {
        "DATASET_ID": "2",
        "DATE_COLUMN": "installation_date",
        "TABLE_NAME_PREFIX": "JOT_",
    }
    volumes = {
        # Assuming the LocalFileSystem block `data_dir_volume` is mounted at /data
        "data_dir_volume": "/data",
    }
    network = _load_secret("app_network")
    _docker_task(
        image="i2t-backendwithintertwino6-load-and-modify:latest",
        env=env,
        volumes=volumes,
        network=network,
    )


@task(retries=1, retry_delay_seconds=30, name="Geocode Reconciliation")
def geocode_reconciliation() -> None:
    """
    Perform geocoding reconciliation using the HERE API.
    """
    env = {
        "PRIMARY_COLUMN": "location",
        "RECONCILIATOR_ID": "geocodingHere",
        "API_TOKEN": _load_secret("here_geocoding_api"),
        "DATASET_ID": "2",
    }
    volumes = {"data_dir_volume": "/data"}
    network = _load_secret("app_network")
    _docker_task(
        image="i2t-backendwithintertwino6-reconciliation:latest",
        env=env,
        volumes=volumes,
        network=network,
    )


@task(retries=1, retry_delay_seconds=30, name="OpenMeteo Data Extension")
def open_meteo_extension() -> None:
    """
    Extend the dataset with weather data from OpenMeteo.
    """
    env = {
        "LAT_COLUMN": "latitude",
        "LON_COLUMN": "longitude",
        "DATE_COLUMN": "installation_date",
        "WEATHER_VARIABLES": "apparent_temperature_max,apparent_temperature_min,precipitation_sum,precipitation_hours",
        "DATE_SEPARATOR_FORMAT": "YYYYMMDD",
    }
    volumes = {"data_dir_volume": "/data"}
    network = _load_secret("app_network")
    _docker_task(
        image="i2t-backendwithintertwino6-openmeteo-extension:latest",
        env=env,
        volumes=volumes,
        network=network,
    )


@task(retries=1, retry_delay_seconds=30, name="Land Use Extension")
def land_use_extension() -> None:
    """
    Add land‑use information using the Geoapify API.
    """
    env = {
        "LAT_COLUMN": "latitude",
        "LON_COLUMN": "longitude",
        "OUTPUT_COLUMN": "land_use_type",
        "API_KEY": _load_secret("geoapify_land_use_api"),
    }
    volumes = {"data_dir_volume": "/data"}
    network = _load_secret("app_network")
    _docker_task(
        image="geoapify-land-use:latest",
        env=env,
        volumes=volumes,
        network=network,
    )


@task(retries=1, retry_delay_seconds=30, name="Population Density Extension")
def population_density_extension() -> None:
    """
    Append population density data using the WorldPop service.
    """
    env = {
        "LAT_COLUMN": "latitude",
        "LON_COLUMN": "longitude",
        "OUTPUT_COLUMN": "population_density",
        "RADIUS": "5000",
    }
    volumes = {"data_dir_volume": "/data"}
    network = _load_secret("app_network")
    _docker_task(
        image="worldpop-density:latest",
        env=env,
        volumes=volumes,
        network=network,
    )


@task(retries=1, retry_delay_seconds=30, name="Environmental Risk Calculation")
def environmental_risk_calculation() -> None:
    """
    Compute an environmental risk score based on weather, population, and land‑use.
    """
    env = {
        "EXTENDER_ID": "environmentalRiskCalculator",
        "INPUT_COLUMNS": "precipitation_sum,population_density,land_use_type",
        "OUTPUT_COLUMN": "risk_score",
        "CALCULATION_FORMULA": "[risk calculation parameters]",
    }
    volumes = {"data_dir_volume": "/data"}
    network = _load_secret("app_network")
    _docker_task(
        image="i2t-backendwithintertwino6-column-extension:latest",
        env=env,
        volumes=volumes,
        network=network,
    )


@task(retries=1, retry_delay_seconds=30, name="Save Final Data")
def save_final_data() -> None:
    """
    Persist the final enriched dataset to the target storage.
    """
    env = {"DATASET_ID": "2"}
    volumes = {"data_dir_volume": "/data"}
    network = _load_secret("app_network")
    _docker_task(
        image="i2t-backendwithintertwino6-save:latest",
        env=env,
        volumes=volumes,
        network=network,
    )


@flow(
    name="load_and_modify_data_pipeline",
    task_runner=SequentialTaskRunner(),
)
def load_and_modify_data_pipeline() -> None:
    """
    Orchestrates the full data loading, enrichment, risk calculation,
    and persistence workflow using Docker containers.
    """
    logger = get_run_logger()
    logger.info("Starting pipeline: load_and_modify_data_pipeline")

    # Sequential execution respecting defined dependencies
    load_and_modify_data()
    geocode_reconciliation()
    open_meteo_extension()
    land_use_extension()
    population_density_extension()
    environmental_risk_calculation()
    save_final_data()

    logger.info("Pipeline completed successfully.")


# If this script is executed directly, run the flow.
if __name__ == "__main__":
    load_and_modify_data_pipeline()