# Generated by Prefect Pipeline Generator
# Pipeline: load_and_modify_data_pipeline
# Generation date: 2024-06-28
# Prefect version: 2.14.0

from __future__ import annotations

from typing import Dict

from prefect import flow, task
from prefect.task_runners import SequentialTaskRunner
from prefect_docker import DockerContainer
from prefect.blocks.system import Secret
from prefect.filesystems import LocalFileSystem


def _load_secret(secret_name: str) -> str:
    """
    Helper to load a secret value from a Prefect Secret block.

    Args:
        secret_name: Name of the Secret block.

    Returns:
        The secret value as a string.
    """
    secret = Secret.load(secret_name)
    return secret.get()


# Load shared filesystem block for mounting into containers
_DATA_DIR_VOLUME = LocalFileSystem.load("data_dir_volume")


def _docker_network() -> str:
    """
    Retrieve the Docker network name from the secret block.

    Returns:
        Docker network name.
    """
    return _load_secret("docker_network_app_network")


def _run_container(
    image: str,
    env: Dict[str, str] | None = None,
    network: str | None = None,
) -> None:
    """
    Execute a Docker container using the Prefect DockerContainer task.

    Args:
        image: Docker image to run.
        env: Environment variables for the container.
        network: Docker network to attach the container to.
    """
    container = DockerContainer(
        image=image,
        env=env or {},
        network=network,
        # Mount the shared DATA_DIR volume to /data inside the container
        volumes={_DATA_DIR_VOLUME.basepath: "/data"},
        # Ensure the container is removed after execution
        remove_container=True,
    )
    container.run()


@task(retries=1, retry_delay_seconds=60)
def load_and_modify_data() -> None:
    """
    Load and modify station data.

    Docker image: i2t-backendwithintertwino6-load-and-modify:latest
    """
    env = {
        "DATASET_ID": "2",
        "DATE_COLUMN": "installation_date",
        "TABLE_NAME_PREFIX": "JOT_",
    }
    _run_container(
        image="i2t-backendwithintertwino6-load-and-modify:latest",
        env=env,
        network=_docker_network(),
    )


@task(retries=1, retry_delay_seconds=60)
def geocode_reconciliation() -> None:
    """
    Perform geocode reconciliation using HERE API.

    Docker image: i2t-backendwithintertwino6-reconciliation:latest
    """
    env = {
        "PRIMARY_COLUMN": "location",
        "RECONCILIATOR_ID": "geocodingHere",
        "API_TOKEN": _load_secret("here_geocoding_api"),
        "DATASET_ID": "2",
    }
    _run_container(
        image="i2t-backendwithintertwino6-reconciliation:latest",
        env=env,
        network=_docker_network(),
    )


@task(retries=1, retry_delay_seconds=60)
def openmeteo_extension() -> None:
    """
    Extend dataset with OpenMeteo weather data.

    Docker image: i2t-backendwithintertwino6-openmeteo-extension:latest
    """
    env = {
        "LAT_COLUMN": "latitude",
        "LON_COLUMN": "longitude",
        "DATE_COLUMN": "installation_date",
        "WEATHER_VARIABLES": "apparent_temperature_max,apparent_temperature_min,precipitation_sum,precipitation_hours",
        "DATE_SEPARATOR_FORMAT": "YYYYMMDD",
    }
    _run_container(
        image="i2t-backendwithintertwino6-openmeteo-extension:latest",
        env=env,
        network=_docker_network(),
    )


@task(retries=1, retry_delay_seconds=60)
def land_use_extension() -> None:
    """
    Add land‑use classification using Geoapify API.

    Docker image: geoapify-land-use:latest
    """
    env = {
        "LAT_COLUMN": "latitude",
        "LON_COLUMN": "longitude",
        "OUTPUT_COLUMN": "land_use_type",
        "API_KEY": _load_secret("geoapify_land_use_api"),
    }
    _run_container(
        image="geoapify-land-use:latest",
        env=env,
        network=_docker_network(),
    )


@task(retries=1, retry_delay_seconds=60)
def population_density_extension() -> None:
    """
    Add population density information using WorldPop API.

    Docker image: worldpop-density:latest
    """
    env = {
        "LAT_COLUMN": "latitude",
        "LON_COLUMN": "longitude",
        "OUTPUT_COLUMN": "population_density",
        "RADIUS": "5000",
    }
    _run_container(
        image="worldpop-density:latest",
        env=env,
        network=_docker_network(),
    )


@task(retries=1, retry_delay_seconds=60)
def environmental_risk_calculation() -> None:
    """
    Calculate environmental risk score.

    Docker image: i2t-backendwithintertwino6-column-extension:latest
    """
    env = {
        "EXTENDER_ID": "environmentalRiskCalculator",
        "INPUT_COLUMNS": "precipitation_sum,population_density,land_use_type",
        "OUTPUT_COLUMN": "risk_score",
        "CALCULATION_FORMULA": "[risk calculation parameters]",
    }
    _run_container(
        image="i2t-backendwithintertwino6-column-extension:latest",
        env=env,
        network=_docker_network(),
    )


@task(retries=1, retry_delay_seconds=60)
def save_final_data() -> None:
    """
    Persist the final enriched dataset.

    Docker image: i2t-backendwithintertwino6-save:latest
    """
    env = {
        "DATASET_ID": "2",
    }
    _run_container(
        image="i2t-backendwithintertwino6-save:latest",
        env=env,
        network=_docker_network(),
    )


@flow(
    name="load_and_modify_data_pipeline",
    task_runner=SequentialTaskRunner(),
)
def load_and_modify_data_pipeline() -> None:
    """
    Orchestrates the full data loading, enrichment, and saving pipeline.

    Execution order:
        1. Load and modify data
        2. Geocode reconciliation
        3. OpenMeteo weather extension
        4. Land‑use classification extension
        5. Population density extension
        6. Environmental risk calculation
        7. Save final dataset
    """
    load_and_modify_data()
    geocode_reconciliation()
    openmeteo_extension()
    land_use_extension()
    population_density_extension()
    environmental_risk_calculation()
    save_final_data()


if __name__ == "__main__":
    # Running the flow directly for debugging / local execution
    load_and_modify_data_pipeline()