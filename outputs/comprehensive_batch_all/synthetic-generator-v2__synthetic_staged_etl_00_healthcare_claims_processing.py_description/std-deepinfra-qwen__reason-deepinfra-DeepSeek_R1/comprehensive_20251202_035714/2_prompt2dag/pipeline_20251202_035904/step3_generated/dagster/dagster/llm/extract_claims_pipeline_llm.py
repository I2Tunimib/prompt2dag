# Generated by Dagster Code Generator
# Generation Metadata:
# - Job Name: extract_claims_pipeline
# - Description: No description provided.
# - Executor Type: multiprocess_executor
# - IO Manager: fs_io_manager
# - Dagster Version: 1.5.0
# - Use Assets: False
# - Required Resources: local_filesystem, data_warehouse

from dagster import job, op, Out, In, RetryPolicy, multiprocess_executor, fs_io_manager, resource, schedule

# Resources
@resource
def local_filesystem():
    """Local File System Resource"""
    pass

@resource
def data_warehouse():
    """Data Warehouse Resource"""
    pass

@resource
def power_bi():
    """Power BI Resource"""
    pass

@resource
def tableau():
    """Tableau Resource"""
    pass

# Ops
@op(
    out={"claims_data": Out()},
    retry_policy=RetryPolicy(max_retries=2),
    required_resource_keys={"local_filesystem"}
)
def extract_claims(context):
    """Extract Claims Data"""
    # Simulate data extraction
    claims_data = "claims_data"
    context.log.info(f"Extracted claims data: {claims_data}")
    return claims_data

@op(
    out={"providers_data": Out()},
    retry_policy=RetryPolicy(max_retries=2),
    required_resource_keys={"local_filesystem"}
)
def extract_providers(context):
    """Extract Providers Data"""
    # Simulate data extraction
    providers_data = "providers_data"
    context.log.info(f"Extracted providers data: {providers_data}")
    return providers_data

@op(
    ins={"claims_data": In(), "providers_data": In()},
    out={"joined_data": Out()},
    retry_policy=RetryPolicy(max_retries=2),
    required_resource_keys={"local_filesystem"}
)
def transform_join(context, claims_data, providers_data):
    """Transform and Join Data"""
    # Simulate data transformation and joining
    joined_data = f"joined_data from {claims_data} and {providers_data}"
    context.log.info(f"Transformed and joined data: {joined_data}")
    return joined_data

@op(
    ins={"joined_data": In()},
    out={"warehouse_data": Out()},
    retry_policy=RetryPolicy(max_retries=2),
    required_resource_keys={"data_warehouse"}
)
def load_warehouse(context, joined_data):
    """Load Data to Warehouse"""
    # Simulate data loading to warehouse
    warehouse_data = f"warehouse_data from {joined_data}"
    context.log.info(f"Loaded data to warehouse: {warehouse_data}")
    return warehouse_data

@op(
    ins={"joined_data": In()},
    retry_policy=RetryPolicy(max_retries=2),
    required_resource_keys={"power_bi", "tableau"}
)
def refresh_bi(context, joined_data):
    """Refresh BI Tools"""
    # Simulate refreshing BI tools
    context.log.info(f"Refreshed BI tools with data: {joined_data}")

# Job
@job(
    name="extract_claims_pipeline",
    description="No description provided.",
    executor_def=multiprocess_executor,
    resource_defs={
        "local_filesystem": local_filesystem,
        "data_warehouse": data_warehouse,
        "power_bi": power_bi,
        "tableau": tableau,
        "io_manager": fs_io_manager
    }
)
def extract_claims_pipeline():
    claims_data = extract_claims()
    providers_data = extract_providers()
    joined_data = transform_join(claims_data, providers_data)
    load_warehouse(joined_data)
    refresh_bi(joined_data)

# Schedule
@schedule(
    job=extract_claims_pipeline,
    cron_schedule="@daily",
    execution_timezone="UTC",
    name="extract_claims_pipeline_schedule",
    should_execute=lambda: True,
    execution_time_window=None,
    tags=None,
    description="No description provided.",
    default_status="RUNNING",
    job_name="extract_claims_pipeline"
)
def extract_claims_pipeline_schedule(_context):
    return {}