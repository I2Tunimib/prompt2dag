# Generated by Prefect 2.x Code Generator
# Date: 2023-10-05
# Prefect Version: 2.14.0

from prefect import flow, task, get_run_logger
from prefect.task_runners import ConcurrentTaskRunner
from prefect.deployments import Deployment
from prefect.infrastructure.docker import DockerContainer
from prefect.filesystems import LocalFileSystem
from prefect.blocks.system import Secret
from prefect import flow, task
from prefect.orion.schemas.schedules import CronSchedule
import os

# Load connections/resources
local_filesystem = LocalFileSystem.load("local_filesystem")
data_warehouse = Secret.load("data_warehouse")
power_bi = Secret.load("power_bi")
tableau = Secret.load("tableau")

@task(retries=2, name="Extract Claims Data")
def extract_claims():
    logger = get_run_logger()
    logger.info("Extracting claims data...")
    # Placeholder for actual data extraction logic
    return "claims_data"

@task(retries=2, name="Extract Providers Data")
def extract_providers():
    logger = get_run_logger()
    logger.info("Extracting providers data...")
    # Placeholder for actual data extraction logic
    return "providers_data"

@task(retries=2, name="Transform and Join Data")
def transform_join(claims_data, providers_data):
    logger = get_run_logger()
    logger.info("Transforming and joining data...")
    # Placeholder for actual data transformation and joining logic
    return "transformed_data"

@task(retries=2, name="Load Data to Warehouse")
def load_warehouse(transformed_data):
    logger = get_run_logger()
    logger.info("Loading data to warehouse...")
    # Placeholder for actual data loading logic
    return "data_loaded"

@task(retries=2, name="Refresh BI Tools")
def refresh_bi(transformed_data):
    logger = get_run_logger()
    logger.info("Refreshing BI tools...")
    # Placeholder for actual BI tool refresh logic
    return "bi_refreshed"

@flow(name="extract_claims_pipeline", task_runner=ConcurrentTaskRunner)
def extract_claims_pipeline():
    logger = get_run_logger()
    logger.info("Starting extract_claims_pipeline...")

    claims_data = extract_claims()
    providers_data = extract_providers()
    transformed_data = transform_join(claims_data, providers_data)
    load_warehouse(transformed_data)
    refresh_bi(transformed_data)

    logger.info("extract_claims_pipeline completed successfully.")

# Schedule configuration
schedule = CronSchedule(cron="0 0 * * *", timezone="UTC", catchup=False)

# Deployment configuration
deployment = Deployment.build_from_flow(
    flow=extract_claims_pipeline,
    name="extract_claims_pipeline_deployment",
    work_pool_name="default-agent-pool",
    schedule=schedule,
    parameters={},
    infra_overrides={"env": {"PREFECT_LOGGING_LEVEL": "INFO"}},
    work_queue_name="default",
)

if __name__ == "__main__":
    deployment.apply()
```
```python
# This is a placeholder for the actual deployment apply call.
# You can run this script to apply the deployment to your Prefect environment.