# Generated by Prefect Pipeline Generator on 2024-06-28
# Pipeline: healthcare_claims_etl
# Description: Comprehensive Pipeline Description
# Pattern: fanout_fanin
# Prefect version: 2.14.0

import logging
from pathlib import Path
from typing import Dict

import pandas as pd
import requests
from sqlalchemy import create_engine
from sqlalchemy.exc import SQLAlchemyError

from prefect import flow, task, get_run_logger
from prefect.task_runners import ConcurrentTaskRunner
from prefect.deployments import DeploymentSpec
from prefect.server.schemas.schedules import CronSchedule
from prefect.filesystems import LocalFileSystem
from prefect.blocks.system import Secret

# -------------------------------------------------------------------------
# Helper Functions
# -------------------------------------------------------------------------

def _load_local_block(block_name: str) -> LocalFileSystem:
    """Load a Prefect LocalFileSystem block."""
    try:
        return LocalFileSystem.load(block_name)
    except Exception as exc:
        raise RuntimeError(f"Failed to load LocalFileSystem block '{block_name}': {exc}") from exc


def _load_secret_block(block_name: str) -> Secret:
    """Load a Prefect Secret block."""
    try:
        return Secret.load(block_name)
    except Exception as exc:
        raise RuntimeError(f"Failed to load Secret block '{block_name}': {exc}") from exc


def _get_secret_dict(secret: Secret) -> Dict[str, str]:
    """Parse a Secret block that stores a JSON string."""
    try:
        secret_value = secret.get()
        if isinstance(secret_value, dict):
            return secret_value
        # Assume JSON string
        import json
        return json.loads(secret_value)
    except Exception as exc:
        raise RuntimeError(f"Unable to parse secret value: {exc}") from exc


# -------------------------------------------------------------------------
# Tasks
# -------------------------------------------------------------------------

@task(retries=2, retry_delay_seconds=60, name="Extract Claims CSV")
def extract_claims() -> pd.DataFrame:
    """
    Load the claims CSV file from the local filesystem.

    Returns:
        pandas.DataFrame: Claims data.
    """
    logger = get_run_logger()
    fs_block = _load_local_block("local_csv_files")
    claims_path = Path(fs_block.base_path) / "claims.csv"

    logger.info("Reading claims data from %s", claims_path)
    try:
        df = pd.read_csv(claims_path)
        logger.info("Loaded %d claim records", len(df))
        return df
    except Exception as exc:
        logger.error("Failed to read claims CSV: %s", exc)
        raise


@task(retries=2, retry_delay_seconds=60, name="Extract Providers CSV")
def extract_providers() -> pd.DataFrame:
    """
    Load the providers CSV file from the local filesystem.

    Returns:
        pandas.DataFrame: Providers data.
    """
    logger = get_run_logger()
    fs_block = _load_local_block("local_csv_files")
    providers_path = Path(fs_block.base_path) / "providers.csv"

    logger.info("Reading providers data from %s", providers_path)
    try:
        df = pd.read_csv(providers_path)
        logger.info("Loaded %d provider records", len(df))
        return df
    except Exception as exc:
        logger.error("Failed to read providers CSV: %s", exc)
        raise


@task(retries=2, retry_delay_seconds=60, name="Join and Anonymize Claims with Providers")
def transform_join(claims: pd.DataFrame, providers: pd.DataFrame) -> pd.DataFrame:
    """
    Join claims with provider information and anonymize PII.

    Args:
        claims: DataFrame containing claim records.
        providers: DataFrame containing provider records.

    Returns:
        pandas.DataFrame: Transformed and anonymized dataset.
    """
    logger = get_run_logger()
    logger.info("Joining claims (%d) with providers (%d)", len(claims), len(providers))

    try:
        merged = claims.merge(providers, how="left", on="provider_id")
        logger.info("Merged dataset contains %d rows", len(merged))
    except Exception as exc:
        logger.error("Failed to merge datasets: %s", exc)
        raise

    # Anonymization: drop or hash PII columns
    pii_columns = ["patient_name", "patient_ssn", "patient_address"]
    for col in pii_columns:
        if col in merged.columns:
            merged.drop(columns=col, inplace=True)
            logger.debug("Dropped PII column: %s", col)

    # Example of hashing a column (if needed)
    if "patient_email" in merged.columns:
        merged["patient_email_hash"] = merged["patient_email"].apply(
            lambda x: pd.util.hash_pandas_object(pd.Series([x])).iloc[0]
        )
        merged.drop(columns="patient_email", inplace=True)
        logger.debug("Hashed patient_email column")

    logger.info("Anonymization complete")
    return merged


@task(retries=2, retry_delay_seconds=60, name="Load Transformed Data to Data Warehouse")
def load_warehouse(transformed_df: pd.DataFrame) -> None:
    """
    Load the transformed DataFrame into a Postgres data warehouse.

    Args:
        transformed_df: The DataFrame to be loaded.
    """
    logger = get_run_logger()
    secret_block = _load_secret_block("postgres_warehouse")
    creds = _get_secret_dict(secret_block)

    required_keys = {"username", "password", "host", "port", "database"}
    if not required_keys.issubset(creds):
        missing = required_keys - set(creds.keys())
        raise RuntimeError(f"Postgres secret missing required keys: {missing}")

    conn_str = (
        f"postgresql://{creds['username']}:{creds['password']}"
        f"@{creds['host']}:{creds['port']}/{creds['database']}"
    )
    logger.info("Creating SQLAlchemy engine")
    try:
        engine = create_engine(conn_str)
        with engine.begin() as conn:
            transformed_df.to_sql(
                name="claims_enriched",
                con=conn,
                if_exists="replace",
                index=False,
                method="multi",
                chunksize=1000,
            )
        logger.info("Data successfully loaded into warehouse table 'claims_enriched'")
    except SQLAlchemyError as exc:
        logger.error("Failed to load data into warehouse: %s", exc)
        raise


@task(retries=2, retry_delay_seconds=60, name="Refresh BI Dashboards")
def refresh_bi(transformed_df: pd.DataFrame) -> None:
    """
    Trigger refreshes for Power BI and Tableau dashboards.

    Args:
        transformed_df: The DataFrame that was just loaded (unused directly but kept for dependency).
    """
    logger = get_run_logger()

    # Power BI Refresh
    power_bi_secret = _load_secret_block("power_bi")
    power_bi_cfg = _get_secret_dict(power_bi_secret)

    power_bi_url = power_bi_cfg.get("refresh_endpoint")
    power_bi_token = power_bi_cfg.get("access_token")
    if power_bi_url and power_bi_token:
        headers = {"Authorization": f"Bearer {power_bi_token}"}
        try:
            resp = requests.post(power_bi_url, headers=headers, timeout=30)
            resp.raise_for_status()
            logger.info("Power BI refresh triggered successfully")
        except Exception as exc:
            logger.error("Power BI refresh failed: %s", exc)
            raise
    else:
        logger.warning("Power BI configuration incomplete; skipping refresh")

    # Tableau Refresh
    tableau_secret = _load_secret_block("tableau")
    tableau_cfg = _get_secret_dict(tableau_secret)

    tableau_url = tableau_cfg.get("refresh_endpoint")
    tableau_token = tableau_cfg.get("access_token")
    if tableau_url and tableau_token:
        headers = {"X-Tableau-Auth": tableau_token}
        try:
            resp = requests.post(tableau_url, headers=headers, timeout=30)
            resp.raise_for_status()
            logger.info("Tableau refresh triggered successfully")
        except Exception as exc:
            logger.error("Tableau refresh failed: %s", exc)
            raise
    else:
        logger.warning("Tableau configuration incomplete; skipping refresh")


# -------------------------------------------------------------------------
# Flow Definition
# -------------------------------------------------------------------------

@flow(
    name="healthcare_claims_etl",
    task_runner=ConcurrentTaskRunner(),
    description="Comprehensive Pipeline Description",
)
def healthcare_claims_etl():
    """
    Orchestrates the ETL pipeline for healthcare claims data.

    Execution order (fan‑out/fan‑in):
        1. extract_claims & extract_providers (parallel)
        2. transform_join (depends on both extracts)
        3. load_warehouse (depends on transform_join)
        4. refresh_bi (depends on transform_join)
    """
    logger = get_run_logger()
    logger.info("Starting ETL pipeline")

    # Fan‑out: parallel extraction
    claims_df = extract_claims.submit()
    providers_df = extract_providers.submit()

    # Fan‑in: join and anonymize
    transformed_df = transform_join.wait_for(claims_df, providers_df)

    # Downstream tasks
    load_warehouse.submit(transformed_df)
    refresh_bi.submit(transformed_df)

    logger.info("ETL pipeline execution scheduled")


# -------------------------------------------------------------------------
# Deployment Specification
# -------------------------------------------------------------------------

DeploymentSpec(
    name="healthcare_claims_etl_deployment",
    flow=healthcare_claims_etl,
    schedule=CronSchedule(cron="0 0 * * *", timezone="UTC"),  # @daily UTC
    tags=["daily"],
    parameters={},
    work_pool_name="default-agent-pool",
    enforce_parameter_schema=False,
    description="Daily ETL pipeline for healthcare claims data",
    flow_runner=ConcurrentTaskRunner(),
    catchup=False,
)

# -------------------------------------------------------------------------
# Entry Point
# -------------------------------------------------------------------------

if __name__ == "__main__":
    # Running the flow directly (useful for local testing)
    healthcare_claims_etl()