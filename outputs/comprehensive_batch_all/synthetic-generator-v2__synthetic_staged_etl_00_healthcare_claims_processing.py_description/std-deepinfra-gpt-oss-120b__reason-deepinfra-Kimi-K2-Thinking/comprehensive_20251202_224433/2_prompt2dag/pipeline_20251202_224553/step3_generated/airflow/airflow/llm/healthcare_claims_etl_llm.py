# Generated by AI on 2024-06-28
# Airflow DAG: healthcare_claims_etl
# Description: Comprehensive Pipeline Description
# Pattern: fanout_fanin

import os
import logging
from datetime import datetime, timedelta

import pandas as pd
import requests
from airflow import DAG
from airflow.decorators import task
from airflow.utils.dates import days_ago
from airflow.exceptions import AirflowException
from airflow.hooks.base import BaseHook
from airflow.providers.postgres.hooks.postgres import PostgresHook

# Default arguments for all tasks
default_args = {
    "owner": "airflow",
    "depends_on_past": False,
    "retries": 2,
    "retry_delay": timedelta(minutes=5),
    "email_on_failure": False,
    "email_on_retry": False,
}

# DAG definition
with DAG(
    dag_id="healthcare_claims_etl",
    description="Comprehensive Pipeline Description",
    schedule_interval="@daily",
    start_date=days_ago(1),
    catchup=False,
    default_args=default_args,
    tags=["healthcare", "etl"],
    max_active_runs=1,
    timezone="UTC",
) as dag:

    @task(retries=2, retry_delay=timedelta(minutes=5))
    def extract_claims() -> pd.DataFrame:
        """
        Extract claims data from local CSV files.
        Returns a pandas DataFrame.
        """
        try:
            conn = BaseHook.get_connection("local_csv_files")
            base_path = conn.extra_dejson.get("base_path", "")
            file_path = os.path.join(base_path, "claims.csv")
            logging.info("Reading claims CSV from %s", file_path)
            df = pd.read_csv(file_path)
            logging.info("Extracted %d claim records", len(df))
            return df
        except Exception as exc:
            logging.error("Failed to extract claims: %s", exc)
            raise AirflowException(exc)

    @task(retries=2, retry_delay=timedelta(minutes=5))
    def extract_providers() -> pd.DataFrame:
        """
        Extract providers data from local CSV files.
        Returns a pandas DataFrame.
        """
        try:
            conn = BaseHook.get_connection("local_csv_files")
            base_path = conn.extra_dejson.get("base_path", "")
            file_path = os.path.join(base_path, "providers.csv")
            logging.info("Reading providers CSV from %s", file_path)
            df = pd.read_csv(file_path)
            logging.info("Extracted %d provider records", len(df))
            return df
        except Exception as exc:
            logging.error("Failed to extract providers: %s", exc)
            raise AirflowException(exc)

    @task(retries=2, retry_delay=timedelta(minutes=5))
    def transform_join(claims: pd.DataFrame, providers: pd.DataFrame) -> pd.DataFrame:
        """
        Join claims with providers and anonymize sensitive fields.
        Returns the transformed DataFrame.
        """
        try:
            logging.info("Joining claims (%d) with providers (%d)", len(claims), len(providers))
            merged = claims.merge(
                providers,
                how="left",
                left_on="provider_id",
                right_on="provider_id",
                suffixes=("", "_prov"),
            )
            # Anonymize PII: drop or hash columns like patient_name, ssn
            pii_columns = ["patient_name", "patient_ssn", "address"]
            for col in pii_columns:
                if col in merged.columns:
                    merged.drop(columns=col, inplace=True)
            logging.info("Transformed dataset contains %d records", len(merged))
            return merged
        except Exception as exc:
            logging.error("Transformation failed: %s", exc)
            raise AirflowException(exc)

    @task(retries=2, retry_delay=timedelta(minutes=5))
    def load_warehouse(transformed_df: pd.DataFrame) -> None:
        """
        Load the transformed data into the Postgres data warehouse.
        """
        try:
            pg_hook = PostgresHook(postgres_conn_id="postgres_warehouse")
            engine = pg_hook.get_sqlalchemy_engine()
            table_name = "claims_transformed"
            logging.info("Loading data into %s.%s", pg_hook.schema, table_name)
            transformed_df.to_sql(
                name=table_name,
                con=engine,
                if_exists="replace",
                index=False,
                method="multi",
            )
            logging.info("Load completed successfully")
        except Exception as exc:
            logging.error("Failed to load data into warehouse: %s", exc)
            raise AirflowException(exc)

    @task(retries=2, retry_delay=timedelta(minutes=5))
    def refresh_bi() -> None:
        """
        Trigger refresh of Power BI and Tableau dashboards via their APIs.
        """
        try:
            # Power BI refresh
            power_bi_conn = BaseHook.get_connection("power_bi")
            power_bi_url = power_bi_conn.host.rstrip("/") + "/refresh"
            power_bi_token = power_bi_conn.password  # Assuming token stored in password field
            headers = {"Authorization": f"Bearer {power_bi_token}"}
            logging.info("Triggering Power BI refresh at %s", power_bi_url)
            resp_pb = requests.post(power_bi_url, headers=headers, timeout=30)
            resp_pb.raise_for_status()
            logging.info("Power BI refresh triggered successfully")

            # Tableau refresh
            tableau_conn = BaseHook.get_connection("tableau")
            tableau_url = tableau_conn.host.rstrip("/") + "/api/refresh"
            tableau_token = tableau_conn.password
            headers = {"X-Tableau-Auth": tableau_token}
            logging.info("Triggering Tableau refresh at %s", tableau_url)
            resp_tb = requests.post(tableau_url, headers=headers, timeout=30)
            resp_tb.raise_for_status()
            logging.info("Tableau refresh triggered successfully")
        except Exception as exc:
            logging.error("BI refresh failed: %s", exc)
            raise AirflowException(exc)

    # Define task pipeline
    claims_df = extract_claims()
    providers_df = extract_providers()
    transformed_df = transform_join(claims_df, providers_df)

    load_warehouse(transformed_df)
    refresh_bi()

    # Set explicit dependencies (fanâ€‘out/fanâ€‘in)
    [claims_df, providers_df] >> transformed_df
    transformed_df >> [load_warehouse, refresh_bi]