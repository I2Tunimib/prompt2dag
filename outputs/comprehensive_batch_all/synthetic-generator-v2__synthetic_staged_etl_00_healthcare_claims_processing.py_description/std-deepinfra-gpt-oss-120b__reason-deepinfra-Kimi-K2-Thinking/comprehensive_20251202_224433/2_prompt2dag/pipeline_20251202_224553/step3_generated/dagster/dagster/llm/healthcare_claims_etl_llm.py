# Generated by Dagster ETL code generator
# Date: 2024-06-28
# Pipeline: healthcare_claims_etl
# Description: Comprehensive Pipeline Description
# Pattern: fanout_fanin

import os
from typing import Any, Dict

import pandas as pd
from dagster import (
    In,
    Out,
    RetryPolicy,
    ResourceDefinition,
    ScheduleDefinition,
    ScheduleStatus,
    asset,
    fs_io_manager,
    job,
    multiprocess_executor,
    op,
    schedule,
)


# -------------------------------------------------------------------------
# Resources
# -------------------------------------------------------------------------

def local_csv_files_resource(init_context) -> Dict[str, str]:
    """Resource providing the base directory for local CSV files."""
    base_path = init_context.resource_config.get("base_path", "./data")
    return {"base_path": base_path}


def postgres_warehouse_resource(init_context):
    """Placeholder resource for a Postgres data warehouse."""
    # In a real implementation you would create a connection pool here.
    class _PostgresWarehouse:
        def load_dataframe(self, df: pd.DataFrame, table_name: str) -> None:
            init_context.log.info(f"Loading {len(df)} rows into Postgres table '{table_name}'.")
            # Placeholder: replace with actual INSERT logic using psycopg2 / SQLAlchemy.

    return _PostgresWarehouse()


def power_bi_resource(init_context):
    """Placeholder resource for Power BI refresh API."""
    class _PowerBI:
        def refresh(self) -> None:
            init_context.log.info("Triggering Power BI dashboard refresh.")
            # Placeholder: call Power BI REST API.

    return _PowerBI()


def tableau_resource(init_context):
    """Placeholder resource for Tableau refresh API."""
    class _Tableau:
        def refresh(self) -> None:
            init_context.log.info("Triggering Tableau dashboard refresh.")
            # Placeholder: call Tableau REST API.

    return _Tableau()


# -------------------------------------------------------------------------
# Ops
# -------------------------------------------------------------------------

@op(
    name="Extract Claims CSV",
    description="Read claims data from a local CSV file.",
    out=Out(pd.DataFrame),
    retry_policy=RetryPolicy(max_retries=2),
)
def extract_claims(context, csv_files: Dict[str, str] = In(dict)):
    """Load claims CSV into a pandas DataFrame."""
    file_path = os.path.join(csv_files["base_path"], "claims.csv")
    context.log.info(f"Reading claims data from {file_path}")
    df = pd.read_csv(file_path)
    context.log.info(f"Loaded {len(df)} claim records.")
    return df


@op(
    name="Extract Providers CSV",
    description="Read providers data from a local CSV file.",
    out=Out(pd.DataFrame),
    retry_policy=RetryPolicy(max_retries=2),
)
def extract_providers(context, csv_files: Dict[str, str] = In(dict)):
    """Load providers CSV into a pandas DataFrame."""
    file_path = os.path.join(csv_files["base_path"], "providers.csv")
    context.log.info(f"Reading providers data from {file_path}")
    df = pd.read_csv(file_path)
    context.log.info(f"Loaded {len(df)} provider records.")
    return df


@op(
    name="Join and Anonymize Claims with Providers",
    description="Join claims with provider information and remove PII.",
    ins={"claims": In(pd.DataFrame), "providers": In(pd.DataFrame)},
    out=Out(pd.DataFrame),
    retry_policy=RetryPolicy(max_retries=2),
)
def transform_join(context, claims: pd.DataFrame, providers: pd.DataFrame) -> pd.DataFrame:
    """Join claims and providers on provider_id and anonymize sensitive fields."""
    context.log.info("Joining claims with providers.")
    merged = claims.merge(providers, how="left", on="provider_id", suffixes=("", "_prov"))

    # Example anonymization: drop patient name and SSN if present
    pii_columns = [col for col in merged.columns if col.lower() in {"patient_name", "ssn"}]
    if pii_columns:
        context.log.info(f"Removing PII columns: {pii_columns}")
        merged = merged.drop(columns=pii_columns)

    context.log.info(f"Resulting dataset has {len(merged)} rows and {len(merged.columns)} columns.")
    return merged


@op(
    name="Load Transformed Data to Data Warehouse",
    description="Persist the transformed dataset into the Postgres warehouse.",
    ins={"transformed": In(pd.DataFrame)},
    out=Out(None),
    retry_policy=RetryPolicy(max_retries=2),
)
def load_warehouse(context, transformed: pd.DataFrame, warehouse=In(dict)):
    """Write the transformed DataFrame to a Postgres table."""
    table_name = context.op_config.get("target_table", "claims_transformed")
    context.log.info(f"Loading data into warehouse table '{table_name}'.")
    warehouse.load_dataframe(transformed, table_name)
    context.log.info("Load completed.")


@op(
    name="Refresh BI Dashboards",
    description="Trigger refreshes for Power BI and Tableau dashboards.",
    ins={"transformed": In(pd.DataFrame)},
    out=Out(None),
    retry_policy=RetryPolicy(max_retries=2),
)
def refresh_bi(context, transformed: pd.DataFrame, power_bi=In(dict), tableau=In(dict)):
    """Call external APIs to refresh BI dashboards."""
    context.log.info("Refreshing Power BI dashboards.")
    power_bi.refresh()
    context.log.info("Refreshing Tableau dashboards.")
    tableau.refresh()
    context.log.info("BI dashboard refreshes triggered.")


# -------------------------------------------------------------------------
# Job
# -------------------------------------------------------------------------

@job(
    name="healthcare_claims_etl",
    description="Comprehensive Pipeline Description",
    executor_def=multiprocess_executor,
    resource_defs={
        "csv_files": ResourceDefinition.resource_fn(local_csv_files_resource).configured(
            {"base_path": "./data"}
        ),
        "fs_io_manager": fs_io_manager,
        "postgres_warehouse": ResourceDefinition.resource_fn(postgres_warehouse_resource),
        "power_bi": ResourceDefinition.resource_fn(power_bi_resource),
        "tableau": ResourceDefinition.resource_fn(tableau_resource),
    },
)
def healthcare_claims_etl():
    """Orchestrates extraction, transformation, loading, and BI refresh for healthcare claims."""
    claims_df = extract_claims(csv_files=healthcare_claims_etl.resource_defs["csv_files"])
    providers_df = extract_providers(csv_files=healthcare_claims_etl.resource_defs["csv_files"])

    transformed_df = transform_join(claims=claims_df, providers=providers_df)

    load_warehouse(
        transformed=transformed_df,
        warehouse=healthcare_claims_etl.resource_defs["postgres_warehouse"],
    )

    refresh_bi(
        transformed=transformed_df,
        power_bi=healthcare_claims_etl.resource_defs["power_bi"],
        tableau=healthcare_claims_etl.resource_defs["tableau"],
    )


# -------------------------------------------------------------------------
# Schedule
# -------------------------------------------------------------------------

@schedule(
    cron_schedule="@daily",
    job=healthcare_claims_etl,
    execution_timezone="UTC",
    default_status=ScheduleStatus.RUNNING,
    description="Daily execution of the healthcare claims ETL pipeline.",
    tags={"catchup": "false"},
)
def healthcare_claims_etl_schedule():
    """Schedule that triggers the healthcare_claims_etl job daily at midnight UTC."""
    return {}

# End of generated file.