# Generated by Airflow DAG Generator on 2024-06-20
"""
Airflow DAG: extract_claims_pipeline
Description: Healthcare claims processing ETL pipeline implementing a staged ETL pattern
with parallel extraction, transformation, and parallel loading stages.
Pattern: fanout_fanin
"""

from datetime import datetime, timedelta
from typing import Any, Dict

import pandas as pd
from airflow import DAG
from airflow.decorators import task
from airflow.exceptions import AirflowException
from airflow.providers.http.hooks.http import HttpHook
from airflow.providers.postgres.hooks.postgres import PostgresHook
from airflow.utils.timezone import utc

# ----------------------------------------------------------------------
# Default arguments applied to all tasks
# ----------------------------------------------------------------------
DEFAULT_ARGS = {
    "owner": "airflow",
    "depends_on_past": False,
    "retries": 2,
    "retry_delay": timedelta(minutes=5),
    "email_on_failure": False,
    "email_on_retry": False,
}

# ----------------------------------------------------------------------
# DAG definition
# ----------------------------------------------------------------------
with DAG(
    dag_id="extract_claims_pipeline",
    description="Healthcare claims processing ETL pipeline implementing a staged ETL pattern with parallel extraction, transformation, and parallel loading stages.",
    schedule_interval="@daily",
    start_date=datetime(2023, 1, 1, tzinfo=utc),
    catchup=False,
    default_args=DEFAULT_ARGS,
    tags=["healthcare", "etl", "fanout_fanin"],
    max_active_runs=1,
) as dag:

    # ------------------------------------------------------------------
    # Extraction tasks
    # ------------------------------------------------------------------
    @task(task_id="extract_claims")
    def extract_claims() -> pd.DataFrame:
        """
        Extract claims data from a local CSV file.
        Returns a pandas DataFrame.
        """
        try:
            # Retrieve connection details (placeholder – adjust as needed)
            # Assuming the connection stores the base path in the 'host' field
            from airflow.hooks.base import BaseHook

            conn = BaseHook.get_connection("local_csv")
            csv_path = f"{conn.host}/claims.csv"

            df = pd.read_csv(csv_path)
            return df
        except Exception as exc:
            raise AirflowException(f"Failed to extract claims: {exc}") from exc

    @task(task_id="extract_providers")
    def extract_providers() -> pd.DataFrame:
        """
        Extract providers data from a local CSV file.
        Returns a pandas DataFrame.
        """
        try:
            from airflow.hooks.base import BaseHook

            conn = BaseHook.get_connection("local_csv")
            csv_path = f"{conn.host}/providers.csv"

            df = pd.read_csv(csv_path)
            return df
        except Exception as exc:
            raise AirflowException(f"Failed to extract providers: {exc}") from exc

    # ------------------------------------------------------------------
    # Transformation task (joins, anonymization, risk scoring)
    # ------------------------------------------------------------------
    @task(task_id="transform_join")
    def transform_join(
        claims_df: pd.DataFrame, providers_df: pd.DataFrame
    ) -> pd.DataFrame:
        """
        Join claims with providers, anonymize PII, and calculate risk scores.
        Returns the transformed DataFrame.
        """
        try:
            # Join on provider_id (adjust column names as needed)
            merged = pd.merge(
                claims_df,
                providers_df,
                how="left",
                left_on="provider_id",
                right_on="provider_id",
                suffixes=("_claim", "_provider"),
            )

            # Anonymize PII: drop or hash sensitive columns
            pii_columns = ["patient_name", "patient_ssn", "patient_address"]
            for col in pii_columns:
                if col in merged.columns:
                    merged[col] = merged[col].apply(lambda x: hash(x))

            # Example risk score calculation (placeholder logic)
            merged["risk_score"] = (
                merged["claim_amount"] * merged["provider_rating"]
            ).fillna(0)

            # Keep only needed columns for downstream loading
            final_columns = [
                "claim_id",
                "provider_id",
                "claim_amount",
                "risk_score",
                "service_date",
            ]
            transformed = merged[final_columns]
            return transformed
        except Exception as exc:
            raise AirflowException(f"Transformation failed: {exc}") from exc

    # ------------------------------------------------------------------
    # Load task – write transformed data to PostgreSQL warehouse
    # ------------------------------------------------------------------
    @task(task_id="load_warehouse")
    def load_warehouse(transformed_df: pd.DataFrame) -> None:
        """
        Load the transformed DataFrame into the PostgreSQL data warehouse.
        """
        try:
            hook = PostgresHook(postgres_conn_id="postgres_warehouse")
            # Create a temporary table name
            temp_table = "stg_claims_transformed"

            # Ensure target table exists (simple upsert logic)
            create_sql = f"""
            CREATE TABLE IF NOT EXISTS claims_processed (
                claim_id VARCHAR PRIMARY KEY,
                provider_id VARCHAR,
                claim_amount NUMERIC,
                risk_score NUMERIC,
                service_date DATE
            );
            """
            hook.run(create_sql)

            # Load data using pandas to_sql via SQLAlchemy engine
            engine = hook.get_sqlalchemy_engine()
            transformed_df.to_sql(
                name=temp_table,
                con=engine,
                if_exists="replace",
                index=False,
                method="multi",
            )

            # Upsert from staging table to final table
            upsert_sql = f"""
            INSERT INTO claims_processed (claim_id, provider_id, claim_amount, risk_score, service_date)
            SELECT claim_id, provider_id, claim_amount, risk_score, service_date
            FROM {temp_table}
            ON CONFLICT (claim_id) DO UPDATE SET
                provider_id = EXCLUDED.provider_id,
                claim_amount = EXCLUDED.claim_amount,
                risk_score = EXCLUDED.risk_score,
                service_date = EXCLUDED.service_date;
            DROP TABLE IF EXISTS {temp_table};
            """
            hook.run(upsert_sql)
        except Exception as exc:
            raise AirflowException(f"Loading to warehouse failed: {exc}") from exc

    # ------------------------------------------------------------------
    # BI Refresh task – trigger Power BI and Tableau refreshes
    # ------------------------------------------------------------------
    @task(task_id="refresh_bi")
    def refresh_bi() -> Dict[str, Any]:
        """
        Call external APIs to refresh Power BI and Tableau dashboards.
        Returns a dict with the status of each refresh.
        """
        results = {}
        try:
            # Power BI refresh
            power_bi_hook = HttpHook(http_conn_id="power_bi", method="POST")
            power_bi_response = power_bi_hook.run(endpoint="/refresh", data={})
            results["power_bi"] = {
                "status_code": power_bi_response.status_code,
                "content": power_bi_response.text,
            }

            # Tableau refresh
            tableau_hook = HttpHook(http_conn_id="tableau", method="POST")
            tableau_response = tableau_hook.run(endpoint="/refresh", data={})
            results["tableau"] = {
                "status_code": tableau_response.status_code,
                "content": tableau_response.text,
            }

            return results
        except Exception as exc:
            raise AirflowException(f"BI refresh failed: {exc}") from exc

    # ------------------------------------------------------------------
    # Define task pipeline
    # ------------------------------------------------------------------
    claims_df = extract_claims()
    providers_df = extract_providers()

    transformed_df = transform_join(claims_df, providers_df)

    load_warehouse(transformed_df)

    refresh_bi_task = refresh_bi()

    # ------------------------------------------------------------------
    # Set explicit dependencies (fan‑out / fan‑in)
    # ------------------------------------------------------------------
    # transform_join already depends on both extraction tasks via arguments
    # load_warehouse depends on transform_join
    # refresh_bi depends on transform_join (but not on load_warehouse)
    transformed_df >> load_warehouse
    transformed_df >> refresh_bi_task

# End of DAG definition.