# Generated by Prefect pipeline generator
# Date: 2024-07-31
# Pipeline: extract_claims_pipeline
# Description: Healthcare claims processing ETL pipeline implementing a staged ETL pattern with parallel extraction,
# transformation, and parallel loading stages (fanout/fanin).

import os
import logging
from datetime import datetime
from typing import Any

import pandas as pd
import requests
from sqlalchemy import create_engine

from prefect import flow, task, get_run_logger
from prefect.task_runners import ConcurrentTaskRunner
from prefect.deployments import Deployment
from prefect.orion.schemas.schedules import CronSchedule

# -------------------------------------------------------------------------
# Configuration & Secrets (expected to be provided via Prefect blocks or env vars)
# -------------------------------------------------------------------------
POSTGRES_DSN = os.getenv("POSTGRES_DSN")  # e.g., "postgresql://user:pass@host:port/db"
POWER_BI_REFRESH_URL = os.getenv("POWER_BI_REFRESH_URL")
POWER_BI_TOKEN = os.getenv("POWER_BI_TOKEN")
TABLEAU_REFRESH_URL = os.getenv("TABLEAU_REFRESH_URL")
TABLEAU_TOKEN = os.getenv("TABLEAU_TOKEN")

CLAIMS_CSV_PATH = os.getenv("CLAIMS_CSV_PATH", "data/claims.csv")
PROVIDERS_CSV_PATH = os.getenv("PROVIDERS_CSV_PATH", "data/providers.csv")


# -------------------------------------------------------------------------
# Extraction Tasks
# -------------------------------------------------------------------------
@task(retries=2, retry_delay_seconds=60)
def extract_claims() -> pd.DataFrame:
    """
    Load the claims CSV file from the local filesystem.

    Returns:
        pandas.DataFrame: Raw claims data.
    """
    logger = get_run_logger()
    logger.info(f"Reading claims data from %s", CLAIMS_CSV_PATH)
    try:
        df = pd.read_csv(CLAIMS_CSV_PATH)
        logger.info("Loaded %d claim records", len(df))
        return df
    except Exception as exc:
        logger.error("Failed to read claims CSV: %s", exc)
        raise


@task(retries=2, retry_delay_seconds=60)
def extract_providers() -> pd.DataFrame:
    """
    Load the providers CSV file from the local filesystem.

    Returns:
        pandas.DataFrame: Raw providers data.
    """
    logger = get_run_logger()
    logger.info(f"Reading providers data from %s", PROVIDERS_CSV_PATH)
    try:
        df = pd.read_csv(PROVIDERS_CSV_PATH)
        logger.info("Loaded %d provider records", len(df))
        return df
    except Exception as exc:
        logger.error("Failed to read providers CSV: %s", exc)
        raise


# -------------------------------------------------------------------------
# Transformation Helpers
# -------------------------------------------------------------------------
def _anonymize(df: pd.DataFrame) -> pd.DataFrame:
    """
    Redact or hash personally identifiable information (PII).

    Args:
        df: DataFrame containing raw data.

    Returns:
        DataFrame with PII columns redacted.
    """
    pii_columns = ["patient_name", "patient_ssn", "patient_address"]
    for col in pii_columns:
        if col in df.columns:
            df[col] = "REDACTED"
    return df


def _calculate_risk_score(df: pd.DataFrame) -> pd.DataFrame:
    """
    Add a simple risk score based on claim amount.

    Args:
        df: DataFrame after join.

    Returns:
        DataFrame with an additional ``risk_score`` column.
    """
    if "claim_amount" in df.columns:
        df["risk_score"] = df["claim_amount"].apply(lambda x: min(1.0, x / 10_000))
    else:
        df["risk_score"] = 0.0
    return df


# -------------------------------------------------------------------------
# Transformation Task
# -------------------------------------------------------------------------
@task(retries=2, retry_delay_seconds=60)
def transform_join(claims: pd.DataFrame, providers: pd.DataFrame) -> pd.DataFrame:
    """
    Join claims with provider data, anonymize PII, and calculate risk scores.

    Args:
        claims: Raw claims DataFrame.
        providers: Raw providers DataFrame.

    Returns:
        Transformed DataFrame ready for loading.
    """
    logger = get_run_logger()
    logger.info("Joining claims with providers on 'provider_id'")
    try:
        merged = claims.merge(
            providers,
            how="left",
            on="provider_id",
            suffixes=("_claim", "_provider"),
        )
        logger.info("Merged dataset contains %d rows", len(merged))
        merged = _anonymize(merged)
        merged = _calculate_risk_score(merged)
        logger.info("Transformation (anonymization & risk scoring) completed")
        return merged
    except Exception as exc:
        logger.error("Transformation failed: %s", exc)
        raise


# -------------------------------------------------------------------------
# Loading Task
# -------------------------------------------------------------------------
@task(retries=2, retry_delay_seconds=60)
def load_warehouse(df: pd.DataFrame) -> None:
    """
    Load the transformed DataFrame into the PostgreSQL data warehouse.

    Args:
        df: Transformed claims DataFrame.
    """
    logger = get_run_logger()
    if not POSTGRES_DSN:
        raise RuntimeError("POSTGRES_DSN secret is not configured")
    logger.info("Creating SQLAlchemy engine")
    engine = create_engine(POSTGRES_DSN)
    table_name = "claims_transformed"
    logger.info("Loading data into table '%s'", table_name)
    try:
        with engine.begin() as conn:
            df.to_sql(name=table_name, con=conn, if_exists="replace", index=False)
        logger.info("Data successfully loaded into warehouse")
    except Exception as exc:
        logger.error("Failed to load data into warehouse: %s", exc)
        raise


# -------------------------------------------------------------------------
# BI Refresh Task
# -------------------------------------------------------------------------
@task(retries=2, retry_delay_seconds=60)
def refresh_bi(df: pd.DataFrame) -> None:  # noqa: ARG001 (df kept for signature consistency)
    """
    Trigger refresh of Power BI and Tableau dashboards.

    Args:
        df: Transformed DataFrame (unused; kept for dependency purposes).
    """
    logger = get_run_logger()

    # Power BI refresh
    if POWER_BI_REFRESH_URL and POWER_BI_TOKEN:
        logger.info("Triggering Power BI dataset refresh")
        try:
            resp = requests.post(
                POWER_BI_REFRESH_URL,
                headers={"Authorization": f"Bearer {POWER_BI_TOKEN}"},
                json={"datasetId": "claims_dataset"},
                timeout=30,
            )
            resp.raise_for_status()
            logger.info("Power BI refresh request succeeded")
        except Exception as exc:
            logger.error("Power BI refresh failed: %s", exc)
            raise
    else:
        logger.warning("Power BI credentials not configured; skipping refresh")

    # Tableau refresh
    if TABLEAU_REFRESH_URL and TABLEAU_TOKEN:
        logger.info("Triggering Tableau datasource refresh")
        try:
            resp = requests.post(
                TABLEAU_REFRESH_URL,
                headers={"X-Tableau-Auth": TABLEAU_TOKEN},
                json={"datasourceId": "claims_datasource"},
                timeout=30,
            )
            resp.raise_for_status()
            logger.info("Tableau refresh request succeeded")
        except Exception as exc:
            logger.error("Tableau refresh failed: %s", exc)
            raise
    else:
        logger.warning("Tableau credentials not configured; skipping refresh")


# -------------------------------------------------------------------------
# Flow Definition (Fanout/Fanin pattern)
# -------------------------------------------------------------------------
@flow(
    name="extract_claims_pipeline",
    task_runner=ConcurrentTaskRunner(),
)
def extract_claims_pipeline() -> None:
    """
    Orchestrate the ETL pipeline for healthcare claims.

    The flow follows a fan‑out/fan‑in pattern:
      * Parallel extraction of claims and providers.
      * Join & transformation (fan‑in).
      * Parallel loading to warehouse and BI dashboard refresh (fan‑out).
    """
    logger = get_run_logger()
    logger.info("Starting ETL pipeline")

    # Fan‑out: parallel extraction
    claims_future = extract_claims.submit()
    providers_future = extract_providers.submit()

    # Fan‑in: join & transform
    transformed = transform_join.wait(claims_future, providers_future)

    # Fan‑out: load and refresh BI dashboards
    load_warehouse.submit(transformed)
    refresh_bi.submit(transformed)

    logger.info("ETL pipeline completed")


# -------------------------------------------------------------------------
# Deployment Configuration
# -------------------------------------------------------------------------
def build_deployment() -> Deployment:
    """
    Build a Prefect deployment for the ``extract_claims_pipeline`` flow.

    The deployment runs daily at midnight UTC without catch‑up.
    """
    schedule = CronSchedule(cron="0 0 * * *", timezone="UTC")  # @daily UTC
    deployment = Deployment.build_from_flow(
        flow=extract_claims_pipeline,
        name="extract_claims_pipeline_deployment",
        schedule=schedule,
        work_pool_name="default-agent-pool",
        description="Daily ETL for healthcare claims data",
        tags=["etl", "healthcare"],
    )
    return deployment


if __name__ == "__main__":
    # Register or update the deployment when this script is executed directly.
    deployment = build_deployment()
    deployment.apply()
    print(f"Deployment '{deployment.name}' applied successfully.")