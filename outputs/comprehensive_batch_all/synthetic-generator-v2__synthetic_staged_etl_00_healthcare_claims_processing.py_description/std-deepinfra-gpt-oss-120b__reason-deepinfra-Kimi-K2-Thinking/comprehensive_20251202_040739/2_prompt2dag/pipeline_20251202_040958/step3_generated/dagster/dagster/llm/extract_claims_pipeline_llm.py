# Generated by Dagster code generator
# Date: 2024-06-12
# Pipeline: extract_claims_pipeline
# Description: Healthcare claims processing ETL pipeline implementing a staged ETL pattern with parallel extraction,
# transformation, and parallel loading stages.

from typing import Any

import pandas as pd
from dagster import (
    ConfigurableResource,
    In,
    InitResourceContext,
    JobDefinition,
    Out,
    RetryPolicy,
    ScheduleDefinition,
    ScheduleStatus,
    asset,
    job,
    multiprocess_executor,
    op,
    resource,
    fs_io_manager,
)


# ----------------------------------------------------------------------
# Resource definitions
# ----------------------------------------------------------------------


class PostgresWarehouseResource(ConfigurableResource):
    """Resource for interacting with the PostgreSQL data warehouse."""

    def load_data(self, df: pd.DataFrame) -> None:
        """Load a DataFrame into the warehouse.

        This is a placeholder implementation; replace with actual DB logic.
        """
        # Example: use SQLAlchemy or psycopg2 to insert data
        print(f"[PostgresWarehouse] Loading {len(df)} rows into the warehouse.")


class PowerBIResource(ConfigurableResource):
    """Resource for triggering Power BI dashboard refreshes."""

    def refresh(self) -> None:
        """Trigger a Power BI refresh.

        Replace with actual Power BI API calls.
        """
        print("[PowerBI] Refresh triggered.")


class TableauResource(ConfigurableResource):
    """Resource for triggering Tableau dashboard refreshes."""

    def refresh(self) -> None:
        """Trigger a Tableau refresh.

        Replace with actual Tableau API calls.
        """
        print("[Tableau] Refresh triggered.")


# ----------------------------------------------------------------------
# Ops
# ----------------------------------------------------------------------


@op(
    name="extract_claims",
    description="Read claims data from a local CSV file.",
    out=Out(pd.DataFrame),
    retry_policy=RetryPolicy(max_retries=2),
)
def extract_claims(context: InitResourceContext) -> pd.DataFrame:
    """Extract claims records from a CSV file stored locally.

    Returns:
        pandas.DataFrame: Raw claims data.
    """
    # Placeholder path; replace with actual file location or config.
    csv_path = "data/claims.csv"
    context.log.info(f"Reading claims CSV from {csv_path}")
    # In a real implementation, the fs_io_manager would handle file I/O.
    df = pd.read_csv(csv_path)
    context.log.info(f"Extracted {len(df)} claim records.")
    return df


@op(
    name="extract_providers",
    description="Read providers data from a local CSV file.",
    out=Out(pd.DataFrame),
    retry_policy=RetryPolicy(max_retries=2),
)
def extract_providers(context: InitResourceContext) -> pd.DataFrame:
    """Extract provider records from a CSV file stored locally.

    Returns:
        pandas.DataFrame: Raw provider data.
    """
    csv_path = "data/providers.csv"
    context.log.info(f"Reading providers CSV from {csv_path}")
    df = pd.read_csv(csv_path)
    context.log.info(f"Extracted {len(df)} provider records.")
    return df


@op(
    name="transform_join",
    description=(
        "Join claims and providers, anonymize PII, and calculate risk scores."
    ),
    ins={"claims": In(pd.DataFrame), "providers": In(pd.DataFrame)},
    out=Out(pd.DataFrame),
    retry_policy=RetryPolicy(max_retries=2),
)
def transform_join(context: InitResourceContext, claims: pd.DataFrame, providers: pd.DataFrame) -> pd.DataFrame:
    """Perform the core transformation logic.

    - Join claims with provider information.
    - Anonymize personally identifiable information.
    - Compute a risk score for each claim.

    Returns:
        pandas.DataFrame: Transformed dataset ready for loading.
    """
    context.log.info("Joining claims with providers.")
    merged = claims.merge(providers, how="left", on="provider_id")

    # Anonymize PII (example: drop or hash columns)
    pii_columns = ["patient_name", "patient_ssn"]
    for col in pii_columns:
        if col in merged.columns:
            merged[col] = "ANONYMIZED"
    context.log.info("PII anonymization completed.")

    # Calculate a dummy risk score
    merged["risk_score"] = merged["claim_amount"] * 0.01
    context.log.info("Risk scores calculated.")

    return merged


@op(
    name="load_warehouse",
    description="Load transformed data into the PostgreSQL data warehouse.",
    ins={"transformed": In(pd.DataFrame)},
    out=Out(None),
    required_resource_keys={"postgres_warehouse"},
    retry_policy=RetryPolicy(max_retries=2),
)
def load_warehouse(context: InitResourceContext, transformed: pd.DataFrame) -> None:
    """Persist the transformed dataset to the data warehouse."""
    warehouse: PostgresWarehouseResource = context.resources.postgres_warehouse
    context.log.info("Loading transformed data into the warehouse.")
    warehouse.load_data(transformed)
    context.log.info("Data load complete.")


@op(
    name="refresh_bi",
    description="Refresh Power BI and Tableau dashboards after transformation.",
    ins={"transformed": In(pd.DataFrame)},
    out=Out(None),
    required_resource_keys={"power_bi", "tableau"},
    retry_policy=RetryPolicy(max_retries=2),
)
def refresh_bi(context: InitResourceContext, transformed: pd.DataFrame) -> None:
    """Trigger downstream BI tools to refresh their data views."""
    power_bi: PowerBIResource = context.resources.power_bi
    tableau: TableauResource = context.resources.tableau

    context.log.info("Refreshing Power BI dashboards.")
    power_bi.refresh()

    context.log.info("Refreshing Tableau dashboards.")
    tableau.refresh()

    context.log.info("BI refresh operations completed.")


# ----------------------------------------------------------------------
# Job definition
# ----------------------------------------------------------------------


@job(
    name="extract_claims_pipeline",
    description=(
        "Healthcare claims processing ETL pipeline implementing a staged ETL pattern "
        "with parallel extraction, transformation, and parallel loading stages."
    ),
    executor_def=multiprocess_executor,
    resource_defs={
        "io_manager": fs_io_manager,
        "postgres_warehouse": PostgresWarehouseResource(),
        "power_bi": PowerBIResource(),
        "tableau": TableauResource(),
    },
    config=None,
)
def extract_claims_pipeline():
    """Orchestrates the ETL workflow."""
    claims = extract_claims()
    providers = extract_providers()
    transformed = transform_join(claims=claims, providers=providers)

    load_warehouse(transformed=transformed)
    refresh_bi(transformed=transformed)


# ----------------------------------------------------------------------
# Schedule definition
# ----------------------------------------------------------------------


daily_schedule = ScheduleDefinition(
    job=extract_claims_pipeline,
    cron_schedule="@daily",  # Runs once per day at midnight UTC
    execution_timezone="UTC",
    default_status=ScheduleStatus.RUNNING,
    description="Daily execution of the extract_claims_pipeline.",
    tags={"catchup": "false"},
)


# ----------------------------------------------------------------------
# Exported symbols
# ----------------------------------------------------------------------


__all__ = [
    "extract_claims_pipeline",
    "daily_schedule",
    "PostgresWarehouseResource",
    "PowerBIResource",
    "TableauResource",
]