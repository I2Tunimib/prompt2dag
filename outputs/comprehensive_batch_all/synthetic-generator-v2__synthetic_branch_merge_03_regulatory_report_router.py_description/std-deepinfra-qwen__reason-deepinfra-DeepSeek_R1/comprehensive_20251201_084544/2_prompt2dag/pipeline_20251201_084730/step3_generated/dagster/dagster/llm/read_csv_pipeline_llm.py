# Generated by Dagster Code Generator
# Generation Metadata:
# - Job Name: read_csv_pipeline
# - Description: No description provided.
# - Executor Type: multiprocess_executor
# - IO Manager: fs_io_manager
# - Dagster Version: 1.5.0
# - Use Assets: False
# - Required Resources: secure_archive, fatca_system, irs_system, local_filesystem

from dagster import (
    job,
    op,
    Out,
    In,
    RetryPolicy,
    multiprocess_executor,
    fs_io_manager,
    schedule,
    ScheduleEvaluationContext,
    RunRequest,
)

# Resources
from dagster_aws.s3 import s3_resource
from dagster import resource

# Custom resources
@resource
def fatca_system():
    # Placeholder for FATCA Regulatory Compliance System
    pass

@resource
def irs_system():
    # Placeholder for IRS Regulatory Compliance System
    pass

@resource
def local_filesystem():
    # Placeholder for Local File System
    pass

# Ops
@op(
    out={"csv_data": Out()},
    retry_policy=RetryPolicy(max_retries=2),
    required_resource_keys={"local_filesystem"},
)
def read_csv(context):
    """Read CSV data from the local file system."""
    # Simulate reading a CSV file
    csv_data = "Sample CSV Data"
    context.log.info(f"Read CSV data: {csv_data}")
    return csv_data

@op(
    in_={"csv_data": In()},
    out={"account_data": Out()},
    retry_policy=RetryPolicy(max_retries=2),
)
def account_check(context, csv_data):
    """Perform account checks on the CSV data."""
    # Simulate account checks
    account_data = "Sample Account Data"
    context.log.info(f"Account check completed: {account_data}")
    return account_data

@op(
    in_={"account_data": In()},
    out={"fatca_data": Out()},
    retry_policy=RetryPolicy(max_retries=2),
    required_resource_keys={"fatca_system"},
)
def route_to_fatca(context, account_data):
    """Route account data to the FATCA system."""
    # Simulate routing to FATCA
    fatca_data = "Sample FATCA Data"
    context.log.info(f"Routed to FATCA: {fatca_data}")
    return fatca_data

@op(
    in_={"account_data": In()},
    out={"irs_data": Out()},
    retry_policy=RetryPolicy(max_retries=2),
    required_resource_keys={"irs_system"},
)
def route_to_irs(context, account_data):
    """Route account data to the IRS system."""
    # Simulate routing to IRS
    irs_data = "Sample IRS Data"
    context.log.info(f"Routed to IRS: {irs_data}")
    return irs_data

@op(
    in_={"fatca_data": In(), "irs_data": In()},
    retry_policy=RetryPolicy(max_retries=2),
    required_resource_keys={"secure_archive"},
)
def archive_reports(context, fatca_data, irs_data):
    """Archive reports to the secure S3 storage."""
    # Simulate archiving reports
    context.log.info(f"Archived FATCA data: {fatca_data}")
    context.log.info(f"Archived IRS data: {irs_data}")

# Job
@job(
    name="read_csv_pipeline",
    description="No description provided.",
    executor_def=multiprocess_executor,
    resource_defs={
        "local_filesystem": local_filesystem,
        "fatca_system": fatca_system,
        "irs_system": irs_system,
        "secure_archive": s3_resource,
        "io_manager": fs_io_manager,
    },
)
def read_csv_pipeline():
    csv_data = read_csv()
    account_data = account_check(csv_data)
    fatca_data = route_to_fatca(account_data)
    irs_data = route_to_irs(account_data)
    archive_reports(fatca_data, irs_data)

# Schedule
@schedule(
    job=read_csv_pipeline,
    cron_schedule="@daily",
    execution_timezone="UTC",
    name="read_csv_pipeline_schedule",
    tags={"dagster/priority": "1"},
)
def read_csv_pipeline_schedule(context: ScheduleEvaluationContext):
    return RunRequest(run_key=None, run_config={})