# Generated by Prefect 2.x Code Generator
# Generation Date: [Insert Date Here]
# Prefect Version: 2.14.0

from prefect import flow, task, get_run_logger
from prefect.task_runners import ConcurrentTaskRunner
from prefect.deployments import Deployment
from prefect.filesystems import LocalFileSystem
from prefect.blocks.secrets import Secret
from prefect.aws import S3Bucket
from prefect import flow, task
import pandas as pd

# Load resources
local_filesystem = LocalFileSystem.load("local_filesystem")
fatca_api = Secret.load("fatca_api")
irs_api = Secret.load("irs_api")
secure_archive = S3Bucket.load("secure_archive")

@task(retries=2, name="Read CSV")
def read_csv(file_path: str) -> pd.DataFrame:
    """Read a CSV file from the local file system."""
    logger = get_run_logger()
    logger.info(f"Reading CSV file from {file_path}")
    with local_filesystem.open(file_path, "r") as file:
        df = pd.read_csv(file)
    return df

@task(retries=2, name="Account Check")
def account_check(df: pd.DataFrame) -> pd.DataFrame:
    """Perform account checks on the data."""
    logger = get_run_logger()
    logger.info("Performing account checks")
    # Example check: filter out invalid accounts
    valid_df = df[df["account_status"] == "valid"]
    return valid_df

@task(retries=2, name="Route to FATCA")
def route_to_fatca(df: pd.DataFrame) -> None:
    """Route data to the FATCA regulatory compliance system."""
    logger = get_run_logger()
    logger.info("Routing data to FATCA")
    # Example API call
    fatca_api_value = fatca_api.get()
    # Simulate API call
    logger.info(f"Sending data to FATCA API with key: {fatca_api_value}")

@task(retries=2, name="Route to IRS")
def route_to_irs(df: pd.DataFrame) -> None:
    """Route data to the IRS regulatory compliance system."""
    logger = get_run_logger()
    logger.info("Routing data to IRS")
    # Example API call
    irs_api_value = irs_api.get()
    # Simulate API call
    logger.info(f"Sending data to IRS API with key: {irs_api_value}")

@task(retries=2, name="Archive Reports")
def archive_reports(df_fatca: pd.DataFrame, df_irs: pd.DataFrame) -> None:
    """Archive the reports to the secure S3 bucket."""
    logger = get_run_logger()
    logger.info("Archiving reports to S3")
    # Example S3 upload
    secure_archive.upload_from_dataframe(df_fatca, "fatca_report.csv")
    secure_archive.upload_from_dataframe(df_irs, "irs_report.csv")

@flow(name="read_csv_pipeline", task_runner=ConcurrentTaskRunner)
def read_csv_pipeline(file_path: str):
    """Main flow for reading CSV, performing account checks, routing to FATCA and IRS, and archiving reports."""
    logger = get_run_logger()
    logger.info("Starting read_csv_pipeline")

    # Read CSV
    df = read_csv(file_path)

    # Perform account checks
    valid_df = account_check(df)

    # Route to FATCA and IRS
    fatca_result = route_to_fatca(valid_df)
    irs_result = route_to_irs(valid_df)

    # Archive reports
    archive_reports(fatca_result, irs_result)

# Schedule the flow
Deployment.build_from_flow(
    flow=read_csv_pipeline,
    name="read_csv_pipeline_deployment",
    work_pool_name="default-agent-pool",
    schedule=(None, "daily", "UTC", False),
)