# Generated by Prefect Pipeline Generator
# Date: 2024-06-28
# Prefect version: 2.14.0
# Pipeline: regulatory_report_router

import logging
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple

import pandas as pd
import requests
from prefect import flow, task, get_run_logger
from prefect.deployments import DeploymentSpec
from prefect.filesystems import LocalFileSystem
from prefect.blocks.system import Secret
from prefect.blocks.s3 import S3Bucket
from prefect.task_runners import ConcurrentTaskRunner, SequentialTaskRunner
from prefect.server.schemas.schedules import CronSchedule

# -------------------------------------------------------------------------
# Resource Blocks
# -------------------------------------------------------------------------

# Load filesystem block for CSV files
LOCAL_FS: LocalFileSystem = LocalFileSystem.load("local_filesystem_csv")

# Load secrets for regulatory APIs
FATCA_API_SECRET: Secret = Secret.load("fatca_api")
IRS_API_SECRET: Secret = Secret.load("irs_api")

# Load S3 bucket block for archival storage
ARCHIVE_BUCKET: S3Bucket = S3Bucket.load("secure_archive_storage")

# -------------------------------------------------------------------------
# Tasks
# -------------------------------------------------------------------------

@task(retries=2, retry_delay_seconds=30)
def extract_transaction_csv(csv_path: str) -> pd.DataFrame:
    """
    Read a CSV file containing financial transactions from the local filesystem.

    Args:
        csv_path: Relative path within the LocalFileSystem block.

    Returns:
        DataFrame with transaction data.
    """
    logger = get_run_logger()
    logger.info("Extracting transaction CSV from %s", csv_path)

    try:
        # Read raw bytes from the block and decode to string
        raw_bytes = LOCAL_FS.read_path(csv_path)
        df = pd.read_csv(Path(csv_path).name, storage_options={"path": raw_bytes})
        logger.info("Loaded %d transaction records", len(df))
        return df
    except Exception as exc:
        logger.error("Failed to read CSV: %s", exc)
        raise


@task(retries=2, retry_delay_seconds=30)
def determine_account_routing(
    transactions: pd.DataFrame,
) -> Tuple[pd.DataFrame, pd.DataFrame]:
    """
    Split transactions into FATCA (international) and IRS (domestic) streams.

    Args:
        transactions: DataFrame of all transactions.

    Returns:
        Tuple containing (fatca_df, irs_df).
    """
    logger = get_run_logger()
    logger.info("Determining routing for %d transactions", len(transactions))

    try:
        # Assume there is a column 'account_type' with values 'international' or 'domestic'
        fatca_df = transactions[transactions["account_type"] == "international"].copy()
        irs_df = transactions[transactions["account_type"] == "domestic"].copy()
        logger.info(
            "Routing results - FATCA: %d, IRS: %d",
            len(fatca_df),
            len(irs_df),
        )
        return fatca_df, irs_df
    except Exception as exc:
        logger.error("Routing determination failed: %s", exc)
        raise


@task(retries=2, retry_delay_seconds=30)
def generate_fatca_report(fatca_df: pd.DataFrame) -> str:
    """
    Generate a FATCA report and submit it to the FATCA regulatory API.

    Args:
        fatca_df: DataFrame containing international transactions.

    Returns:
        Path to the generated report file (local temporary file).
    """
    logger = get_run_logger()
    logger.info("Generating FATCA report for %d records", len(fatca_df))

    if fatca_df.empty:
        logger.warning("No FATCA records to process")
        return ""

    try:
        # Convert to JSON payload
        payload = fatca_df.to_dict(orient="records")
        api_token = FATCA_API_SECRET.get()
        headers = {"Authorization": f"Bearer {api_token}", "Content-Type": "application/json"}

        response = requests.post(
            "https://api.fatca-regulatory.example.com/report",
            json=payload,
            headers=headers,
            timeout=30,
        )
        response.raise_for_status()
        logger.info("FATCA report submitted successfully, status %s", response.status_code)

        # Save a local copy for archival
        timestamp = datetime.utcnow().strftime("%Y%m%dT%H%M%SZ")
        report_path = f"fatca_report_{timestamp}.json"
        with open(report_path, "w", encoding="utf-8") as f:
            f.write(response.text)

        logger.info("FATCA report saved locally at %s", report_path)
        return report_path
    except Exception as exc:
        logger.error("Failed to generate or submit FATCA report: %s", exc)
        raise


@task(retries=2, retry_delay_seconds=30)
def generate_irs_report(irs_df: pd.DataFrame) -> str:
    """
    Generate an IRS report and submit it to the IRS regulatory API.

    Args:
        irs_df: DataFrame containing domestic transactions.

    Returns:
        Path to the generated report file (local temporary file).
    """
    logger = get_run_logger()
    logger.info("Generating IRS report for %d records", len(irs_df))

    if irs_df.empty:
        logger.warning("No IRS records to process")
        return ""

    try:
        # Convert to CSV payload
        csv_buffer = irs_df.to_csv(index=False)
        api_token = IRS_API_SECRET.get()
        headers = {"Authorization": f"Bearer {api_token}", "Content-Type": "text/csv"}

        response = requests.post(
            "https://api.irs-regulatory.example.com/report",
            data=csv_buffer,
            headers=headers,
            timeout=30,
        )
        response.raise_for_status()
        logger.info("IRS report submitted successfully, status %s", response.status_code)

        # Save a local copy for archival
        timestamp = datetime.utcnow().strftime("%Y%m%dT%H%M%SZ")
        report_path = f"irs_report_{timestamp}.csv"
        with open(report_path, "w", encoding="utf-8") as f:
            f.write(response.text)

        logger.info("IRS report saved locally at %s", report_path)
        return report_path
    except Exception as exc:
        logger.error("Failed to generate or submit IRS report: %s", exc)
        raise


@task(retries=2, retry_delay_seconds=30)
def archive_regulatory_reports(
    fatca_report_path: str, irs_report_path: str
) -> List[str]:
    """
    Archive generated regulatory reports to secure S3 storage.

    Args:
        fatca_report_path: Local path to the FATCA report file.
        irs_report_path: Local path to the IRS report file.

    Returns:
        List of S3 object keys where the reports were stored.
    """
    logger = get_run_logger()
    archived_keys = []

    for local_path in [fatca_report_path, irs_report_path]:
        if not local_path:
            logger.info("Skipping empty report path")
            continue

        try:
            s3_key = f"regulatory_reports/{Path(local_path).name}"
            ARCHIVE_BUCKET.upload_from_path(local_path, to_path=s3_key)
            archived_keys.append(s3_key)
            logger.info("Archived %s to s3://%s/%s", local_path, ARCHIVE_BUCKET.bucket, s3_key)
        except Exception as exc:
            logger.error("Failed to archive %s: %s", local_path, exc)
            raise

    return archived_keys


# -------------------------------------------------------------------------
# Flow
# -------------------------------------------------------------------------

@flow(
    name="regulatory_report_router",
    task_runner=ConcurrentTaskRunner(),
)
def regulatory_report_router(csv_path: str = "data/transactions.csv") -> List[str]:
    """
    Orchestrates the regulatory reporting pipeline.

    The flow extracts transaction data, determines routing, generates
    FATCA and IRS reports in parallel, and archives the results.

    Args:
        csv_path: Relative path to the transaction CSV within the
                  LocalFileSystem block.

    Returns:
        List of S3 keys for the archived reports.
    """
    logger = get_run_logger()
    logger.info("Starting regulatory_report_router flow")

    # Step 1: Extract CSV
    transactions = extract_transaction_csv(csv_path)

    # Step 2: Determine routing (branch)
    fatca_df, irs_df = determine_account_routing(transactions)

    # Step 3: Parallel report generation
    fatca_report_path = generate_fatca_report.submit(fatca_df).result()
    irs_report_path = generate_irs_report.submit(irs_df).result()

    # Step 4: Archive both reports
    archived_keys = archive_regulatory_reports(fatca_report_path, irs_report_path)

    logger.info("Flow completed successfully")
    return archived_keys


# -------------------------------------------------------------------------
# Deployment
# -------------------------------------------------------------------------

DeploymentSpec(
    name="regulatory_report_router_deployment",
    flow=regulatory_report_router,
    schedule=CronSchedule(cron="0 0 * * *", timezone="UTC"),
    tags=["regulatory", "daily"],
    work_pool_name="default-agent-pool",
    enforce_parameter_schema=False,
    description="Daily regulatory reporting pipeline for FATCA and IRS.",
)

# End of file.