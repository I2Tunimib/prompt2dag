# Generated by Airflow DAG Generator on 2024-06-13
# DAG: regulatory_report_router
# Description: Processes financial transaction data and routes it to FATCA and IRS reporting systems,
# then archives the generated reports. Implements a fan‑out/fan‑in pattern.

import logging
from datetime import datetime, timedelta

import pandas as pd
from airflow import DAG
from airflow.decorators import task
from airflow.exceptions import AirflowFailException
from airflow.utils.dates import days_ago
from airflow.providers.http.hooks.http import HttpHook
from airflow.providers.amazon.aws.hooks.s3 import S3Hook

# -------------------------------------------------------------------------
# Default arguments applied to all tasks
# -------------------------------------------------------------------------
default_args = {
    "owner": "airflow",
    "depends_on_past": False,
    "retries": 2,
    "retry_delay": timedelta(minutes=5),
    "email_on_failure": False,
    "email_on_retry": False,
}

# -------------------------------------------------------------------------
# DAG definition
# -------------------------------------------------------------------------
with DAG(
    dag_id="regulatory_report_router",
    description="Regulatory reporting pipeline for financial transactions (FATCA/IRS).",
    schedule_interval="@daily",
    start_date=days_ago(1),
    catchup=False,
    default_args=default_args,
    tags=["regulatory", "reporting", "fanout_fanin"],
    max_active_runs=1,
    timezone="UTC",
) as dag:

    @task(task_id="extract_transaction_csv", retries=2)
    def extract_transaction_csv() -> pd.DataFrame:
        """
        Extract transaction data from a local CSV file.
        Returns a pandas DataFrame.
        """
        try:
            # Path could be stored in an Airflow Variable or Connection; using a placeholder here.
            csv_path = "/opt/airflow/data/transactions.csv"
            logging.info("Reading CSV from %s", csv_path)
            df = pd.read_csv(csv_path)
            logging.info("Extracted %d rows", len(df))
            return df
        except Exception as exc:
            logging.error("Failed to extract CSV: %s", exc)
            raise AirflowFailException(f"CSV extraction failed: {exc}")

    @task(task_id="determine_account_routing", retries=2)
    def determine_account_routing(df: pd.DataFrame) -> dict:
        """
        Split the transaction DataFrame into FATCA (international) and IRS (domestic) subsets.
        Returns a dictionary with keys 'fatca' and 'irs' containing the respective DataFrames.
        """
        try:
            if "account_type" not in df.columns:
                raise ValueError("Column 'account_type' not found in input data")

            fatca_df = df[df["account_type"] == "international"].copy()
            irs_df = df[df["account_type"] == "domestic"].copy()

            logging.info("FATCA records: %d, IRS records: %d", len(fatca_df), len(irs_df))

            return {"fatca": fatca_df, "irs": irs_df}
        except Exception as exc:
            logging.error("Routing determination failed: %s", exc)
            raise AirflowFailException(f"Routing determination failed: {exc}")

    @task(task_id="generate_fatca_report", retries=2)
    def generate_fatca_report(routing: dict) -> dict:
        """
        Generate a FATCA report and send it to the FATCA API.
        Returns the API response payload.
        """
        try:
            fatca_df: pd.DataFrame = routing.get("fatca")
            if fatca_df is None or fatca_df.empty:
                logging.info("No FATCA records to process.")
                return {"status": "skipped", "detail": "No FATCA data"}

            # Convert DataFrame to JSON payload
            payload = fatca_df.to_dict(orient="records")
            http = HttpHook(http_conn_id="fatca_api", method="POST")
            response = http.run(endpoint="/reports/fatca", json=payload, headers={"Content-Type": "application/json"})

            if response.status_code != 200:
                raise ValueError(f"FATCA API returned {response.status_code}: {response.text}")

            logging.info("FATCA report successfully sent.")
            return {"status": "success", "response": response.json()}
        except Exception as exc:
            logging.error("FATCA report generation failed: %s", exc)
            raise AirflowFailException(f"FATCA report generation failed: {exc}")

    @task(task_id="generate_irs_report", retries=2)
    def generate_irs_report(routing: dict) -> dict:
        """
        Generate an IRS report and send it to the IRS API.
        Returns the API response payload.
        """
        try:
            irs_df: pd.DataFrame = routing.get("irs")
            if irs_df is None or irs_df.empty:
                logging.info("No IRS records to process.")
                return {"status": "skipped", "detail": "No IRS data"}

            payload = irs_df.to_dict(orient="records")
            http = HttpHook(http_conn_id="irs_api", method="POST")
            response = http.run(endpoint="/reports/irs", json=payload, headers={"Content-Type": "application/json"})

            if response.status_code != 200:
                raise ValueError(f"IRS API returned {response.status_code}: {response.text}")

            logging.info("IRS report successfully sent.")
            return {"status": "success", "response": response.json()}
        except Exception as exc:
            logging.error("IRS report generation failed: %s", exc)
            raise AirflowFailException(f"IRS report generation failed: {exc}")

    @task(task_id="archive_regulatory_reports", retries=2)
    def archive_regulatory_reports(fatca_result: dict, irs_result: dict) -> None:
        """
        Archive the generated reports (or their metadata) to secure S3 storage.
        """
        try:
            s3 = S3Hook(aws_conn_id="secure_archive_storage")
            timestamp = datetime.utcnow().strftime("%Y%m%dT%H%M%SZ")
            archive_key_prefix = f"regulatory_reports/{timestamp}/"

            # Archive FATCA result
            fatca_key = f"{archive_key_prefix}fatca_report.json"
            s3.load_string(
                string_data=str(fatca_result),
                key=fatca_key,
                bucket_name="secure-archive-bucket",
                replace=True,
            )
            logging.info("Archived FATCA report to s3://secure-archive-bucket/%s", fatca_key)

            # Archive IRS result
            irs_key = f"{archive_key_prefix}irs_report.json"
            s3.load_string(
                string_data=str(irs_result),
                key=irs_key,
                bucket_name="secure-archive-bucket",
                replace=True,
            )
            logging.info("Archived IRS report to s3://secure-archive-bucket/%s", irs_key)

        except Exception as exc:
            logging.error("Archiving reports failed: %s", exc)
            raise AirflowFailException(f"Archiving reports failed: {exc}")

    # -------------------------------------------------------------------------
    # Define task pipeline (fan‑out / fan‑in)
    # -------------------------------------------------------------------------
    df = extract_transaction_csv()
    routing = determine_account_routing(df)

    fatca_report = generate_fatca_report(routing)
    irs_report = generate_irs_report(routing)

    archive_regulatory_reports(fatca_report, irs_report)

    # Set explicit dependencies (optional, as the above chaining already defines them)
    df >> routing
    routing >> [fatca_report, irs_report] >> archive_regulatory_reports

# End of DAG definition.