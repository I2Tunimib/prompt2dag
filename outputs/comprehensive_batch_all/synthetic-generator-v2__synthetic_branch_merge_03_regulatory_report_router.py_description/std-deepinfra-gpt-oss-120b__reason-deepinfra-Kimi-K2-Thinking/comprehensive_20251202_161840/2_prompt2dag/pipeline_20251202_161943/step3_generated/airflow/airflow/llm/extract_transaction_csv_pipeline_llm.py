# Generated by Airflow DAG generator on 2024-06-13
"""
DAG: extract_transaction_csv_pipeline
Description: Comprehensive Pipeline Description
Pattern: fanout_fanin
"""

from datetime import datetime, timedelta
from airflow import DAG
from airflow.decorators import task
from airflow.utils.dates import days_ago
from airflow.exceptions import AirflowFailException
from airflow.hooks.base import BaseHook
import logging
import pandas as pd
import boto3
import requests


# Default arguments applied to all tasks
default_args = {
    "owner": "airflow",
    "retries": 2,
    "retry_delay": timedelta(minutes=5),
    "depends_on_past": False,
}


def get_connection(conn_id: str):
    """
    Helper to retrieve Airflow connection objects.
    """
    try:
        return BaseHook.get_connection(conn_id)
    except Exception as exc:
        raise AirflowFailException(f"Connection '{conn_id}' not found: {exc}")


with DAG(
    dag_id="extract_transaction_csv_pipeline",
    description="Comprehensive Pipeline Description",
    schedule_interval="@daily",
    start_date=days_ago(1),
    catchup=False,
    default_args=default_args,
    tags=["fanout_fanin"],
    is_paused_upon_creation=True,  # Disabled by default
    max_active_runs=1,
) as dag:

    @task(retries=2, retry_delay=timedelta(minutes=5))
    def extract_transaction_csv() -> pd.DataFrame:
        """
        Extracts transaction data from a local CSV file.
        Returns a pandas DataFrame for downstream processing.
        """
        conn = get_connection("local_filesystem_csv")
        csv_path = conn.extra_dejson.get("path", "/tmp/transactions.csv")
        try:
            df = pd.read_csv(csv_path)
            logging.info("Extracted %d rows from %s", len(df), csv_path)
            return df
        except Exception as exc:
            raise AirflowFailException(f"Failed to read CSV at {csv_path}: {exc}")

    @task(retries=2, retry_delay=timedelta(minutes=5))
    def branch_account_type_check(df: pd.DataFrame) -> str:
        """
        Determines the account type and returns a branch key.
        This simple example checks if any account is 'Corporate'.
        """
        try:
            if df["account_type"].str.contains("Corporate", case=False).any():
                branch = "corporate"
            else:
                branch = "individual"
            logging.info("Branch decision based on account type: %s", branch)
            return branch
        except Exception as exc:
            raise AirflowFailException(f"Account type check failed: {exc}")

    @task(retries=2, retry_delay=timedelta(minutes=5))
    def generate_fatca_report(df: pd.DataFrame) -> str:
        """
        Generates a FATCA report and posts it to the FATCA regulatory system.
        Returns the report identifier.
        """
        conn = get_connection("fatca_system")
        endpoint = conn.host.rstrip("/") + "/api/fatca/report"
        try:
            payload = df.to_dict(orient="records")
            response = requests.post(endpoint, json=payload, timeout=30)
            response.raise_for_status()
            report_id = response.json().get("report_id")
            logging.info("FATCA report generated with ID: %s", report_id)
            return report_id
        except Exception as exc:
            raise AirflowFailException(f"Failed to generate FATCA report: {exc}")

    @task(retries=2, retry_delay=timedelta(minutes=5))
    def generate_irs_report(df: pd.DataFrame) -> str:
        """
        Generates an IRS report and posts it to the IRS regulatory system.
        Returns the report identifier.
        """
        conn = get_connection("irs_system")
        endpoint = conn.host.rstrip("/") + "/api/irs/report"
        try:
            payload = df.to_dict(orient="records")
            response = requests.post(endpoint, json=payload, timeout=30)
            response.raise_for_status()
            report_id = response.json().get("report_id")
            logging.info("IRS report generated with ID: %s", report_id)
            return report_id
        except Exception as exc:
            raise AirflowFailException(f"Failed to generate IRS report: {exc}")

    @task(retries=2, retry_delay=timedelta(minutes=5))
    def archive_regulatory_reports(
        fatca_report_id: str, irs_report_id: str
    ) -> None:
        """
        Archives both FATCA and IRS reports to secure S3 storage.
        """
        conn = get_connection("secure_archive_storage")
        s3 = boto3.client(
            "s3",
            aws_access_key_id=conn.login,
            aws_secret_access_key=conn.password,
            region_name=conn.extra_dejson.get("region", "us-east-1"),
        )
        bucket = conn.schema or "secure-archive"
        try:
            # Placeholder: In a real scenario, retrieve report contents via API.
            for report_id, prefix in [
                (fatca_report_id, "fatca"),
                (irs_report_id, "irs"),
            ]:
                key = f"{prefix}/report_{report_id}.json"
                # Simulate empty JSON payload
                s3.put_object(Bucket=bucket, Key=key, Body=b"{}")
                logging.info("Archived %s report to s3://%s/%s", prefix, bucket, key)
        except Exception as exc:
            raise AirflowFailException(f"Failed to archive reports: {exc}")

    # Define task pipeline
    df_extracted = extract_transaction_csv()
    branch_key = branch_account_type_check(df_extracted)

    # Fan-out: generate both reports in parallel
    fatca_report = generate_fatca_report(df_extracted)
    irs_report = generate_irs_report(df_extracted)

    # Fan-in: archive after both reports are ready
    archive = archive_regulatory_reports(fatca_report, irs_report)

    # Set explicit dependencies
    df_extracted >> branch_key
    branch_key >> [fatca_report, irs_report]
    [fatca_report, irs_report] >> archive

# End of DAG definition.