# Generated by Prefect Pipeline Generator
# Pipeline: extract_transaction_csv_pipeline
# Description: Comprehensive Pipeline Description
# Generation Timestamp: 2024-06-13T12:00:00Z

from pathlib import Path
from typing import List

import pandas as pd
from prefect import flow, task, get_run_logger
from prefect.task_runners import ConcurrentTaskRunner
from prefect.exceptions import PrefectException
from prefect.filesystems import LocalFileSystem
from prefect.blocks.system import Secret
from prefect.blocks.storage import S3Bucket


@task(retries=2, retry_delay_seconds=10)
def extract_transaction_csv(csv_path: str) -> pd.DataFrame:
    """
    Extracts transaction data from a CSV file stored on the local filesystem.

    Args:
        csv_path: Relative path to the CSV file within the LocalFileSystem block.

    Returns:
        DataFrame containing the transaction records.

    Raises:
        PrefectException: If the file cannot be read.
    """
    logger = get_run_logger()
    logger.info("Loading LocalFileSystem block 'local_filesystem_csv'")
    fs_block: LocalFileSystem = LocalFileSystem.load("local_filesystem_csv")

    try:
        full_path: Path = fs_block.get_path(csv_path)
        logger.info(f"Reading CSV from {full_path}")
        df = pd.read_csv(full_path)
        logger.info(f"Extracted {len(df)} rows")
        return df
    except Exception as exc:
        logger.error(f"Failed to read CSV: {exc}")
        raise PrefectException("CSV extraction failed") from exc


@task(retries=2, retry_delay_seconds=10)
def branch_account_type_check(df: pd.DataFrame) -> pd.DataFrame:
    """
    Performs a branching check based on account type.
    In this simplified example, the function just logs the distinct account types
    and returns the original DataFrame for downstream processing.

    Args:
        df: DataFrame produced by ``extract_transaction_csv``.

    Returns:
        The same DataFrame, unchanged.
    """
    logger = get_run_logger()
    if "account_type" in df.columns:
        account_types = df["account_type"].unique()
        logger.info(f"Detected account types: {account_types}")
    else:
        logger.warning("Column 'account_type' not found in DataFrame")
    return df


@task(retries=2, retry_delay_seconds=10)
def generate_fatca_report(df: pd.DataFrame) -> Path:
    """
    Generates a FATCA regulatory report from the transaction data.

    Args:
        df: DataFrame after account type branching.

    Returns:
        Path to the generated FATCA report CSV file.
    """
    logger = get_run_logger()
    logger.info("Loading FATCA system secret")
    fatca_secret: Secret = Secret.load("fatca_system")
    # In a real scenario, you would use the secret to authenticate with the FATCA system.
    # Here we simply log its presence.
    logger.debug(f"FATCA secret retrieved: {fatca_secret.get()}")

    report_path = Path("reports/fatca_report.csv")
    report_path.parent.mkdir(parents=True, exist_ok=True)
    logger.info(f"Writing FATCA report to {report_path}")
    df.to_csv(report_path, index=False)
    return report_path


@task(retries=2, retry_delay_seconds=10)
def generate_irs_report(df: pd.DataFrame) -> Path:
    """
    Generates an IRS regulatory report from the transaction data.

    Args:
        df: DataFrame after account type branching.

    Returns:
        Path to the generated IRS report CSV file.
    """
    logger = get_run_logger()
    logger.info("Loading IRS system secret")
    irs_secret: Secret = Secret.load("irs_system")
    # In a real scenario, you would use the secret to authenticate with the IRS system.
    logger.debug(f"IRS secret retrieved: {irs_secret.get()}")

    report_path = Path("reports/irs_report.csv")
    report_path.parent.mkdir(parents=True, exist_ok=True)
    logger.info(f"Writing IRS report to {report_path}")
    df.to_csv(report_path, index=False)
    return report_path


@task(retries=2, retry_delay_seconds=10)
def archive_regulatory_reports(
    fatca_report_path: Path, irs_report_path: Path
) -> List[str]:
    """
    Archives the generated regulatory reports to a secure S3 bucket.

    Args:
        fatca_report_path: Path to the FATCA report file.
        irs_report_path: Path to the IRS report file.

    Returns:
        List of S3 object keys where the reports were stored.
    """
    logger = get_run_logger()
    logger.info("Loading S3 bucket block 'secure_archive_storage'")
    s3_bucket: S3Bucket = S3Bucket.load("secure_archive_storage")

    uploaded_keys = []
    for local_path in [fatca_report_path, irs_report_path]:
        s3_key = f"regulatory_reports/{local_path.name}"
        logger.info(f"Uploading {local_path} to s3://{s3_bucket.bucket}/{s3_key}")
        try:
            s3_bucket.upload_from_path(
                from_path=str(local_path),
                to_path=s3_key,
                overwrite=True,
            )
            uploaded_keys.append(s3_key)
        except Exception as exc:
            logger.error(f"Failed to upload {local_path}: {exc}")
            raise PrefectException("Archiving failed") from exc

    logger.info(f"Successfully archived reports: {uploaded_keys}")
    return uploaded_keys


@flow(
    name="extract_transaction_csv_pipeline",
    task_runner=ConcurrentTaskRunner(),
)
def extract_transaction_csv_pipeline():
    """
    Prefect flow orchestrating the extraction, processing, and archiving of
    transaction CSV data with regulatory reporting for FATCA and IRS.
    """
    logger = get_run_logger()
    logger.info("Starting extract_transaction_csv_pipeline flow")

    # Step 1: Extract CSV
    raw_df = extract_transaction_csv(csv_path="transactions/transaction_data.csv")

    # Step 2: Branch based on account type
    branched_df = branch_account_type_check(raw_df)

    # Step 3: Parallel report generation
    fatca_report_path = generate_fatca_report.submit(branched_df)
    irs_report_path = generate_irs_report.submit(branched_df)

    # Step 4: Archive both reports after they are ready
    archive_regulatory_reports.wait_for(fatca_report_path, irs_report_path)
    archive_regulatory_reports(
        fatca_report_path.result(),
        irs_report_path.result(),
    )

    logger.info("extract_transaction_csv_pipeline flow completed")


if __name__ == "__main__":
    # Execute the flow locally; in production this flow would be deployed
    # with a Prefect deployment using the provided orchestration metadata.
    extract_transaction_csv_pipeline()