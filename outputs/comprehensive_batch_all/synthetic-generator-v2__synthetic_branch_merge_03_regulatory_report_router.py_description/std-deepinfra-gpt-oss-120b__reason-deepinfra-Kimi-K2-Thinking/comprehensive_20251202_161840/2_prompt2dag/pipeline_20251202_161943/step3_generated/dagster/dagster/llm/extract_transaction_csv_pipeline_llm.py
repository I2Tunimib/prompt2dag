# Generated by Dagster Code Generator
# Date: 2024-06-28
# Pipeline: extract_transaction_csv_pipeline
# Description: Comprehensive Pipeline Description
# Pattern: fanout_fanin

from typing import Any, List, Dict
import pandas as pd
import json
import os

from dagster import (
    op,
    job,
    In,
    Out,
    RetryPolicy,
    ResourceDefinition,
    ConfigurableResource,
    IOManager,
    io_manager,
    InputContext,
    OutputContext,
    MultiRunConfig,
    multiprocess_executor,
    in_process_executor,
    ScheduleDefinition,
    DefaultScheduleStatus,
)


# ----------------------------------------------------------------------
# Resources
# ----------------------------------------------------------------------


class LocalFilesystemIOManager(IOManager):
    """Simple IO manager that reads/writes CSV files from a local directory."""

    def __init__(self, base_dir: str):
        self.base_dir = base_dir

    def _full_path(self, context: OutputContext) -> str:
        return os.path.join(self.base_dir, f"{context.step_key}.csv")

    def handle_output(self, context: OutputContext, obj: pd.DataFrame) -> None:
        path = self._full_path(context)
        os.makedirs(os.path.dirname(path), exist_ok=True)
        obj.to_csv(path, index=False)

    def load_input(self, context: InputContext) -> pd.DataFrame:
        path = self._full_path(context)
        return pd.read_csv(path)


@io_manager(config_schema={"base_dir": str})
def local_filesystem_io_manager(init_context) -> LocalFilesystemIOManager:
    return LocalFilesystemIOManager(base_dir=init_context.resource_config["base_dir"])


class FatcaSystemResource(ConfigurableResource):
    """Placeholder for FATCA regulatory system integration."""

    endpoint: str = "https://fatca.example.com/api"

    def submit_report(self, data: List[Dict[str, Any]]) -> str:
        # In a real implementation this would POST to the FATCA endpoint.
        # Here we just simulate a report ID.
        return f"fatca_report_{hash(json.dumps(data)) % 10000}"


class IRSSystemResource(ConfigurableResource):
    """Placeholder for IRS regulatory system integration."""

    endpoint: str = "https://irs.example.com/api"

    def submit_report(self, data: List[Dict[str, Any]]) -> str:
        # Simulated IRS report ID.
        return f"irs_report_{hash(json.dumps(data)) % 10000}"


class SecureArchiveS3Resource(ConfigurableResource):
    """Placeholder for secure S3 archive storage."""

    bucket_name: str = "secure-archive-bucket"

    def upload(self, key: str, content: str) -> None:
        # Simulated upload; replace with boto3 logic in production.
        print(f"Uploading to s3://{self.bucket_name}/{key}")
        # No-op for placeholder.


# ----------------------------------------------------------------------
# Ops
# ----------------------------------------------------------------------


@op(
    name="Extract Transaction CSV",
    description="Read transaction data from a CSV file stored in the local filesystem.",
    out=Out(pd.DataFrame),
    retry_policy=RetryPolicy(max_retries=2),
    required_resource_keys={"fs_io_manager"},
    tags={"executor": "in_process_executor"},
)
def extract_transaction_csv(context) -> pd.DataFrame:
    """Loads the transaction CSV using the configured IO manager."""
    # The IO manager will handle loading based on the step key.
    df: pd.DataFrame = context.resources.fs_io_manager.load_input(
        InputContext(step_key=context.op.name, name="input")
    )
    context.log.info(f"Loaded {len(df)} transaction rows.")
    return df


@op(
    name="Account Type Branch Check",
    description="Pass-through op that could include branching logic based on account type.",
    ins={"transactions": In(pd.DataFrame)},
    out=Out(pd.DataFrame),
    retry_policy=RetryPolicy(max_retries=2),
    tags={"executor": "in_process_executor"},
)
def branch_account_type_check(context, transactions: pd.DataFrame) -> pd.DataFrame:
    """Placeholder for accountâ€‘type branching logic. Currently forwards data unchanged."""
    context.log.info("Branch check completed.")
    return transactions


@op(
    name="Generate FATCA Report",
    description="Generate a FATCA report from transaction data.",
    ins={"transactions": In(pd.DataFrame)},
    out=Out(str),
    retry_policy=RetryPolicy(max_retries=2),
    required_resource_keys={"fatca_system"},
    tags={"executor": "in_process_executor"},
)
def generate_fatca_report(context, transactions: pd.DataFrame) -> str:
    """Transforms transaction data into a FATCA report and submits it."""
    data = transactions.to_dict(orient="records")
    report_id = context.resources.fatca_system.submit_report(data)
    context.log.info(f"FATCA report generated with ID: {report_id}")
    return report_id


@op(
    name="Generate IRS Report",
    description="Generate an IRS report from transaction data.",
    ins={"transactions": In(pd.DataFrame)},
    out=Out(str),
    retry_policy=RetryPolicy(max_retries=2),
    required_resource_keys={"irs_system"},
    tags={"executor": "in_process_executor"},
)
def generate_irs_report(context, transactions: pd.DataFrame) -> str:
    """Transforms transaction data into an IRS report and submits it."""
    data = transactions.to_dict(orient="records")
    report_id = context.resources.irs_system.submit_report(data)
    context.log.info(f"IRS report generated with ID: {report_id}")
    return report_id


@op(
    name="Archive Regulatory Reports",
    description="Archive both FATCA and IRS reports to secure S3 storage.",
    ins={
        "fatca_report_id": In(str),
        "irs_report_id": In(str),
    },
    out=Out(None),
    retry_policy=RetryPolicy(max_retries=2),
    required_resource_keys={"secure_archive_storage"},
    tags={"executor": "in_process_executor"},
)
def archive_regulatory_reports(context, fatca_report_id: str, irs_report_id: str) -> None:
    """Uploads report identifiers to the secure archive."""
    s3 = context.resources.secure_archive_storage
    s3.upload(key=f"fatca/{fatca_report_id}.json", content=json.dumps({"report_id": fatca_report_id}))
    s3.upload(key=f"irs/{irs_report_id}.json", content=json.dumps({"report_id": irs_report_id}))
    context.log.info("Both reports archived successfully.")


# ----------------------------------------------------------------------
# Job Definition
# ----------------------------------------------------------------------


@job(
    name="extract_transaction_csv_pipeline",
    description="Comprehensive Pipeline Description",
    executor_def=multiprocess_executor,
    resource_defs={
        "fs_io_manager": local_filesystem_io_manager,
        "fatca_system": FatcaSystemResource(),
        "irs_system": IRSSystemResource(),
        "secure_archive_storage": SecureArchiveS3Resource(),
    },
    tags={"dagster_version": "1.5.0"},
)
def extract_transaction_csv_pipeline():
    """Orchestrates extraction, branching, report generation, and archiving."""
    # Step 1: Extract CSV
    transactions = extract_transaction_csv()

    # Step 2: Branch check
    branched = branch_account_type_check(transactions)

    # Step 3: Parallel report generation
    fatca_report = generate_fatca_report(branched)
    irs_report = generate_irs_report(branched)

    # Step 4: Archive both reports
    archive_regulatory_reports(fatca_report_id=fatca_report, irs_report_id=irs_report)


# ----------------------------------------------------------------------
# Schedule (disabled)
# ----------------------------------------------------------------------


daily_schedule = ScheduleDefinition(
    job=extract_transaction_csv_pipeline,
    cron_schedule="@daily",
    execution_timezone="UTC",
    default_status=DefaultScheduleStatus.INACTIVE,  # Disabled
    description="Daily schedule for the extract_transaction_csv_pipeline (currently disabled).",
)

# End of file.