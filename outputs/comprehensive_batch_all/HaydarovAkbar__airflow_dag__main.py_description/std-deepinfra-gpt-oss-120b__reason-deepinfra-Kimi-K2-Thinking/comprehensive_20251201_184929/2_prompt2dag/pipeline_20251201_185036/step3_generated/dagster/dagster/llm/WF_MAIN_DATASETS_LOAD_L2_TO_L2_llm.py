# Generated by Dagster Code Generator
# Date: 2024-06-28
# Pipeline: WF_MAIN_DATASETS_LOAD_L2_TO_L2

from dagster import (
    op,
    job,
    In,
    Out,
    RetryPolicy,
    ResourceDefinition,
    fs_io_manager,
    multiprocess_executor,
    Failure,
    get_dagster_logger,
)

# ----------------------------------------------------------------------
# Resource definitions (placeholders – replace with real implementations)
# ----------------------------------------------------------------------


def dwh_postgres_resource(init_context):
    """Placeholder for a PostgreSQL connection resource."""
    # Replace with actual connection logic, e.g., psycopg2 or SQLAlchemy engine.
    return {"connection": "dwh_postgres_connection"}


def sap_http_resource(init_context):
    """Placeholder for an HTTP client to communicate with SAP."""
    # Replace with actual HTTP client, e.g., requests.Session().
    return {"session": "sap_http_session"}


def smtp_email_resource(init_context):
    """Placeholder for an SMTP client used to send emails."""
    # Replace with actual SMTP client, e.g., smtplib.SMTP().
    return {"client": "smtp_email_client"}


# ----------------------------------------------------------------------
# Op definitions
# ----------------------------------------------------------------------


@op(
    name="wait_for_l2_full_load",
    description="Wait for the L2 full load flag to become true.",
    out=Out(str),
    retry_policy=RetryPolicy(max_retries=0),
)
def wait_for_l2_full_load(context) -> str:
    logger = get_dagster_logger()
    logger.info("Checking L2 full load flag...")
    # Placeholder logic – replace with actual flag check.
    flag = "L2_FULL_LOAD_READY"
    logger.info(f"Flag status: {flag}")
    return flag


@op(
    name="get_load_id",
    description="Generate a unique load identifier for the current run.",
    ins={"flag": In(str)},
    out=Out(str),
    retry_policy=RetryPolicy(max_retries=0),
)
def get_load_id(context, flag: str) -> str:
    logger = get_dagster_logger()
    logger.info(f"Received flag: {flag}. Generating load ID...")
    load_id = f"load_{context.run_id}"
    logger.info(f"Generated load ID: {load_id}")
    return load_id


@op(
    name="workflow_registration",
    description="Register the workflow session in the metadata store.",
    ins={"load_id": In(str)},
    out=Out(str),
    required_resource_keys={"dwh_postgres"},
    retry_policy=RetryPolicy(max_retries=0),
)
def workflow_registration(context, load_id: str) -> str:
    logger = get_dagster_logger()
    dwh = context.resources.dwh_postgres
    logger.info(f"Registering workflow with load ID {load_id} using {dwh}")
    # Placeholder registration logic.
    session_id = f"session_{load_id}"
    logger.info(f"Workflow registered with session ID: {session_id}")
    return session_id


@op(
    name="wait_for_success_end",
    description="Wait for the previous day workflow to complete successfully.",
    ins={"session_id": In(str)},
    out=Out(str),
    retry_policy=RetryPolicy(max_retries=0),
)
def wait_for_success_end(context, session_id: str) -> str:
    logger = get_dagster_logger()
    logger.info(f"Waiting for previous day workflow to finish for session {session_id}...")
    # Placeholder wait logic.
    status = "PREV_WORKFLOW_SUCCESS"
    logger.info(f"Previous workflow status: {status}")
    return status


@op(
    name="run_sys_kill_all_session_pg",
    description="Trigger PostgreSQL session cleanup after previous workflow success.",
    ins={"status": In(str)},
    out=Out(bool),
    required_resource_keys={"dwh_postgres"},
    retry_policy=RetryPolicy(max_retries=0),
)
def run_sys_kill_all_session_pg(context, status: str) -> bool:
    logger = get_dagster_logger()
    dwh = context.resources.dwh_postgres
    logger.info(f"Running session cleanup for status {status} using {dwh}")
    # Placeholder cleanup logic.
    cleaned = True
    logger.info(f"Session cleanup completed: {cleaned}")
    return cleaned


@op(
    name="load_ds_client_segmentation",
    description="Trigger the client segmentation dataset load.",
    out=Out(bool),
    required_resource_keys={"dwh_postgres"},
    retry_policy=RetryPolicy(max_retries=0),
)
def load_ds_client_segmentation(context) -> bool:
    logger = get_dagster_logger()
    dwh = context.resources.dwh_postgres
    logger.info(f"Starting client segmentation load using {dwh}")
    # Placeholder load logic.
    success = True
    logger.info(f"Client segmentation load success: {success}")
    return success


@op(
    name="run_wf_data_preparation_for_reports",
    description="Trigger data preparation steps required for reporting.",
    out=Out(bool),
    required_resource_keys={"dwh_postgres"},
    retry_policy=RetryPolicy(max_retries=0),
)
def run_wf_data_preparation_for_reports(context) -> bool:
    logger = get_dagster_logger()
    dwh = context.resources.dwh_postgres
    logger.info(f"Running reporting data preparation using {dwh}")
    # Placeholder preparation logic.
    prepared = True
    logger.info(f"Reporting data preparation success: {prepared}")
    return prepared


@op(
    name="send_flg_to_sap",
    description="Send completion flag to SAP system after client segmentation load.",
    ins={"segmentation_success": In(bool)},
    out=Out(bool),
    required_resource_keys={"sap_http"},
    retry_policy=RetryPolicy(max_retries=0),
)
def send_flg_to_sap(context, segmentation_success: bool) -> bool:
    logger = get_dagster_logger()
    sap = context.resources.sap_http
    if not segmentation_success:
        logger.error("Segmentation load failed; not sending flag to SAP.")
        raise Failure("Segmentation load failed.")
    logger.info(f"Sending completion flag to SAP using {sap}")
    # Placeholder SAP communication logic.
    sent = True
    logger.info(f"Flag sent to SAP: {sent}")
    return sent


@op(
    name="end",
    description="Update workflow completion status after all parallel branches finish.",
    ins={
        "cleanup_done": In(bool),
        "report_prep_done": In(bool),
        "sap_flag_sent": In(bool),
    },
    out=Out(str),
    retry_policy=RetryPolicy(max_retries=0),
)
def end(context, cleanup_done: bool, report_prep_done: bool, sap_flag_sent: bool) -> str:
    logger = get_dagster_logger()
    logger.info(
        f"Finalizing workflow: cleanup={cleanup_done}, report_prep={report_prep_done}, sap_flag={sap_flag_sent}"
    )
    if not all([cleanup_done, report_prep_done, sap_flag_sent]):
        logger.error("One or more parallel steps failed; marking workflow as failed.")
        raise Failure("Workflow finalization failed due to upstream errors.")
    status = "WORKFLOW_COMPLETED"
    logger.info(f"Workflow status updated to: {status}")
    return status


@op(
    name="email_on_failure",
    description="Send a failure notification email if the workflow does not complete successfully.",
    ins={"final_status": In(str)},
    required_resource_keys={"smtp_email"},
    retry_policy=RetryPolicy(max_retries=0),
)
def email_on_failure(context, final_status: str):
    logger = get_dagster_logger()
    smtp = context.resources.smtp_email
    if final_status != "WORKFLOW_COMPLETED":
        logger.info(f"Sending failure email via {smtp}")
        # Placeholder email logic.
        logger.info("Failure notification email sent.")
    else:
        logger.info("Workflow succeeded; no failure email sent.")


# ----------------------------------------------------------------------
# Job definition
# ----------------------------------------------------------------------


@job(
    name="wf_main_datasets_load_l2_to_l2",
    description="Comprehensive Pipeline Description",
    executor_def=multiprocess_executor,
    resource_defs={
        "dwh_postgres": ResourceDefinition.resource(dwh_postgres_resource),
        "sap_http": ResourceDefinition.resource(sap_http_resource),
        "smtp_email": ResourceDefinition.resource(smtp_email_resource),
        "io_manager": fs_io_manager,
    },
)
def WF_MAIN_DATASETS_LOAD_L2_TO_L2():
    # Linear chain up to the fan‑out point
    flag = wait_for_l2_full_load()
    load_id = get_load_id(flag)
    session_id = workflow_registration(load_id)
    prev_status = wait_for_success_end(session_id)
    cleanup_done = run_sys_kill_all_session_pg(prev_status)

    # Parallel fan‑out
    report_prep_done = run_wf_data_preparation_for_reports()
    segmentation_success = load_ds_client_segmentation()

    # Continue after fan‑out
    sap_flag_sent = send_flg_to_sap(segmentation_success)

    # Join parallel branches
    final_status = end(
        cleanup_done=cleanup_done,
        report_prep_done=report_prep_done,
        sap_flag_sent=sap_flag_sent,
    )

    # Failure handling (executed regardless of success/failure)
    email_on_failure(final_status)


# ----------------------------------------------------------------------
# Entry point for local execution
# ----------------------------------------------------------------------


if __name__ == "__main__":
    # Execute the job locally for testing/debugging purposes.
    result = WF_MAIN_DATASETS_LOAD_L2_TO_L2.execute_in_process()
    assert result.success