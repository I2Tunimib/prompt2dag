{
  "metadata": {
    "schema_version": "1.0",
    "analysis_timestamp": "2025-12-01T02:38:16.170040",
    "source_file": "Pipeline_Description_Dataset/HaydarovAkbar__airflow_dag__main.py_description.txt",
    "llm_provider": "deepinfra",
    "llm_model": "gpt-oss-120b",
    "analysis_results": {
      "detected_patterns": [
        "sequential",
        "parallel",
        "sensor_driven",
        "event_driven",
        "hybrid"
      ],
      "task_executors_used": [
        "python",
        "sql"
      ],
      "has_branching": false,
      "has_parallelism": true,
      "has_sensors": true,
      "total_components": 10,
      "complexity_score": "medium"
    },
    "orchestrator_compatibility": {
      "airflow": {
        "supported": true,
        "confidence": "high",
        "notes": [
          "Sequential pattern fully supported",
          "Parallelism via TaskFlow API expand()",
          "Native sensor support"
        ]
      },
      "prefect": {
        "supported": true,
        "confidence": "high",
        "notes": [
          "Sequential flow with task dependencies",
          "map() for parallel execution",
          "wait_for() or custom sensors"
        ]
      },
      "dagster": {
        "supported": true,
        "confidence": "high",
        "notes": [
          "Op graph with dependencies",
          "DynamicOutput for fan-out",
          "Sensors via run_status_sensor"
        ]
      }
    },
    "validation_warnings": []
  },
  "pipeline_summary": {
    "name": "WF_MAIN_DATASETS_LOAD_L2_TO_L2",
    "description": "Comprehensive Pipeline Description",
    "flow_patterns": [
      "sequential",
      "parallel",
      "sensor_driven",
      "event_driven",
      "hybrid"
    ],
    "task_executors": [
      "python",
      "sql"
    ],
    "complexity": "medium"
  },
  "components": [
    {
      "id": "wait_for_l2_full_load",
      "name": "Wait for L2 Full Load Flag",
      "category": "Sensor",
      "description": "Polls the md.dwh_flag table in the DWH until the 'l1_to_l2_load_successfull' flag for the previous business day is set.",
      "inputs": [
        "md.dwh_flag table (PostgreSQL)"
      ],
      "outputs": [
        "sensor_success signal to downstream tasks"
      ],
      "executor_type": "sql",
      "executor_config": {
        "image": null,
        "command": null,
        "script_path": null,
        "entry_point": null,
        "environment": {},
        "resources": {},
        "network": null
      },
      "io_spec": [
        {
          "name": "dwh_flag_table",
          "direction": "input",
          "kind": "table",
          "format": "sql",
          "path_pattern": "md.dwh_flag",
          "connection_id": "dwh"
        }
      ],
      "upstream_policy": {
        "type": "custom",
        "description": "First task in the pipeline, no upstream dependencies",
        "timeout_seconds": null
      },
      "retry_policy": {
        "max_attempts": 0,
        "delay_seconds": 0,
        "exponential_backoff": false,
        "retry_on": []
      },
      "concurrency": {
        "supports_parallelism": false,
        "supports_dynamic_mapping": false,
        "map_over_param": null,
        "max_parallel_instances": null
      },
      "connections": [
        {
          "id": "dwh",
          "type": "database",
          "purpose": "PostgreSQL DWH access"
        }
      ],
      "datasets": {
        "consumes": [
          "dwh_flag_table"
        ],
        "produces": [
          "sensor_success"
        ]
      }
    },
    {
      "id": "get_load_id",
      "name": "Generate Load Identifier",
      "category": "Transformer",
      "description": "Creates a unique load identifier for the workflow run and pushes it via XCom for downstream tasks.",
      "inputs": [
        "sensor_success signal from wait_for_l2_full_load"
      ],
      "outputs": [
        "load_id (XCom)"
      ],
      "executor_type": "python",
      "executor_config": {
        "image": null,
        "command": null,
        "script_path": null,
        "entry_point": null,
        "environment": {},
        "resources": {},
        "network": null
      },
      "io_spec": [
        {
          "name": "load_id",
          "direction": "output",
          "kind": "object",
          "format": "json",
          "path_pattern": "xcom://load_id",
          "connection_id": null
        }
      ],
      "upstream_policy": {
        "type": "all_success",
        "description": "Runs after the L2 load flag sensor succeeds",
        "timeout_seconds": null
      },
      "retry_policy": {
        "max_attempts": 0,
        "delay_seconds": 0,
        "exponential_backoff": false,
        "retry_on": []
      },
      "concurrency": {
        "supports_parallelism": false,
        "supports_dynamic_mapping": false,
        "map_over_param": null,
        "max_parallel_instances": null
      },
      "connections": [],
      "datasets": {
        "consumes": [
          "sensor_success"
        ],
        "produces": [
          "load_id"
        ]
      }
    },
    {
      "id": "workflow_registration",
      "name": "Register Workflow Session",
      "category": "Enricher",
      "description": "Registers the workflow session in metadata tables using the generated load identifier.",
      "inputs": [
        "load_id (XCom)"
      ],
      "outputs": [
        "metadata registration record"
      ],
      "executor_type": "python",
      "executor_config": {
        "image": null,
        "command": null,
        "script_path": null,
        "entry_point": null,
        "environment": {},
        "resources": {},
        "network": null
      },
      "io_spec": [
        {
          "name": "metadata_registration",
          "direction": "output",
          "kind": "table",
          "format": "sql",
          "path_pattern": "metadata.workflow_sessions",
          "connection_id": "dwh"
        }
      ],
      "upstream_policy": {
        "type": "all_success",
        "description": "Executes after load identifier generation",
        "timeout_seconds": null
      },
      "retry_policy": {
        "max_attempts": 0,
        "delay_seconds": 0,
        "exponential_backoff": false,
        "retry_on": []
      },
      "concurrency": {
        "supports_parallelism": false,
        "supports_dynamic_mapping": false,
        "map_over_param": null,
        "max_parallel_instances": null
      },
      "connections": [
        {
          "id": "dwh",
          "type": "database",
          "purpose": "Metadata storage"
        }
      ],
      "datasets": {
        "consumes": [
          "load_id"
        ],
        "produces": [
          "metadata_registration"
        ]
      }
    },
    {
      "id": "wait_for_success_end",
      "name": "Wait for Previous Day DAG Completion",
      "category": "Sensor",
      "description": "Blocks execution until the previous day's run of the same DAG reaches its end task.",
      "inputs": [
        "external DAG execution status"
      ],
      "outputs": [
        "sensor_success signal to downstream tasks"
      ],
      "executor_type": "custom",
      "executor_config": {
        "image": null,
        "command": null,
        "script_path": null,
        "entry_point": null,
        "environment": {},
        "resources": {},
        "network": null
      },
      "io_spec": [
        {
          "name": "external_dag_status",
          "direction": "input",
          "kind": "api",
          "format": "json",
          "path_pattern": "airflow://dag_status/WF_MAIN_DATASETS_LOAD_L2_TO_L2",
          "connection_id": null
        }
      ],
      "upstream_policy": {
        "type": "all_success",
        "description": "Runs after workflow registration",
        "timeout_seconds": null
      },
      "retry_policy": {
        "max_attempts": 0,
        "delay_seconds": 0,
        "exponential_backoff": false,
        "retry_on": []
      },
      "concurrency": {
        "supports_parallelism": false,
        "supports_dynamic_mapping": false,
        "map_over_param": null,
        "max_parallel_instances": null
      },
      "connections": [],
      "datasets": {
        "consumes": [
          "external_dag_status"
        ],
        "produces": [
          "sensor_success"
        ]
      }
    },
    {
      "id": "trigger_sys_kill_all_session_pg",
      "name": "Trigger PostgreSQL Session Cleanup",
      "category": "Orchestrator",
      "description": "Triggers the sys_kill_all_session_pg DAG to clean up PostgreSQL sessions and waits for its completion.",
      "inputs": [
        "sensor_success signal from wait_for_success_end"
      ],
      "outputs": [
        "session_cleanup_done signal"
      ],
      "executor_type": "custom",
      "executor_config": {
        "dag_id": "sys_kill_all_session_pg",
        "wait_for_completion": true,
        "image": null,
        "command": null,
        "script_path": null,
        "entry_point": null,
        "environment": {},
        "resources": {},
        "network": null
      },
      "io_spec": [
        {
          "name": "session_cleanup_done",
          "direction": "output",
          "kind": "object",
          "format": "json",
          "path_pattern": "xcom://session_cleanup_done",
          "connection_id": null
        }
      ],
      "upstream_policy": {
        "type": "all_success",
        "description": "Runs after previous‑day DAG completion sensor",
        "timeout_seconds": null
      },
      "retry_policy": {
        "max_attempts": 0,
        "delay_seconds": 0,
        "exponential_backoff": false,
        "retry_on": []
      },
      "concurrency": {
        "supports_parallelism": false,
        "supports_dynamic_mapping": false,
        "map_over_param": null,
        "max_parallel_instances": null
      },
      "connections": [],
      "datasets": {
        "consumes": [
          "sensor_success"
        ],
        "produces": [
          "session_cleanup_done"
        ]
      }
    },
    {
      "id": "trigger_wf_data_preparation_for_reports",
      "name": "Trigger Reporting Data Preparation",
      "category": "Orchestrator",
      "description": "Triggers the wf_data_preparation_for_reports DAG to prepare reporting datasets.",
      "inputs": [
        "session_cleanup_done signal"
      ],
      "outputs": [
        "reporting_preparation_done signal"
      ],
      "executor_type": "custom",
      "executor_config": {
        "dag_id": "wf_data_preparation_for_reports",
        "pool": "dwh_l2",
        "pool_slots": 1,
        "image": null,
        "command": null,
        "script_path": null,
        "entry_point": null,
        "environment": {},
        "resources": {},
        "network": null
      },
      "io_spec": [
        {
          "name": "reporting_preparation_done",
          "direction": "output",
          "kind": "object",
          "format": "json",
          "path_pattern": "xcom://reporting_preparation_done",
          "connection_id": null
        }
      ],
      "upstream_policy": {
        "type": "all_success",
        "description": "Runs after PostgreSQL session cleanup completes",
        "timeout_seconds": null
      },
      "retry_policy": {
        "max_attempts": 0,
        "delay_seconds": 0,
        "exponential_backoff": false,
        "retry_on": []
      },
      "concurrency": {
        "supports_parallelism": false,
        "supports_dynamic_mapping": false,
        "map_over_param": null,
        "max_parallel_instances": null
      },
      "connections": [],
      "datasets": {
        "consumes": [
          "session_cleanup_done"
        ],
        "produces": [
          "reporting_preparation_done"
        ]
      }
    },
    {
      "id": "trigger_load_ds_client_segmentation",
      "name": "Trigger Client Segmentation Load",
      "category": "Orchestrator",
      "description": "Triggers the l1_to_l2_p_load_data_ds_client_segmentation_full DAG to load client segmentation data.",
      "inputs": [
        "session_cleanup_done signal"
      ],
      "outputs": [
        "client_segmentation_loaded signal"
      ],
      "executor_type": "custom",
      "executor_config": {
        "dag_id": "l1_to_l2_p_load_data_ds_client_segmentation_full",
        "image": null,
        "command": null,
        "script_path": null,
        "entry_point": null,
        "environment": {},
        "resources": {},
        "network": null
      },
      "io_spec": [
        {
          "name": "client_segmentation_loaded",
          "direction": "output",
          "kind": "object",
          "format": "json",
          "path_pattern": "xcom://client_segmentation_loaded",
          "connection_id": null
        }
      ],
      "upstream_policy": {
        "type": "all_success",
        "description": "Runs after PostgreSQL session cleanup completes (parallel branch)",
        "timeout_seconds": null
      },
      "retry_policy": {
        "max_attempts": 0,
        "delay_seconds": 0,
        "exponential_backoff": false,
        "retry_on": []
      },
      "concurrency": {
        "supports_parallelism": false,
        "supports_dynamic_mapping": false,
        "map_over_param": null,
        "max_parallel_instances": null
      },
      "connections": [],
      "datasets": {
        "consumes": [
          "session_cleanup_done"
        ],
        "produces": [
          "client_segmentation_loaded"
        ]
      }
    },
    {
      "id": "send_flag_to_sap",
      "name": "Send Completion Flag to SAP",
      "category": "Notifier",
      "description": "Posts a completion flag to the SAP system via HTTP POST after client segmentation data is loaded.",
      "inputs": [
        "client_segmentation_loaded signal"
      ],
      "outputs": [
        "sap_flag_acknowledgment"
      ],
      "executor_type": "http",
      "executor_config": {
        "url": "<SAP_ENDPOINT_URL>",
        "method": "POST",
        "headers": {
          "Content-Type": "application/json"
        },
        "image": null,
        "command": null,
        "script_path": null,
        "entry_point": null,
        "environment": {},
        "resources": {},
        "network": null
      },
      "io_spec": [
        {
          "name": "sap_flag_acknowledgment",
          "direction": "output",
          "kind": "object",
          "format": "json",
          "path_pattern": "xcom://sap_flag_acknowledgment",
          "connection_id": "sap_conn"
        }
      ],
      "upstream_policy": {
        "type": "all_success",
        "description": "Executes after client segmentation load completes",
        "timeout_seconds": null
      },
      "retry_policy": {
        "max_attempts": 0,
        "delay_seconds": 0,
        "exponential_backoff": false,
        "retry_on": []
      },
      "concurrency": {
        "supports_parallelism": false,
        "supports_dynamic_mapping": false,
        "map_over_param": null,
        "max_parallel_instances": null
      },
      "connections": [
        {
          "id": "sap_conn",
          "type": "api",
          "purpose": "SAP HTTP endpoint"
        }
      ],
      "datasets": {
        "consumes": [
          "client_segmentation_loaded"
        ],
        "produces": [
          "sap_flag_acknowledgment"
        ]
      }
    },
    {
      "id": "finalize_workflow",
      "name": "Finalize Workflow Session",
      "category": "Transformer",
      "description": "Updates metadata tables to mark the workflow session as successful after reporting preparation and SAP notification complete.",
      "inputs": [
        "reporting_preparation_done signal",
        "sap_flag_acknowledgment"
      ],
      "outputs": [
        "workflow_success_record"
      ],
      "executor_type": "python",
      "executor_config": {
        "image": null,
        "command": null,
        "script_path": null,
        "entry_point": null,
        "environment": {},
        "resources": {},
        "network": null
      },
      "io_spec": [
        {
          "name": "workflow_success_record",
          "direction": "output",
          "kind": "table",
          "format": "sql",
          "path_pattern": "metadata.workflow_sessions",
          "connection_id": "dwh"
        }
      ],
      "upstream_policy": {
        "type": "all_success",
        "description": "Runs after both parallel branches (reporting preparation and SAP flag) have succeeded",
        "timeout_seconds": null
      },
      "retry_policy": {
        "max_attempts": 0,
        "delay_seconds": 0,
        "exponential_backoff": false,
        "retry_on": []
      },
      "concurrency": {
        "supports_parallelism": false,
        "supports_dynamic_mapping": false,
        "map_over_param": null,
        "max_parallel_instances": null
      },
      "connections": [
        {
          "id": "dwh",
          "type": "database",
          "purpose": "Metadata storage"
        }
      ],
      "datasets": {
        "consumes": [
          "reporting_preparation_done",
          "sap_flag_acknowledgment"
        ],
        "produces": [
          "workflow_success_record"
        ]
      }
    },
    {
      "id": "email_on_failure",
      "name": "Failure Notification Email",
      "category": "Notifier",
      "description": "Sends an email to configured recipients when any upstream task fails.",
      "inputs": [
        "failure signals from all upstream tasks"
      ],
      "outputs": [
        "failure_email_sent"
      ],
      "executor_type": "python",
      "executor_config": {
        "recipients": [
          "test@gmail.com"
        ],
        "subject": "Pipeline Failure Alert",
        "image": null,
        "command": null,
        "script_path": null,
        "entry_point": null,
        "environment": {},
        "resources": {},
        "network": null
      },
      "io_spec": [
        {
          "name": "failure_email_sent",
          "direction": "output",
          "kind": "object",
          "format": "json",
          "path_pattern": "xcom://failure_email_sent",
          "connection_id": null
        }
      ],
      "upstream_policy": {
        "type": "custom",
        "description": "Triggered when any upstream task fails (trigger_rule='one_failed')",
        "timeout_seconds": null
      },
      "retry_policy": {
        "max_attempts": 0,
        "delay_seconds": 0,
        "exponential_backoff": false,
        "retry_on": []
      },
      "concurrency": {
        "supports_parallelism": false,
        "supports_dynamic_mapping": false,
        "map_over_param": null,
        "max_parallel_instances": null
      },
      "connections": [],
      "datasets": {
        "consumes": [
          "failure_signals"
        ],
        "produces": [
          "failure_email_sent"
        ]
      }
    }
  ],
  "flow_structure": {
    "pattern": "hybrid",
    "entry_points": [
      "wait_for_l2_full_load"
    ],
    "nodes": {
      "wait_for_l2_full_load": {
        "kind": "Sensor",
        "component_type_id": "wait_for_l2_full_load",
        "upstream_policy": {
          "type": "all_success",
          "timeout_seconds": null
        },
        "next_nodes": [
          "get_load_id"
        ],
        "sensor_config": {
          "sensor_type": "sql",
          "target": "md.dwh_flag table flag l1_to_l2_load_successfull",
          "poke_interval_seconds": 60,
          "timeout_seconds": null,
          "mode": "poke"
        },
        "branch_config": null,
        "parallel_config": null
      },
      "get_load_id": {
        "kind": "Task",
        "component_type_id": "get_load_id",
        "upstream_policy": {
          "type": "all_success",
          "timeout_seconds": null
        },
        "next_nodes": [
          "workflow_registration"
        ],
        "branch_config": null,
        "sensor_config": null,
        "parallel_config": null
      },
      "workflow_registration": {
        "kind": "Task",
        "component_type_id": "workflow_registration",
        "upstream_policy": {
          "type": "all_success",
          "timeout_seconds": null
        },
        "next_nodes": [
          "wait_for_success_end"
        ],
        "branch_config": null,
        "sensor_config": null,
        "parallel_config": null
      },
      "wait_for_success_end": {
        "kind": "Sensor",
        "component_type_id": "wait_for_success_end",
        "upstream_policy": {
          "type": "all_success",
          "timeout_seconds": null
        },
        "next_nodes": [
          "trigger_sys_kill_all_session_pg"
        ],
        "sensor_config": {
          "sensor_type": "external_task",
          "target": "previous day's WF_MAIN_DATASETS_LOAD_L2_TO_L2 end task",
          "poke_interval_seconds": 100,
          "timeout_seconds": null,
          "mode": "poke"
        },
        "branch_config": null,
        "parallel_config": null
      },
      "trigger_sys_kill_all_session_pg": {
        "kind": "Task",
        "component_type_id": "trigger_sys_kill_all_session_pg",
        "upstream_policy": {
          "type": "all_success",
          "timeout_seconds": null
        },
        "next_nodes": [
          "trigger_wf_data_preparation_for_reports",
          "trigger_load_ds_client_segmentation"
        ],
        "branch_config": null,
        "sensor_config": null,
        "parallel_config": null
      },
      "trigger_wf_data_preparation_for_reports": {
        "kind": "Task",
        "component_type_id": "trigger_wf_data_preparation_for_reports",
        "upstream_policy": {
          "type": "all_success",
          "timeout_seconds": null
        },
        "next_nodes": [
          "join_parallel"
        ],
        "branch_config": null,
        "sensor_config": null,
        "parallel_config": null
      },
      "trigger_load_ds_client_segmentation": {
        "kind": "Task",
        "component_type_id": "trigger_load_ds_client_segmentation",
        "upstream_policy": {
          "type": "all_success",
          "timeout_seconds": null
        },
        "next_nodes": [
          "send_flag_to_sap"
        ],
        "branch_config": null,
        "sensor_config": null,
        "parallel_config": null
      },
      "send_flag_to_sap": {
        "kind": "Task",
        "component_type_id": "send_flag_to_sap",
        "upstream_policy": {
          "type": "all_success",
          "timeout_seconds": null
        },
        "next_nodes": [
          "join_parallel"
        ],
        "branch_config": null,
        "sensor_config": null,
        "parallel_config": null
      },
      "join_parallel": {
        "kind": "Join",
        "component_type_id": null,
        "upstream_policy": {
          "type": "all_success",
          "timeout_seconds": null
        },
        "next_nodes": [
          "finalize_workflow"
        ],
        "branch_config": null,
        "sensor_config": null,
        "parallel_config": null
      },
      "finalize_workflow": {
        "kind": "Task",
        "component_type_id": "finalize_workflow",
        "upstream_policy": {
          "type": "all_success",
          "timeout_seconds": null
        },
        "next_nodes": [],
        "branch_config": null,
        "sensor_config": null,
        "parallel_config": null
      },
      "email_on_failure": {
        "kind": "Task",
        "component_type_id": "email_on_failure",
        "upstream_policy": {
          "type": "any_success",
          "timeout_seconds": null
        },
        "next_nodes": [],
        "branch_config": null,
        "sensor_config": null,
        "parallel_config": null
      }
    },
    "edges": [
      {
        "from": "wait_for_l2_full_load",
        "to": "get_load_id",
        "edge_type": "success",
        "condition": null,
        "metadata": {}
      },
      {
        "from": "get_load_id",
        "to": "workflow_registration",
        "edge_type": "success",
        "condition": null,
        "metadata": {}
      },
      {
        "from": "workflow_registration",
        "to": "wait_for_success_end",
        "edge_type": "success",
        "condition": null,
        "metadata": {}
      },
      {
        "from": "wait_for_success_end",
        "to": "trigger_sys_kill_all_session_pg",
        "edge_type": "success",
        "condition": null,
        "metadata": {}
      },
      {
        "from": "trigger_sys_kill_all_session_pg",
        "to": "trigger_wf_data_preparation_for_reports",
        "edge_type": "success",
        "condition": null,
        "metadata": {}
      },
      {
        "from": "trigger_sys_kill_all_session_pg",
        "to": "trigger_load_ds_client_segmentation",
        "edge_type": "success",
        "condition": null,
        "metadata": {}
      },
      {
        "from": "trigger_wf_data_preparation_for_reports",
        "to": "join_parallel",
        "edge_type": "success",
        "condition": null,
        "metadata": {}
      },
      {
        "from": "trigger_load_ds_client_segmentation",
        "to": "send_flag_to_sap",
        "edge_type": "success",
        "condition": null,
        "metadata": {}
      },
      {
        "from": "send_flag_to_sap",
        "to": "join_parallel",
        "edge_type": "success",
        "condition": null,
        "metadata": {}
      },
      {
        "from": "join_parallel",
        "to": "finalize_workflow",
        "edge_type": "success",
        "condition": null,
        "metadata": {}
      },
      {
        "from": "wait_for_l2_full_load",
        "to": "email_on_failure",
        "edge_type": "failure",
        "condition": null,
        "metadata": {}
      },
      {
        "from": "get_load_id",
        "to": "email_on_failure",
        "edge_type": "failure",
        "condition": null,
        "metadata": {}
      },
      {
        "from": "workflow_registration",
        "to": "email_on_failure",
        "edge_type": "failure",
        "condition": null,
        "metadata": {}
      },
      {
        "from": "wait_for_success_end",
        "to": "email_on_failure",
        "edge_type": "failure",
        "condition": null,
        "metadata": {}
      },
      {
        "from": "trigger_sys_kill_all_session_pg",
        "to": "email_on_failure",
        "edge_type": "failure",
        "condition": null,
        "metadata": {}
      },
      {
        "from": "trigger_wf_data_preparation_for_reports",
        "to": "email_on_failure",
        "edge_type": "failure",
        "condition": null,
        "metadata": {}
      },
      {
        "from": "trigger_load_ds_client_segmentation",
        "to": "email_on_failure",
        "edge_type": "failure",
        "condition": null,
        "metadata": {}
      },
      {
        "from": "send_flag_to_sap",
        "to": "email_on_failure",
        "edge_type": "failure",
        "condition": null,
        "metadata": {}
      },
      {
        "from": "finalize_workflow",
        "to": "email_on_failure",
        "edge_type": "failure",
        "condition": null,
        "metadata": {}
      }
    ]
  },
  "parameters": {
    "pipeline": {
      "name": {
        "description": "Pipeline identifier",
        "type": "string",
        "default": "WF_MAIN_DATASETS_LOAD_L2_TO_L2",
        "required": true,
        "constraints": null
      },
      "description": {
        "description": "Pipeline description",
        "type": "string",
        "default": "Comprehensive Pipeline Description",
        "required": false,
        "constraints": null
      },
      "tags": {
        "description": "Classification tags",
        "type": "array",
        "default": [],
        "required": false
      }
    },
    "schedule": {
      "enabled": {
        "description": "Whether pipeline runs on schedule",
        "type": "boolean",
        "default": false,
        "required": false
      },
      "cron_expression": {
        "description": "Cron or preset (e.g., @daily, 0 0 * * *)",
        "type": "string",
        "default": null,
        "required": false
      },
      "start_date": {
        "description": "When to start scheduling",
        "type": "datetime",
        "default": "2024-12-22T00:00:00Z",
        "required": false,
        "format": "ISO8601"
      },
      "end_date": {
        "description": "When to stop scheduling",
        "type": "datetime",
        "default": null,
        "required": false
      },
      "timezone": {
        "description": "Schedule timezone",
        "type": "string",
        "default": null,
        "required": false
      },
      "catchup": {
        "description": "Run missed intervals",
        "type": "boolean",
        "default": false,
        "required": false
      },
      "batch_window": {
        "description": "Batch window parameter name (e.g., ds, execution_date)",
        "type": "string",
        "default": null,
        "required": false
      },
      "partitioning": {
        "description": "Data partitioning strategy (e.g., daily, hourly, monthly)",
        "type": "string",
        "default": null,
        "required": false
      }
    },
    "execution": {
      "max_active_runs": {
        "description": "Max concurrent pipeline runs",
        "type": "integer",
        "default": 20,
        "required": false
      },
      "timeout_seconds": {
        "description": "Pipeline execution timeout",
        "type": "integer",
        "default": null,
        "required": false
      },
      "retry_policy": {
        "description": "Pipeline-level retry behavior",
        "type": "object",
        "default": null,
        "required": false
      },
      "depends_on_past": {
        "description": "Whether execution depends on previous run success",
        "type": "boolean",
        "default": null,
        "required": false
      }
    },
    "components": {
      "wait_for_l2_full_load": {
        "conn_id": {
          "description": "Connection identifier for the DWH",
          "type": "string",
          "default": "dwh",
          "required": false,
          "constraints": null
        },
        "poke_interval": {
          "description": "Polling interval in seconds for the SqlSensor",
          "type": "integer",
          "default": 60,
          "required": false,
          "constraints": null
        },
        "fail_on_empty": {
          "description": "Whether the sensor should fail if no rows are returned",
          "type": "boolean",
          "default": false,
          "required": false,
          "constraints": null
        }
      },
      "get_load_id": {},
      "workflow_registration": {},
      "wait_for_success_end": {
        "external_dag_id": {
          "description": "Identifier of the external DAG to monitor",
          "type": "string",
          "default": "WF_MAIN_DATASETS_LOAD_L2_TO_L2",
          "required": false,
          "constraints": null
        },
        "execution_delta": {
          "description": "Time delta to look back for the external DAG run (in days)",
          "type": "integer",
          "default": 1,
          "required": false,
          "constraints": null
        },
        "poke_interval": {
          "description": "Polling interval in seconds for the ExternalTaskSensor",
          "type": "integer",
          "default": 100,
          "required": false,
          "constraints": null
        }
      },
      "trigger_sys_kill_all_session_pg": {
        "trigger_dag_id": {
          "description": "DAG to trigger for PostgreSQL session cleanup",
          "type": "string",
          "default": "sys_kill_all_session_pg",
          "required": false,
          "constraints": null
        },
        "wait_for_completion": {
          "description": "Whether to wait for the triggered DAG to finish",
          "type": "boolean",
          "default": true,
          "required": false,
          "constraints": null
        }
      },
      "trigger_wf_data_preparation_for_reports": {
        "trigger_dag_id": {
          "description": "DAG to trigger for reporting data preparation",
          "type": "string",
          "default": "wf_data_preparation_for_reports",
          "required": false,
          "constraints": null
        },
        "pool": {
          "description": "Airflow pool name to limit concurrency",
          "type": "string",
          "default": "dwh_l2",
          "required": false,
          "constraints": null
        },
        "pool_slots": {
          "description": "Number of slots to consume from the pool",
          "type": "integer",
          "default": 1,
          "required": false,
          "constraints": null
        }
      },
      "trigger_load_ds_client_segmentation": {
        "trigger_dag_id": {
          "description": "DAG to trigger for client segmentation data load",
          "type": "string",
          "default": "l1_to_l2_p_load_data_ds_client_segmentation_full",
          "required": false,
          "constraints": null
        }
      },
      "send_flag_to_sap": {
        "conn_id": {
          "description": "Connection identifier for the SAP HTTP endpoint",
          "type": "string",
          "default": "sap_conn",
          "required": false,
          "constraints": null
        }
      },
      "finalize_workflow": {},
      "email_on_failure": {
        "trigger_rule": {
          "description": "Rule that determines when the email task is triggered",
          "type": "string",
          "default": "one_failed",
          "required": false,
          "constraints": null
        },
        "recipients": {
          "description": "Email address(es) to receive failure notifications",
          "type": "string",
          "default": "test@gmail.com",
          "required": false,
          "constraints": null
        }
      }
    },
    "environment": {}
  },
  "integrations": {
    "connections": [
      {
        "id": "dwh_postgres",
        "name": "PostgreSQL Data Warehouse",
        "type": "database",
        "config": {
          "base_path": null,
          "base_url": null,
          "host": null,
          "port": null,
          "protocol": "jdbc",
          "database": "dwh",
          "schema": "md",
          "bucket": null,
          "queue_name": null
        },
        "authentication": {
          "type": "basic",
          "token_env_var": null,
          "username_env_var": "DWH_USER",
          "password_env_var": "DWH_PASSWORD",
          "credentials_path": null
        },
        "used_by_components": [
          "wait_for_l2_full_load",
          "workflow_registration",
          "finalize_workflow"
        ],
        "direction": "both",
        "rate_limit": {
          "requests_per_second": null,
          "burst": null
        },
        "datasets": {
          "produces": [
            "metadata.workflow_session",
            "metadata.workflow_status"
          ],
          "consumes": [
            "md.dwh_flag",
            "metadata.workflow_registration"
          ]
        }
      },
      {
        "id": "sap_http",
        "name": "SAP System API",
        "type": "api",
        "config": {
          "base_path": null,
          "base_url": "https://sap.example.com/api",
          "host": "sap.example.com",
          "port": null,
          "protocol": "https",
          "database": null,
          "schema": null,
          "bucket": null,
          "queue_name": null
        },
        "authentication": {
          "type": "token",
          "token_env_var": "SAP_API_TOKEN",
          "username_env_var": null,
          "password_env_var": null,
          "credentials_path": null
        },
        "used_by_components": [
          "send_flag_to_sap"
        ],
        "direction": "output",
        "rate_limit": {
          "requests_per_second": null,
          "burst": null
        },
        "datasets": {
          "produces": [
            "sap.completion_flag"
          ],
          "consumes": []
        }
      },
      {
        "id": "smtp_email",
        "name": "SMTP Email Service",
        "type": "other",
        "config": {
          "base_path": null,
          "base_url": null,
          "host": null,
          "port": null,
          "protocol": "smtp",
          "database": null,
          "schema": null,
          "bucket": null,
          "queue_name": null
        },
        "authentication": {
          "type": "none",
          "token_env_var": null,
          "username_env_var": null,
          "password_env_var": null,
          "credentials_path": null
        },
        "used_by_components": [
          "email_on_failure"
        ],
        "direction": "output",
        "rate_limit": {
          "requests_per_second": null,
          "burst": null
        },
        "datasets": {
          "produces": [
            "email.failure_notification"
          ],
          "consumes": []
        }
      },
      {
        "id": "trigger_sys_kill_all_session_pg",
        "name": "External DAG – PostgreSQL Session Cleanup",
        "type": "other",
        "config": {
          "base_path": null,
          "base_url": null,
          "host": null,
          "port": null,
          "protocol": null,
          "database": null,
          "schema": null,
          "bucket": null,
          "queue_name": null
        },
        "authentication": {
          "type": "none",
          "token_env_var": null,
          "username_env_var": null,
          "password_env_var": null,
          "credentials_path": null
        },
        "used_by_components": [
          "trigger_sys_kill_all_session_pg"
        ],
        "direction": "output",
        "rate_limit": {
          "requests_per_second": null,
          "burst": null
        },
        "datasets": {
          "produces": [],
          "consumes": []
        }
      },
      {
        "id": "trigger_wf_data_preparation_for_reports",
        "name": "External DAG – Reporting Data Preparation",
        "type": "other",
        "config": {
          "base_path": null,
          "base_url": null,
          "host": null,
          "port": null,
          "protocol": null,
          "database": null,
          "schema": null,
          "bucket": null,
          "queue_name": null
        },
        "authentication": {
          "type": "none",
          "token_env_var": null,
          "username_env_var": null,
          "password_env_var": null,
          "credentials_path": null
        },
        "used_by_components": [
          "trigger_wf_data_preparation_for_reports"
        ],
        "direction": "output",
        "rate_limit": {
          "requests_per_second": null,
          "burst": null
        },
        "datasets": {
          "produces": [],
          "consumes": []
        }
      },
      {
        "id": "trigger_load_ds_client_segmentation",
        "name": "External DAG – Client Segmentation Load",
        "type": "other",
        "config": {
          "base_path": null,
          "base_url": null,
          "host": null,
          "port": null,
          "protocol": null,
          "database": null,
          "schema": null,
          "bucket": null,
          "queue_name": null
        },
        "authentication": {
          "type": "none",
          "token_env_var": null,
          "username_env_var": null,
          "password_env_var": null,
          "credentials_path": null
        },
        "used_by_components": [
          "trigger_load_ds_client_segmentation"
        ],
        "direction": "output",
        "rate_limit": {
          "requests_per_second": null,
          "burst": null
        },
        "datasets": {
          "produces": [],
          "consumes": []
        }
      }
    ],
    "data_lineage": {
      "sources": [
        "md.dwh_flag table in the PostgreSQL DWH (flag indicating L1‑to‑L2 load success)",
        "Completion status of the previous day's WF_MAIN_DATASETS_LOAD_L2_TO_L2 DAG (via ExternalTaskSensor)",
        "Client segmentation source data processed by the external DAG l1_to_l2_p_load_data_ds_client_segmentation_full"
      ],
      "sinks": [
        "Metadata tables in the DWH (workflow registration and status updates)",
        "SAP system receiving a completion flag via HTTP POST",
        "Failure notification email sent through the SMTP service"
      ],
      "intermediate_datasets": [
        "XCom load_id generated by get_load_id",
        "Reporting preparation datasets produced by wf_data_preparation_for_reports DAG",
        "ds_client_segmentation_last_v table (row count used for verification)",
        "SAP completion flag payload"
      ]
    }
  }
}