# Generated by Dagster code generator
# Date: 2024-06-13
# Pipeline: wait_for_sales_aggregation_pipeline
# Description: No description provided.

from pathlib import Path
from typing import Any, Dict

import pandas as pd
from dagster import (
    In,
    Out,
    RetryPolicy,
    ResourceDefinition,
    ScheduleDefinition,
    ScheduleStatus,
    asset,
    job,
    op,
    fs_io_manager,
    in_process_executor,
    schedule,
)


# ----------------------------------------------------------------------
# Resources
# ----------------------------------------------------------------------


def _path_resource(base_path: str) -> ResourceDefinition:
    """Factory to create a simple path resource."""
    return ResourceDefinition(
        resource_fn=lambda _: Path(base_path),
        description=f"Base path for {base_path}",
    )


# Resource that provides the base directory for raw sales data
fs_sales_data = _path_resource("data/sales")

# Resource that provides the base directory for generated reports
fs_reports = _path_resource("data/reports")

# Placeholder SMTP email resource
def _email_smtp_resource(_):
    """A stub SMTP resource. Replace with a real implementation."""
    class EmailClient:
        def send(self, subject: str, body: str, to: str) -> None:
            # In production, integrate with an actual SMTP server.
            print(f"Sending email to {to}: {subject}")

    return EmailClient()


email_smtp = ResourceDefinition(
    resource_fn=_email_smtp_resource,
    description="SMTP Email Server",
)


# ----------------------------------------------------------------------
# Ops
# ----------------------------------------------------------------------


@op(
    name="wait_for_sales_aggregation",
    description="Waits for the sales aggregation DAG to produce the CSV file.",
    out=Out(str, description="Path to the aggregated sales CSV file."),
    retry_policy=RetryPolicy(max_retries=0),
    tags={"executor": "in_process_executor"},
)
def wait_for_sales_aggregation(context) -> str:
    """
    In a real environment this op would poll or listen for the completion of an upstream
    DAG that writes the aggregated sales CSV. Here we simulate the presence of the file.
    """
    csv_path = context.resources.fs_sales_data / "aggregated_sales.csv"
    # Simulate waiting logic (e.g., polling a filesystem, checking a DB flag, etc.)
    if not csv_path.exists():
        context.log.info(f"Aggregated sales CSV not found at {csv_path}.")
        # In a real implementation you might raise RetryRequested or similar.
    else:
        context.log.info(f"Found aggregated sales CSV at {csv_path}.")
    return str(csv_path)


@op(
    name="load_sales_csv",
    description="Loads the aggregated sales CSV into a Pandas DataFrame.",
    ins={"csv_path": In(str)},
    out=Out(pd.DataFrame, description="DataFrame with sales data."),
    retry_policy=RetryPolicy(max_retries=2, delay=5),
    tags={"executor": "in_process_executor"},
    required_resource_keys={"aggregated_sales_csv"},
)
def load_sales_csv(context, csv_path: str) -> pd.DataFrame:
    """
    Reads the CSV file produced by the upstream aggregation step.
    The `aggregated_sales_csv` IO manager handles file persistence.
    """
    # Use the built‑in fs_io_manager to read the file.
    # Since we are not using the IO manager directly, we read via pandas.
    context.log.info(f"Loading sales data from {csv_path}.")
    df = pd.read_csv(csv_path)
    context.log.info(f"Loaded {len(df)} rows.")
    return df


@op(
    name="generate_dashboard",
    description="Generates an executive dashboard from the sales DataFrame.",
    ins={"sales_df": In(pd.DataFrame)},
    out=Out(str, description="Path to the generated dashboard file."),
    retry_policy=RetryPolicy(max_retries=2, delay=5),
    tags={"executor": "in_process_executor"},
    required_resource_keys={"fs_reports", "email_smtp"},
)
def generate_dashboard(context, sales_df: pd.DataFrame) -> str:
    """
    Creates a simple dashboard (CSV summary) and stores it in the reports directory.
    Sends a notification email upon successful generation.
    """
    # Example: create a summary CSV
    summary_path = context.resources.fs_reports / "executive_dashboard.csv"
    context.log.info(f"Generating dashboard at {summary_path}.")

    # Simple aggregation – total sales per region (adjust as needed)
    summary = (
        sales_df.groupby("region")["sales_amount"]
        .sum()
        .reset_index()
        .rename(columns={"sales_amount": "total_sales"})
    )
    summary.to_csv(summary_path, index=False)
    context.log.info(f"Dashboard written with {len(summary)} rows.")

    # Send notification email
    email_client = context.resources.email_smtp
    email_client.send(
        subject="Executive Dashboard Ready",
        body=f"The executive dashboard has been generated at {summary_path}.",
        to="executives@example.com",
    )
    return str(summary_path)


# ----------------------------------------------------------------------
# Job
# ----------------------------------------------------------------------


@job(
    name="wait_for_sales_aggregation_pipeline",
    description="No description provided.",
    executor_def=in_process_executor,
    resource_defs={
        "aggregated_sales_csv": fs_io_manager,
        "fs_sales_data": fs_sales_data,
        "fs_reports": fs_reports,
        "email_smtp": email_smtp,
    },
)
def wait_for_sales_aggregation_pipeline():
    """
    Sequential pipeline that:
    1. Waits for the sales aggregation CSV.
    2. Loads the CSV into a DataFrame.
    3. Generates an executive dashboard and notifies stakeholders.
    """
    csv_path = wait_for_sales_aggregation()
    sales_df = load_sales_csv(csv_path)
    generate_dashboard(sales_df)


# ----------------------------------------------------------------------
# Schedule
# ----------------------------------------------------------------------


@schedule(
    cron_schedule="@daily",
    job=wait_for_sales_aggregation_pipeline,
    execution_timezone="UTC",
    default_status=ScheduleStatus.RUNNING,
    description="Daily execution of the sales aggregation pipeline.",
    tags={"catchup": "false"},
)
def wait_for_sales_aggregation_schedule():
    """Runs the pipeline once per day at midnight UTC without catch‑up."""
    return {}


# ----------------------------------------------------------------------
# End of file
# ----------------------------------------------------------------------