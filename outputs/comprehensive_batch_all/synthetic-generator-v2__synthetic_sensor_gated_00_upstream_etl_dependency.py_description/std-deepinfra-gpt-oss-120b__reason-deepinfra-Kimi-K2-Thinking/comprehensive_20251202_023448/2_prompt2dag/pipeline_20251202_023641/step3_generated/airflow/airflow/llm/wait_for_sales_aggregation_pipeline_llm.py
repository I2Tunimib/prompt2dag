# Generated by AI on 2024-06-28
# Airflow DAG: wait_for_sales_aggregation_pipeline
# Description: Sequential pipeline that waits for sales aggregation, loads CSV data,
# generates an executive dashboard, and emails it.

import logging
import json
import smtplib
from datetime import datetime, timedelta

import pandas as pd
import pendulum
from airflow import DAG
from airflow.decorators import dag, task
from airflow.hooks.base import BaseHook
from email.mime.text import MIMEText


default_args = {
    "owner": "airflow",
    "depends_on_past": False,
    "email_on_failure": False,
    "email_on_retry": False,
    "retries": 1,
    "retry_delay": timedelta(minutes=5),
}


@dag(
    dag_id="wait_for_sales_aggregation_pipeline",
    description="Sequential pipeline that waits for sales aggregation, loads CSV data, "
    "generates an executive dashboard, and emails it.",
    schedule="@daily",
    start_date=datetime(2024, 1, 1),
    catchup=False,
    default_args=default_args,
    tags=["sales", "dashboard"],
    timezone=pendulum.timezone("UTC"),
)
def pipeline():
    """Define the DAG tasks and their dependencies."""

    @task(retries=0, retry_delay=timedelta(minutes=1))
    def wait_for_sales_aggregation():
        """
        Placeholder task that waits for the external sales aggregation DAG to finish.
        In a production environment, replace this with a DagRunSensor or similar.
        """
        logging.info("Waiting for the sales aggregation DAG to complete...")
        # Simulated wait; replace with real logic as needed.
        import time

        time.sleep(5)
        logging.info("Sales aggregation DAG completed.")
        return True

    @task(retries=2, retry_delay=timedelta(minutes=2))
    def load_sales_csv():
        """
        Load the aggregated sales CSV from the filesystem connection
        ``aggregated_sales_csv`` and push the data to XCom as JSON.
        """
        try:
            conn = BaseHook.get_connection("aggregated_sales_csv")
            # Expect the connection's host field to contain the directory path.
            csv_path = f"{conn.host.rstrip('/')}/aggregated_sales.csv"
            logging.info("Loading CSV from %s", csv_path)

            df = pd.read_csv(csv_path)
            # Convert DataFrame to JSON for XCom transport.
            sales_json = df.to_json(date_format="iso", orient="split")
            logging.info("CSV loaded successfully, rows=%d", len(df))
            return sales_json
        except Exception as exc:
            logging.error("Failed to load CSV: %s", exc)
            raise

    @task(retries=2, retry_delay=timedelta(minutes=2))
    def generate_dashboard(sales_json: str):
        """
        Generate an executive dashboard from the sales data and email it using the
        ``email_smtp`` connection.
        """
        try:
            # Recreate DataFrame from JSON.
            df = pd.read_json(sales_json, orient="split")
            logging.info("Generating HTML dashboard, rows=%d", len(df))

            # Simple HTML representation; replace with real dashboard generation logic.
            html_body = df.to_html(index=False, border=0)

            # Retrieve SMTP connection details.
            conn = BaseHook.get_connection("email_smtp")
            smtp_host = conn.host
            smtp_port = conn.port or 25
            smtp_user = conn.login
            smtp_password = conn.password

            msg = MIMEText(html_body, "html")
            msg["Subject"] = "Executive Sales Dashboard"
            msg["From"] = smtp_user or "no-reply@example.com"
            msg["To"] = "executive@example.com"

            logging.info(
                "Sending dashboard email via %s:%s", smtp_host, smtp_port
            )
            with smtplib.SMTP(smtp_host, smtp_port) as server:
                if smtp_user and smtp_password:
                    server.starttls()
                    server.login(smtp_user, smtp_password)
                server.send_message(msg)

            logging.info("Dashboard email sent successfully.")
        except Exception as exc:
            logging.error("Failed to generate or send dashboard: %s", exc)
            raise

    # Task orchestration
    wait = wait_for_sales_aggregation()
    load = load_sales_csv()
    dashboard = generate_dashboard(load)

    wait >> load >> dashboard


# Instantiate the DAG
pipeline_dag = pipeline()