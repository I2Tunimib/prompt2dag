# Generated by Prefect 2.x Code Generator
# Date: 2023-10-05
# Prefect Version: 2.14.0

from prefect import flow, task, get_run_logger
from prefect_docker import DockerContainer
from prefect_docker.credentials import DockerHost
from prefect.filesystems import LocalFileSystem
from prefect.task_runners import SequentialTaskRunner
from prefect.deployments import Deployment
from prefect.infrastructure import DockerContainer as DockerContainerInfra
from prefect.blocks.system import Secret

# Define the LocalFileSystem blocks for data and JSON
data_dir = LocalFileSystem(basepath="/path/to/data_dir")
json_data = LocalFileSystem(basepath="/path/to/json_data")

# Define the DockerContainer tasks
@task(retries=1, name="Load and Modify Data")
def load_and_modify_data():
    logger = get_run_logger()
    logger.info("Starting Load and Modify Data task")
    container = DockerContainer(
        image="i2t-backendwithintertwino6-load-and-modify:latest",
        environment={
            "DATASET_ID": "2",
            "DATE_COLUMN": "submission_date",
            "TABLE_NAME_PREFIX": "JOT_"
        },
        docker_host=DockerHost(base_url="unix:///var/run/docker.sock"),
        local_file_system=data_dir
    )
    container.run()
    logger.info("Completed Load and Modify Data task")

@task(retries=1, name="Language Detection")
def language_detection():
    logger = get_run_logger()
    logger.info("Starting Language Detection task")
    container = DockerContainer(
        image="jmockit/language-detection",
        environment={
            "TEXT_COLUMN": "review_text",
            "LANG_CODE_COLUMN": "language_code",
            "OUTPUT_FILE": "lang_detected_2.json"
        },
        docker_host=DockerHost(base_url="unix:///var/run/docker.sock"),
        local_file_system=json_data
    )
    container.run()
    logger.info("Completed Language Detection task")

@task(retries=1, name="Sentiment Analysis")
def sentiment_analysis():
    logger = get_run_logger()
    logger.info("Starting Sentiment Analysis task")
    container = DockerContainer(
        image="huggingface/transformers-inference",
        environment={
            "MODEL_NAME": "distilbert-base-uncased-finetuned-sst-2-english",
            "TEXT_COLUMN": "review_text",
            "OUTPUT_COLUMN": "sentiment_score"
        },
        docker_host=DockerHost(base_url="unix:///var/run/docker.sock"),
        local_file_system=json_data
    )
    container.run()
    logger.info("Completed Sentiment Analysis task")

@task(retries=1, name="Category Extraction")
def category_extraction():
    logger = get_run_logger()
    logger.info("Starting Category Extraction task")
    container = DockerContainer(
        image="i2t-backendwithintertwino6-column-extension:latest",
        environment={
            "EXTENDER_ID": "featureExtractor",
            "TEXT_COLUMN": "review_text",
            "OUTPUT_COLUMN": "mentioned_features"
        },
        docker_host=DockerHost(base_url="unix:///var/run/docker.sock"),
        local_file_system=json_data
    )
    container.run()
    logger.info("Completed Category Extraction task")

@task(retries=1, name="Save Final Data")
def save_final_data():
    logger = get_run_logger()
    logger.info("Starting Save Final Data task")
    container = DockerContainer(
        image="i2t-backendwithintertwino6-save:latest",
        environment={
            "DATASET_ID": "2"
        },
        docker_host=DockerHost(base_url="unix:///var/run/docker.sock"),
        local_file_system=data_dir
    )
    container.run()
    logger.info("Completed Save Final Data task")

# Define the flow
@flow(name="load_and_modify_data_pipeline", task_runner=SequentialTaskRunner)
def load_and_modify_data_pipeline():
    logger = get_run_logger()
    logger.info("Starting load_and_modify_data_pipeline")

    load_and_modify_data_result = load_and_modify_data()
    language_detection_result = language_detection(wait_for=[load_and_modify_data_result])
    sentiment_analysis_result = sentiment_analysis(wait_for=[language_detection_result])
    category_extraction_result = category_extraction(wait_for=[sentiment_analysis_result])
    save_final_data_result = save_final_data(wait_for=[category_extraction_result])

    logger.info("Completed load_and_modify_data_pipeline")

# Define the deployment
deployment = Deployment.build_from_flow(
    flow=load_and_modify_data_pipeline,
    name="load_and_modify_data_pipeline_deployment",
    work_pool_name="default-agent-pool",
    work_queue_name="default",
    parameters={},
    schedule=None,
    tags=[],
    infra_overrides={
        "env": {
            "PREFECT_LOGGING_LEVEL": "INFO"
        }
    }
)

if __name__ == "__main__":
    deployment.apply()