# Generated by Prefect Pipeline Generator
# Date: 2024-06-28
# Pipeline: Content Moderation Pipeline
# Description: Scans user‑generated content for toxicity, branches based on a 0.7 threshold,
#              and merges results for audit logging.

from __future__ import annotations

import json
import os
from typing import List, Dict, Any

from prefect import flow, task, get_run_logger
from prefect.task_runners import SequentialTaskRunner
from prefect.deployments import DeploymentSpec
from prefect.orion.schemas.schedules import CronSchedule
from prefect.blocks.system import Secret
from prefect.filesystems import LocalFileSystem

# -------------------------------------------------------------------------
# Resource Blocks (placeholders – configure in Prefect UI or via CLI)
# -------------------------------------------------------------------------
# Local CSV filesystem
local_filesystem_csv = LocalFileSystem(basepath="data/")  # type: ignore

# Secrets (replace with actual block names in your Prefect workspace)
content_management_api = Secret.load("content-management-api")  # type: ignore
publishing_api = Secret.load("publishing-api")  # type: ignore
audit_logging_api = Secret.load("audit-logging-api")  # type: ignore
email_smtp_service = Secret.load("email-smtp-service")  # type: ignore


@task(retries=2, retry_delay_seconds=10, name="Extract User Content")
def extract_user_content(csv_path: str) -> List[Dict[str, Any]]:
    """
    Load user‑generated content from a CSV file stored in the local filesystem.

    Args:
        csv_path: Relative path to the CSV file within the configured filesystem.

    Returns:
        A list of dictionaries, each representing a content record.
    """
    logger = get_run_logger()
    logger.info("Reading user content from %s", csv_path)

    try:
        # Use the filesystem block to read the file
        with local_filesystem_csv.open(csv_path, "r") as f:
            # Simple CSV parsing – replace with pandas or csv module as needed
            header = f.readline().strip().split(",")
            records = []
            for line in f:
                values = line.strip().split(",")
                record = dict(zip(header, values))
                records.append(record)
        logger.info("Loaded %d content records", len(records))
        return records
    except Exception as exc:
        logger.error("Failed to read CSV: %s", exc)
        raise


@task(retries=2, retry_delay_seconds=10, name="Evaluate Toxicity")
def evaluate_toxicity(
    content_records: List[Dict[str, Any]], threshold: float = 0.7
) -> List[Dict[str, Any]]:
    """
    Evaluate each piece of content for toxicity using the Content Management API.

    Args:
        content_records: List of content dictionaries.
        threshold: Toxicity score threshold for branching.

    Returns:
        The same list with an added ``toxicity_score`` and ``is_toxic`` flag.
    """
    logger = get_run_logger()
    api_key = content_management_api.get()  # type: ignore
    logger.info("Evaluating toxicity for %d records (threshold=%.2f)", len(content_records), threshold)

    evaluated = []
    for record in content_records:
        try:
            # Placeholder for real API call – replace with requests or SDK call
            # Simulate a toxicity score between 0 and 1
            score = float(record.get("simulated_score", 0.5))
            is_toxic = score >= threshold
            record.update({"toxicity_score": score, "is_toxic": is_toxic})
            evaluated.append(record)
        except Exception as exc:
            logger.error("Error evaluating record %s: %s", record.get("id"), exc)
            raise

    logger.info("Toxicity evaluation completed")
    return evaluated


@task(retries=2, retry_delay_seconds=10, name="Publish Safe Content")
def publish_content(safe_records: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    Publish content that passed the toxicity check.

    Args:
        safe_records: List of content dictionaries where ``is_toxic`` is False.

    Returns:
        The list of records that were successfully published.
    """
    logger = get_run_logger()
    api_key = publishing_api.get()  # type: ignore
    logger.info("Publishing %d safe records", len(safe_records))

    published = []
    for record in safe_records:
        try:
            # Placeholder for real publishing call
            # e.g., requests.post(publishing_endpoint, json=record, headers=...)
            record["published"] = True
            published.append(record)
        except Exception as exc:
            logger.error("Failed to publish record %s: %s", record.get("id"), exc)
            raise

    logger.info("Publishing completed")
    return published


@task(retries=2, retry_delay_seconds=10, name="Remove and Flag Toxic Content")
def remove_and_flag_content(toxic_records: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    Remove toxic content from the platform and flag it for review.

    Args:
        toxic_records: List of content dictionaries where ``is_toxic`` is True.

    Returns:
        The list of records that were removed and flagged.
    """
    logger = get_run_logger()
    api_key = content_management_api.get()  # type: ignore
    logger.info("Removing and flagging %d toxic records", len(toxic_records))

    processed = []
    for record in toxic_records:
        try:
            # Placeholder for removal/flagging API call
            record["removed"] = True
            record["flagged"] = True
            processed.append(record)
        except Exception as exc:
            logger.error("Failed to process toxic record %s: %s", record.get("id"), exc)
            raise

    logger.info("Removal and flagging completed")
    return processed


@task(retries=2, retry_delay_seconds=10, name="Audit Log Consolidation")
def audit_log(
    published: List[Dict[str, Any]],
    removed: List[Dict[str, Any]],
) -> None:
    """
    Consolidate audit information for both published and removed content and
    send it to the audit logging system. Also sends a summary email.

    Args:
        published: List of successfully published records.
        removed: List of removed/flagged records.
    """
    logger = get_run_logger()
    api_key = audit_logging_api.get()  # type: ignore
    smtp_credentials = email_smtp_service.get()  # type: ignore

    logger.info("Consolidating audit logs")
    audit_payload = {
        "published": [r["id"] for r in published],
        "removed": [r["id"] for r in removed],
        "summary": {
            "published_count": len(published),
            "removed_count": len(removed),
        },
    }

    try:
        # Placeholder for audit logging API call
        logger.debug("Audit payload: %s", json.dumps(audit_payload))
        # e.g., requests.post(audit_endpoint, json=audit_payload, headers=...)
    except Exception as exc:
        logger.error("Failed to send audit log: %s", exc)
        raise

    # Send summary email (placeholder)
    try:
        logger.info("Sending audit summary email")
        # e.g., smtplib.SMTP(...).sendmail(...)
    except Exception as exc:
        logger.error("Failed to send audit email: %s", exc)
        raise

    logger.info("Audit logging completed")


@flow(
    name="Content Moderation Pipeline",
    task_runner=SequentialTaskRunner(),
    description="Comprehensive pipeline that scans user‑generated content for toxicity, "
                "branches based on a 0.7 threshold, and merges results for audit logging.",
)
def content_moderation_pipeline(csv_path: str = "user_content.csv", toxicity_threshold: float = 0.7) -> None:
    """
    Orchestrates the content moderation workflow.

    Args:
        csv_path: Path to the CSV file containing user content.
        toxicity_threshold: Score above which content is considered toxic.
    """
    logger = get_run_logger()
    logger.info("Starting Content Moderation Pipeline")

    # Extract
    raw_content = extract_user_content(csv_path)

    # Evaluate toxicity
    evaluated_content = evaluate_toxicity(raw_content, threshold=toxicity_threshold)

    # Fan‑out: split into safe and toxic streams
    safe_content = [rec for rec in evaluated_content if not rec.get("is_toxic")]
    toxic_content = [rec for rec in evaluated_content if rec.get("is_toxic")]

    # Parallel branches
    published = publish_content(safe_content)
    removed = remove_and_flag_content(toxic_content)

    # Fan‑in: audit logging
    audit_log(published, removed)

    logger.info("Content Moderation Pipeline completed")


# -------------------------------------------------------------------------
# Deployment Specification
# -------------------------------------------------------------------------
# This creates a deployment that runs daily at midnight UTC without catch‑up.
DeploymentSpec(
    name="content_moderation_pipeline_deployment",
    flow=content_moderation_pipeline,
    schedule=CronSchedule(cron="0 0 * * *", timezone="UTC", day_or=False),
    tags=["content-moderation"],
    work_pool_name="default-agent-pool",
    enforce_parameter_schema=False,
    description="Daily execution of the Content Moderation Pipeline.",
    parameters={"csv_path": "user_content.csv", "toxicity_threshold": 0.7},
    flow_runner=SequentialTaskRunner(),
    # Prefect 2.14+ supports `catchup` via schedule; set to False by default.
)

if __name__ == "__main__":
    # For local debugging; runs the flow immediately.
    content_moderation_pipeline()