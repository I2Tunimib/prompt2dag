# Generated by Prefect Pipeline Generator
# Date: 2024-06-28
# Pipeline: extract_user_content_pipeline
# Description: Content Moderation Pipeline (fanout_fanin pattern)

from __future__ import annotations

import json
from pathlib import Path
from typing import List, Dict, Any

import pandas as pd
from prefect import flow, task, get_run_logger
from prefect.task_runners import SequentialTaskRunner
from prefect.deployments import DeploymentSpec
from prefect.server.schemas.schedules import CronSchedule
from prefect.filesystems import LocalFileSystem
from prefect.blocks.system import Secret


# -------------------------------------------------------------------------
# Block loading (replace block names with actual block identifiers in your
# Prefect workspace)
# -------------------------------------------------------------------------
local_csv_filesystem: LocalFileSystem = LocalFileSystem.load("local_csv_filesystem")
content_management_api: Secret = Secret.load("content_management_api")
publishing_api: Secret = Secret.load("publishing_api")
audit_logging_api: Secret = Secret.load("audit_logging_api")
email_smtp_service: Secret = Secret.load("email_smtp_service")


# -------------------------------------------------------------------------
# Tasks
# -------------------------------------------------------------------------
@task(retries=2, retry_delay_seconds=60)
def extract_user_content(csv_path: str) -> pd.DataFrame:
    """
    Load user‑generated content from a CSV file stored in the local filesystem.

    Args:
        csv_path: Relative path inside the ``local_csv_filesystem`` block.

    Returns:
        DataFrame containing the raw content.
    """
    logger = get_run_logger()
    logger.info("Reading CSV from %s", csv_path)

    # Resolve the absolute path using the filesystem block
    full_path = Path(local_csv_filesystem.basepath) / csv_path
    if not full_path.is_file():
        raise FileNotFoundError(f"CSV file not found at {full_path}")

    df = pd.read_csv(full_path)
    logger.info("Loaded %d rows of user content", len(df))
    return df


@task(retries=2, retry_delay_seconds=60)
def evaluate_toxicity(content_df: pd.DataFrame) -> pd.DataFrame:
    """
    Evaluate each piece of content for toxicity using the Content Management API.

    The API credentials are stored in the ``content_management_api`` secret block.

    Args:
        content_df: DataFrame with a column ``text`` containing user content.

    Returns:
        DataFrame with an additional ``toxicity_score`` column (0‑1 range).
    """
    logger = get_run_logger()
    logger.info("Evaluating toxicity for %d rows", len(content_df))

    # Placeholder for real API call – here we simulate scores
    def mock_score(text: str) -> float:
        # Simple heuristic: longer text gets higher score
        return min(len(text) / 500.0, 1.0)

    content_df = content_df.copy()
    content_df["toxicity_score"] = content_df["text"].apply(mock_score)
    logger.debug("Toxicity scores added")
    return content_df


@task(retries=2, retry_delay_seconds=60)
def publish_content(toxic_df: pd.DataFrame) -> List[Dict[str, Any]]:
    """
    Publish content that is deemed safe (toxicity_score < 0.5) via the Publishing API.

    Args:
        toxic_df: DataFrame that includes ``toxicity_score``.

    Returns:
        List of dictionaries representing the published items.
    """
    logger = get_run_logger()
    safe_content = toxic_df[toxic_df["toxicity_score"] < 0.5]
    logger.info("Publishing %d safe content items", len(safe_content))

    # Placeholder for real publishing logic
    published = []
    for _, row in safe_content.iterrows():
        # Simulate a publish call
        published.append(
            {
                "id": row.get("id"),
                "status": "published",
                "text": row["text"],
            }
        )
    logger.debug("Published items: %s", json.dumps(published, default=str))
    return published


@task(retries=2, retry_delay_seconds=60)
def remove_and_flag_content(toxic_df: pd.DataFrame) -> List[Dict[str, Any]]:
    """
    Remove and flag content that exceeds the toxicity threshold.

    Args:
        toxic_df: DataFrame that includes ``toxicity_score``.

    Returns:
        List of dictionaries describing the removed/flagged items.
    """
    logger = get_run_logger()
    toxic_content = toxic_df[toxic_df["toxicity_score"] >= 0.5]
    logger.info("Removing and flagging %d toxic content items", len(toxic_content))

    # Placeholder for real removal/flagging logic
    flagged = []
    for _, row in toxic_content.iterrows():
        flagged.append(
            {
                "id": row.get("id"),
                "status": "removed",
                "reason": "toxicity_score >= 0.5",
                "toxicity_score": row["toxicity_score"],
            }
        )
    logger.debug("Flagged items: %s", json.dumps(flagged, default=str))
    return flagged


@task(retries=2, retry_delay_seconds=60)
def audit_log(
    published: List[Dict[str, Any]],
    flagged: List[Dict[str, Any]],
) -> None:
    """
    Create an audit log entry summarising the moderation run.

    The audit is sent to the Audit Logging API and an email notification is
    dispatched via the SMTP service.

    Args:
        published: List of published content items.
        flagged: List of removed/flagged content items.
    """
    logger = get_run_logger()
    audit_entry = {
        "published_count": len(published),
        "flagged_count": len(flagged),
        "published_ids": [item["id"] for item in published],
        "flagged_ids": [item["id"] for item in flagged],
    }

    logger.info("Sending audit log to audit_logging_api")
    # Placeholder for real API call
    audit_logging_api.get()  # simulate secret usage

    logger.info("Audit entry: %s", json.dumps(audit_entry, default=str))

    logger.info("Sending notification email via email_smtp_service")
    # Placeholder for real email sending
    email_smtp_service.get()  # simulate secret usage


# -------------------------------------------------------------------------
# Flow definition
# -------------------------------------------------------------------------
@flow(
    name="extract_user_content_pipeline",
    task_runner=SequentialTaskRunner(),
)
def extract_user_content_pipeline(csv_path: str = "user_content.csv") -> None:
    """
    Orchestrates the content moderation pipeline.

    The flow follows a fan‑out/fan‑in pattern:
        1. Extract raw content.
        2. Evaluate toxicity.
        3. Fan‑out to publishing safe content and removing toxic content.
        4. Fan‑in to create an audit log.

    Args:
        csv_path: Path to the CSV file within the ``local_csv_filesystem`` block.
    """
    logger = get_run_logger()
    logger.info("Starting Content Moderation Pipeline")

    # Step 1: Extract
    raw_df = extract_user_content(csv_path)

    # Step 2: Evaluate toxicity
    evaluated_df = evaluate_toxicity(raw_df)

    # Step 3: Fan‑out
    published = publish_content(evaluated_df)
    flagged = remove_and_flag_content(evaluated_df)

    # Step 4: Fan‑in (audit)
    audit_log(published, flagged)

    logger.info("Content Moderation Pipeline completed successfully")


# -------------------------------------------------------------------------
# Deployment specification
# -------------------------------------------------------------------------
DeploymentSpec(
    name="extract_user_content_pipeline_deployment",
    flow=extract_user_content_pipeline,
    schedule=CronSchedule(cron="0 0 * * *", timezone="UTC"),  # @daily UTC
    tags=["content-moderation"],
    parameters={"csv_path": "user_content.csv"},
    work_pool_name="default-agent-pool",
    description="Daily content moderation pipeline that extracts, evaluates, publishes, "
    "removes, and audits user‑generated content.",
    enforce_parameter_schema=False,
)