# ------------------------------------------------------------
# Generated by Prefect Pipeline Generator
# Date: 2024-06-28
# Pipeline: Content Moderation Pipeline
# ------------------------------------------------------------

from __future__ import annotations

import pandas as pd
from typing import List, Tuple, Dict

from prefect import flow, task, get_run_logger
from prefect.task_runners import SequentialTaskRunner
from prefect.deployments import DeploymentSpec
from prefect.server.schemas.schedules import CronSchedule
from prefect.blocks.system import Secret
from prefect.filesystems import LocalFileSystem


@task(retries=2, description="Extract user‑generated content from a CSV file.")
def extract_user_content(csv_path: str) -> List[Dict]:
    """
    Read a CSV file containing user‑generated content and return a list of
    dictionaries, one per row.

    Args:
        csv_path: Absolute path to the CSV file.

    Returns:
        List of dictionaries representing the content rows.
    """
    logger = get_run_logger()
    logger.info("Reading user content from %s", csv_path)

    try:
        df = pd.read_csv(csv_path)
    except Exception as exc:
        logger.error("Failed to read CSV: %s", exc)
        raise

    records = df.to_dict(orient="records")
    logger.info("Extracted %d rows of content.", len(records))
    return records


@task(retries=2, description="Evaluate toxicity scores and split content.")
def evaluate_toxicity(
    content: List[Dict], threshold: float = 0.7
) -> Tuple[List[Dict], List[Dict]]:
    """
    Mock toxicity evaluation. Each row is expected to have a ``text`` field.
    A dummy score is generated; rows with a score >= ``threshold`` are
    considered toxic.

    Args:
        content: List of content dictionaries.
        threshold: Toxicity score threshold.

    Returns:
        Tuple of (safe_content, toxic_content).
    """
    logger = get_run_logger()
    safe_content: List[Dict] = []
    toxic_content: List[Dict] = []

    for row in content:
        # Placeholder: deterministic pseudo‑score based on text length
        text = str(row.get("text", ""))
        score = min(len(text) / 100.0, 1.0)  # simple heuristic
        row["toxicity_score"] = score

        if score >= threshold:
            toxic_content.append(row)
        else:
            safe_content.append(row)

    logger.info(
        "Toxicity evaluation complete: %d safe, %d toxic (threshold=%.2f).",
        len(safe_content),
        len(toxic_content),
        threshold,
    )
    return safe_content, toxic_content


@task(retries=2, description="Publish safe content via the publishing API.")
def publish_content(safe_content: List[Dict], publishing_api: Secret) -> None:
    """
    Simulate publishing safe content.

    Args:
        safe_content: List of non‑toxic content dictionaries.
        publishing_api: Secret block containing publishing credentials.
    """
    logger = get_run_logger()
    api_key = publishing_api.get()
    logger.info("Publishing %d safe items using API key: %s", len(safe_content), api_key[:4] + "***")
    # Placeholder for real publishing logic
    for item in safe_content:
        logger.debug("Published item ID %s", item.get("id"))


@task(
    retries=2,
    description="Remove toxic content, flag users, and send notification emails.",
)
def remove_and_flag_content(
    toxic_content: List[Dict],
    content_mgmt_api: Secret,
    email_smtp: Secret,
) -> None:
    """
    Simulate removal of toxic content and flagging of associated users.

    Args:
        toxic_content: List of toxic content dictionaries.
        content_mgmt_api: Secret block for the content‑management system.
        email_smtp: Secret block for the SMTP server.
    """
    logger = get_run_logger()
    cm_api_key = content_mgmt_api.get()
    smtp_credentials = email_smtp.get()
    logger.info(
        "Removing %d toxic items using CM API key: %s and notifying via SMTP.",
        len(toxic_content),
        cm_api_key[:4] + "***",
    )
    # Placeholder for real removal/flagging logic
    for item in toxic_content:
        user_id = item.get("user_id")
        logger.debug("Removed toxic item ID %s; flagged user %s", item.get("id"), user_id)
        # Simulate email notification
        logger.debug("Sent notification email to user %s via SMTP credentials %s***", user_id, smtp_credentials[:4])


@task(retries=2, description="Create a consolidated audit log entry.")
def audit_log(
    safe_content: List[Dict],
    toxic_content: List[Dict],
    audit_logging_api: Secret,
) -> None:
    """
    Consolidate the results of the moderation run and send them to the audit
    logging service.

    Args:
        safe_content: List of safe content dictionaries.
        toxic_content: List of toxic content dictionaries.
        audit_logging_api: Secret block for the audit logging system.
    """
    logger = get_run_logger()
    audit_key = audit_logging_api.get()
    logger.info(
        "Creating audit log (safe=%d, toxic=%d) using audit key: %s",
        len(safe_content),
        len(toxic_content),
        audit_key[:4] + "***",
    )
    # Placeholder for real audit logging
    audit_record = {
        "safe_count": len(safe_content),
        "toxic_count": len(toxic_content),
        "details": {
            "safe_ids": [item.get("id") for item in safe_content],
            "toxic_ids": [item.get("id") for item in toxic_content],
        },
    }
    logger.debug("Audit record: %s", audit_record)


@flow(
    name="Content Moderation Pipeline",
    description="Scans user‑generated content for toxicity, branches based on a 0.7 threshold, and merges results for audit logging.",
    task_runner=SequentialTaskRunner(),
)
def content_moderation_pipeline() -> None:
    """
    Orchestrates the content moderation workflow:
    1. Extract content from CSV.
    2. Evaluate toxicity and split into safe/toxic streams.
    3. Publish safe content.
    4. Remove and flag toxic content.
    5. Create a consolidated audit log.
    """
    logger = get_run_logger()

    # Load infrastructure blocks
    csv_fs = LocalFileSystem.load("local_csv_filesystem")
    publishing_api = Secret.load("publishing_api")
    content_mgmt_api = Secret.load("content_mgmt_api")
    audit_logging_api = Secret.load("audit_logging_api")
    email_smtp = Secret.load("email_smtp")

    # Assume the CSV file is located at the root of the filesystem block
    csv_path = f"{csv_fs.basepath.rstrip('/')}/user_content.csv"
    logger.info("Starting pipeline with CSV path: %s", csv_path)

    # Step 1: Extract
    raw_content = extract_user_content(csv_path)

    # Step 2: Evaluate toxicity
    safe_content, toxic_content = evaluate_toxicity(raw_content)

    # Step 3 & 4: Parallel processing of safe and toxic streams
    publish_future = publish_content.submit(safe_content, publishing_api)
    remove_future = remove_and_flag_content.submit(toxic_content, content_mgmt_api, email_smtp)

    # Step 5: Audit log (wait for both upstream tasks)
    audit_log.submit(
        safe_content,
        toxic_content,
        audit_logging_api,
        wait_for=[publish_future, remove_future],
    )

    logger.info("Content moderation pipeline completed.")


# -------------------------------------------------------------------------
# Deployment specification
# -------------------------------------------------------------------------
DeploymentSpec(
    name="content_moderation_pipeline_deployment",
    flow=content_moderation_pipeline,
    schedule=CronSchedule(cron="0 0 * * *", timezone="UTC"),  # @daily UTC
    tags=["content-moderation"],
    parameters={},
    work_pool_name="default-agent-pool",
    enforce_parameter_schema=False,
    flow_name="content_moderation_pipeline",
    description="Daily execution of the content moderation pipeline.",
    catchup=False,
)