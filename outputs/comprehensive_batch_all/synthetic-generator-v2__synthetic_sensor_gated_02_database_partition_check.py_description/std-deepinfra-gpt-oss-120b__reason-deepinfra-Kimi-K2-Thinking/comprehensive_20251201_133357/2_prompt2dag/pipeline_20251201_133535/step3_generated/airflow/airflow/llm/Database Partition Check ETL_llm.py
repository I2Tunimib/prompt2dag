# Generated by Airflow DAG Generator
# Date: 2024-06-27
# Description: Database Partition Check ETL - sensor-gated daily ETL pipeline

import datetime
import time
import logging

import pendulum
from airflow import DAG
from airflow.exceptions import AirflowException
from airflow.models import Variable
from airflow.providers.common.sql.hooks.sql import DbApiHook
from airflow.utils.task_group import TaskGroup
from airflow.decorators import task

# Default arguments for all tasks
default_args = {
    "owner": "airflow",
    "depends_on_past": False,
    "retries": 2,
    "retry_delay": datetime.timedelta(minutes=5),
}

# DAG definition
with DAG(
    dag_id="database_partition_check_etl",
    description="Sensor-gated daily ETL pipeline that waits for database partition availability before extracting, transforming, and loading incremental orders data.",
    schedule_interval="@daily",
    start_date=pendulum.datetime(2024, 1, 1, tz="UTC"),
    catchup=False,
    default_args=default_args,
    tags=["etl", "partition", "orders"],
    max_active_runs=1,
) as dag:

    @task(retries=2, retry_delay=datetime.timedelta(minutes=5))
    def wait_partition():
        """
        Waits for the daily partition to become available in the source database.
        This task polls the database every 30 seconds for up to 10 minutes.
        """
        conn_id = "database_conn"
        hook = DbApiHook.get_hook(conn_id=conn_id)
        partition_name = f"orders_{pendulum.now(tz='UTC').format('YYYYMMDD')}"
        max_attempts = 20
        attempt = 0

        while attempt < max_attempts:
            try:
                sql = f"SELECT 1 FROM information_schema.tables WHERE table_name = '{partition_name}'"
                result = hook.get_first(sql)
                if result:
                    logging.info("Partition %s is available.", partition_name)
                    return partition_name
                else:
                    logging.info("Partition %s not yet available. Retrying...", partition_name)
            except Exception as e:
                logging.error("Error checking partition existence: %s", e)
                raise AirflowException(f"Failed to query partition: {e}")

            attempt += 1
            time.sleep(30)  # wait before next poll

        raise AirflowException(f"Partition {partition_name} not available after waiting.")

    @task(retries=2, retry_delay=datetime.timedelta(minutes=5))
    def extract_incremental(partition_name: str):
        """
        Extracts incremental orders from the identified partition.
        Returns a list of order records (as dicts) via XCom.
        """
        conn_id = "database_conn"
        hook = DbApiHook.get_hook(conn_id=conn_id)

        try:
            sql = f"SELECT * FROM {partition_name} WHERE order_timestamp >= (SELECT MAX(order_timestamp) FROM staging_orders)"
            rows = hook.get_records(sql)
            orders = [dict(zip([desc[0] for desc in hook.get_cursor().description], row)) for row in rows]
            logging.info("Extracted %d incremental orders.", len(orders))
            return orders
        except Exception as e:
            logging.error("Failed to extract incremental orders: %s", e)
            raise AirflowException(f"Extraction error: {e}")

    @task(retries=2, retry_delay=datetime.timedelta(minutes=5))
    def transform_orders(orders: list):
        """
        Transforms raw order records into the schema required by the data warehouse.
        Simple example: convert timestamps to ISO format and calculate total_price.
        """
        transformed = []
        try:
            for order in orders:
                transformed_order = {
                    "order_id": order["order_id"],
                    "customer_id": order["customer_id"],
                    "order_timestamp": order["order_timestamp"].isoformat(),
                    "total_price": float(order["quantity"]) * float(order["unit_price"]),
                    "status": order.get("status", "NEW"),
                }
                transformed.append(transformed_order)
            logging.info("Transformed %d orders.", len(transformed))
            return transformed
        except Exception as e:
            logging.error("Transformation failed: %s", e)
            raise AirflowException(f"Transformation error: {e}")

    @task(retries=2, retry_delay=datetime.timedelta(minutes=5))
    def load_orders(transformed_orders: list):
        """
        Loads transformed orders into the data warehouse.
        Assumes a separate connection 'warehouse_conn' is defined in Airflow.
        """
        conn_id = "warehouse_conn"
        hook = DbApiHook.get_hook(conn_id=conn_id)

        try:
            insert_sql = """
                INSERT INTO dw_orders (order_id, customer_id, order_timestamp, total_price, status)
                VALUES (%(order_id)s, %(customer_id)s, %(order_timestamp)s, %(total_price)s, %(status)s)
                ON CONFLICT (order_id) DO UPDATE SET
                    customer_id = EXCLUDED.customer_id,
                    order_timestamp = EXCLUDED.order_timestamp,
                    total_price = EXCLUDED.total_price,
                    status = EXCLUDED.status;
            """
            for order in transformed_orders:
                hook.run(insert_sql, parameters=order)
            logging.info("Loaded %d orders into the warehouse.", len(transformed_orders))
        except Exception as e:
            logging.error("Loading to warehouse failed: %s", e)
            raise AirflowException(f"Load error: {e}")

    # Define task pipeline
    partition = wait_partition()
    extracted = extract_incremental(partition)
    transformed = transform_orders(extracted)
    load = load_orders(transformed)

    # Set dependencies (sequential flow)
    partition >> extracted >> transformed >> load