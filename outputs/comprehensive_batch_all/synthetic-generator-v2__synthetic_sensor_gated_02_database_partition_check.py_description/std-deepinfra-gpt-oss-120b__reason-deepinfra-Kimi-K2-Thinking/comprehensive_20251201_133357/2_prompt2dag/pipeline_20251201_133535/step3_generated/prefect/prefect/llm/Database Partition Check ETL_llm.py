# Generated by Prefect Pipeline Generator
# Date: 2024-06-28
# Prefect version: 2.14.0
# Flow: database_partition_check_etl

import time
from datetime import datetime, timedelta

import pandas as pd
from prefect import flow, task, get_run_logger
from prefect.deployments import DeploymentSpec
from prefect.infrastructure import Process
from prefect.task_runners import SequentialTaskRunner
from prefect.server.schemas.schedules import CronSchedule
from prefect.blocks.system import Secret


@task(retries=2, retry_delay_seconds=30, name="Wait for Daily Partition")
def wait_partition(db_conn: Secret) -> bool:
    """
    Poll the primary database until the daily partition for the current date
    becomes available.

    Args:
        db_conn: Secret block containing the database connection string.

    Returns:
        True if the partition is detected within the timeout period.

    Raises:
        RuntimeError: If the partition is not found after the maximum wait time.
    """
    logger = get_run_logger()
    connection_str = db_conn.get()
    logger.info("Using database connection: %s", connection_str)

    # Placeholder logic: simulate checking for a partition.
    max_wait_minutes = 30
    poll_interval_seconds = 60
    deadline = datetime.utcnow() + timedelta(minutes=max_wait_minutes)

    while datetime.utcnow() < deadline:
        logger.info("Checking for partition for date %s", datetime.utcnow().date())
        # TODO: Replace with actual DB query to verify partition existence.
        partition_exists = True  # Mocked as always true for demo purposes.
        if partition_exists:
            logger.info("Partition is available.")
            return True
        logger.info("Partition not yet available, sleeping for %s seconds.", poll_interval_seconds)
        time.sleep(poll_interval_seconds)

    raise RuntimeError("Timed out waiting for daily partition.")


@task(retries=2, retry_delay_seconds=30, name="Extract Incremental Orders")
def extract_incremental(db_conn: Secret) -> pd.DataFrame:
    """
    Extract incremental orders data from the primary database based on the
    latest processed timestamp.

    Args:
        db_conn: Secret block containing the database connection string.

    Returns:
        DataFrame containing the extracted orders.

    Raises:
        RuntimeError: If extraction fails.
    """
    logger = get_run_logger()
    connection_str = db_conn.get()
    logger.info("Extracting incremental orders using connection: %s", connection_str)

    # Placeholder extraction logic.
    # TODO: Replace with actual extraction query.
    data = {
        "order_id": [1, 2, 3],
        "order_date": [datetime.utcnow()] * 3,
        "amount": [100.0, 150.5, 200.75],
    }
    df = pd.DataFrame(data)
    logger.info("Extracted %d rows.", len(df))
    return df


@task(retries=2, retry_delay_seconds=30, name="Transform Orders Data")
def transform_orders(df: pd.DataFrame) -> pd.DataFrame:
    """
    Apply business transformations to the extracted orders data.

    Args:
        df: DataFrame with raw orders data.

    Returns:
        Transformed DataFrame ready for loading.

    Raises:
        ValueError: If the input DataFrame is empty.
    """
    logger = get_run_logger()
    if df.empty:
        raise ValueError("Received empty DataFrame for transformation.")

    logger.info("Starting transformation on %d rows.", len(df))

    # Example transformation: calculate total with tax.
    tax_rate = 0.07
    df["total_with_tax"] = df["amount"] * (1 + tax_rate)
    logger.info("Transformation complete.")
    return df


@task(retries=2, retry_delay_seconds=30, name="Load Orders to Warehouse")
def load_orders(df: pd.DataFrame, warehouse_conn: Secret) -> None:
    """
    Load the transformed orders data into the data warehouse.

    Args:
        df: Transformed orders DataFrame.
        warehouse_conn: Secret block containing the warehouse connection string.

    Raises:
        RuntimeError: If loading fails.
    """
    logger = get_run_logger()
    connection_str = warehouse_conn.get()
    logger.info("Loading %d rows into warehouse using connection: %s", len(df), connection_str)

    # Placeholder loading logic.
    # TODO: Replace with actual loading code (e.g., using SQLAlchemy, pandas.to_sql, etc.)
    time.sleep(2)  # Simulate load time.
    logger.info("Load completed successfully.")


@flow(
    name="Database Partition Check ETL",
    task_runner=SequentialTaskRunner(),
    description="Sensor-gated daily ETL pipeline that waits for database partition availability before extracting, transforming, and loading incremental orders data.",
)
def database_partition_check_etl():
    """
    Orchestrates the ETL pipeline:
    1. Waits for the daily partition.
    2. Extracts incremental orders.
    3. Transforms the orders data.
    4. Loads the transformed data into the warehouse.
    """
    logger = get_run_logger()

    # Load secret blocks
    db_secret = Secret.load("database_conn")
    warehouse_secret = Secret.load("warehouse_conn")  # Assumes a separate secret for the warehouse.

    # Task execution respecting dependencies
    partition_ready = wait_partition(db_secret)
    orders_df = extract_incremental(db_secret).wait_for(partition_ready)
    transformed_df = transform_orders(orders_df)
    load_orders(transformed_df, warehouse_secret)


# Deployment specification
DeploymentSpec(
    name="database_partition_check_etl_deployment",
    flow=database_partition_check_etl,
    schedule=CronSchedule(cron="@daily", timezone="UTC", catchup=False),
    work_pool_name="default-agent-pool",
    infra=Process(),
    tags=["etl", "daily", "partition-check"],
    description="Daily deployment of the Database Partition Check ETL flow.",
)