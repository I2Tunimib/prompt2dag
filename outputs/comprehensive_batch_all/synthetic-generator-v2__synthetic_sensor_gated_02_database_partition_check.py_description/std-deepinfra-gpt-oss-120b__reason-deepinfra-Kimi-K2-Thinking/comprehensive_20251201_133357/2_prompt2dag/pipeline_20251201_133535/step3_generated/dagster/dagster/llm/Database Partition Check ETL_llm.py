# Generated by Dagster Code Generator
# Date: 2024-06-28
# Description: Database Partition Check ETL pipeline implementation

from datetime import datetime
from typing import Any, Dict, List

from dagster import (
    op,
    job,
    In,
    Out,
    RetryPolicy,
    ResourceDefinition,
    resource,
    fs_io_manager,
    ScheduleDefinition,
    DefaultScheduleStatus,
    in_process_executor,
    ConfigurableResource,
)


# ----------------------------------------------------------------------
# Resources
# ----------------------------------------------------------------------
@resource(required_resource_keys=set())
def database_conn_resource(context) -> Dict[str, Any]:
    """
    Placeholder resource for the primary database connection.

    In a production environment, replace this stub with an actual
    database client (e.g., SQLAlchemy engine, psycopg2 connection, etc.).
    """
    # Example placeholder connection info
    return {"connection_string": "postgresql://user:password@host:5432/database"}


# ----------------------------------------------------------------------
# Ops
# ----------------------------------------------------------------------
@op(
    name="wait_partition",
    description="Wait for the daily partition to become available in the source database.",
    out=Out(bool),
    retry_policy=RetryPolicy(max_retries=2, delay=5),
)
def wait_partition(context) -> bool:
    """
    Polls the source database until the expected daily partition exists.
    Returns ``True`` when the partition is ready.
    """
    db = context.resources.database_conn
    # Placeholder logic – replace with real partition check
    context.log.info(f"Checking partition availability using {db['connection_string']}")
    # Simulate successful check
    partition_ready = True
    if not partition_ready:
        raise Exception("Partition not yet available.")
    context.log.info("Partition is available.")
    return partition_ready


@op(
    name="extract_incremental",
    description="Extract incremental orders data since the last successful run.",
    ins={"partition_ready": In(bool)},
    out=Out(List[Dict[str, Any]]),
    retry_policy=RetryPolicy(max_retries=2, delay=5),
)
def extract_incremental(context, partition_ready: bool) -> List[Dict[str, Any]]:
    """
    Pulls new orders from the source database after confirming the partition is ready.
    """
    if not partition_ready:
        raise Exception("Cannot extract orders because partition is not ready.")
    db = context.resources.database_conn
    context.log.info(f"Extracting incremental orders using {db['connection_string']}")
    # Placeholder extraction logic
    extracted_data = [
        {"order_id": 1, "amount": 100.0, "order_date": datetime.utcnow()},
        {"order_id": 2, "amount": 250.5, "order_date": datetime.utcnow()},
    ]
    context.log.info(f"Extracted {len(extracted_data)} orders.")
    return extracted_data


@op(
    name="transform_orders",
    description="Transform raw orders data into the schema required by the warehouse.",
    ins={"orders": In(List[Dict[str, Any]])},
    out=Out(List[Dict[str, Any]]),
    retry_policy=RetryPolicy(max_retries=2, delay=5),
)
def transform_orders(context, orders: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    Applies business logic and schema transformations to the extracted orders.
    """
    context.log.info(f"Transforming {len(orders)} orders.")
    transformed = []
    for order in orders:
        transformed_order = {
            "order_id": order["order_id"],
            "order_amount_usd": round(order["amount"], 2),
            "order_timestamp": order["order_date"].isoformat(),
        }
        transformed.append(transformed_order)
    context.log.info("Transformation complete.")
    return transformed


@op(
    name="load_orders",
    description="Load transformed orders into the data warehouse.",
    ins={"orders": In(List[Dict[str, Any]])},
    out=Out(None),
    retry_policy=RetryPolicy(max_retries=2, delay=5),
)
def load_orders(context, orders: List[Dict[str, Any]]) -> None:
    """
    Persists the transformed orders into the target warehouse.
    """
    context.log.info(f"Loading {len(orders)} orders into the warehouse.")
    # Placeholder load logic – replace with actual warehouse write code
    for order in orders:
        context.log.debug(f"Loaded order: {order}")
    context.log.info("All orders have been loaded successfully.")


# ----------------------------------------------------------------------
# Job
# ----------------------------------------------------------------------
@job(
    name="database_partition_check_etl",
    description=(
        "Sensor-gated daily ETL pipeline that waits for database partition availability "
        "before extracting, transforming, and loading incremental orders data."
    ),
    executor_def=in_process_executor,
    resource_defs={
        "database_conn": database_conn_resource,
        "io_manager": fs_io_manager,
    },
)
def database_partition_check_etl():
    """
    Orchestrates the ETL steps:
    1. Wait for the daily partition.
    2. Extract incremental orders.
    3. Transform orders data.
    4. Load orders into the warehouse.
    """
    partition_ready = wait_partition()
    extracted = extract_incremental(partition_ready)
    transformed = transform_orders(extracted)
    load_orders(transformed)


# ----------------------------------------------------------------------
# Schedule
# ----------------------------------------------------------------------
daily_etl_schedule = ScheduleDefinition(
    job=database_partition_check_etl,
    cron_schedule="0 0 * * *",  # Runs daily at 00:00 UTC
    execution_timezone="UTC",
    default_status=DefaultScheduleStatus.RUNNING,
    description="Daily schedule for the Database Partition Check ETL job.",
)

# Export symbols for Dagster discovery
__all__ = [
    "database_partition_check_etl",
    "daily_etl_schedule",
    "database_conn_resource",
]