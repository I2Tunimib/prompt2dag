# Generated by Dagster Pipeline Generator
# Timestamp: 2024-06-28 12:00:00 UTC
# Pipeline: wait_partition_pipeline
# Description: No description provided.

from dagster import (
    op,
    job,
    In,
    Out,
    RetryPolicy,
    ResourceDefinition,
    fs_io_manager,
    ConfigurableResource,
    DefaultScheduleStatus,
    ScheduleDefinition,
    Definitions,
    get_dagster_logger,
)
from dagster import in_process_executor
from datetime import datetime, timedelta
import time


class DatabaseConnection(ConfigurableResource):
    """Placeholder resource for a source/target database connection."""

    def connect(self):
        # Implement actual connection logic here
        get_dagster_logger().info("Connecting to the database...")
        return self

    def query(self, sql: str):
        get_dagster_logger().info(f"Executing query: {sql}")
        # Return mock data
        return [{"order_id": 1, "amount": 100}]

    def insert(self, table: str, rows):
        get_dagster_logger().info(f"Inserting {len(rows)} rows into {table}")


@op(
    name="wait_partition",
    description="Wait for the daily orders partition to become available.",
    out=Out(str),
    retry_policy=RetryPolicy(max_retries=2),
)
def wait_partition(_: None) -> str:
    """Polls for the existence of the daily orders partition."""
    logger = get_dagster_logger()
    logger.info("Starting partition wait...")
    # Placeholder wait logic
    time.sleep(2)  # Simulate waiting
    partition_date = (datetime.utcnow() - timedelta(days=1)).strftime("%Y-%m-%d")
    logger.info(f"Partition for {partition_date} is now available.")
    return partition_date


@op(
    name="extract_incremental",
    description="Extract incremental orders from the source database.",
    ins={"partition": In(str)},
    out=Out(list),
    retry_policy=RetryPolicy(max_retries=2),
    required_resource_keys={"database_conn"},
)
def extract_incremental(context, partition: str) -> list:
    """Runs a query to fetch orders for the given partition."""
    logger = context.log
    db: DatabaseConnection = context.resources.database_conn
    db.connect()
    sql = f"SELECT * FROM orders WHERE order_date = '{partition}'"
    rows = db.query(sql)
    logger.info(f"Extracted {len(rows)} rows for partition {partition}.")
    return rows


@op(
    name="transform",
    description="Transform raw orders data into the target schema.",
    ins={"orders": In(list)},
    out=Out(list),
    retry_policy=RetryPolicy(max_retries=2),
)
def transform(_: None, orders: list) -> list:
    """Applies business logic transformations to the extracted orders."""
    logger = get_dagster_logger()
    logger.info("Starting transformation of orders data...")
    transformed = []
    for order in orders:
        # Example transformation: add a processed timestamp
        transformed.append(
            {
                **order,
                "processed_at": datetime.utcnow().isoformat(),
                "status": "processed",
            }
        )
    logger.info(f"Transformed {len(transformed)} orders.")
    return transformed


@op(
    name="load",
    description="Load transformed orders into the data warehouse.",
    ins={"orders": In(list)},
    out=Out(None),
    retry_policy=RetryPolicy(max_retries=2),
    required_resource_keys={"database_conn"},
)
def load(context, orders: list) -> None:
    """Inserts the transformed orders into the target warehouse table."""
    logger = context.log
    db: DatabaseConnection = context.resources.database_conn
    db.connect()
    target_table = "dw_orders"
    db.insert(target_table, orders)
    logger.info(f"Loaded {len(orders)} orders into {target_table}.")


@job(
    name="wait_partition_pipeline",
    description="No description provided.",
    executor_def=in_process_executor,
    resource_defs={
        "database_conn": DatabaseConnection(),
        "io_manager": fs_io_manager,
    },
)
def wait_partition_pipeline():
    """Sequential pipeline that waits for a partition, extracts, transforms, and loads orders."""
    partition = wait_partition()
    extracted = extract_incremental(partition)
    transformed = transform(extracted)
    load(transformed)


# Schedule (disabled by default)
daily_schedule = ScheduleDefinition(
    job=wait_partition_pipeline,
    cron_schedule="@daily",
    execution_timezone="UTC",
    default_status=DefaultScheduleStatus.INACTIVE,
    description="Daily execution of wait_partition_pipeline.",
)


# Assemble definitions for Dagster UI / CLI
defs = Definitions(
    jobs=[wait_partition_pipeline],
    schedules=[daily_schedule],
    resources={"database_conn": DatabaseConnection()},
    sensors=[],
    assets=[],
)