# Generated by Prefect Pipeline Generator
# Date: 2024-06-28
# Pipeline: wait_partition_pipeline
# Description: Database Partition Check ETL
# Prefect version: 2.14.0

from __future__ import annotations

import os
from typing import Any, Dict, List

import pandas as pd
from sqlalchemy import create_engine, text
from sqlalchemy.engine import Engine

from prefect import flow, task, get_run_logger
from prefect.task_runners import SequentialTaskRunner
from prefect.blocks.system import Secret

# -------------------------------------------------------------------------
# Helper utilities
# -------------------------------------------------------------------------

def _get_db_engine(secret_name: str) -> Engine:
    """
    Retrieve a SQLAlchemy engine using a Prefect Secret block.

    Args:
        secret_name: Name of the Prefect Secret block that stores the DB URL.

    Returns:
        SQLAlchemy Engine instance.
    """
    logger = get_run_logger()
    try:
        secret_block = Secret.load(secret_name)  # type: ignore[attr-defined]
        db_url = secret_block.get()
        logger.info("Loaded database URL from secret block '%s'.", secret_name)
        return create_engine(db_url)
    except Exception as exc:
        logger.error("Failed to load database secret '%s': %s", secret_name, exc)
        raise


# -------------------------------------------------------------------------
# Tasks
# -------------------------------------------------------------------------

@task(retries=2, retry_delay_seconds=30)
def wait_partition(db_engine: Engine, partition_date: str) -> None:
    """
    Wait until the daily partition for ``partition_date`` exists in the source table.

    The task polls the database every 30 seconds and raises an exception if the
    partition does not appear after a configurable timeout.

    Args:
        db_engine: SQLAlchemy engine connected to the source database.
        partition_date: Date string (YYYY-MM-DD) representing the expected partition.
    """
    logger = get_run_logger()
    query = text(
        """
        SELECT EXISTS (
            SELECT 1
            FROM information_schema.tables
            WHERE table_name = :partition_name
        ) AS partition_exists;
        """
    )
    partition_name = f"orders_{partition_date.replace('-', '')}"
    timeout_seconds = 600  # 10 minutes
    poll_interval = 30
    elapsed = 0

    logger.info("Waiting for partition '%s' to become available.", partition_name)

    while elapsed < timeout_seconds:
        with db_engine.connect() as conn:
            result = conn.execute(query, {"partition_name": partition_name}).fetchone()
            if result and result["partition_exists"]:
                logger.info("Partition '%s' is now available.", partition_name)
                return
        logger.debug(
            "Partition '%s' not yet available; sleeping %s seconds.", partition_name, poll_interval
        )
        import time

        time.sleep(poll_interval)
        elapsed += poll_interval

    raise TimeoutError(f"Partition '{partition_name}' did not appear within {timeout_seconds}s.")


@task(retries=2, retry_delay_seconds=30)
def extract_incremental(
    db_engine: Engine, partition_date: str
) -> pd.DataFrame:
    """
    Extract incremental order records for the given partition date.

    Args:
        db_engine: SQLAlchemy engine connected to the source database.
        partition_date: Date string (YYYY-MM-DD) for which to extract orders.

    Returns:
        DataFrame containing the extracted orders.
    """
    logger = get_run_logger()
    partition_name = f"orders_{partition_date.replace('-', '')}"
    sql = text(f"SELECT * FROM {partition_name};")
    logger.info("Extracting incremental orders from partition '%s'.", partition_name)

    with db_engine.connect() as conn:
        df = pd.read_sql(sql, conn)

    logger.info("Extracted %d rows from partition '%s'.", len(df), partition_name)
    return df


@task(retries=2, retry_delay_seconds=30)
def transform_orders(raw_orders: pd.DataFrame) -> pd.DataFrame:
    """
    Apply business transformations to the raw orders DataFrame.

    Transformations include:
    * Converting timestamps to UTC.
    * Calculating order totals.
    * Normalising column names.

    Args:
        raw_orders: DataFrame with raw order data.

    Returns:
        Transformed DataFrame ready for loading.
    """
    logger = get_run_logger()
    logger.info("Starting transformation of %d rows.", len(raw_orders))

    df = raw_orders.copy()

    # Example transformation: ensure datetime columns are timezone-aware UTC
    if "order_timestamp" in df.columns:
        df["order_timestamp"] = pd.to_datetime(df["order_timestamp"], utc=True)

    # Example: calculate total price if not present
    if {"quantity", "unit_price"}.issubset(df.columns) and "total_price" not in df.columns:
        df["total_price"] = df["quantity"] * df["unit_price"]

    # Normalise column names to snake_case
    df.columns = [col.lower().replace(" ", "_") for col in df.columns]

    logger.info("Transformation complete. Resulting DataFrame has %d columns.", len(df.columns))
    return df


@task(retries=2, retry_delay_seconds=30)
def load_orders(
    transformed_orders: pd.DataFrame,
    warehouse_conn_secret: str = "warehouse_db_secret",
) -> None:
    """
    Load the transformed orders into the data warehouse.

    Args:
        transformed_orders: DataFrame containing transformed order data.
        warehouse_conn_secret: Name of the Prefect Secret block holding the warehouse DB URL.
    """
    logger = get_run_logger()
    logger.info("Loading %d rows into the data warehouse.", len(transformed_orders))

    warehouse_engine = _get_db_engine(warehouse_conn_secret)

    # Assuming a target table named `dim_orders` exists in the warehouse
    target_table = "dim_orders"

    with warehouse_engine.begin() as conn:
        transformed_orders.to_sql(
            name=target_table,
            con=conn,
            if_exists="append",
            index=False,
            method="multi",
        )

    logger.info("Successfully loaded data into warehouse table '%s'.", target_table)


# -------------------------------------------------------------------------
# Flow definition
# -------------------------------------------------------------------------

@flow(
    name="wait_partition_pipeline",
    task_runner=SequentialTaskRunner(),
)
def wait_partition_pipeline(
    partition_date: str,
    db_secret_name: str = "primary_database_secret",
    warehouse_secret_name: str = "warehouse_db_secret",
) -> None:
    """
    Orchestrates the ETL pipeline that waits for a daily partition,
    extracts incremental orders, transforms them, and loads them into a warehouse.

    The flow runs tasks sequentially as defined by the pipeline specification.

    Args:
        partition_date: Date string (YYYY-MM-DD) for which the partition is expected.
        db_secret_name: Prefect Secret block name for the source database connection.
        warehouse_secret_name: Prefect Secret block name for the warehouse connection.
    """
    logger = get_run_logger()
    logger.info("Starting wait_partition_pipeline for date %s.", partition_date)

    # Create a DB engine for the source database
    source_engine = _get_db_engine(db_secret_name)

    # Step 1: Wait for the daily partition to be ready
    wait_partition(source_engine, partition_date)

    # Step 2: Extract incremental orders
    raw_orders = extract_incremental(source_engine, partition_date)

    # Step 3: Transform the extracted data
    transformed = transform_orders(raw_orders)

    # Step 4: Load transformed data into the warehouse
    load_orders(transformed, warehouse_conn_secret=warehouse_secret_name)

    logger.info("wait_partition_pipeline completed successfully.")


# -------------------------------------------------------------------------
# Entry point for local execution
# -------------------------------------------------------------------------

if __name__ == "__main__":
    # Example execution; in production the flow will be triggered via a Prefect deployment.
    example_date = os.getenv("PARTITION_DATE", "2024-06-27")
    wait_partition_pipeline(partition_date=example_date)