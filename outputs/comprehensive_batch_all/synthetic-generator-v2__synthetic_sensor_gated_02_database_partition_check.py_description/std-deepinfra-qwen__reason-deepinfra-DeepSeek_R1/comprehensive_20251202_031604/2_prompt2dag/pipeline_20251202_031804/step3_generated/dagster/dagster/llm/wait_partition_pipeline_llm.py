# Generated by Dagster Code Generator
# Metadata:
# - Job Name: wait_partition_pipeline
# - Description: This is a sensor-gated daily ETL pipeline that waits for database partition availability before extracting, transforming, and loading incremental orders data.
# - Executor Type: in_process_executor
# - IO Manager: fs_io_manager
# - Dagster Version: 1.5.0
# - Use Assets: False
# - Required Resources: database_conn, data_warehouse_conn

from dagster import (
    job,
    op,
    Out,
    In,
    RetryPolicy,
    resource,
    in_process_executor,
    fs_io_manager,
    ScheduleDefinition,
)

@resource
def database_conn():
    """Resource for database connection."""
    # Implement the actual database connection logic here
    pass

@resource
def data_warehouse_conn():
    """Resource for data warehouse connection."""
    # Implement the actual data warehouse connection logic here
    pass

@op(
    name="wait_partition",
    description="Wait for database partition availability.",
    retry_policy=RetryPolicy(max_retries=0),
    required_resource_keys={"database_conn"},
)
def wait_partition(context):
    """Wait for the database partition to be available."""
    # Implement the logic to wait for the partition
    context.log.info("Waiting for partition availability...")
    # Example: context.resources.database_conn.wait_for_partition()
    return "Partition is available"

@op(
    name="extract_incremental",
    description="Extract incremental orders data.",
    retry_policy=RetryPolicy(max_retries=2),
    required_resource_keys={"database_conn"},
    ins={"partition_status": In(str)},
    out=Out(str),
)
def extract_incremental(context, partition_status):
    """Extract incremental orders data from the database."""
    context.log.info("Extracting incremental orders data...")
    # Example: orders = context.resources.database_conn.extract_incremental_orders()
    return "Incremental orders data extracted"

@op(
    name="transform",
    description="Transform orders data.",
    retry_policy=RetryPolicy(max_retries=2),
    ins={"orders_data": In(str)},
    out=Out(str),
)
def transform(context, orders_data):
    """Transform the extracted orders data."""
    context.log.info("Transforming orders data...")
    # Example: transformed_data = transform_orders(orders_data)
    return "Transformed orders data"

@op(
    name="load",
    description="Load orders data into the data warehouse.",
    retry_policy=RetryPolicy(max_retries=2),
    required_resource_keys={"data_warehouse_conn"},
    ins={"transformed_data": In(str)},
)
def load(context, transformed_data):
    """Load the transformed orders data into the data warehouse."""
    context.log.info("Loading orders data into the data warehouse...")
    # Example: context.resources.data_warehouse_conn.load_orders(transformed_data)
    return "Orders data loaded successfully"

@job(
    name="wait_partition_pipeline",
    description="This is a sensor-gated daily ETL pipeline that waits for database partition availability before extracting, transforming, and loading incremental orders data.",
    executor_def=in_process_executor,
    resource_defs={
        "database_conn": database_conn,
        "data_warehouse_conn": data_warehouse_conn,
        "io_manager": fs_io_manager,
    },
)
def wait_partition_pipeline():
    """Define the wait_partition_pipeline job."""
    load(transform(extract_incremental(wait_partition())))

# Define the schedule
wait_partition_pipeline_schedule = ScheduleDefinition(
    job=wait_partition_pipeline,
    cron_schedule="@daily",
    execution_timezone="UTC",
    name="wait_partition_pipeline_schedule",
    description="Daily schedule for the wait_partition_pipeline.",
    should_execute=lambda _context: True,
    tags={"dagster/priority": "1"},
    execution_time_window=None,
    execution_timezone="UTC",
    default_status=ScheduleDefinition.DEFAULT_STATUS_RUNNING,
    job_name="wait_partition_pipeline",
    run_config=None,
    should_execute=None,
    tags_fn=None,
    execution_timezone="UTC",
    execution_time_window=None,
    job=None,
    cron_schedule="@daily",
    description="Daily schedule for the wait_partition_pipeline.",
    name="wait_partition_pipeline_schedule",
    default_status=ScheduleDefinition.DEFAULT_STATUS_RUNNING,
)