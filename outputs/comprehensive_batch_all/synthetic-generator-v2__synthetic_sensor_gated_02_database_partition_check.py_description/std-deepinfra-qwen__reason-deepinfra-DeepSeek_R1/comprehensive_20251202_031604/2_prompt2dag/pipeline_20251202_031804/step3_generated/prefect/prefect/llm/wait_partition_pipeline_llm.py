# Generated by Prefect 2.x Code Generator
# Generation Metadata:
# - Name: wait_partition_pipeline
# - Description: This is a sensor-gated daily ETL pipeline that waits for database partition availability before extracting, transforming, and loading incremental orders data.
# - Pattern: sequential
# - Schedule: @daily
# - Timezone: UTC
# - Catchup: False
# - Prefect Version: 2.14.0

from prefect import flow, task, get_run_logger
from prefect.orion.schemas.schedules import CronSchedule
from prefect.deployments import DeploymentSpec
from prefect.infrastructure.docker import DockerContainer
from prefect.task_runners import SequentialTaskRunner
from prefect.blocks.system import Secret

# Load secrets
database_conn = Secret.load("database_conn")
data_warehouse_conn = Secret.load("data_warehouse_conn")

@task(retries=0, name="Wait for Partition")
def wait_partition():
    """Waits for the database partition to be available."""
    logger = get_run_logger()
    logger.info("Waiting for partition availability...")
    # Simulate partition check
    # Replace with actual partition check logic
    partition_available = True
    if partition_available:
        logger.info("Partition is available.")
    else:
        logger.warning("Partition is not available.")
    return partition_available

@task(retries=2, name="Extract Incremental Orders")
def extract_incremental():
    """Extracts incremental orders data from the database."""
    logger = get_run_logger()
    logger.info("Extracting incremental orders data...")
    # Simulate data extraction
    # Replace with actual data extraction logic
    orders_data = ["order1", "order2", "order3"]
    logger.info(f"Extracted {len(orders_data)} orders.")
    return orders_data

@task(retries=2, name="Transform Orders Data")
def transform(orders_data):
    """Transforms the extracted orders data."""
    logger = get_run_logger()
    logger.info("Transforming orders data...")
    # Simulate data transformation
    # Replace with actual data transformation logic
    transformed_data = [f"transformed_{order}" for order in orders_data]
    logger.info(f"Transformed {len(transformed_data)} orders.")
    return transformed_data

@task(retries=2, name="Load Orders Data")
def load(transformed_data):
    """Loads the transformed orders data into the data warehouse."""
    logger = get_run_logger()
    logger.info("Loading orders data into the data warehouse...")
    # Simulate data loading
    # Replace with actual data loading logic
    for order in transformed_data:
        logger.info(f"Loaded order: {order}")
    logger.info("Data loading complete.")

@flow(
    name="wait_partition_pipeline",
    schedule=CronSchedule(cron="0 0 * * *", timezone="UTC", catchup=False),
    task_runner=SequentialTaskRunner()
)
def wait_partition_pipeline():
    """Main pipeline flow for waiting for partition, extracting, transforming, and loading incremental orders data."""
    partition_available = wait_partition()
    if partition_available:
        orders_data = extract_incremental()
        transformed_data = transform(orders_data)
        load(transformed_data)
    else:
        logger = get_run_logger()
        logger.warning("Partition not available, skipping ETL process.")

# Deployment configuration
DeploymentSpec(
    flow=wait_partition_pipeline,
    name="wait_partition_pipeline_deployment",
    work_pool_name="default-agent-pool",
    parameters={},
    tags=["etl", "daily"],
    skip_upload=True
)