# Generated by Prefect 2.x Code Generator
# Generation Metadata:
# - Name: wait_partition_pipeline
# - Description: Comprehensive Pipeline Description: This is a sensor-gated daily ETL pipeline that waits for database partition availability before extracting, transforming, and loading incremental orders data.
# - Pattern: sequential
# - Schedule Configuration: Enabled: True, Expression: @daily, Timezone: UTC, Catchup: False
# - Connections/Resources: database_conn, data_warehouse_conn
# - Orchestrator-Specific Configuration: flow_name: wait_partition_pipeline, deployment_name: wait_partition_pipeline_deployment, work_pool: default-agent-pool, task_runner: SequentialTaskRunner, prefect_version: 2.14.0

from prefect import flow, task, get_run_logger
from prefect.tasks import task_input_hash
from prefect.orion.schemas.states import Retrying
from prefect.deployments import Deployment
from prefect.infrastructure.docker import DockerContainer
from prefect.blocks.system import Secret
from prefect.task_runners import SequentialTaskRunner
from datetime import datetime, timedelta
import time

# Load secrets
database_conn = Secret.load("database_conn")
data_warehouse_conn = Secret.load("data_warehouse_conn")

@task(retries=0, retry_delay_seconds=10)
def wait_partition():
    """Wait for the database partition to be available."""
    logger = get_run_logger()
    logger.info("Waiting for partition availability...")
    # Simulate partition check
    time.sleep(10)
    logger.info("Partition is available.")

@task(retries=2, retry_delay_seconds=10)
def extract_incremental():
    """Extract incremental orders data from the database."""
    logger = get_run_logger()
    logger.info("Extracting incremental orders data...")
    # Simulate data extraction
    time.sleep(5)
    logger.info("Incremental orders data extracted.")
    return "extracted_data"

@task(retries=2, retry_delay_seconds=10)
def transform(data):
    """Transform the extracted orders data."""
    logger = get_run_logger()
    logger.info("Transforming orders data...")
    # Simulate data transformation
    time.sleep(5)
    logger.info("Orders data transformed.")
    return "transformed_data"

@task(retries=2, retry_delay_seconds=10)
def load(data):
    """Load the transformed orders data into the data warehouse."""
    logger = get_run_logger()
    logger.info("Loading orders data into the data warehouse...")
    # Simulate data loading
    time.sleep(5)
    logger.info("Orders data loaded.")

@flow(name="wait_partition_pipeline", task_runner=SequentialTaskRunner, schedule="0 0 * * *", timezone="UTC", catchup=False)
def wait_partition_pipeline():
    """Comprehensive Pipeline Description: This is a sensor-gated daily ETL pipeline that waits for database partition availability before extracting, transforming, and loading incremental orders data."""
    wait_partition()
    extracted_data = extract_incremental()
    transformed_data = transform(extracted_data)
    load(transformed_data)

if __name__ == "__main__":
    deployment = Deployment.build_from_flow(
        flow=wait_partition_pipeline,
        name="wait_partition_pipeline_deployment",
        work_pool_name="default-agent-pool",
        version="1.0",
        parameters={},
        tags=["etl", "daily"],
    )
    deployment.apply()