# Generated by Prefect 2.x Code Generator
# Pipeline Name: wait_partition_pipeline
# Description: Comprehensive Pipeline Description for Database Partition Check ETL
# Pattern: sequential
# Schedule Configuration: @daily, UTC, catchup=False
# Prefect Version: 2.14.0

from prefect import flow, task, get_run_logger
from prefect.orion.schemas.schedules import CronSchedule
from prefect.deployments import DeploymentSpec
from prefect.infrastructure.docker import DockerContainer
from prefect.task_runners import SequentialTaskRunner
from prefect.blocks.system import Secret

# Load secrets
database_conn = Secret.load("database_conn")
data_warehouse_conn = Secret.load("data_warehouse_conn")

@task(retries=0, name="Wait for Partition")
def wait_partition():
    """
    Task to wait for the partition to be available in the database.
    """
    logger = get_run_logger()
    logger.info("Waiting for partition to be available...")
    # Simulate waiting for partition
    # Replace with actual logic to check partition availability
    logger.info("Partition is available.")

@task(retries=2, name="Extract Incremental Orders")
def extract_incremental():
    """
    Task to extract incremental orders from the database.
    """
    logger = get_run_logger()
    logger.info("Extracting incremental orders...")
    # Simulate data extraction
    # Replace with actual logic to extract incremental orders
    logger.info("Incremental orders extracted successfully.")

@task(retries=2, name="Transform Orders Data")
def transform():
    """
    Task to transform the extracted orders data.
    """
    logger = get_run_logger()
    logger.info("Transforming orders data...")
    # Simulate data transformation
    # Replace with actual logic to transform orders data
    logger.info("Orders data transformed successfully.")

@task(retries=2, name="Load Orders Data")
def load():
    """
    Task to load the transformed orders data into the data warehouse.
    """
    logger = get_run_logger()
    logger.info("Loading orders data into the data warehouse...")
    # Simulate data loading
    # Replace with actual logic to load orders data
    logger.info("Orders data loaded successfully.")

@flow(name="wait_partition_pipeline", task_runner=SequentialTaskRunner(), schedule=CronSchedule(cron="0 0 * * *", timezone="UTC", day_or=True))
def wait_partition_pipeline():
    """
    Comprehensive Pipeline Description for Database Partition Check ETL.
    """
    wait_partition()
    extract_incremental()
    transform()
    load()

# Deployment configuration
DeploymentSpec(
    name="wait_partition_pipeline_deployment",
    flow=wait_partition_pipeline,
    work_pool_name="default-agent-pool",
    parameters={},
    tags=["etl", "database", "data-warehouse"],
    description="Deployment for the wait_partition_pipeline",
)