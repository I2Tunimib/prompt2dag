# Generated by Prefect Pipeline Generator
# Pipeline: analyze_transactions_pipeline
# Description: Comprehensive Pipeline Description
# Pattern: fanout_fanin
# Prefect version: 2.14.0
# Orchestrator configuration:
# {
#   "flow_name": "analyze_transactions_pipeline",
#   "deployment_name": "analyze_transactions_pipeline_deployment",
#   "work_pool": "default-agent-pool",
#   "task_runner": "ConcurrentTaskRunner",
#   "prefect_version": "2.14.0"
# }

from __future__ import annotations

import json
from pathlib import Path
from typing import List, Dict, Any

import pandas as pd
from prefect import flow, task, get_run_logger
from prefect.task_runners import ConcurrentTaskRunner
from prefect.deployments import DeploymentSpec
from prefect.server.schemas.schedules import CronSchedule
from prefect.blocks.system import Secret
from prefect.filesystems import LocalFileSystem


@task(retries=2, retry_delay_seconds=30, name="Analyze Transactions")
def analyze_transactions(csv_block: LocalFileSystem) -> List[Dict[str, Any]]:
    """
    Load daily transaction CSV files from a LocalFileSystem block,
    perform basic analysis, and return a list of transaction records
    enriched with a risk score.

    Returns:
        List[Dict[str, Any]]: List of transaction dictionaries with a
        ``risk_score`` field (0 = low risk, 1 = high risk).
    """
    logger = get_run_logger()
    logger.info("Fetching CSV files from LocalFileSystem block %s", csv_block._block_document_id)

    # Assume the block points to a directory containing a single CSV file for the day.
    # In a real implementation you might iterate over multiple files.
    csv_path = Path(csv_block.base_path) / "transactions.csv"
    if not csv_path.is_file():
        raise FileNotFoundError(f"Transaction CSV not found at {csv_path}")

    df = pd.read_csv(csv_path)
    logger.debug("Loaded %d rows from %s", len(df), csv_path)

    # Simple risk scoring: flag transactions > $10,000 as high risk
    df["risk_score"] = (df["amount"] > 10_000).astype(int)

    records = df.to_dict(orient="records")
    logger.info("Analyzed %d transactions, %d high‑risk", len(records), sum(r["risk_score"] for r in records))
    return records


@task(retries=2, retry_delay_seconds=30, name="Route Transaction")
def route_transaction(
    transactions: List[Dict[str, Any]],
) -> Dict[str, List[Dict[str, Any]]]:
    """
    Split transactions into two streams based on risk score.

    Returns:
        Dict[str, List[Dict[str, Any]]]: Mapping with keys
        ``auto_approve`` and ``manual_review``.
    """
    logger = get_run_logger()
    auto_approve = [t for t in transactions if t["risk_score"] == 0]
    manual_review = [t for t in transactions if t["risk_score"] == 1]

    logger.info(
        "Routing %d transactions: %d auto‑approve, %d manual review",
        len(transactions),
        len(auto_approve),
        len(manual_review),
    )
    return {"auto_approve": auto_approve, "manual_review": manual_review}


@task(retries=2, retry_delay_seconds=30, name="Route to Auto Approve")
def route_to_auto_approve(
    auto_transactions: List[Dict[str, Any]],
    payment_api_secret: Secret,
) -> List[Dict[str, Any]]:
    """
    Process auto‑approved transactions via the payment processing API.

    Returns:
        List[Dict[str, Any]]: List of processed transaction responses.
    """
    logger = get_run_logger()
    api_key = payment_api_secret.get()
    logger.debug("Using payment processing API key: %s", api_key[:4] + "****")

    # Placeholder for API call – in production replace with real HTTP request.
    processed = []
    for tx in auto_transactions:
        response = {"transaction_id": tx["id"], "status": "approved"}
        processed.append(response)

    logger.info("Auto‑approved %d transactions", len(processed))
    return processed


@task(retries=2, retry_delay_seconds=30, name="Route to Manual Review")
def route_to_manual_review(
    manual_transactions: List[Dict[str, Any]],
    review_queue_secret: Secret,
) -> List[Dict[str, Any]]:
    """
    Enqueue high‑risk transactions for manual review.

    Returns:
        List[Dict[str, Any]]: List of enqueued transaction identifiers.
    """
    logger = get_run_logger()
    queue_name = review_queue_secret.get()
    logger.debug("Enqueuing to manual review queue: %s", queue_name)

    # Placeholder for queue interaction – replace with real queue client.
    enqueued = []
    for tx in manual_transactions:
        enqueued.append({"transaction_id": tx["id"], "queue": queue_name})

    logger.info("Enqueued %d transactions for manual review", len(enqueued))
    return enqueued


@task(retries=2, retry_delay_seconds=30, name="Send Notification")
def send_notification(
    auto_results: List[Dict[str, Any]],
    manual_results: List[Dict[str, Any]],
    notification_email_secret: Secret,
) -> None:
    """
    Send a summary notification email after processing both branches.

    Args:
        auto_results: Results from auto‑approve branch.
        manual_results: Results from manual‑review branch.
        notification_email_secret: Secret containing the destination email address.
    """
    logger = get_run_logger()
    email = notification_email_secret.get()
    logger.debug("Preparing notification for %s", email)

    summary = {
        "auto_approved": len(auto_results),
        "manual_review_enqueued": len(manual_results),
    }
    message = f"Fraud detection run completed:\n{json.dumps(summary, indent=2)}"

    # Placeholder for email sending – replace with actual email service.
    logger.info("Sending notification email to %s:\n%s", email, message)


@flow(
    name="analyze_transactions_pipeline",
    task_runner=ConcurrentTaskRunner(),
)
def analyze_transactions_pipeline():
    """
    Prefect flow that ingests daily transaction CSV files, analyses risk,
    routes transactions to auto‑approval or manual review, and finally
    sends a notification summarising the run.
    """
    logger = get_run_logger()

    # Load infrastructure blocks
    csv_block = LocalFileSystem.load("transaction_csv_files")
    review_queue_secret = Secret.load("manual_review_queue")
    payment_api_secret = Secret.load("payment_processing_api")
    notification_email_secret = Secret.load("notification_email")

    # Step 1: Analyze
    transactions = analyze_transactions(csv_block)

    # Step 2: Route based on risk
    routed = route_transaction(transactions)

    # Fan‑out branches
    auto_results = route_to_auto_approve(
        routed["auto_approve"], payment_api_secret
    )
    manual_results = route_to_manual_review(
        routed["manual_review"], review_queue_secret
    )

    # Fan‑in: send notification after both branches complete
    send_notification(auto_results, manual_results, notification_email_secret)

    logger.info("Pipeline execution completed.")


# Deployment specification with daily schedule
DeploymentSpec(
    name="analyze_transactions_pipeline_deployment",
    flow=analyze_transactions_pipeline,
    schedule=CronSchedule(cron="0 0 * * *", timezone="UTC"),  # @daily at midnight UTC
    tags=["daily", "transaction-analysis"],
    work_pool_name="default-agent-pool",
    enforce_parameter_schema=False,
)