# Generated by Airflow DAG generator on 2024-06-28
"""
DAG: start_backup_process_pipeline
Description: No description provided.
Pattern: fanout_fanin
"""

from datetime import datetime, timedelta
import logging

from airflow import DAG
from airflow.decorators import task
from airflow.operators.bash import BashOperator
from airflow.utils.trigger_rule import TriggerRule

# Default arguments applied to all tasks
default_args = {
    "owner": "airflow",
    "retries": 2,
    "retry_delay": timedelta(minutes=5),
    "email_on_failure": False,
    "email_on_retry": False,
}


def _on_failure_callback(context):
    """Simple failure callback that logs the exception."""
    dag_id = context.get("dag_id")
    task_id = context.get("task_instance").task_id
    exception = context.get("exception")
    logging.error(
        "Task failed: dag=%s, task=%s, exception=%s", dag_id, task_id, exception
    )


with DAG(
    dag_id="start_backup_process_pipeline",
    description="No description provided.",
    schedule="@daily",
    start_date=datetime(2024, 1, 1),
    catchup=False,
    default_args=default_args,
    tags=["fanout_fanin"],
    is_paused_upon_creation=True,  # Disabled by default
    max_active_runs=1,
    on_failure_callback=_on_failure_callback,
    render_template_as_native_obj=True,
    timezone="UTC",
) as dag:

    @task(task_id="start_backup_process", retries=2, on_failure_callback=_on_failure_callback)
    def start_backup_process():
        """Placeholder for starting the backup process."""
        logging.info("Backup process started.")
        # Return a flag that could be used downstream if needed
        return {"started": True}

    @task(task_id="determine_backup_strategy", retries=2, on_failure_callback=_on_failure_callback)
    def determine_backup_strategy(start_info: dict):
        """Determine whether to run a full or incremental backup."""
        logging.info("Determining backup strategy based on start info: %s", start_info)
        # Simple logic placeholder: always run both for demonstration
        return {"run_full": True, "run_incremental": True}

    # Bash tasks for actual backup steps
    full_backup = BashOperator(
        task_id="full_backup",
        bash_command="echo 'Running full backup...'; sleep 10; echo 'Full backup completed.'",
        retries=2,
        on_failure_callback=_on_failure_callback,
    )

    incremental_backup = BashOperator(
        task_id="incremental_backup",
        bash_command="echo 'Running incremental backup...'; sleep 5; echo 'Incremental backup completed.'",
        retries=2,
        on_failure_callback=_on_failure_callback,
    )

    verify_backup = BashOperator(
        task_id="verify_backup",
        bash_command="echo 'Verifying backup integrity...'; sleep 3; echo 'Verification successful.'",
        retries=2,
        on_failure_callback=_on_failure_callback,
        # Ensure this runs even if one of the upstream backups is skipped
        trigger_rule=TriggerRule.ALL_SUCCESS,
    )

    @task(task_id="backup_complete", retries=2, on_failure_callback=_on_failure_callback)
    def backup_complete():
        """Final step indicating backup workflow completion."""
        logging.info("Backup workflow completed successfully.")
        return "SUCCESS"

    # -------------------------------------------------------------------------
    # Task dependencies (fanout -> fanin)
    # -------------------------------------------------------------------------
    start_info = start_backup_process()
    strategy = determine_backup_strategy(start_info)

    # Fan‑out: both backup types depend on the strategy decision
    strategy >> [full_backup, incremental_backup]

    # Fan‑in: verification runs after both backup tasks finish
    [full_backup, incremental_backup] >> verify_backup

    # Final task
    verify_backup >> backup_complete()