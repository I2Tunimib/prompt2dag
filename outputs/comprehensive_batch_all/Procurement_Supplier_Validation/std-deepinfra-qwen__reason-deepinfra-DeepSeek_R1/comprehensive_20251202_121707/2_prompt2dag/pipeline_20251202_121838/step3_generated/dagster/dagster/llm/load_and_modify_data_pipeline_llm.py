# Generated by Dagster Code Generator
# Generation Metadata:
# - Job Name: load_and_modify_data_pipeline
# - Description: No description provided.
# - Executor Type: docker_executor
# - IO Manager: fs_io_manager
# - Dagster Version: 1.5.0
# - Use Assets: False
# - Required Resources: app_network

from dagster import job, op, Out, In, RetryPolicy, fs_io_manager, Field, String, resource, execute_job, docker_executor

# Resources
@resource(config_schema={"data_dir": Field(String)})
def data_dir(init_context):
    return init_context.resource_config["data_dir"]

@resource
def load_and_modify_service():
    pass

@resource
def reconciliation_service():
    pass

@resource
def save_service():
    pass

@resource
def mongodb():
    pass

@resource
def intertwino_api():
    pass

# Ops
@op(
    required_resource_keys={"data_dir", "load_and_modify_service"},
    out={"modified_data": Out()},
    retry_policy=RetryPolicy(max_retries=1),
)
def load_and_modify_data(context):
    """
    Load and modify data using the load_and_modify_service.
    """
    data_dir = context.resources.data_dir
    # Simulate data loading and modification
    modified_data = "modified_data"
    context.log.info(f"Loaded and modified data from {data_dir}")
    return modified_data

@op(
    required_resource_keys={"reconciliation_service"},
    out={"reconciled_data": Out()},
    retry_policy=RetryPolicy(max_retries=1),
)
def entity_reconciliation(context, modified_data):
    """
    Perform entity reconciliation using the reconciliation_service.
    """
    # Simulate entity reconciliation
    reconciled_data = "reconciled_data"
    context.log.info(f"Reconciled data: {reconciled_data}")
    return reconciled_data

@op(
    required_resource_keys={"save_service", "data_dir"},
    retry_policy=RetryPolicy(max_retries=1),
)
def save_final_data(context, reconciled_data):
    """
    Save the final data using the save_service.
    """
    data_dir = context.resources.data_dir
    # Simulate saving final data
    context.log.info(f"Saved final data to {data_dir}")

# Job
@job(
    name="load_and_modify_data_pipeline",
    description="No description provided.",
    executor_def=docker_executor,
    resource_defs={
        "data_dir": data_dir,
        "load_and_modify_service": load_and_modify_service,
        "reconciliation_service": reconciliation_service,
        "save_service": save_service,
        "mongodb": mongodb,
        "intertwino_api": intertwino_api,
        "io_manager": fs_io_manager,
    },
)
def load_and_modify_data_pipeline():
    """
    Sequential pipeline to load, modify, reconcile, and save data.
    """
    modified_data = load_and_modify_data()
    reconciled_data = entity_reconciliation(modified_data)
    save_final_data(reconciled_data)

# Example execution
if __name__ == "__main__":
    execute_job(
        load_and_modify_data_pipeline,
        run_config={
            "resources": {
                "data_dir": {"config": {"data_dir": "/path/to/data"}},
            }
        },
    )