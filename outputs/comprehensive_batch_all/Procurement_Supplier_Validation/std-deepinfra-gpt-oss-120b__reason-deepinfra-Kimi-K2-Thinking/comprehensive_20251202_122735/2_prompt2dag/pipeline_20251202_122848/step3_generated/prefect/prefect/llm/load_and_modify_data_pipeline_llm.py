# Generated by Prefect Pipeline Generator
# Pipeline: load_and_modify_data_pipeline
# Prefect version: 2.14.0
# Task runner: SequentialTaskRunner
# Note: This flow is intended to be deployed with a Prefect deployment that disables scheduling.

from __future__ import annotations

import os
from typing import Dict

from prefect import flow, task, get_run_logger
from prefect.task_runners import SequentialTaskRunner
from prefect.blocks.system import Secret
from prefect.filesystems import LocalFileSystem
from prefect_docker import DockerContainer


def _get_secret(secret_name: str) -> str:
    """
    Retrieve the value of a Prefect Secret block.

    Args:
        secret_name: The name of the Secret block to load.

    Returns:
        The secret value as a string.

    Raises:
        ValueError: If the secret block cannot be loaded or is empty.
    """
    logger = get_run_logger()
    try:
        secret_block = Secret.load(secret_name)
        secret_value = secret_block.get()
        if not secret_value:
            raise ValueError(f"Secret '{secret_name}' is empty.")
        logger.debug(f"Loaded secret '{secret_name}'.")
        return secret_value
    except Exception as exc:
        logger.error(f"Failed to load secret '{secret_name}': {exc}")
        raise


def _get_filesystem_base_path(block_name: str) -> str:
    """
    Retrieve the base path from a LocalFileSystem block.

    Args:
        block_name: The name of the LocalFileSystem block.

    Returns:
        The base path as a string.

    Raises:
        ValueError: If the block cannot be loaded or the base path is missing.
    """
    logger = get_run_logger()
    try:
        fs_block = LocalFileSystem.load(block_name)
        base_path = fs_block.basepath
        if not base_path:
            raise ValueError(f"LocalFileSystem block '{block_name}' has no basepath.")
        logger.debug(f"Loaded filesystem base path from '{block_name}': {base_path}")
        return base_path
    except Exception as exc:
        logger.error(f"Failed to load filesystem block '{block_name}': {exc}")
        raise


def _run_docker_container(
    image: str,
    env: Dict[str, str],
    network: str | None = None,
) -> None:
    """
    Execute a Docker container using the Prefect DockerContainer task.

    Args:
        image: Docker image to run.
        env: Environment variables to pass to the container.
        network: Optional Docker network name.

    Raises:
        RuntimeError: If the container exits with a non‑zero status.
    """
    logger = get_run_logger()
    container = DockerContainer(
        image=image,
        env=env,
        network=network,
        # Pull the latest image each run to ensure freshness.
        pull=True,
    )
    logger.info(f"Starting Docker container: {image}")
    result = container.run()
    logger.info(
        f"Docker container finished. Exit code: {result.exit_code}. "
        f"Stdout: {result.stdout.strip()}"
    )
    if result.exit_code != 0:
        raise RuntimeError(
            f"Docker container for image '{image}' failed with exit code {result.exit_code}"
        )


@task(retries=1, name="Load and Modify Supplier Data")
def load_and_modify_data(data_dir: str, docker_network: str) -> None:
    """
    Run the load‑and‑modify Docker image.

    Args:
        data_dir: Path to the shared data directory.
        docker_network: Docker network to attach the container to.
    """
    env = {
        "DATASET_ID": "2",
        "TABLE_NAME_PREFIX": "JOT_",
        "DATA_DIR": data_dir,
    }
    _run_docker_container(
        image="i2t-backendwithintertwino6-load-and-modify:latest",
        env=env,
        network=docker_network,
    )


@task(retries=1, name="Entity Reconciliation (Wikidata)")
def reconcile_supplier_names(data_dir: str, docker_network: str) -> None:
    """
    Run the reconciliation Docker image.

    Args:
        data_dir: Path to the shared data directory.
        docker_network: Docker network to attach the container to.
    """
    env = {
        "PRIMARY_COLUMN": "supplier_name",
        "RECONCILIATOR_ID": "wikidataEntity",
        "DATASET_ID": "2",
        "DATA_DIR": data_dir,
    }
    _run_docker_container(
        image="i2t-backendwithintertwino6-reconciliation:latest",
        env=env,
        network=docker_network,
    )


@task(retries=1, name="Save Final Validated Supplier Data")
def save_validated_data(data_dir: str, docker_network: str) -> None:
    """
    Run the save Docker image.

    Args:
        data_dir: Path to the shared data directory.
        docker_network: Docker network to attach the container to.
    """
    env = {
        "DATASET_ID": "2",
        "DATA_DIR": data_dir,
    }
    _run_docker_container(
        image="i2t-backendwithintertwino6-save:latest",
        env=env,
        network=docker_network,
    )


@flow(
    name="load_and_modify_data_pipeline",
    task_runner=SequentialTaskRunner(),
)
def load_and_modify_data_pipeline() -> None:
    """
    Orchestrates the loading, reconciliation, and saving of supplier data.

    The flow follows a strict sequential pattern:
        1. Load and modify supplier data.
        2. Reconcile supplier names via Wikidata.
        3. Save the validated data.

    All Docker containers run on the custom Docker network defined in the
    ``docker_network_app`` secret.
    """
    logger = get_run_logger()

    # Load shared resources
    data_dir = _get_filesystem_base_path("filesystem_data_dir")
    docker_network = _get_secret("docker_network_app")

    logger.info(f"Using data directory: {data_dir}")
    logger.info(f"Using Docker network: {docker_network}")

    # Execute tasks in the required order
    load_and_modify_data(data_dir=data_dir, docker_network=docker_network)
    reconcile_supplier_names(data_dir=data_dir, docker_network=docker_network)
    save_validated_data(data_dir=data_dir, docker_network=docker_network)


if __name__ == "__main__":
    # Running the flow locally (useful for testing)
    load_and_modify_data_pipeline()