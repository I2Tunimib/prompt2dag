# Generated by Airflow DAG generator on 2024-06-28
# DAG: load_and_modify_data_pipeline
# Description: Sequential pipeline to load, reconcile, and save supplier data using Docker containers.

from datetime import datetime, timedelta

from airflow import DAG
from airflow.models import Variable
from airflow.providers.docker.operators.docker import DockerOperator
from airflow.utils.task_group import TaskGroup
from airflow.utils.email import send_email

# -------------------------------------------------------------------------
# Default arguments applied to all tasks
# -------------------------------------------------------------------------
default_args = {
    "owner": "airflow",
    "depends_on_past": False,
    "retries": 1,
    "retry_delay": timedelta(minutes=5),
    "email_on_failure": False,
    "email_on_retry": False,
}


def _failure_callback(context):
    """
    Simple failure callback that sends an email with the task failure details.
    Adjust the recipient list as needed.
    """
    dag_id = context.get("dag").dag_id
    task_id = context.get("task_instance").task_id
    execution_date = context.get("execution_date")
    log_url = context.get("task_instance").log_url

    subject = f"[Airflow] DAG {dag_id} - Task {task_id} Failed"
    html_content = f"""
    <p>Task <strong>{task_id}</strong> in DAG <strong>{dag_id}</strong> failed.</p>
    <p><strong>Execution date:</strong> {execution_date}</p>
    <p><strong>Log URL:</strong> <a href="{log_url}">{log_url}</a></p>
    """
    # Replace with your actual email list
    send_email(to=["airflow-alerts@example.com"], subject=subject, html_content=html_content)


# -------------------------------------------------------------------------
# Retrieve shared configuration values
# -------------------------------------------------------------------------
# The shared data directory is stored as an Airflow Variable named DATA_DIR.
# It can also be provided via a connection if preferred.
DATA_DIR = Variable.get("DATA_DIR", default_var="/opt/data")

# Docker network to be used for all containers
DOCKER_NETWORK = "app_network"


with DAG(
    dag_id="load_and_modify_data_pipeline",
    description="Sequential pipeline to load, reconcile, and save supplier data using Docker containers.",
    schedule_interval=None,  # Disabled schedule
    start_date=datetime(2024, 1, 1),
    catchup=False,
    default_args=default_args,
    tags=["load", "reconcile", "save"],
    on_failure_callback=_failure_callback,
    render_template_as_native_obj=True,
) as dag:

    # ---------------------------------------------------------------------
    # Task: Load and Modify Supplier Data
    # ---------------------------------------------------------------------
    load_and_modify = DockerOperator(
        task_id="load_and_modify_data",
        image="i2t-backendwithintertwino6-load-and-modify:latest",
        api_version="auto",
        auto_remove=True,
        docker_url="unix://var/run/docker.sock",
        network_mode=DOCKER_NETWORK,
        environment={
            "DATASET_ID": "2",
            "TABLE_NAME_PREFIX": "JOT_",
            "DATA_DIR": DATA_DIR,
        },
        # Optional: mount the data directory into the container if needed
        # mounts=[Mount(source=DATA_DIR, target="/data", type="bind")],
        on_failure_callback=_failure_callback,
    )

    # ---------------------------------------------------------------------
    # Task: Entity Reconciliation (Wikidata)
    # ---------------------------------------------------------------------
    reconcile_supplier_names = DockerOperator(
        task_id="reconcile_supplier_names",
        image="i2t-backendwithintertwino6-reconciliation:latest",
        api_version="auto",
        auto_remove=True,
        docker_url="unix://var/run/docker.sock",
        network_mode=DOCKER_NETWORK,
        environment={
            "PRIMARY_COLUMN": "supplier_name",
            "RECONCILIATOR_ID": "wikidataEntity",
            "DATASET_ID": "2",
            "DATA_DIR": DATA_DIR,
        },
        on_failure_callback=_failure_callback,
    )

    # ---------------------------------------------------------------------
    # Task: Save Final Validated Supplier Data
    # ---------------------------------------------------------------------
    save_validated = DockerOperator(
        task_id="save_validated_data",
        image="i2t-backendwithintertwino6-save:latest",
        api_version="auto",
        auto_remove=True,
        docker_url="unix://var/run/docker.sock",
        network_mode=DOCKER_NETWORK,
        environment={
            "DATASET_ID": "2",
            "DATA_DIR": DATA_DIR,
        },
        on_failure_callback=_failure_callback,
    )

    # ---------------------------------------------------------------------
    # Define task dependencies (sequential execution)
    # ---------------------------------------------------------------------
    load_and_modify >> reconcile_supplier_names >> save_validated

# End of DAG definition.