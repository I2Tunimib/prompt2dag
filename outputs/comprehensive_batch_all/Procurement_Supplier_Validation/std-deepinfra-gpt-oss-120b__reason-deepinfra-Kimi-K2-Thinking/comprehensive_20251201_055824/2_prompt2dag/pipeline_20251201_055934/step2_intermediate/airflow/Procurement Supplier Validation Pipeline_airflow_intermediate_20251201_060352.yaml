metadata:
  target_orchestrator: airflow
  generated_at: 2025-12-01 06:03:52.427802
  source_analysis_file: Pipeline_Description_Dataset/Procurement_Supplier_Validation.txt
  pipeline_name: Procurement Supplier Validation Pipeline
  pipeline_description: Validates and standardizes supplier data by reconciling names against a known database 
    (Wikidata) for improved data quality in procurement systems. Ingests a CSV of basic supplier information, converts 
    it to JSON, reconciles supplier names, and exports the enriched data to CSV.
  orchestrator_specific: {}
schedule:
  enabled: false
  schedule_expression:
  start_date:
  end_date:
  timezone: UTC
  catchup: false
connections:
  - conn_id: filesystem_data_dir
    conn_type: fs
    description: Shared Data Directory (DATA_DIR)
    config:
      base_path: /data
      base_url:
      host:
      port:
      protocol: file
      database:
      schema:
      bucket:
      queue_name:
  - conn_id: api_load_modify
    conn_type: http
    description: Load‑and‑Modify Service API
    config:
      base_path:
      base_url: http://load-and-modify:3003
      host: load-and-modify
      port: 3003
      protocol: http
      database:
      schema:
      bucket:
      queue_name:
  - conn_id: api_reconciliation
    conn_type: http
    description: Entity Reconciliation Service API
    config:
      base_path:
      base_url: http://reconciliation:3003
      host: reconciliation
      port: 3003
      protocol: http
      database:
      schema:
      bucket:
      queue_name:
  - conn_id: api_save
    conn_type: http
    description: Save Service API
    config:
      base_path:
      base_url: http://save-service:3000
      host: save-service
      port: 3000
      protocol: http
      database:
      schema:
      bucket:
      queue_name:
  - conn_id: db_mongodb
    conn_type: mongo
    description: MongoDB Instance
    config:
      base_path:
      base_url:
      host: mongodb
      port: 27017
      protocol: mongodb
      database:
      schema:
      bucket:
      queue_name:
  - conn_id: api_intertwino
    conn_type: http
    description: Intertwino API
    config:
      base_path:
      base_url: http://intertwino:5005
      host: intertwino
      port: 5005
      protocol: http
      database:
      schema:
      bucket:
      queue_name:
  - conn_id: docker_network_app
    conn_type: generic
    description: Custom Docker Network (app_network)
    config:
      base_path:
      base_url:
      host:
      port:
      protocol:
      database:
      schema:
      bucket:
      queue_name:
tasks:
  - task_id: load_and_modify_data
    task_name: Load and Modify Data
    operator_class: DockerOperator
    operator_module: airflow.providers.docker.operators.docker
    component_ref: load_and_modify_data
    config:
      image: i2t-backendwithintertwino6-load-and-modify:latest
      environment:
        DATASET_ID: '2'
        TABLE_NAME_PREFIX: JOT_
        DATA_DIR: ${DATA_DIR}
      network_mode: app_network
      auto_remove: true
      docker_url: unix://var/run/docker.sock
    upstream_task_ids: []
    trigger_rule: all_success
    retries: 1
    retry_delay_seconds: 0
    validation_warnings:
      - Uses image default command - ensure Dockerfile has ENTRYPOINT/CMD
  - task_id: reconcile_supplier_names
    task_name: Entity Reconciliation (Wikidata)
    operator_class: DockerOperator
    operator_module: airflow.providers.docker.operators.docker
    component_ref: reconcile_supplier_names
    config:
      image: i2t-backendwithintertwino6-reconciliation:latest
      environment:
        PRIMARY_COLUMN: supplier_name
        RECONCILIATOR_ID: wikidataEntity
        DATASET_ID: '2'
        DATA_DIR: ${DATA_DIR}
      network_mode: app_network
      auto_remove: true
      docker_url: unix://var/run/docker.sock
    upstream_task_ids:
      - load_and_modify_data
    trigger_rule: all_success
    retries: 1
    retry_delay_seconds: 0
    validation_warnings:
      - Uses image default command - ensure Dockerfile has ENTRYPOINT/CMD
  - task_id: save_final_data
    task_name: Save Final Data
    operator_class: DockerOperator
    operator_module: airflow.providers.docker.operators.docker
    component_ref: save_final_data
    config:
      image: i2t-backendwithintertwino6-save:latest
      environment:
        DATASET_ID: '2'
        DATA_DIR: ${DATA_DIR}
      network_mode: app_network
      auto_remove: true
      docker_url: unix://var/run/docker.sock
    upstream_task_ids:
      - reconcile_supplier_names
    trigger_rule: all_success
    retries: 1
    retry_delay_seconds: 0
    validation_warnings:
      - Uses image default command - ensure Dockerfile has ENTRYPOINT/CMD
