# Generated by Airflow DAG Generator
# Pipeline: Procurement Supplier Validation Pipeline
# Description: Validates and standardizes supplier data by reconciling names against Wikidata.
# This DAG is intentionally not scheduled (schedule_interval=None) and must be triggered manually.

from __future__ import annotations

import logging
from datetime import datetime, timedelta

from airflow import DAG
from airflow.models import Variable
from airflow.utils.task_group import TaskGroup
from airflow.utils.trigger_rule import TriggerRule
from airflow.providers.docker.operators.docker import DockerOperator
from airflow.utils.email import send_email

# -------------------------------------------------------------------------
# Default arguments applied to all tasks
# -------------------------------------------------------------------------
default_args = {
    "owner": "airflow",
    "depends_on_past": False,
    "retries": 1,
    "retry_delay": timedelta(minutes=5),
    "email_on_failure": False,
    "email_on_retry": False,
}


def failure_callback(context):
    """
    Callback executed when a task fails.
    Sends a simple log entry and optionally an email notification.
    """
    dag_id = context.get("dag").dag_id
    task_id = context.get("task_instance").task_id
    execution_date = context.get("execution_date")
    log_msg = (
        f"Task failed: dag_id={dag_id}, task_id={task_id}, execution_date={execution_date}"
    )
    logging.error(log_msg)

    # Example email notification (uncomment and configure if needed)
    # send_email(
    #     to="data-eng@example.com",
    #     subject=f"[Airflow] Task Failure: {task_id}",
    #     html_content=log_msg,
    # )


# -------------------------------------------------------------------------
# DAG definition
# -------------------------------------------------------------------------
with DAG(
    dag_id="procurement_supplier_validation_pipeline",
    description="Validates and standardizes supplier data by reconciling names against Wikidata.",
    schedule_interval=None,  # Disabled; trigger manually
    start_date=datetime(2024, 1, 1),
    catchup=False,
    default_args=default_args,
    tags=["procurement", "validation", "wikidata"],
    on_failure_callback=failure_callback,
    is_paused_upon_creation=True,
) as dag:

    # Retrieve shared data directory from Airflow Variable (set via UI/CLI)
    DATA_DIR = Variable.get("DATA_DIR", default_var="/opt/airflow/data")

    # -----------------------------------------------------------------
    # Task: Load and Modify Data
    # -----------------------------------------------------------------
    load_and_modify_data = DockerOperator(
        task_id="load_and_modify_data",
        image="i2t-backendwithintertwino6-load-and-modify:latest",
        api_version="auto",
        auto_remove=True,
        command=None,
        docker_url="unix://var/run/docker.sock",
        network_mode="app_network",  # Custom Docker network
        environment={
            "DATASET_ID": "2",
            "TABLE_NAME_PREFIX": "JOT_",
            "DATA_DIR": DATA_DIR,
        },
        retries=1,
        retry_delay=timedelta(minutes=5),
        on_failure_callback=failure_callback,
        # Mount the shared data directory into the container
        mounts=[
            # Example mount; adjust host path as needed
            # {"source": "/host/path/to/data", "target": DATA_DIR, "type": "bind"}
        ],
    )

    # -----------------------------------------------------------------
    # Task: Entity Reconciliation (Wikidata)
    # -----------------------------------------------------------------
    reconcile_supplier_names = DockerOperator(
        task_id="reconcile_supplier_names",
        image="i2t-backendwithintertwino6-reconciliation:latest",
        api_version="auto",
        auto_remove=True,
        command=None,
        docker_url="unix://var/run/docker.sock",
        network_mode="app_network",
        environment={
            "PRIMARY_COLUMN": "supplier_name",
            "RECONCILIATOR_ID": "wikidataEntity",
            "DATASET_ID": "2",
            "DATA_DIR": DATA_DIR,
        },
        retries=1,
        retry_delay=timedelta(minutes=5),
        on_failure_callback=failure_callback,
        mounts=[
            # Same mount as above if required
        ],
    )

    # -----------------------------------------------------------------
    # Task: Save Final Data
    # -----------------------------------------------------------------
    save_final_data = DockerOperator(
        task_id="save_final_data",
        image="i2t-backendwithintertwino6-save:latest",
        api_version="auto",
        auto_remove=True,
        command=None,
        docker_url="unix://var/run/docker.sock",
        network_mode="app_network",
        environment={
            "DATASET_ID": "2",
            "DATA_DIR": DATA_DIR,
        },
        retries=1,
        retry_delay=timedelta(minutes=5),
        on_failure_callback=failure_callback,
        mounts=[
            # Same mount as above if required
        ],
    )

    # -----------------------------------------------------------------
    # Set task dependencies (sequential flow)
    # -----------------------------------------------------------------
    load_and_modify_data >> reconcile_supplier_names >> save_final_data

# End of DAG definition.