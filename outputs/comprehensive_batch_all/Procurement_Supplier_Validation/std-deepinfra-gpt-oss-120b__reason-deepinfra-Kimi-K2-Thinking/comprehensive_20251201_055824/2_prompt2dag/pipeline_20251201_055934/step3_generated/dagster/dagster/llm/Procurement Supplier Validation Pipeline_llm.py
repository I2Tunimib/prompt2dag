# Generated by Dagster Pipeline Generator
# Date: 2024-06-28
# Pipeline: Procurement Supplier Validation Pipeline
# Description: Validates and standardizes supplier data by reconciling names against Wikidata.
# This file is intended to be used with Dagster >=1.5.0.

from __future__ import annotations

import json
import os
from typing import Any, Dict, List

import pandas as pd
from dagster import (
    Config,
    In,
    Out,
    RetryPolicy,
    ResourceDefinition,
    String,
    asset,
    job,
    op,
    resource,
)
from dagster._core.execution.executors import docker_executor

# ----------------------------------------------------------------------
# Resource Definitions
# ----------------------------------------------------------------------


@resource(config_schema={"data_dir": String})
def filesystem_data_dir(init_context) -> str:
    """Filesystem data directory used as the base path for I/O operations."""
    data_dir = init_context.resource_config["data_dir"]
    if not os.path.isdir(data_dir):
        raise FileNotFoundError(f"Data directory does not exist: {data_dir}")
    return data_dir


@resource(config_schema={"base_url": String})
def api_load_modify(init_context):
    """Placeholder for a Load‑and‑Modify Service API client."""
    class LoadModifyClient:
        def __init__(self, base_url: str):
            self.base_url = base_url

        def fetch_csv(self, path: str) -> pd.DataFrame:
            full_path = os.path.join(path)
            return pd.read_csv(full_path)

        def upload_json(self, data: List[Dict[str, Any]]) -> None:
            # In a real implementation this would POST to an endpoint.
            pass

    return LoadModifyClient(init_context.resource_config["base_url"])


@resource(config_schema={"base_url": String})
def api_reconciliation(init_context):
    """Placeholder for an Entity Reconciliation Service API client (e.g., Wikidata)."""
    class ReconciliationClient:
        def __init__(self, base_url: str):
            self.base_url = base_url

        def reconcile(self, name: str) -> Dict[str, Any]:
            # Simulated reconciliation response.
            return {"original_name": name, "wikidata_id": f"Q{hash(name) % 100000}"}

    return ReconciliationClient(init_context.resource_config["base_url"])


@resource(config_schema={"base_url": String})
def api_save(init_context):
    """Placeholder for a Save Service API client."""
    class SaveClient:
        def __init__(self, base_url: str):
            self.base_url = base_url

        def write_csv(self, data: List[Dict[str, Any]], path: str) -> None:
            df = pd.DataFrame(data)
            df.to_csv(path, index=False)

    return SaveClient(init_context.resource_config["base_url"])


@resource(config_schema={"connection_string": String})
def db_mongodb(init_context):
    """Placeholder for a MongoDB resource."""
    class MongoDBClient:
        def __init__(self, conn_str: str):
            self.conn_str = conn_str

        def insert_many(self, collection: str, documents: List[Dict[str, Any]]) -> None:
            # Real implementation would use pymongo.
            pass

    return MongoDBClient(init_context.resource_config["connection_string"])


@resource(config_schema={"api_key": String, "base_url": String})
def api_intertwino(init_context):
    """Placeholder for an Intertwino API client."""
    class IntertwinoClient:
        def __init__(self, api_key: str, base_url: str):
            self.api_key = api_key
            self.base_url = base_url

        def enrich(self, data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
            # Simulated enrichment.
            return data

    cfg = init_context.resource_config
    return IntertwinoClient(api_key=cfg["api_key"], base_url=cfg["base_url"])


@resource(config_schema={"network_name": String})
def docker_network_app(init_context):
    """Docker network resource used by the Docker executor."""
    return init_context.resource_config["network_name"]


# ----------------------------------------------------------------------
# Op Definitions
# ----------------------------------------------------------------------


@op(
    name="load_and_modify_data",
    description="Loads a CSV of supplier information, converts it to JSON, and applies any required modifications.",
    out=Out(dagster_type=List[Dict[str, Any]]),
    retry_policy=RetryPolicy(max_retries=1),
    required_resource_keys={"filesystem_data_dir", "api_load_modify"},
)
def load_and_modify_data(context) -> List[Dict[str, Any]]:
    """Load CSV from the shared data directory, convert to JSON, and return a list of records."""
    data_dir: str = context.resources.filesystem_data_dir
    csv_path = os.path.join(data_dir, "suppliers_input.csv")
    context.log.info(f"Loading supplier CSV from {csv_path}")

    # Use the load‑and‑modify API client to read the CSV.
    df = context.resources.api_load_modify.fetch_csv(csv_path)

    # Example modification: strip whitespace from column names.
    df.columns = df.columns.str.strip()
    records = df.to_dict(orient="records")

    # Optionally upload the raw JSON to a staging endpoint.
    context.resources.api_load_modify.upload_json(records)

    context.log.info(f"Loaded and transformed {len(records)} supplier records.")
    return records


@op(
    name="reconcile_supplier_names",
    description="Reconciles supplier names against Wikidata to standardize entity identifiers.",
    ins={"supplier_data": In(dagster_type=List[Dict[str, Any]])},
    out=Out(dagster_type=List[Dict[str, Any]]),
    retry_policy=RetryPolicy(max_retries=1),
    required_resource_keys={"api_reconciliation"},
)
def reconcile_supplier_names(context, supplier_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """For each supplier record, call the reconciliation service and augment the record."""
    client = context.resources.api_reconciliation
    enriched = []

    for record in supplier_data:
        name = record.get("supplier_name")
        if not name:
            context.log.warning("Record missing 'supplier_name'; skipping reconciliation.")
            enriched.append(record)
            continue

        reconciliation_result = client.reconcile(name)
        enriched_record = {**record, **reconciliation_result}
        enriched.append(enriched_record)

    context.log.info(f"Reconciled {len(enriched)} supplier records.")
    return enriched


@op(
    name="save_final_data",
    description="Writes the enriched supplier data back to CSV for downstream consumption.",
    ins={"enriched_data": In(dagster_type=List[Dict[str, Any]])},
    out=Out(dagster_type=String),
    retry_policy=RetryPolicy(max_retries=1),
    required_resource_keys={"filesystem_data_dir", "api_save"},
)
def save_final_data(context, enriched_data: List[Dict[str, Any]]) -> str:
    """Persist the enriched supplier data to a CSV file in the shared data directory."""
    data_dir: str = context.resources.filesystem_data_dir
    output_path = os.path.join(data_dir, "suppliers_enriched.csv")
    context.log.info(f"Saving enriched supplier data to {output_path}")

    context.resources.api_save.write_csv(enriched_data, output_path)

    context.log.info("Save completed.")
    return output_path


# ----------------------------------------------------------------------
# Job Definition
# ----------------------------------------------------------------------


@job(
    name="procurement_supplier_validation_pipeline",
    description=(
        "Validates and standardizes supplier data by reconciling names against a known database "
        "(Wikidata) for improved data quality in procurement systems. Ingests a CSV of basic "
        "supplier information, converts it to JSON, reconciles supplier names, and exports the "
        "enriched data to CSV."
    ),
    executor_def=docker_executor,
    resource_defs={
        "filesystem_data_dir": filesystem_data_dir,
        "api_load_modify": api_load_modify,
        "api_reconciliation": api_reconciliation,
        "api_save": api_save,
        "db_mongodb": db_mongodb,
        "api_intertwino": api_intertwino,
        "docker_network_app": docker_network_app,
    },
    config={
        "execution": {
            "config": {
                "image": "python:3.9-slim",
                "network": {"name": {"env": "DOCKER_NETWORK_NAME"}},
                "env_vars": {"DATA_DIR": {"env": "DATA_DIR"}},
            }
        },
        "resources": {
            "filesystem_data_dir": {"config": {"data_dir": {"env": "DATA_DIR"}}},
            "api_load_modify": {"config": {"base_url": "https://load-modify.example.com"}},
            "api_reconciliation": {"config": {"base_url": "https://reconcile.example.com"}},
            "api_save": {"config": {"base_url": "https://save.example.com"}},
            "db_mongodb": {"config": {"connection_string": "mongodb://mongo:27017"}},
            "api_intertwino": {
                "config": {
                    "api_key": {"env": "INTERTWINO_API_KEY"},
                    "base_url": "https://api.intertwino.com",
                }
            },
            "docker_network_app": {"config": {"network_name": {"env": "DOCKER_NETWORK_NAME"}}},
        },
    },
)
def procurement_supplier_validation_pipeline():
    """Orchestrates the sequential steps required to validate and enrich supplier data."""
    raw_data = load_and_modify_data()
    reconciled = reconcile_supplier_names(supplier_data=raw_data)
    save_final_data(enriched_data=reconciled)