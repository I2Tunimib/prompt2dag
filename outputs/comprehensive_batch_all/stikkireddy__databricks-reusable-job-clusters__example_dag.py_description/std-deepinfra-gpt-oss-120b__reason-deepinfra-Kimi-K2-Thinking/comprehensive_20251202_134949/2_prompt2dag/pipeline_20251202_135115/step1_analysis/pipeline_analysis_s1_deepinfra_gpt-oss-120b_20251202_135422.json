{
  "metadata": {
    "schema_version": "1.0",
    "analysis_timestamp": "2025-12-02T13:54:22.167740",
    "source_file": "Pipeline_Description_Dataset/stikkireddy__databricks-reusable-job-clusters__example_dag.py_description.txt",
    "llm_provider": "deepinfra",
    "llm_model": "gpt-oss-120b",
    "analysis_results": {
      "detected_patterns": [
        "sequential",
        "branching",
        "hybrid"
      ],
      "task_executors_used": [
        "python",
        "spark"
      ],
      "has_branching": true,
      "has_parallelism": false,
      "has_sensors": false,
      "total_components": 8,
      "complexity_score": "medium"
    },
    "orchestrator_compatibility": {
      "airflow": {
        "supported": true,
        "confidence": "high",
        "notes": [
          "Sequential pattern fully supported",
          "Branching via BranchPythonOperator"
        ]
      },
      "prefect": {
        "supported": true,
        "confidence": "high",
        "notes": [
          "Sequential flow with task dependencies",
          "Conditional logic via Python control flow"
        ]
      },
      "dagster": {
        "supported": true,
        "confidence": "high",
        "notes": [
          "Op graph with dependencies",
          "Dynamic graphs or branching ops"
        ]
      }
    },
    "validation_warnings": []
  },
  "pipeline_summary": {
    "name": "test_dbx_aws_dag_reuse",
    "description": "Comprehensive pipeline that orchestrates Databricks notebook executions with conditional branching and reusable clusters. Manual trigger only.",
    "flow_patterns": [
      "sequential",
      "branching",
      "hybrid"
    ],
    "task_executors": [
      "python",
      "spark"
    ],
    "complexity": "medium"
  },
  "components": [
    {
      "id": "start_pipeline",
      "name": "Start Pipeline",
      "category": "Other",
      "description": "Initial dummy task that starts the pipeline execution when manually triggered.",
      "inputs": [
        "manual_trigger"
      ],
      "outputs": [
        "trigger_notebook_task"
      ],
      "executor_type": "python",
      "executor_config": {
        "image": null,
        "command": null,
        "script_path": null,
        "entry_point": null,
        "environment": {},
        "resources": {},
        "network": null
      },
      "io_spec": [
        {
          "name": "manual_trigger",
          "direction": "input",
          "kind": "api",
          "format": "json",
          "path_pattern": "airflow_trigger",
          "connection_id": null
        },
        {
          "name": "trigger_notebook_task",
          "direction": "output",
          "kind": "object",
          "format": "json",
          "path_pattern": "internal_signal",
          "connection_id": null
        }
      ],
      "upstream_policy": {
        "type": "all_success",
        "description": "Root task, no upstream dependencies",
        "timeout_seconds": null
      },
      "retry_policy": {
        "max_attempts": 0,
        "delay_seconds": 0,
        "exponential_backoff": false,
        "retry_on": []
      },
      "concurrency": {
        "supports_parallelism": false,
        "supports_dynamic_mapping": false,
        "map_over_param": null,
        "max_parallel_instances": null
      },
      "connections": [],
      "datasets": {
        "consumes": [],
        "produces": []
      }
    },
    {
      "id": "execute_primary_notebook",
      "name": "Execute Primary Databricks Notebook",
      "category": "Transformer",
      "description": "Runs the primary Databricks notebook on a reusable cluster for data processing.",
      "inputs": [
        "completion_of_start_pipeline"
      ],
      "outputs": [
        "trigger_dummy_task_1"
      ],
      "executor_type": "custom",
      "executor_config": {
        "image": null,
        "command": null,
        "script_path": null,
        "entry_point": null,
        "environment": {
          "DATABRICKS_CONN_ID": "databricks_default",
          "EXISTING_CLUSTER_ID": "existing_cluster_id",
          "NOTEBOOK_PATH": "/Users/sri.tikkireddy@databricks.com/workflow-mirroring/helloworld"
        },
        "resources": {},
        "network": null
      },
      "io_spec": [
        {
          "name": "completion_of_start_pipeline",
          "direction": "input",
          "kind": "object",
          "format": "json",
          "path_pattern": "internal_signal",
          "connection_id": null
        },
        {
          "name": "trigger_dummy_task_1",
          "direction": "output",
          "kind": "object",
          "format": "json",
          "path_pattern": "internal_signal",
          "connection_id": null
        }
      ],
      "upstream_policy": {
        "type": "all_success",
        "description": "Runs after successful completion of start_pipeline",
        "timeout_seconds": null
      },
      "retry_policy": {
        "max_attempts": 1,
        "delay_seconds": 300,
        "exponential_backoff": false,
        "retry_on": [
          "timeout",
          "network_error"
        ]
      },
      "concurrency": {
        "supports_parallelism": false,
        "supports_dynamic_mapping": false,
        "map_over_param": null,
        "max_parallel_instances": null
      },
      "connections": [
        {
          "id": "databricks_default",
          "type": "databricks",
          "purpose": "Execute notebook on Databricks cluster"
        }
      ],
      "datasets": {
        "consumes": [],
        "produces": []
      }
    },
    {
      "id": "intermediate_dummy_1",
      "name": "Intermediate Dummy 1",
      "category": "Other",
      "description": "Placeholder dummy task between primary notebook execution and branching decision.",
      "inputs": [
        "completion_of_execute_primary_notebook"
      ],
      "outputs": [
        "trigger_branch_task"
      ],
      "executor_type": "python",
      "executor_config": {
        "image": null,
        "command": null,
        "script_path": null,
        "entry_point": null,
        "environment": {},
        "resources": {},
        "network": null
      },
      "io_spec": [
        {
          "name": "completion_of_execute_primary_notebook",
          "direction": "input",
          "kind": "object",
          "format": "json",
          "path_pattern": "internal_signal",
          "connection_id": null
        },
        {
          "name": "trigger_branch_task",
          "direction": "output",
          "kind": "object",
          "format": "json",
          "path_pattern": "internal_signal",
          "connection_id": null
        }
      ],
      "upstream_policy": {
        "type": "all_success",
        "description": "Runs after successful primary notebook execution",
        "timeout_seconds": null
      },
      "retry_policy": {
        "max_attempts": 0,
        "delay_seconds": 0,
        "exponential_backoff": false,
        "retry_on": []
      },
      "concurrency": {
        "supports_parallelism": false,
        "supports_dynamic_mapping": false,
        "map_over_param": null,
        "max_parallel_instances": null
      },
      "connections": [],
      "datasets": {
        "consumes": [],
        "produces": []
      }
    },
    {
      "id": "determine_branch_path",
      "name": "Determine Branch Path",
      "category": "Orchestrator",
      "description": "Evaluates a Python callable to decide which execution branch to follow.",
      "inputs": [
        "completion_of_intermediate_dummy_1"
      ],
      "outputs": [
        "dummy_task_3",
        "execute_secondary_notebook"
      ],
      "executor_type": "python",
      "executor_config": {
        "image": null,
        "command": null,
        "script_path": null,
        "entry_point": "branch_func",
        "environment": {},
        "resources": {},
        "network": null
      },
      "io_spec": [
        {
          "name": "completion_of_intermediate_dummy_1",
          "direction": "input",
          "kind": "object",
          "format": "json",
          "path_pattern": "internal_signal",
          "connection_id": null
        },
        {
          "name": "branch_decision",
          "direction": "output",
          "kind": "object",
          "format": "json",
          "path_pattern": "internal_signal",
          "connection_id": null
        }
      ],
      "upstream_policy": {
        "type": "all_success",
        "description": "Runs after intermediate dummy task completes",
        "timeout_seconds": null
      },
      "retry_policy": {
        "max_attempts": 0,
        "delay_seconds": 0,
        "exponential_backoff": false,
        "retry_on": []
      },
      "concurrency": {
        "supports_parallelism": false,
        "supports_dynamic_mapping": false,
        "map_over_param": null,
        "max_parallel_instances": null
      },
      "connections": [],
      "datasets": {
        "consumes": [],
        "produces": []
      }
    },
    {
      "id": "terminal_branch_dummy",
      "name": "Terminal Branch Dummy",
      "category": "Other",
      "description": "Terminal dummy task executed when branch function selects the first branch.",
      "inputs": [
        "branch_path_1_selected"
      ],
      "outputs": [],
      "executor_type": "python",
      "executor_config": {
        "image": null,
        "command": null,
        "script_path": null,
        "entry_point": null,
        "environment": {},
        "resources": {},
        "network": null
      },
      "io_spec": [
        {
          "name": "branch_path_1_selected",
          "direction": "input",
          "kind": "object",
          "format": "json",
          "path_pattern": "internal_signal",
          "connection_id": null
        }
      ],
      "upstream_policy": {
        "type": "any_success",
        "description": "Runs when branch_task routes to this path",
        "timeout_seconds": null
      },
      "retry_policy": {
        "max_attempts": 0,
        "delay_seconds": 0,
        "exponential_backoff": false,
        "retry_on": []
      },
      "concurrency": {
        "supports_parallelism": false,
        "supports_dynamic_mapping": false,
        "map_over_param": null,
        "max_parallel_instances": null
      },
      "connections": [],
      "datasets": {
        "consumes": [],
        "produces": []
      }
    },
    {
      "id": "execute_secondary_notebook",
      "name": "Execute Secondary Databricks Notebook",
      "category": "Transformer",
      "description": "Runs a secondary Databricks notebook on the same reusable cluster when the alternative branch is chosen.",
      "inputs": [
        "branch_path_2_selected"
      ],
      "outputs": [
        "trigger_dummy_task_2"
      ],
      "executor_type": "custom",
      "executor_config": {
        "image": null,
        "command": null,
        "script_path": null,
        "entry_point": null,
        "environment": {
          "DATABRICKS_CONN_ID": "databricks_default",
          "EXISTING_CLUSTER_ID": "existing_cluster_id",
          "NOTEBOOK_PATH": "/Users/sri.tikkireddy@databricks.com/workflow-mirroring/helloworld"
        },
        "resources": {},
        "network": null
      },
      "io_spec": [
        {
          "name": "branch_path_2_selected",
          "direction": "input",
          "kind": "object",
          "format": "json",
          "path_pattern": "internal_signal",
          "connection_id": null
        },
        {
          "name": "trigger_dummy_task_2",
          "direction": "output",
          "kind": "object",
          "format": "json",
          "path_pattern": "internal_signal",
          "connection_id": null
        }
      ],
      "upstream_policy": {
        "type": "all_success",
        "description": "Runs after branch decision selects secondary path",
        "timeout_seconds": null
      },
      "retry_policy": {
        "max_attempts": 1,
        "delay_seconds": 300,
        "exponential_backoff": false,
        "retry_on": [
          "timeout",
          "network_error"
        ]
      },
      "concurrency": {
        "supports_parallelism": false,
        "supports_dynamic_mapping": false,
        "map_over_param": null,
        "max_parallel_instances": null
      },
      "connections": [
        {
          "id": "databricks_default",
          "type": "databricks",
          "purpose": "Execute secondary notebook on Databricks cluster"
        }
      ],
      "datasets": {
        "consumes": [],
        "produces": []
      }
    },
    {
      "id": "intermediate_dummy_2",
      "name": "Intermediate Dummy 2",
      "category": "Other",
      "description": "Placeholder dummy task between secondary notebook execution and pipeline completion.",
      "inputs": [
        "completion_of_execute_secondary_notebook"
      ],
      "outputs": [
        "trigger_end_task"
      ],
      "executor_type": "python",
      "executor_config": {
        "image": null,
        "command": null,
        "script_path": null,
        "entry_point": null,
        "environment": {},
        "resources": {},
        "network": null
      },
      "io_spec": [
        {
          "name": "completion_of_execute_secondary_notebook",
          "direction": "input",
          "kind": "object",
          "format": "json",
          "path_pattern": "internal_signal",
          "connection_id": null
        },
        {
          "name": "trigger_end_task",
          "direction": "output",
          "kind": "object",
          "format": "json",
          "path_pattern": "internal_signal",
          "connection_id": null
        }
      ],
      "upstream_policy": {
        "type": "all_success",
        "description": "Runs after secondary notebook completes",
        "timeout_seconds": null
      },
      "retry_policy": {
        "max_attempts": 0,
        "delay_seconds": 0,
        "exponential_backoff": false,
        "retry_on": []
      },
      "concurrency": {
        "supports_parallelism": false,
        "supports_dynamic_mapping": false,
        "map_over_param": null,
        "max_parallel_instances": null
      },
      "connections": [],
      "datasets": {
        "consumes": [],
        "produces": []
      }
    },
    {
      "id": "pipeline_completion",
      "name": "Pipeline Completion",
      "category": "Other",
      "description": "Final dummy task marking successful completion of the pipeline for the secondary branch.",
      "inputs": [
        "completion_of_intermediate_dummy_2"
      ],
      "outputs": [],
      "executor_type": "python",
      "executor_config": {
        "image": null,
        "command": null,
        "script_path": null,
        "entry_point": null,
        "environment": {},
        "resources": {},
        "network": null
      },
      "io_spec": [
        {
          "name": "completion_of_intermediate_dummy_2",
          "direction": "input",
          "kind": "object",
          "format": "json",
          "path_pattern": "internal_signal",
          "connection_id": null
        }
      ],
      "upstream_policy": {
        "type": "all_success",
        "description": "Runs after intermediate dummy 2 completes",
        "timeout_seconds": null
      },
      "retry_policy": {
        "max_attempts": 0,
        "delay_seconds": 0,
        "exponential_backoff": false,
        "retry_on": []
      },
      "concurrency": {
        "supports_parallelism": false,
        "supports_dynamic_mapping": false,
        "map_over_param": null,
        "max_parallel_instances": null
      },
      "connections": [],
      "datasets": {
        "consumes": [],
        "produces": []
      }
    }
  ],
  "flow_structure": {
    "pattern": "branching",
    "entry_points": [
      "start_pipeline"
    ],
    "nodes": {
      "start_pipeline": {
        "kind": "Task",
        "component_type_id": "start_pipeline",
        "upstream_policy": {
          "type": "all_success",
          "timeout_seconds": null
        },
        "next_nodes": [
          "execute_primary_notebook"
        ],
        "branch_config": null,
        "sensor_config": null,
        "parallel_config": null
      },
      "execute_primary_notebook": {
        "kind": "Task",
        "component_type_id": "execute_primary_notebook",
        "upstream_policy": {
          "type": "all_success",
          "timeout_seconds": null
        },
        "next_nodes": [
          "intermediate_dummy_1"
        ],
        "branch_config": null,
        "sensor_config": null,
        "parallel_config": null
      },
      "intermediate_dummy_1": {
        "kind": "Task",
        "component_type_id": "intermediate_dummy_1",
        "upstream_policy": {
          "type": "all_success",
          "timeout_seconds": null
        },
        "next_nodes": [
          "determine_branch_path"
        ],
        "branch_config": null,
        "sensor_config": null,
        "parallel_config": null
      },
      "determine_branch_path": {
        "kind": "Branch",
        "component_type_id": null,
        "upstream_policy": {
          "type": "all_success",
          "timeout_seconds": null
        },
        "next_nodes": [],
        "branch_config": {
          "type": "conditional",
          "branches": [
            {
              "label": "Path 1 - Terminal Dummy",
              "condition": "branch_func == \"terminal_branch_dummy\"",
              "next_node": "terminal_branch_dummy"
            },
            {
              "label": "Path 2 - Secondary Notebook",
              "condition": "else",
              "next_node": "execute_secondary_notebook"
            }
          ]
        },
        "sensor_config": null,
        "parallel_config": null
      },
      "terminal_branch_dummy": {
        "kind": "Task",
        "component_type_id": "terminal_branch_dummy",
        "upstream_policy": {
          "type": "all_success",
          "timeout_seconds": null
        },
        "next_nodes": [],
        "branch_config": null,
        "sensor_config": null,
        "parallel_config": null
      },
      "execute_secondary_notebook": {
        "kind": "Task",
        "component_type_id": "execute_secondary_notebook",
        "upstream_policy": {
          "type": "all_success",
          "timeout_seconds": null
        },
        "next_nodes": [
          "intermediate_dummy_2"
        ],
        "branch_config": null,
        "sensor_config": null,
        "parallel_config": null
      },
      "intermediate_dummy_2": {
        "kind": "Task",
        "component_type_id": "intermediate_dummy_2",
        "upstream_policy": {
          "type": "all_success",
          "timeout_seconds": null
        },
        "next_nodes": [
          "pipeline_completion"
        ],
        "branch_config": null,
        "sensor_config": null,
        "parallel_config": null
      },
      "pipeline_completion": {
        "kind": "Task",
        "component_type_id": "pipeline_completion",
        "upstream_policy": {
          "type": "all_success",
          "timeout_seconds": null
        },
        "next_nodes": [],
        "branch_config": null,
        "sensor_config": null,
        "parallel_config": null
      }
    },
    "edges": [
      {
        "from": "start_pipeline",
        "to": "execute_primary_notebook",
        "edge_type": "success",
        "condition": null,
        "metadata": {}
      },
      {
        "from": "execute_primary_notebook",
        "to": "intermediate_dummy_1",
        "edge_type": "success",
        "condition": null,
        "metadata": {}
      },
      {
        "from": "intermediate_dummy_1",
        "to": "determine_branch_path",
        "edge_type": "success",
        "condition": null,
        "metadata": {}
      },
      {
        "from": "determine_branch_path",
        "to": "terminal_branch_dummy",
        "edge_type": "conditional",
        "condition": "Path 1 - Terminal Dummy",
        "metadata": {}
      },
      {
        "from": "determine_branch_path",
        "to": "execute_secondary_notebook",
        "edge_type": "conditional",
        "condition": "Path 2 - Secondary Notebook",
        "metadata": {}
      },
      {
        "from": "execute_secondary_notebook",
        "to": "intermediate_dummy_2",
        "edge_type": "success",
        "condition": null,
        "metadata": {}
      },
      {
        "from": "intermediate_dummy_2",
        "to": "pipeline_completion",
        "edge_type": "success",
        "condition": null,
        "metadata": {}
      }
    ]
  },
  "parameters": {
    "pipeline": {
      "name": {
        "description": "Pipeline identifier",
        "type": "string",
        "default": "test_dbx_aws_dag_reuse",
        "required": false,
        "constraints": null
      },
      "description": {
        "description": "Pipeline description",
        "type": "string",
        "default": "Comprehensive pipeline that orchestrates Databricks notebook executions with conditional branching and reusable clusters. Manual trigger only.",
        "required": false,
        "constraints": null
      },
      "tags": {
        "description": "Classification tags",
        "type": "array",
        "default": [],
        "required": false,
        "constraints": null
      }
    },
    "schedule": {
      "enabled": {
        "description": "Whether pipeline runs on schedule",
        "type": "boolean",
        "default": false,
        "required": false,
        "constraints": null
      },
      "cron_expression": {
        "description": "Cron or preset (e.g., @daily, 0 0 * * *)",
        "type": "string",
        "default": null,
        "required": false,
        "constraints": null
      },
      "start_date": {
        "description": "When to start scheduling",
        "type": "datetime",
        "default": "2023-06-06T00:00:00Z",
        "required": false,
        "format": "ISO8601",
        "constraints": null
      },
      "end_date": {
        "description": "When to stop scheduling",
        "type": "datetime",
        "default": null,
        "required": false,
        "constraints": null
      },
      "timezone": {
        "description": "Schedule timezone",
        "type": "string",
        "default": null,
        "required": false,
        "constraints": null
      },
      "catchup": {
        "description": "Run missed intervals",
        "type": "boolean",
        "default": null,
        "required": false,
        "constraints": null
      },
      "batch_window": {
        "description": "Batch window parameter name (e.g., ds, execution_date)",
        "type": "string",
        "default": null,
        "required": false,
        "constraints": null
      },
      "partitioning": {
        "description": "Data partitioning strategy (e.g., daily, hourly, monthly)",
        "type": "string",
        "default": null,
        "required": false,
        "constraints": null
      }
    },
    "execution": {
      "max_active_runs": {
        "description": "Max concurrent pipeline runs",
        "type": "integer",
        "default": null,
        "required": false,
        "constraints": null
      },
      "timeout_seconds": {
        "description": "Pipeline execution timeout",
        "type": "integer",
        "default": null,
        "required": false,
        "constraints": null
      },
      "retry_policy": {
        "description": "Pipeline-level retry behavior (1 retry, 5â€‘minute delay, owner='airflow')",
        "type": "object",
        "default": null,
        "required": false,
        "constraints": null
      },
      "depends_on_past": {
        "description": "Whether execution depends on previous run success",
        "type": "boolean",
        "default": null,
        "required": false,
        "constraints": null
      }
    },
    "components": {
      "start_pipeline": {},
      "execute_primary_notebook": {
        "databricks_conn_id": {
          "description": "Connection ID for Databricks",
          "type": "string",
          "default": "databricks_default",
          "required": false,
          "constraints": null
        },
        "existing_cluster_id": {
          "description": "Identifier of the reusable Databricks cluster",
          "type": "string",
          "default": "existing_cluster_id",
          "required": false,
          "constraints": null
        },
        "notebook_path": {
          "description": "Path to the Databricks notebook to execute",
          "type": "string",
          "default": "/Users/sri.tikkireddy@databricks.com/workflow-mirroring/helloworld",
          "required": false,
          "constraints": null
        }
      },
      "intermediate_dummy_1": {},
      "determine_branch_path": {
        "python_callable": {
          "description": "Python callable that determines the branch",
          "type": "string",
          "default": "branch_func",
          "required": false,
          "constraints": null
        },
        "provide_context": {
          "description": "Whether to provide Airflow context to the callable",
          "type": "boolean",
          "default": true,
          "required": false,
          "constraints": null
        }
      },
      "terminal_branch_dummy": {},
      "execute_secondary_notebook": {
        "databricks_conn_id": {
          "description": "Connection ID for Databricks",
          "type": "string",
          "default": "databricks_default",
          "required": false,
          "constraints": null
        },
        "existing_cluster_id": {
          "description": "Identifier of the reusable Databricks cluster",
          "type": "string",
          "default": "existing_cluster_id",
          "required": false,
          "constraints": null
        },
        "notebook_path": {
          "description": "Path to the secondary Databricks notebook to execute",
          "type": "string",
          "default": "/Users/sri.tikkireddy@databricks.com/workflow-mirroring/helloworld",
          "required": false,
          "constraints": null
        }
      },
      "intermediate_dummy_2": {},
      "pipeline_completion": {}
    },
    "environment": {
      "AIRFLOW_HOST": {
        "description": "Databricks secret storing the Airflow host URL",
        "type": "string",
        "default": null,
        "required": false,
        "associated_component_id": null
      },
      "AIRFLOW_AUTH_HEADER": {
        "description": "Databricks secret storing the Airflow authentication header",
        "type": "string",
        "default": null,
        "required": false,
        "associated_component_id": null
      }
    }
  },
  "integrations": {
    "connections": [
      {
        "id": "databricks_api",
        "name": "Databricks Workspace API",
        "type": "api",
        "config": {
          "base_path": null,
          "base_url": "https://<databricks-instance>.cloud.databricks.com",
          "host": "<databricks-instance>.cloud.databricks.com",
          "port": null,
          "protocol": "https",
          "database": null,
          "schema": null,
          "bucket": null,
          "queue_name": null
        },
        "authentication": {
          "type": "token",
          "token_env_var": "DATABRICKS_TOKEN",
          "username_env_var": null,
          "password_env_var": null,
          "credentials_path": null
        },
        "used_by_components": [
          "execute_primary_notebook",
          "execute_secondary_notebook"
        ],
        "direction": "both",
        "rate_limit": {
          "requests_per_second": null,
          "burst": null
        },
        "datasets": {
          "produces": [
            "primary_notebook_execution_result",
            "secondary_notebook_execution_result"
          ],
          "consumes": [
            "primary_notebook",
            "secondary_notebook"
          ]
        }
      }
    ],
    "data_lineage": {
      "sources": [
        "Primary Databricks notebook located at /Users/sri.tikkireddy@databricks.com/workflow-mirroring/helloworld",
        "Secondary Databricks notebook located at /Users/sri.tikkireddy@databricks.com/workflow-mirroring/helloworld"
      ],
      "sinks": [
        "No external data sink; notebook execution results remain within the Databricks workspace"
      ],
      "intermediate_datasets": [
        "primary_notebook_execution_result",
        "secondary_notebook_execution_result"
      ]
    }
  }
}