# Generated by Prefect Pipeline Generator
# Date: 2024-06-28
# Pipeline: test_dbx_aws_dag_reuse
# Description: Orchestrates Databricks notebook executions with conditional branching and reusable clusters.

import json
import random
import logging
from typing import Any, Dict

import requests
from prefect import flow, task, get_run_logger
from prefect.task_runners import SequentialTaskRunner
from prefect.blocks.system import Secret

# -------------------------------------------------------------------------
# Helper Functions
# -------------------------------------------------------------------------

def _load_databricks_credentials() -> Dict[str, str]:
    """
    Load Databricks workspace credentials from a Prefect Secret block.

    Returns
    -------
    dict
        Dictionary containing ``host`` and ``token`` keys.
    """
    secret = Secret.load("databricks_api")
    # Expect the secret value to be a JSON string with host and token.
    try:
        credentials = json.loads(secret.get())
        if not {"host", "token"} <= credentials.keys():
            raise ValueError("Databricks secret must contain 'host' and 'token'.")
        return credentials
    except json.JSONDecodeError as exc:
        raise ValueError("Databricks secret must be valid JSON.") from exc


def _submit_databricks_run(notebook_path: str, existing_cluster_id: str) -> Dict[str, Any]:
    """
    Submit a Databricks notebook run using the Jobs API.

    Parameters
    ----------
    notebook_path : str
        Absolute path to the notebook in the workspace.
    existing_cluster_id : str
        ID of an existing reusable cluster.

    Returns
    -------
    dict
        JSON response from the Databricks API.
    """
    creds = _load_databricks_credentials()
    url = f"{creds['host'].rstrip('/')}/api/2.0/jobs/runs/submit"
    payload = {
        "run_name": f"Prefect run for {notebook_path}",
        "existing_cluster_id": existing_cluster_id,
        "notebook_task": {"notebook_path": notebook_path},
    }
    headers = {"Authorization": f"Bearer {creds['token']}", "Content-Type": "application/json"}
    response = requests.post(url, headers=headers, json=payload, timeout=60)
    response.raise_for_status()
    return response.json()


# -------------------------------------------------------------------------
# Tasks
# -------------------------------------------------------------------------

@task(retries=0, name="Start Pipeline")
def start_pipeline() -> None:
    """
    Initial task that logs the start of the pipeline.
    """
    logger = get_run_logger()
    logger.info("Pipeline execution started.")


@task(retries=1, name="Execute Primary Databricks Notebook")
def execute_primary_notebook() -> None:
    """
    Executes the primary Databricks notebook on a reusable cluster.
    """
    logger = get_run_logger()
    logger.info("Submitting primary notebook run to Databricks.")
    try:
        result = _submit_databricks_run(
            notebook_path="/Shared/primary_notebook",
            existing_cluster_id="cluster-primary-id"
        )
        logger.info("Primary notebook submitted successfully: %s", result.get("run_id"))
    except Exception as exc:
        logger.error("Failed to submit primary notebook: %s", exc)
        raise


@task(retries=0, name="Intermediate Dummy 1")
def intermediate_dummy_1() -> None:
    """
    Placeholder task representing an intermediate processing step.
    """
    logger = get_run_logger()
    logger.info("Running intermediate dummy task 1.")


@task(retries=0, name="Determine Branch Path")
def determine_branch_path() -> str:
    """
    Determines which branch of the workflow to follow.

    Returns
    -------
    str
        Either ``'primary'`` or ``'secondary'`` indicating the chosen branch.
    """
    logger = get_run_logger()
    choice = random.choice(["primary", "secondary"])
    logger.info("Branch decision made: %s", choice)
    return choice


@task(retries=1, name="Execute Secondary Databricks Notebook")
def execute_secondary_notebook() -> None:
    """
    Executes the secondary Databricks notebook on a reusable cluster.
    """
    logger = get_run_logger()
    logger.info("Submitting secondary notebook run to Databricks.")
    try:
        result = _submit_databricks_run(
            notebook_path="/Shared/secondary_notebook",
            existing_cluster_id="cluster-secondary-id"
        )
        logger.info("Secondary notebook submitted successfully: %s", result.get("run_id"))
    except Exception as exc:
        logger.error("Failed to submit secondary notebook: %s", exc)
        raise


@task(retries=0, name="Terminal Branch Dummy")
def terminal_branch_dummy(branch: str) -> None:
    """
    Dummy task that runs at the end of each branch.

    Parameters
    ----------
    branch : str
        The branch identifier returned by ``determine_branch_path``.
    """
    logger = get_run_logger()
    logger.info("Running terminal dummy for branch: %s", branch)


@task(retries=0, name="Intermediate Dummy 2")
def intermediate_dummy_2() -> None:
    """
    Placeholder task representing a later intermediate processing step.
    """
    logger = get_run_logger()
    logger.info("Running intermediate dummy task 2.")


@task(retries=0, name="Pipeline Completion")
def pipeline_completion() -> None:
    """
    Final task that logs the successful completion of the pipeline.
    """
    logger = get_run_logger()
    logger.info("Pipeline execution completed successfully.")


# -------------------------------------------------------------------------
# Flow Definition
# -------------------------------------------------------------------------

@flow(
    name="test_dbx_aws_dag_reuse",
    task_runner=SequentialTaskRunner(),
)
def test_dbx_aws_dag_reuse() -> None:
    """
    Main Prefect flow orchestrating Databricks notebook executions with
    conditional branching and reusable clusters.
    """
    # Step 1: Start
    start = start_pipeline()

    # Step 2: Primary notebook
    primary = execute_primary_notebook(wait_for=[start])

    # Step 3: Intermediate dummy 1
    dummy1 = intermediate_dummy_1(wait_for=[primary])

    # Step 4: Branch decision
    branch = determine_branch_path(wait_for=[dummy1])

    # Step 5: Parallel branch tasks
    terminal = terminal_branch_dummy(branch, wait_for=[branch])
    secondary = execute_secondary_notebook(wait_for=[branch])

    # Step 6: Intermediate dummy 2 (depends on secondary notebook)
    dummy2 = intermediate_dummy_2(wait_for=[secondary])

    # Step 7: Completion
    pipeline_completion(wait_for=[dummy2, terminal])


# -------------------------------------------------------------------------
# Entry Point
# -------------------------------------------------------------------------

if __name__ == "__main__":
    # Execute the flow locally; in production this flow would be deployed
    # via Prefect Cloud or Prefect Server.
    test_dbx_aws_dag_reuse()