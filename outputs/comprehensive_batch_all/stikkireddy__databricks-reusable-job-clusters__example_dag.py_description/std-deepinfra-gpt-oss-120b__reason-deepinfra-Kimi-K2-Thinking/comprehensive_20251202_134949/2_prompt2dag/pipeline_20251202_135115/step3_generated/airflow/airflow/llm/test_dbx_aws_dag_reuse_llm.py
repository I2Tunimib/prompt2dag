# Generated by Airflow DAG generator on 2024-06-13
"""
DAG: test_dbx_aws_dag_reuse
Description: Comprehensive pipeline that orchestrates Databricks notebook executions with
conditional branching and reusable clusters. Manual trigger only.
"""

from datetime import datetime
from airflow import DAG
from airflow.decorators import task
from airflow.utils.dates import days_ago
from airflow.exceptions import AirflowException
from airflow.models import Variable
from airflow.hooks.base import BaseHook
import logging
import json
import requests

# Default arguments applied to all tasks
default_args = {
    "owner": "airflow",
    "depends_on_past": False,
    "retries": 0,
    "retry_delay": None,
    "email_on_failure": False,
    "email_on_retry": False,
}

# Helper to retrieve Databricks connection details
def get_databricks_connection():
    """
    Retrieves the Databricks workspace API connection details from Airflow.
    Expected connection type: HTTP with host and token stored in the password field.
    """
    try:
        conn = BaseHook.get_connection("databricks_api")
        host = conn.host.rstrip("/")
        token = conn.password  # Assuming token stored as password
        if not host or not token:
            raise ValueError("Databricks connection is missing host or token.")
        return host, token
    except Exception as exc:
        raise AirflowException(f"Failed to load Databricks connection: {exc}") from exc


# DAG definition â€“ manual trigger only (no schedule)
with DAG(
    dag_id="test_dbx_aws_dag_reuse",
    description="Comprehensive pipeline that orchestrates Databricks notebook executions with conditional branching and reusable clusters.",
    schedule_interval=None,
    start_date=days_ago(1),
    catchup=False,
    default_args=default_args,
    tags=["databricks", "fanout"],
    max_active_runs=1,
    is_paused_upon_creation=True,
) as dag:

    @task(task_id="start_pipeline", retries=0)
    def start_pipeline():
        """Entry point of the pipeline."""
        logging.info("Pipeline started.")
        return "started"

    @task(task_id="execute_primary_notebook", retries=1)
    def execute_primary_notebook(_):
        """Executes the primary Databricks notebook."""
        host, token = get_databricks_connection()
        notebook_path = "/Shared/PrimaryNotebook"
        payload = {
            "run_name": "Primary Notebook Run",
            "existing_cluster_id": "my-reusable-cluster",
            "notebook_task": {"notebook_path": notebook_path},
        }
        headers = {"Authorization": f"Bearer {token}"}
        try:
            response = requests.post(
                f"{host}/api/2.0/jobs/runs/submit", json=payload, headers=headers, timeout=30
            )
            response.raise_for_status()
            run_id = response.json().get("run_id")
            logging.info("Submitted primary notebook run, run_id=%s", run_id)
            return run_id
        except Exception as exc:
            raise AirflowException(f"Primary notebook execution failed: {exc}") from exc

    @task(task_id="intermediate_dummy_1", retries=0)
    def intermediate_dummy_1(_):
        """Placeholder task representing intermediate processing."""
        logging.info("Intermediate dummy 1 executed.")
        return "dummy1_done"

    @task(task_id="determine_branch_path", retries=0)
    def determine_branch_path(_):
        """
        Determines which branch to follow.
        For demonstration, we simply log and return a static value.
        """
        logging.info("Determining branch path.")
        # In a real scenario, logic would decide which path to take.
        return "branch_determined"

    @task(task_id="execute_secondary_notebook", retries=1)
    def execute_secondary_notebook(_):
        """Executes the secondary Databricks notebook."""
        host, token = get_databricks_connection()
        notebook_path = "/Shared/SecondaryNotebook"
        payload = {
            "run_name": "Secondary Notebook Run",
            "existing_cluster_id": "my-reusable-cluster",
            "notebook_task": {"notebook_path": notebook_path},
        }
        headers = {"Authorization": f"Bearer {token}"}
        try:
            response = requests.post(
                f"{host}/api/2.0/jobs/runs/submit", json=payload, headers=headers, timeout=30
            )
            response.raise_for_status()
            run_id = response.json().get("run_id")
            logging.info("Submitted secondary notebook run, run_id=%s", run_id)
            return run_id
        except Exception as exc:
            raise AirflowException(f"Secondary notebook execution failed: {exc}") from exc

    @task(task_id="terminal_branch_dummy", retries=0)
    def terminal_branch_dummy(_):
        """Terminal dummy task representing the end of a branch."""
        logging.info("Terminal branch dummy executed.")
        return "terminal_dummy_done"

    @task(task_id="intermediate_dummy_2", retries=0)
    def intermediate_dummy_2(_):
        """Second intermediate dummy task after secondary notebook."""
        logging.info("Intermediate dummy 2 executed.")
        return "dummy2_done"

    @task(task_id="pipeline_completion", retries=0)
    def pipeline_completion(_):
        """Final task marking pipeline completion."""
        logging.info("Pipeline completed successfully.")
        return "completed"

    # Define task pipeline
    start = start_pipeline()
    primary = execute_primary_notebook(start)
    dummy1 = intermediate_dummy_1(primary)
    branch_decision = determine_branch_path(dummy1)

    # Parallel branch tasks
    terminal_dummy = terminal_branch_dummy(branch_decision)
    secondary = execute_secondary_notebook(branch_decision)

    dummy2 = intermediate_dummy_2(secondary)

    completion = pipeline_completion(dummy2)

    # Set explicit dependencies for clarity (optional due to upstream chaining)
    start >> primary >> dummy1 >> branch_decision
    branch_decision >> terminal_dummy
    branch_decision >> secondary >> dummy2 >> completion

    # Ensure terminal_dummy does not affect downstream flow
    terminal_dummy >> completion  # optional: can be omitted if not required

# End of DAG definition.