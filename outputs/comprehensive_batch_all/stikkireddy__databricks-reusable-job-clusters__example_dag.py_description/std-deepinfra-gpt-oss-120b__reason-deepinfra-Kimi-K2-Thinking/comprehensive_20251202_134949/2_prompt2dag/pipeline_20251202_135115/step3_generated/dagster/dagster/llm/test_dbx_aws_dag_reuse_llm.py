# Generated by Dagster Pipeline Generator
# Pipeline: test_dbx_aws_dag_reuse
# Description: Comprehensive pipeline that orchestrates Databricks notebook executions with conditional branching and reusable clusters. Manual trigger only.
# Generation date: 2024-06-10

from dagster import (
    op,
    job,
    In,
    Out,
    RetryPolicy,
    ResourceDefinition,
    InProcessExecutor,
    fs_io_manager,
    ConfigurableResource,
    InitResourceContext,
)


class DatabricksAPI(ConfigurableResource):
    """Placeholder resource for interacting with Databricks Workspace API."""

    def run_notebook(self, notebook_path: str, parameters: dict | None = None) -> str:
        """Simulate running a Databricks notebook.

        Args:
            notebook_path: Path to the notebook in Databricks.
            parameters: Optional dictionary of notebook parameters.

        Returns:
            A string representing the notebook execution result.
        """
        # In a real implementation, this would call the Databricks REST API.
        return f"Executed notebook {notebook_path} with params {parameters or {}}"  # pragma: no cover


def databricks_api_resource(init_context: InitResourceContext) -> DatabricksAPI:
    """Factory for the DatabricksAPI resource."""
    return DatabricksAPI()


@op(
    name="start_pipeline",
    description="Initial step that marks the start of the pipeline execution.",
    out=Out(str),
    retry_policy=RetryPolicy(max_retries=0),
)
def start_pipeline(context) -> str:
    context.log.info("Pipeline started.")
    return "pipeline_started"


@op(
    name="execute_primary_notebook",
    description="Executes the primary Databricks notebook.",
    ins={"start_token": In(str)},
    out=Out(str),
    retry_policy=RetryPolicy(max_retries=1),
)
def execute_primary_notebook(context, start_token: str) -> str:
    context.log.info(f"Running primary notebook, received token: {start_token}")
    result = context.resources.databricks_api.run_notebook(
        notebook_path="/Workspace/PrimaryNotebook", parameters={"token": start_token}
    )
    context.log.info(f"Primary notebook result: {result}")
    return result


@op(
    name="intermediate_dummy_1",
    description="Placeholder operation after primary notebook execution.",
    ins={"primary_result": In(str)},
    out=Out(str),
    retry_policy=RetryPolicy(max_retries=0),
)
def intermediate_dummy_1(context, primary_result: str) -> str:
    context.log.info(f"Intermediate dummy 1 received: {primary_result}")
    return "dummy_1_completed"


@op(
    name="determine_branch_path",
    description="Determines which branch to follow based on prior results.",
    ins={"dummy_input": In(str)},
    out=Out(bool),
    retry_policy=RetryPolicy(max_retries=0),
)
def determine_branch_path(context, dummy_input: str) -> bool:
    context.log.info(f"Evaluating branch decision from: {dummy_input}")
    # Simple deterministic decision for illustration; replace with real logic as needed.
    decision = True
    context.log.info(f"Branch decision: {'secondary' if decision else 'terminal'}")
    return decision


@op(
    name="terminal_branch_dummy",
    description="Terminal dummy operation for the terminal branch.",
    ins={"branch_decision": In(bool)},
    out=Out(str),
    retry_policy=RetryPolicy(max_retries=0),
)
def terminal_branch_dummy(context, branch_decision: bool) -> str:
    context.log.info(f"Terminal branch dummy executed. Decision was: {branch_decision}")
    return "terminal_branch_completed"


@op(
    name="execute_secondary_notebook",
    description="Executes the secondary Databricks notebook when the branch decision is true.",
    ins={"branch_decision": In(bool)},
    out=Out(str),
    retry_policy=RetryPolicy(max_retries=1),
)
def execute_secondary_notebook(context, branch_decision: bool) -> str:
    if not branch_decision:
        context.log.info("Branch decision false; skipping secondary notebook.")
        return "secondary_not_executed"
    context.log.info("Running secondary notebook as branch decision is true.")
    result = context.resources.databricks_api.run_notebook(
        notebook_path="/Workspace/SecondaryNotebook", parameters={}
    )
    context.log.info(f"Secondary notebook result: {result}")
    return result


@op(
    name="intermediate_dummy_2",
    description="Placeholder operation after secondary notebook execution.",
    ins={"secondary_result": In(str)},
    out=Out(str),
    retry_policy=RetryPolicy(max_retries=0),
)
def intermediate_dummy_2(context, secondary_result: str) -> str:
    context.log.info(f"Intermediate dummy 2 received: {secondary_result}")
    return "dummy_2_completed"


@op(
    name="pipeline_completion",
    description="Final step marking the successful completion of the pipeline.",
    ins={"final_input": In(str)},
    out=Out(str),
    retry_policy=RetryPolicy(max_retries=0),
)
def pipeline_completion(context, final_input: str) -> str:
    context.log.info(f"Pipeline completed with final input: {final_input}")
    return "pipeline_successful"


@job(
    name="test_dbx_aws_dag_reuse",
    description="Comprehensive pipeline that orchestrates Databricks notebook executions with conditional branching and reusable clusters. Manual trigger only.",
    executor_def=InProcessExecutor(),
    resource_defs={
        "databricks_api": ResourceDefinition.resource_fn(databricks_api_resource),
        "io_manager": fs_io_manager,
    },
)
def test_dbx_aws_dag_reuse():
    """Dagster job implementing the specified fanâ€‘out pipeline."""
    start = start_pipeline()
    primary = execute_primary_notebook(start)
    dummy1 = intermediate_dummy_1(primary)
    branch_decision = determine_branch_path(dummy1)

    # Branches
    terminal = terminal_branch_dummy(branch_decision)
    secondary = execute_secondary_notebook(branch_decision)

    # Continue only after secondary branch (as per specification)
    dummy2 = intermediate_dummy_2(secondary)
    pipeline_completion(dummy2)

    # Note: `terminal` is intentionally not connected downstream, representing a terminal branch.