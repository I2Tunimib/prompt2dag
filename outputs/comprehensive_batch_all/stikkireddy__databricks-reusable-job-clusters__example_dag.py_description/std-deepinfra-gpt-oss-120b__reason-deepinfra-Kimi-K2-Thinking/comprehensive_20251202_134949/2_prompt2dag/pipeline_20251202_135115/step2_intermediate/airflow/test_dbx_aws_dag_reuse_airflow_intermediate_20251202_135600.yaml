metadata:
  target_orchestrator: airflow
  generated_at: 2025-12-02 13:56:00.392856
  source_analysis_file: 
    Pipeline_Description_Dataset/stikkireddy__databricks-reusable-job-clusters__example_dag.py_description.txt
  pipeline_name: test_dbx_aws_dag_reuse
  pipeline_description: Comprehensive pipeline that orchestrates Databricks notebook executions with conditional 
    branching and reusable clusters. Manual trigger only.
  orchestrator_specific: {}
schedule:
  enabled: false
  schedule_expression:
  start_date: '2023-06-06T00:00:00Z'
  end_date:
  timezone: UTC
  catchup: false
connections:
  - conn_id: databricks_api
    conn_type: http
    description: Databricks Workspace API
    config:
      base_path:
      base_url: https://<databricks-instance>.cloud.databricks.com
      host: <databricks-instance>.cloud.databricks.com
      port:
      protocol: https
      database:
      schema:
      bucket:
      queue_name:
      extra:
        token_env_var: DATABRICKS_TOKEN
tasks:
  - task_id: start_pipeline
    task_name: Start Pipeline
    operator_class: PythonOperator
    operator_module: airflow.operators.python
    component_ref: start_pipeline
    config: {}
    upstream_task_ids: []
    trigger_rule: all_success
    retries: 0
    retry_delay_seconds: 0
    validation_warnings: []
  - task_id: execute_primary_notebook
    task_name: Execute Primary Databricks Notebook
    operator_class: PythonOperator
    operator_module: airflow.operators.python
    component_ref: execute_primary_notebook
    config:
      retry_delay: timedelta(seconds=300)
    upstream_task_ids:
      - start_pipeline
    trigger_rule: all_success
    retries: 1
    retry_delay_seconds: 300
    validation_warnings: []
  - task_id: intermediate_dummy_1
    task_name: Intermediate Dummy 1
    operator_class: PythonOperator
    operator_module: airflow.operators.python
    component_ref: intermediate_dummy_1
    config: {}
    upstream_task_ids:
      - execute_primary_notebook
    trigger_rule: all_success
    retries: 0
    retry_delay_seconds: 0
    validation_warnings: []
  - task_id: determine_branch_path
    task_name: Determine Branch Path
    operator_class: PythonOperator
    operator_module: airflow.operators.python
    component_ref: determine_branch_path
    config:
      python_callable: branch_func
    upstream_task_ids:
      - intermediate_dummy_1
    trigger_rule: all_success
    retries: 0
    retry_delay_seconds: 0
    validation_warnings: []
  - task_id: terminal_branch_dummy
    task_name: Terminal Branch Dummy
    operator_class: PythonOperator
    operator_module: airflow.operators.python
    component_ref: terminal_branch_dummy
    config: {}
    upstream_task_ids:
      - determine_branch_path
    trigger_rule: all_success
    retries: 0
    retry_delay_seconds: 0
    validation_warnings: []
  - task_id: execute_secondary_notebook
    task_name: Execute Secondary Databricks Notebook
    operator_class: PythonOperator
    operator_module: airflow.operators.python
    component_ref: execute_secondary_notebook
    config:
      retry_delay: timedelta(seconds=300)
    upstream_task_ids:
      - determine_branch_path
    trigger_rule: all_success
    retries: 1
    retry_delay_seconds: 300
    validation_warnings: []
  - task_id: intermediate_dummy_2
    task_name: Intermediate Dummy 2
    operator_class: PythonOperator
    operator_module: airflow.operators.python
    component_ref: intermediate_dummy_2
    config: {}
    upstream_task_ids:
      - execute_secondary_notebook
    trigger_rule: all_success
    retries: 0
    retry_delay_seconds: 0
    validation_warnings: []
  - task_id: pipeline_completion
    task_name: Pipeline Completion
    operator_class: PythonOperator
    operator_module: airflow.operators.python
    component_ref: pipeline_completion
    config: {}
    upstream_task_ids:
      - intermediate_dummy_2
    trigger_rule: all_success
    retries: 0
    retry_delay_seconds: 0
    validation_warnings: []
