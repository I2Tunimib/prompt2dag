# Generated by Dagster Code Generator
# Generation Metadata:
# - Job Name: initialize_pipeline_pipeline
# - Description: No description provided.
# - Executor Type: docker_executor
# - IO Manager: fs_io_manager
# - Dagster Version: 1.5.0
# - Use Assets: False
# - Required Resources: ['databricks_default']

from dagster import job, op, Out, In, RetryPolicy, in_process_executor, docker_executor, fs_io_manager, resource

# Define the required resources
@resource
def databricks_default():
    """
    Databricks Default Connection
    """
    return None

# Define the ops
@op(out=Out())
def initialize_pipeline(context):
    """
    Initialize Pipeline
    """
    context.log.info("Initializing pipeline")
    return "initialized"

@op(
    retry_policy=RetryPolicy(max_retries=1),
    ins={"pipeline_status": In()},
    out=Out()
)
def execute_primary_notebook(context, pipeline_status):
    """
    Execute Primary Notebook
    """
    context.log.info("Executing primary notebook")
    return "primary_notebook_executed"

@op(
    ins={"notebook_status": In()},
    out=Out()
)
def intermediate_step_1(context, notebook_status):
    """
    Intermediate Step 1
    """
    context.log.info("Executing intermediate step 1")
    return "intermediate_step_1_executed"

@op(
    ins={"step_status": In()},
    out=Out()
)
def branch_decision(context, step_status):
    """
    Branch Decision
    """
    context.log.info("Making branch decision")
    return "branch_decision_made"

@op(
    retry_policy=RetryPolicy(max_retries=1),
    ins={"decision_status": In()},
    out=Out()
)
def execute_secondary_notebook(context, decision_status):
    """
    Execute Secondary Notebook
    """
    context.log.info("Executing secondary notebook")
    return "secondary_notebook_executed"

@op(
    ins={"notebook_status": In()},
    out=Out()
)
def terminal_branch_path_1(context, notebook_status):
    """
    Terminal Branch Path 1
    """
    context.log.info("Executing terminal branch path 1")
    return "terminal_branch_path_1_executed"

@op(
    ins={"notebook_status": In()},
    out=Out()
)
def intermediate_step_2(context, notebook_status):
    """
    Intermediate Step 2
    """
    context.log.info("Executing intermediate step 2")
    return "intermediate_step_2_executed"

@op(
    ins={"step_status": In()},
    out=Out()
)
def pipeline_completion(context, step_status):
    """
    Pipeline Completion
    """
    context.log.info("Pipeline completed")
    return "pipeline_completed"

# Define the job
@job(
    name="initialize_pipeline_pipeline",
    description="No description provided.",
    executor_def=docker_executor,
    resource_defs={"databricks_default": databricks_default},
    io_manager_def=fs_io_manager
)
def initialize_pipeline_pipeline():
    pipeline_status = initialize_pipeline()
    primary_notebook_status = execute_primary_notebook(pipeline_status)
    intermediate_step_1_status = intermediate_step_1(primary_notebook_status)
    branch_decision_status = branch_decision(intermediate_step_1_status)
    
    terminal_branch_path_1_status = terminal_branch_path_1(branch_decision_status)
    secondary_notebook_status = execute_secondary_notebook(branch_decision_status)
    
    intermediate_step_2_status = intermediate_step_2(secondary_notebook_status)
    pipeline_completion(intermediate_step_2_status)