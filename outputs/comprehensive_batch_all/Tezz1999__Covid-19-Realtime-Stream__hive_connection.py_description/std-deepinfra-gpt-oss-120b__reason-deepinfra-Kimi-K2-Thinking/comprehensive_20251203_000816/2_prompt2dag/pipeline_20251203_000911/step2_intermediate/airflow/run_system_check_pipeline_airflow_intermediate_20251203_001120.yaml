metadata:
  target_orchestrator: airflow
  generated_at: 2025-12-03 00:11:20.651812
  source_analysis_file: 
    Pipeline_Description_Dataset/Tezz1999__Covid-19-Realtime-Stream__hive_connection.py_description.txt
  pipeline_name: run_system_check_pipeline
  pipeline_description: This is a simple linear data pipeline that executes Hive database operations for COVID-19 
    realtime streaming data. The pipeline follows a sequential topology pattern with two tasks executing in strict 
    order. Key infrastructure features include Hive database connectivity and scheduled daily execution at 1:00 AM.
  orchestrator_specific: {}
schedule:
  enabled: true
  schedule_expression: 00 1 * * *
  start_date:
  end_date:
  timezone: UTC
  catchup: false
connections:
  - conn_id: hive_local_conn
    conn_type: generic
    description: Hive Local Connection
    config:
      base_path:
      base_url:
      host: localhost
      port:
      protocol: hive
      database: mydb
      schema:
      bucket:
      queue_name:
tasks:
  - task_id: run_system_check
    task_name: Run System Check
    operator_class: BashOperator
    operator_module: airflow.operators.bash
    component_ref: run_system_check
    config:
      bash_command:
        - bash
        - -c
        - echo `whoami`
    upstream_task_ids: []
    trigger_rule: all_success
    retries: 0
    retry_delay_seconds: 0
    validation_warnings: []
  - task_id: execute_hive_script
    task_name: Execute Hive Script
    operator_class: SQLExecuteQueryOperator
    operator_module: airflow.providers.common.sql.operators.sql
    component_ref: execute_hive_script
    config: {}
    upstream_task_ids:
      - run_system_check
    trigger_rule: all_success
    retries: 0
    retry_delay_seconds: 0
    validation_warnings: []
