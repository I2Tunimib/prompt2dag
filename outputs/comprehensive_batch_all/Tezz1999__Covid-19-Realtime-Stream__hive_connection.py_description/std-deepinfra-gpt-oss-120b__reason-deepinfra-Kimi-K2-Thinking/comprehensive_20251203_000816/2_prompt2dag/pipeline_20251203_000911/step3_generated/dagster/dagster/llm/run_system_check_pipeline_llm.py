# Generated by Dagster code generator on 2024-06-28
# Pipeline: run_system_check_pipeline
# Description: Linear pipeline executing Hive operations for COVID-19 realtime streaming data.
# Executor: in_process_executor
# Resources: hive_local (Hive Local Connection)
# Schedule: Daily at 01:00 UTC (cron: 00 1 * * *)

from dagster import (
    op,
    job,
    In,
    Out,
    ResourceDefinition,
    ScheduleDefinition,
    DefaultScheduleStatus,
    fs_io_manager,
    in_process_executor,
    get_dagster_logger,
)
from typing import Any


# -------------------------------------------------------------------------
# Resource Definitions
# -------------------------------------------------------------------------

def hive_local_resource(init_context) -> Any:
    """
    Placeholder resource for a Hive local connection.

    In a production environment, replace this stub with an actual Hive client
    (e.g., PyHive, hivecli, etc.) and configure connection parameters via
    environment variables or secrets management.
    """
    class HiveConnection:
        def __init__(self):
            self.connected = False

        def connect(self):
            # Simulate establishing a connection
            self.connected = True
            get_dagster_logger().info("Hive connection established.")

        def execute(self, script: str):
            if not self.connected:
                raise RuntimeError("Hive connection not established.")
            # Placeholder for script execution logic
            get_dagster_logger().info(f"Executing Hive script:\n{script}")

        def close(self):
            self.connected = False
            get_dagster_logger().info("Hive connection closed.")

    conn = HiveConnection()
    conn.connect()
    try:
        yield conn
    finally:
        conn.close()


# -------------------------------------------------------------------------
# Op Definitions
# -------------------------------------------------------------------------

@op(
    name="run_system_check",
    description="Performs a lightweight system health check before Hive operations.",
    required_resource_keys=set(),
    out=Out(str),
    retry_policy=None,
)
def run_system_check(context) -> str:
    """
    Simple health check op. In a real deployment this could verify
    connectivity, required services, or data availability.
    """
    logger = get_dagster_logger()
    logger.info("Running system health check...")
    # Placeholder logic; always succeeds
    result = "system_check_passed"
    logger.info(f"System check result: {result}")
    return result


@op(
    name="execute_hive_script",
    description="Executes a Hive script using the provided Hive connection.",
    required_resource_keys={"hive_local"},
    ins={"system_check": In(str)},
    out=Out(str),
    retry_policy=None,
)
def execute_hive_script(context, system_check: str) -> str:
    """
    Executes a Hive script after confirming the system check passed.

    Args:
        system_check (str): Output from the ``run_system_check`` op.

    Returns:
        str: Confirmation message after script execution.
    """
    logger = get_dagster_logger()
    if system_check != "system_check_passed":
        raise RuntimeError("System check failed; aborting Hive script execution.")

    hive_conn = context.resources.hive_local
    hive_script = """
    -- Example Hive script for COVID-19 realtime streaming data
    CREATE DATABASE IF NOT EXISTS covid19_streaming;
    USE covid19_streaming;
    -- Additional DDL/DML statements would go here
    """
    logger.info("Executing Hive script...")
    hive_conn.execute(hive_script)
    logger.info("Hive script execution completed.")
    return "hive_script_executed"


# -------------------------------------------------------------------------
# Job Definition
# -------------------------------------------------------------------------

@job(
    name="run_system_check_pipeline",
    description=(
        "This is a simple linear data pipeline that executes Hive database operations "
        "for COVID-19 realtime streaming data. The pipeline follows a sequential topology "
        "pattern with two tasks executing in strict order."
    ),
    executor_def=in_process_executor,
    resource_defs={
        "hive_local": ResourceDefinition.hardcoded_resource(hive_local_resource),
        "io_manager": fs_io_manager,
    },
)
def run_system_check_job():
    """
    Dagster job orchestrating the system check followed by Hive script execution.
    """
    check_result = run_system_check()
    execute_hive_script(system_check=check_result)


# -------------------------------------------------------------------------
# Schedule Definition
# -------------------------------------------------------------------------

run_system_check_schedule = ScheduleDefinition(
    job=run_system_check_job,
    cron_schedule="00 1 * * *",  # Daily at 01:00 UTC
    execution_timezone="UTC",
    default_status=DefaultScheduleStatus.RUNNING,
    description="Daily schedule for the run_system_check_pipeline at 01:00 UTC.",
)

# Export symbols for Dagster discovery
__all__ = [
    "run_system_check_job",
    "run_system_check_schedule",
    "hive_local_resource",
]