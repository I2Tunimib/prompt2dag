# Generated by Airflow DAG Generator on 2024-06-28
# Description: DAG for running a system check followed by a Hive script execution.
# This DAG is scheduled daily at 01:00 UTC and does not perform catchup.

from datetime import timedelta
import pendulum

from airflow import DAG
from airflow.utils.dates import days_ago
from airflow.operators.bash import BashOperator
from airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator
from airflow.utils.email import send_email

# ----------------------------------------------------------------------
# Helper functions
# ----------------------------------------------------------------------
def failure_callback(context):
    """
    Sends an email notification on task failure.
    """
    dag_id = context.get('dag').dag_id
    task_id = context.get('task_instance').task_id
    execution_date = context.get('execution_date')
    log_url = context.get('task_instance').log_url

    subject = f"[Airflow] DAG {dag_id} - Task {task_id} Failed"
    html_content = f"""
    <p>Task <strong>{task_id}</strong> in DAG <strong>{dag_id}</strong> failed.</p>
    <p><strong>Execution date:</strong> {execution_date}</p>
    <p><strong>Log URL:</strong> <a href="{log_url}">{log_url}</a></p>
    """

    # Adjust the recipient list as needed
    send_email(to=["airflow-alerts@example.com"], subject=subject, html_content=html_content)


# ----------------------------------------------------------------------
# Default arguments applied to all tasks
# ----------------------------------------------------------------------
default_args = {
    "owner": "airflow",
    "depends_on_past": False,
    "email_on_failure": False,
    "email_on_retry": False,
    "retries": 0,                     # No retries as per specification
    "retry_delay": timedelta(minutes=5),
    "on_failure_callback": failure_callback,
}

# ----------------------------------------------------------------------
# DAG definition
# ----------------------------------------------------------------------
with DAG(
    dag_id="run_system_check_pipeline",
    description="Sequential pipeline executing a system check and a Hive script for COVID-19 realtime data.",
    schedule_interval="00 1 * * *",   # Daily at 01:00 UTC
    start_date=days_ago(1),           # Reasonable start date; will be overridden by catchup=False
    catchup=False,
    default_args=default_args,
    tags=["system_check", "hive", "covid19"],
    max_active_runs=1,
    timezone=pendulum.timezone("UTC"),
) as dag:

    # ------------------------------------------------------------------
    # Task: Run System Check
    # ------------------------------------------------------------------
    run_system_check = BashOperator(
        task_id="run_system_check",
        bash_command='echo "Running system check for COVID-19 realtime streaming pipeline..."',
        retries=0,                     # Explicitly set to 0 per task spec
    )

    # ------------------------------------------------------------------
    # Task: Execute Hive Script
    # ------------------------------------------------------------------
    execute_hive_script = SQLExecuteQueryOperator(
        task_id="execute_hive_script",
        conn_id="hive_local_conn",     # Hive connection defined in Airflow UI
        sql="""
        -- Example Hive query; replace with actual script as needed
        SELECT COUNT(*) AS record_count
        FROM covid19_realtime_streaming;
        """,
        retries=0,                     # Explicitly set to 0 per task spec
    )

    # ------------------------------------------------------------------
    # Define task dependencies (sequential execution)
    # ------------------------------------------------------------------
    run_system_check >> execute_hive_script