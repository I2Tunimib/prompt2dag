# Generated by Airflow DAG Generator
# Date: 2024-06-28
# Pipeline: run_system_check_pipeline
# Description: Sequential pipeline executing a system check followed by a Hive script.

from datetime import datetime, timedelta
import logging

from airflow import DAG
from airflow.utils.dates import days_ago
from airflow.utils.timezone import timezone
from airflow.operators.bash import BashOperator
from airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator
from airflow.models import Variable

# ----------------------------------------------------------------------
# Default arguments for the DAG
# ----------------------------------------------------------------------
default_args = {
    "owner": "airflow",
    "depends_on_past": False,
    "email_on_failure": False,
    "email_on_retry": False,
    "retries": 0,                     # No retries as per specification
    "retry_delay": timedelta(minutes=5),
    "start_date": days_ago(1),        # Ensures the DAG can be triggered immediately
}

# ----------------------------------------------------------------------
# Failure callback for better observability
# ----------------------------------------------------------------------
def task_failure_callback(context):
    """Log task failure details."""
    dag_id = context.get("dag").dag_id
    task_id = context.get("task_instance").task_id
    execution_date = context.get("execution_date")
    log_msg = (
        f"Task failed: dag_id={dag_id}, task_id={task_id}, "
        f"execution_date={execution_date}"
    )
    logging.error(log_msg)

# ----------------------------------------------------------------------
# DAG definition
# ----------------------------------------------------------------------
with DAG(
    dag_id="run_system_check_pipeline",
    description="No description provided.",
    schedule_interval="00 1 * * *",          # Cron expression: 01:00 UTC daily
    catchup=False,
    default_args=default_args,
    tags=["system_check", "hive"],
    on_failure_callback=task_failure_callback,
    timezone=timezone("UTC"),
) as dag:

    # ------------------------------------------------------------------
    # Task: Run System Check
    # ------------------------------------------------------------------
    run_system_check = BashOperator(
        task_id="run_system_check",
        bash_command="echo 'Running system check...'; sleep 5; echo 'System check completed.'",
        retries=0,
        on_failure_callback=task_failure_callback,
    )

    # ------------------------------------------------------------------
    # Task: Run Hive Script
    # ------------------------------------------------------------------
    # The Hive script can be stored in a Variable or a file; here we use a simple query.
    hive_query = Variable.get("hive_system_check_query", default_var="SELECT 1;")

    run_hive_script = SQLExecuteQueryOperator(
        task_id="run_hive_script",
        conn_id="hive_local_conn",          # Hive connection defined in Airflow UI
        sql=hive_query,
        retries=0,
        on_failure_callback=task_failure_callback,
    )

    # ------------------------------------------------------------------
    # Set task dependencies (sequential execution)
    # ------------------------------------------------------------------
    run_system_check >> run_hive_script

# End of DAG definition.