# Generated by Airflow DAG Generator
# Date: 2024-06-13
# Description: Simple linear data pipeline that executes Hive database operations for COVID-19 realtime streaming data.

import logging
from datetime import timedelta

import pendulum
from airflow import DAG
from airflow.utils.dates import days_ago
from airflow.operators.bash import BashOperator
from airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator
from airflow.utils.email import send_email

# ----------------------------------------------------------------------
# Default arguments applied to all tasks unless overridden
# ----------------------------------------------------------------------
default_args = {
    "owner": "airflow",
    "depends_on_past": False,
    "email_on_failure": False,
    "email_on_retry": False,
    "retries": 0,                     # Global default – overridden per task if needed
    "retry_delay": timedelta(minutes=5),
    "start_date": days_ago(1),
}

# ----------------------------------------------------------------------
# Failure callback – logs the failure and optionally sends an alert
# ----------------------------------------------------------------------
def task_failure_callback(context):
    """Callback executed when a task fails."""
    dag_id = context.get("dag").dag_id
    task_id = context.get("task_instance").task_id
    execution_date = context.get("execution_date")
    log_msg = (
        f"Task failed: dag_id={dag_id}, task_id={task_id}, "
        f"execution_date={execution_date}"
    )
    logging.error(log_msg)

    # Example: send an email alert (requires proper email config in Airflow)
    # send_email(
    #     to=["alert@example.com"],
    #     subject=f"[Airflow] Task failure in {dag_id}",
    #     html_content=log_msg,
    # )

# ----------------------------------------------------------------------
# DAG definition
# ----------------------------------------------------------------------
with DAG(
    dag_id="pipeline_name",                                 # Replace with actual pipeline name if needed
    description="Simple linear data pipeline that executes Hive database operations for COVID-19 realtime streaming data.",
    schedule_interval="00 1 * * *",                         # Cron expression: run daily at 01:00 UTC
    catchup=False,
    default_args=default_args,
    tags=["example", "hive", "covid19"],
    max_active_runs=1,
    timezone=pendulum.timezone("UTC"),
) as dag:

    # ------------------------------------------------------------------
    # Task: Run System Check
    # ------------------------------------------------------------------
    run_system_check = BashOperator(
        task_id="run_system_check",
        bash_command="echo 'System check passed.'",
        retries=0,
        on_failure_callback=task_failure_callback,
    )

    # ------------------------------------------------------------------
    # Task: Run Hive Script
    # ------------------------------------------------------------------
    # This task uses the generic SQLExecuteQueryOperator with the Hive connection.
    # Replace the SQL statement with the actual Hive script you need to execute.
    run_hive_script = SQLExecuteQueryOperator(
        task_id="run_hive_script",
        sql="""
            -- Example Hive query; replace with your actual script
            SELECT COUNT(*) FROM covid19_realtime;
        """,
        conn_id="hive_local",          # Hive connection defined in Airflow UI
        retries=0,
        on_failure_callback=task_failure_callback,
    )

    # ------------------------------------------------------------------
    # Define task dependencies (sequential execution)
    # ------------------------------------------------------------------
    run_system_check >> run_hive_script

# End of DAG definition.