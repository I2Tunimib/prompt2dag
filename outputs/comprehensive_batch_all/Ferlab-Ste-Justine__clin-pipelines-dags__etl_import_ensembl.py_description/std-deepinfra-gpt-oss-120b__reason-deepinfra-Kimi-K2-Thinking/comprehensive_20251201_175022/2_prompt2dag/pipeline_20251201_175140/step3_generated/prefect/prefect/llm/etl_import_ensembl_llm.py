# Generated by Prefect Pipeline Generator
# Pipeline: etl_import_ensembl
# Description: Comprehensive Pipeline Description
# Prefect version: 2.14.0
# Task runner: SequentialTaskRunner
# Schedule: disabled

import os
import subprocess
import tempfile
from pathlib import Path

from prefect import flow, task, get_run_logger
from prefect.task_runners import SequentialTaskRunner
from prefect.exceptions import PrefectException

# Prefect blocks
from prefect.blocks.system import Secret
from prefect_aws.s3 import S3Bucket

# ----------------------------------------------------------------------
# Helper functions
# ----------------------------------------------------------------------


def _load_secret(secret_name: str) -> str:
    """
    Retrieve a secret value from a Prefect Secret block.

    Args:
        secret_name: Name of the Secret block.

    Returns:
        The secret value as a string.

    Raises:
        PrefectException: If the secret cannot be loaded.
    """
    try:
        secret_block = Secret.load(secret_name)
        return secret_block.get()
    except Exception as exc:
        raise PrefectException(f"Failed to load secret '{secret_name}': {exc}") from exc


def _upload_to_s3(bucket_name: str, local_path: Path, s3_key: str) -> None:
    """
    Upload a local file or directory to an S3 bucket.

    Args:
        bucket_name: Name of the S3 bucket block.
        local_path: Path to the local file or directory.
        s3_key: Destination key (prefix) in the bucket.

    Raises:
        PrefectException: If the upload fails.
    """
    try:
        bucket = S3Bucket.load(bucket_name)
        if local_path.is_dir():
            for file_path in local_path.rglob("*"):
                if file_path.is_file():
                    relative_path = file_path.relative_to(local_path)
                    destination_key = f"{s3_key}/{relative_path}"
                    bucket.upload_from_path(
                        from_path=str(file_path), to_path=destination_key
                    )
        else:
            bucket.upload_from_path(
                from_path=str(local_path), to_path=f"{s3_key}/{local_path.name}"
            )
    except Exception as exc:
        raise PrefectException(f"S3 upload failed: {exc}") from exc


def _run_subprocess(command: list[str]) -> None:
    """
    Execute a command via subprocess, raising an exception on failure.

    Args:
        command: List of command arguments.

    Raises:
        PrefectException: If the subprocess exits with a nonâ€‘zero status.
    """
    logger = get_run_logger()
    logger.info(f"Running command: {' '.join(command)}")
    try:
        result = subprocess.run(
            command, check=True, capture_output=True, text=True
        )
        logger.debug(f"STDOUT: {result.stdout}")
        logger.debug(f"STDERR: {result.stderr}")
    except subprocess.CalledProcessError as exc:
        logger.error(f"Command failed with exit code {exc.returncode}")
        logger.error(f"STDOUT: {exc.stdout}")
        logger.error(f"STDERR: {exc.stderr}")
        raise PrefectException(f"Subprocess failed: {exc}") from exc


# ----------------------------------------------------------------------
# Tasks
# ----------------------------------------------------------------------


@task(retries=0, name="Extract Ensembl Mapping Files")
def extract_ensembl_files(
    ftp_secret_name: str = "ensembl_ftp",
    s3_bucket_name: str = "s3_data_lake",
    s3_prefix: str = "ensembl/mapping",
) -> Path:
    """
    Connect to the Ensembl FTP server, download mapping files, and upload them to S3.

    Returns:
        Path to the temporary directory containing the downloaded files.

    Raises:
        PrefectException: If any step of the extraction fails.
    """
    logger = get_run_logger()
    logger.info("Starting extraction of Ensembl mapping files.")

    # Load FTP credentials
    ftp_credentials = _load_secret(ftp_secret_name)
    # Expected format: user:password@host
    try:
        user_pass, host = ftp_credentials.split("@")
        user, password = user_pass.split(":")
    except Exception as exc:
        raise PrefectException(
            "FTP secret must be in the format 'user:password@host'"
        ) from exc

    # Create a temporary directory for downloads
    temp_dir = Path(tempfile.mkdtemp(prefix="ensembl_"))
    logger.debug(f"Created temporary directory at {temp_dir}")

    # Example: use wget to download files (placeholder logic)
    # In a real implementation, you might use ftplib or a more robust solution.
    ftp_url = f"ftp://{host}"
    download_command = [
        "wget",
        "--recursive",
        "--no-parent",
        "--user",
        user,
        "--password",
        password,
        "-P",
        str(temp_dir),
        ftp_url,
    ]
    _run_subprocess(download_command)

    # Upload to S3
    _upload_to_s3(s3_bucket_name, temp_dir, s3_prefix)

    logger.info("Extraction and upload completed successfully.")
    return temp_dir


@task(retries=0, name="Transform Ensembl Mapping with Spark")
def transform_ensembl_mapping(
    input_path: Path,
    spark_submit_path: str = "/opt/spark/bin/spark-submit",
    spark_app: str = "transform_ensembl_mapping.py",
    s3_bucket_name: str = "s3_data_lake",
    s3_output_prefix: str = "ensembl/transformed",
) -> None:
    """
    Run a Spark job to transform the extracted Ensembl mapping files.

    Args:
        input_path: Path to the directory containing the raw mapping files.
        spark_submit_path: Path to the spark-submit executable.
        spark_app: Path to the Spark application script.
        s3_bucket_name: S3 bucket where transformed data will be stored.
        s3_output_prefix: Destination prefix for transformed data.

    Raises:
        PrefectException: If the Spark job fails.
    """
    logger = get_run_logger()
    logger.info("Starting Spark transformation of Ensembl mapping files.")

    # Build the Spark submit command
    command = [
        spark_submit_path,
        "--master",
        "local[*]",
        "--conf",
        f"spark.hadoop.fs.s3a.bucket.{s3_bucket_name}=s3://{s3_bucket_name}",
        spark_app,
        "--input",
        str(input_path),
        "--output",
        f"s3://{s3_bucket_name}/{s3_output_prefix}",
    ]

    _run_subprocess(command)

    logger.info("Spark transformation completed successfully.")


# ----------------------------------------------------------------------
# Flow
# ----------------------------------------------------------------------


@flow(
    name="etl_import_ensembl",
    task_runner=SequentialTaskRunner(),
)
def etl_import_ensembl() -> None:
    """
    ETL pipeline that extracts Ensembl mapping files from FTP,
    uploads them to an S3 data lake, and transforms them using Spark.

    The flow follows a sequential pattern:
        1. extract_ensembl_files
        2. transform_ensembl_mapping (depends on extraction)
    """
    logger = get_run_logger()
    logger.info("ETL pipeline started.")

    # Step 1: Extraction
    extracted_path = extract_ensembl_files()

    # Step 2: Transformation (depends on extraction)
    transform_ensembl_mapping(input_path=extracted_path)

    logger.info("ETL pipeline completed successfully.")


# ----------------------------------------------------------------------
# Entry point
# ----------------------------------------------------------------------


if __name__ == "__main__":
    # Running the flow directly (useful for local testing)
    etl_import_ensembl()