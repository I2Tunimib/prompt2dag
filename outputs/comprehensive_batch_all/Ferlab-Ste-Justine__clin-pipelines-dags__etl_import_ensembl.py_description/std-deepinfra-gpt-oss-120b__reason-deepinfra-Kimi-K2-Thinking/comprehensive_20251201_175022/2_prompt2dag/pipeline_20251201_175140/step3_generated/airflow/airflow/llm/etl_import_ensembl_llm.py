# Generated by Airflow DAG generator on 2024-06-13
"""
DAG: etl_import_ensembl
Description: Comprehensive Pipeline Description
Pattern: sequential
"""

from datetime import timedelta

from airflow import DAG
from airflow.utils.dates import days_ago
from airflow.utils.task_group import TaskGroup
from airflow.providers.http.hooks.http import HttpHook
from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator
from airflow.operators.python import get_current_context
from airflow.decorators import task

# ----------------------------------------------------------------------
# Default arguments applied to all tasks
# ----------------------------------------------------------------------
default_args = {
    "owner": "airflow",
    "depends_on_past": False,
    "email_on_failure": False,
    "email_on_retry": False,
    "retries": 0,                     # As per specification
    "retry_delay": timedelta(minutes=5),
    "on_failure_callback": None,      # Will be set per task below
}

# ----------------------------------------------------------------------
# Slack notification helper
# ----------------------------------------------------------------------
def _slack_failure_callback(context):
    """
    Sends a simple failure notification to Slack using the
    ``slack_notifications`` HTTP connection.
    """
    hook = HttpHook(http_conn_id="slack_notifications", method="POST")
    dag_id = context.get("dag").dag_id
    task_id = context.get("task_instance").task_id
    execution_date = context.get("execution_date")
    message = (
        f":x: *Airflow task failed*\n"
        f"*DAG*: `{dag_id}`\n"
        f"*Task*: `{task_id}`\n"
        f"*Execution*: `{execution_date}`"
    )
    payload = {"text": message}
    try:
        hook.run(endpoint="", json=payload, headers={"Content-Type": "application/json"})
    except Exception as exc:  # pragma: no cover
        # Log the exception; Airflow will already mark the task as failed.
        print(f"Failed to send Slack notification: {exc}")

# ----------------------------------------------------------------------
# DAG definition
# ----------------------------------------------------------------------
with DAG(
    dag_id="etl_import_ensembl",
    description="Comprehensive Pipeline Description",
    schedule_interval=None,          # Disabled schedule
    start_date=days_ago(1),
    catchup=False,
    default_args=default_args,
    tags=["etl", "ensembl"],
    render_template_as_native_obj=True,
) as dag:

    # ------------------------------------------------------------------
    # Task: Extract Ensembl Mapping Files
    # ------------------------------------------------------------------
    @task(
        task_id="extract_ensembl_files",
        retries=0,
        retry_delay=timedelta(minutes=5),
        on_failure_callback=_slack_failure_callback,
    )
    def extract_ensembl_files():
        """
        Placeholder implementation for extracting Ensembl mapping files
        from the ``ensembl_ftp`` connection.
        """
        # Example: using HttpHook to list files on the FTP server.
        # In a real implementation you would replace this with proper FTP logic.
        ftp_hook = HttpHook(http_conn_id="ensembl_ftp", method="GET")
        response = ftp_hook.run(endpoint="/pub/release-*/mapping/")
        if response.status_code != 200:
            raise RuntimeError("Failed to list Ensembl mapping files")
        # Simulate download logic
        print("Successfully listed Ensembl mapping files")
        # Return a list of file paths for downstream tasks
        return ["file1.txt", "file2.txt"]

    # ------------------------------------------------------------------
    # Task: Transform Ensembl Mapping with Spark
    # ------------------------------------------------------------------
    transform_ensembl_mapping = SparkSubmitOperator(
        task_id="transform_ensembl_mapping",
        application="/opt/airflow/dags/spark_jobs/transform_ensembl_mapping.py",
        name="transform_ensembl_mapping",
        conn_id="spark_default",
        conf={"spark.master": "k8s://https://kubernetes_etl_cluster"},
        application_args="{{ ti.xcom_pull(task_ids='extract_ensembl_files') }}",
        retries=0,
        retry_delay=timedelta(minutes=5),
        on_failure_callback=_slack_failure_callback,
        # Example of passing S3 bucket via Spark config
        spark_conf={
            "spark.hadoop.fs.s3a.endpoint": "s3.amazonaws.com",
            "spark.hadoop.fs.s3a.aws.credentials.provider": "com.amazonaws.auth.DefaultAWSCredentialsProviderChain",
            "spark.hadoop.fs.s3a.bucket": "s3_data_lake",
        },
    )

    # ------------------------------------------------------------------
    # Set task dependencies (sequential pattern)
    # ------------------------------------------------------------------
    extract_ensembl_files() >> transform_ensembl_mapping

# End of DAG definition.