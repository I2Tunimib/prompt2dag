# Generated by Dagster Code Generator
# Date: 2024-06-13
# Pipeline: etl_import_ensembl
# Description: Comprehensive Pipeline Description
# Executor: in_process_executor
# Resources: ensembl_ftp, s3_data_lake, slack_notifications, kubernetes_etl_cluster

from typing import List

from dagster import (
    In,
    Out,
    RetryPolicy,
    ResourceDefinition,
    String,
    asset,
    fs_io_manager,
    job,
    op,
    resource,
    in_process_executor,
    HookContext,
    Failure,
)


# -------------------------------------------------------------------------
# Resource Definitions
# -------------------------------------------------------------------------

@resource
def ensembl_ftp_resource(init_context) -> dict:
    """Placeholder resource for connecting to the Ensembl FTP server."""
    # In a real implementation, you would establish an FTP client here.
    return {"host": "ftp.ensembl.org", "user": "anonymous", "password": ""}


@resource
def s3_data_lake_resource(init_context) -> dict:
    """Placeholder resource for interacting with an AWS S3 data lake."""
    # In a real implementation, you would configure boto3 client here.
    return {"bucket": "my-data-lake", "region": "us-east-1"}


@resource
def slack_notifications_resource(init_context) -> dict:
    """Placeholder resource for sending Slack notifications."""
    # In a real implementation, you would store a webhook URL or token.
    return {"webhook_url": "https://hooks.slack.com/services/XXX/YYY/ZZZ"}


@resource
def kubernetes_etl_cluster_resource(init_context) -> dict:
    """Placeholder resource for submitting Spark jobs to a Kubernetes cluster."""
    # In a real implementation, you would configure the K8s client here.
    return {"namespace": "etl-jobs", "context": "my-k8s-context"}


# -------------------------------------------------------------------------
# Helper Functions
# -------------------------------------------------------------------------

def _notify_slack(context: HookContext, message: str) -> None:
    """Send a simple Slack notification using the provided resource."""
    slack_cfg = context.resources.slack_notifications
    webhook_url = slack_cfg.get("webhook_url")
    # This is a stub; replace with an actual HTTP POST to the webhook.
    context.log.info(f"Slack notification (to {webhook_url}): {message}")


def _on_failure_hook(context: HookContext) -> None:
    """Hook that runs on op failure to send a Slack alert."""
    error_msg = f"Op `{context.op.name}` failed with error: {context.failure_event.message}"
    _notify_slack(context, error_msg)


# -------------------------------------------------------------------------
# Ops
# -------------------------------------------------------------------------

@op(
    name="extract_ensembl_files",
    description="Download Ensembl mapping files from the FTP server.",
    required_resource_keys={"ensembl_ftp", "slack_notifications"},
    out=Out(dagster_type=List[String]),
    retry_policy=RetryPolicy(max_retries=0),
    on_failure=_on_failure_hook,
)
def extract_ensembl_files(context) -> List[str]:
    """
    Connects to the Ensembl FTP server, downloads required mapping files,
    and returns a list of local file paths.
    """
    ftp_cfg = context.resources.ensembl_ftp
    context.log.info(f"Connecting to Ensembl FTP at {ftp_cfg['host']}")

    # Placeholder logic – replace with real FTP download code.
    downloaded_files = [
        "/tmp/ensembl_gene_mapping.tsv",
        "/tmp/ensembl_transcript_mapping.tsv",
    ]
    context.log.info(f"Downloaded files: {downloaded_files}")

    # Notify success via Slack.
    _notify_slack(context, "Ensembl files successfully extracted.")
    return downloaded_files


@op(
    name="transform_ensembl_mapping",
    description="Transform the extracted Ensembl mapping files using Spark on a Kubernetes cluster.",
    required_resource_keys={"kubernetes_etl_cluster", "s3_data_lake", "slack_notifications"},
    ins={"files": In(dagster_type=List[String])},
    out=Out(dagster_type=String),  # Path to transformed data in S3
    retry_policy=RetryPolicy(max_retries=0),
    on_failure=_on_failure_hook,
)
def transform_ensembl_mapping(context, files: List[str]) -> str:
    """
    Submits a Spark job to the Kubernetes ETL cluster to transform the raw
    Ensembl mapping files and writes the result to the S3 data lake.
    """
    k8s_cfg = context.resources.kubernetes_etl_cluster
    s3_cfg = context.resources.s3_data_lake
    context.log.info(
        f"Submitting Spark job to K8s namespace {k8s_cfg['namespace']} with files: {files}"
    )

    # Placeholder Spark submission – replace with actual Spark job logic.
    transformed_s3_path = f"s3://{s3_cfg['bucket']}/ensembl/transformed/mapping.parquet"
    context.log.info(f"Transformed data will be stored at {transformed_s3_path}")

    # Notify success via Slack.
    _notify_slack(context, f"Ensembl mapping transformed and stored at {transformed_s3_path}.")
    return transformed_s3_path


# -------------------------------------------------------------------------
# Job Definition
# -------------------------------------------------------------------------

@job(
    name="etl_import_ensembl",
    description="Comprehensive Pipeline Description",
    executor_def=in_process_executor,
    resource_defs={
        "ensembl_ftp": ensembl_ftp_resource,
        "s3_data_lake": s3_data_lake_resource,
        "slack_notifications": slack_notifications_resource,
        "kubernetes_etl_cluster": kubernetes_etl_cluster_resource,
        "io_manager": fs_io_manager,
    },
)
def etl_import_ensembl():
    """
    Sequential ETL pipeline that extracts Ensembl mapping files from FTP,
    transforms them with Spark on a Kubernetes cluster, and stores the
    result in an S3 data lake.
    """
    extracted_files = extract_ensembl_files()
    transform_ensembl_mapping(extracted_files)


# -------------------------------------------------------------------------
# Schedule (disabled)
# -------------------------------------------------------------------------

# The schedule is intentionally disabled as per the specification.
# If you wish to enable it later, uncomment and adjust the cron expression.

# from dagster import schedule
#
# @schedule(
#     cron_schedule="0 0 * * *",  # Example: daily at midnight UTC
#     job=etl_import_ensembl,
#     execution_timezone="UTC",
#     default_status=ScheduleStatus.INACTIVE,
# )
# def etl_import_ensembl_schedule(_context):
#     return {}