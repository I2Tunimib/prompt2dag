# Generated by Prefect Pipeline Generator
# Date: 2024-06-13
# Prefect version: 2.14.0
# Flow name: etl_import_ensembl

import logging
import os
import subprocess
import ftplib
import tempfile
from pathlib import Path

import requests
from prefect import flow, task
from prefect.task_runners import SequentialTaskRunner
from prefect.blocks.system import Secret
from prefect_aws.s3 import S3Bucket

logger = logging.getLogger(__name__)


@task(retries=1, retry_delay_seconds=60)
def notify_slack_start(slack_token: str) -> None:
    """
    Send a Slack message indicating the start of the ETL pipeline.

    Args:
        slack_token: Slack Bot User OAuth token.
    """
    logger.info("Sending start notification to Slack.")
    payload = {
        "text": ":rocket: ETL pipeline *etl_import_ensembl* has started."
    }
    headers = {"Authorization": f"Bearer {slack_token}"}
    response = requests.post(
        "https://slack.com/api/chat.postMessage",
        json=payload,
        headers=headers,
        timeout=10,
    )
    response.raise_for_status()
    logger.debug("Slack start notification response: %s", response.json())


@task(retries=2, retry_delay_seconds=120)
def check_and_download_ensembl_files(
    ftp_host: str,
    ftp_user: str,
    ftp_pass: str,
    s3_bucket_name: str,
) -> None:
    """
    Connect to the Ensembl FTP server, verify required mapping files,
    download them to a temporary directory, and upload them to the
    configured S3 data lake bucket.

    Args:
        ftp_host: Hostname of the Ensembl FTP server.
        ftp_user: Username for FTP authentication.
        ftp_pass: Password for FTP authentication.
        s3_bucket_name: Name of the target S3 bucket.
    """
    required_files = [
        "Homo_sapiens.GRCh38.mapping.txt.gz",
        "Mus_musculus.GRCm39.mapping.txt.gz",
    ]

    logger.info("Connecting to Ensembl FTP server %s.", ftp_host)
    with ftplib.FTP(ftp_host) as ftp:
        ftp.login(user=ftp_user, passwd=ftp_pass)
        ftp.cwd("/pub/current_release/gtf/")

        available = ftp.nlst()
        missing = [f for f in required_files if f not in available]
        if missing:
            raise FileNotFoundError(f"Missing required files on FTP: {missing}")

        with tempfile.TemporaryDirectory() as tmp_dir:
            for filename in required_files:
                local_path = Path(tmp_dir) / filename
                logger.info("Downloading %s.", filename)
                with open(local_path, "wb") as f:
                    ftp.retrbinary(f"RETR {filename}", f.write)

                # Upload to S3
                s3_block = S3Bucket.load("s3_data_lake")
                s3_key = f"ensembl/mapping/{filename}"
                logger.info("Uploading %s to s3://%s/%s.", filename, s3_bucket_name, s3_key)
                s3_block.upload_from_path(
                    from_path=str(local_path),
                    to_path=s3_key,
                )
    logger.info("All required Ensembl mapping files have been uploaded to S3.")


@task(retries=3, retry_delay_seconds=180)
def process_ensembl_mapping_spark(
    s3_bucket_name: str,
    kubernetes_config: str,
) -> None:
    """
    Run a Spark job (submitted to a Kubernetes cluster) that processes the
    Ensembl mapping files stored in S3.

    Args:
        s3_bucket_name: Name of the S3 bucket containing the mapping files.
        kubernetes_config: Kubernetes configuration (e.g., kubeconfig content)
            required to submit the Spark job.
    """
    spark_submit_cmd = [
        "spark-submit",
        "--master", "k8s://https://kubernetes.default.svc",
        "--deploy-mode", "cluster",
        "--conf", f"spark.kubernetes.container.image=your-spark-image:latest",
        "--conf", f"spark.kubernetes.namespace=default",
        "--conf", f"spark.kubernetes.authenticate.driver.serviceAccountName=spark",
        "--conf", f"spark.kubernetes.authenticate.submission.caCertFile=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt",
        "--conf", f"spark.kubernetes.authenticate.submission.oauthTokenFile=/var/run/secrets/kubernetes.io/serviceaccount/token",
        "--conf", f"spark.hadoop.fs.s3a.bucket.{s3_bucket_name}.access.key={os.getenv('AWS_ACCESS_KEY_ID')}",
        "--conf", f"spark.hadoop.fs.s3a.bucket.{s3_bucket_name}.secret.key={os.getenv('AWS_SECRET_ACCESS_KEY')}",
        "s3a://{}/ensembl/mapping/ensembl_mapping_job.py".format(s3_bucket_name),
    ]

    logger.info("Submitting Spark job with command: %s", " ".join(spark_submit_cmd))

    # Write the Kubernetes config to a temporary file if needed
    with tempfile.NamedTemporaryFile(delete=False, mode="w") as kube_file:
        kube_file.write(kubernetes_config)
        kube_path = kube_file.name

    env = os.environ.copy()
    env["KUBECONFIG"] = kube_path

    try:
        result = subprocess.run(
            spark_submit_cmd,
            check=True,
            capture_output=True,
            text=True,
            env=env,
        )
        logger.info("Spark job completed successfully. stdout: %s", result.stdout)
    except subprocess.CalledProcessError as exc:
        logger.error(
            "Spark job failed with return code %s. stderr: %s",
            exc.returncode,
            exc.stderr,
        )
        raise
    finally:
        os.remove(kube_path)


@task(retries=1, retry_delay_seconds=60)
def notify_slack_success(slack_token: str) -> None:
    """
    Send a Slack message indicating successful completion of the ETL pipeline.

    Args:
        slack_token: Slack Bot User OAuth token.
    """
    logger.info("Sending success notification to Slack.")
    payload = {
        "text": ":white_check_mark: ETL pipeline *etl_import_ensembl* completed successfully."
    }
    headers = {"Authorization": f"Bearer {slack_token}"}
    response = requests.post(
        "https://slack.com/api/chat.postMessage",
        json=payload,
        headers=headers,
        timeout=10,
    )
    response.raise_for_status()
    logger.debug("Slack success notification response: %s", response.json())


@task(retries=1, retry_delay_seconds=60)
def notify_slack_failure(slack_token: str, error_message: str) -> None:
    """
    Send a Slack message indicating failure of the ETL pipeline.

    Args:
        slack_token: Slack Bot User OAuth token.
        error_message: Description of the error that caused the failure.
    """
    logger.info("Sending failure notification to Slack.")
    payload = {
        "text": (
            ":x: ETL pipeline *etl_import_ensembl* failed.\n"
            f"```{error_message}```"
        )
    }
    headers = {"Authorization": f"Bearer {slack_token}"}
    response = requests.post(
        "https://slack.com/api/chat.postMessage",
        json=payload,
        headers=headers,
        timeout=10,
    )
    response.raise_for_status()
    logger.debug("Slack failure notification response: %s", response.json())


@flow(
    name="etl_import_ensembl",
    task_runner=SequentialTaskRunner(),
)
def etl_import_ensembl() -> None:
    """
    Orchestrates the ETL pipeline for importing Ensembl mapping files.
    The flow follows a fan‑out pattern with explicit error handling and
    Slack notifications for start, success, and failure events.
    """
    # Load connection secrets and resources
    ensembl_ftp_secret = Secret.load("ensembl_ftp")
    slack_secret = Secret.load("slack_api")
    s3_bucket_block = S3Bucket.load("s3_data_lake")
    kubernetes_secret = Secret.load("kubernetes_etl")

    # Extract values from secrets
    ftp_host = ensembl_ftp_secret.get("host")
    ftp_user = ensembl_ftp_secret.get("user")
    ftp_pass = ensembl_ftp_secret.get("password")
    slack_token = slack_secret.get()
    s3_bucket_name = s3_bucket_block.bucket_name
    kubernetes_config = kubernetes_secret.get()

    # Notify start
    notify_slack_start(slack_token)

    # Check and download files
    check_and_download_ensembl_files(
        ftp_host=ftp_host,
        ftp_user=ftp_user,
        ftp_pass=ftp_pass,
        s3_bucket_name=s3_bucket_name,
    )

    # Process with Spark and handle outcome
    try:
        process_ensembl_mapping_spark(
            s3_bucket_name=s3_bucket_name,
            kubernetes_config=kubernetes_config,
        )
        # Success branch
        notify_slack_success(slack_token)
    except Exception as exc:  # pylint: disable=broad-except
        # Failure branch
        error_msg = str(exc)
        notify_slack_failure(slack_token, error_msg)
        # Re‑raise to mark the flow as failed
        raise


if __name__ == "__main__":
    # Execute the flow locally; in production this flow would be
    # registered as a deployment with the desired schedule.
    etl_import_ensembl()