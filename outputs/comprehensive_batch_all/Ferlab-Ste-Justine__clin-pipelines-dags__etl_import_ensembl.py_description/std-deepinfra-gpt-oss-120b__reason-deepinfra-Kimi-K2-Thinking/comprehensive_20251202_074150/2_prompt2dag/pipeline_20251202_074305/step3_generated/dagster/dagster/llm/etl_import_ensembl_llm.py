# Generated by Dagster Code Generator
# Date: 2024-06-28
# Pipeline: etl_import_ensembl
# Description: Comprehensive Pipeline Description

from dagster import (
    op,
    job,
    RetryPolicy,
    In,
    Out,
    Nothing,
    ResourceDefinition,
    ConfigurableResource,
    InitResourceContext,
    get_dagster_logger,
)
from dagster_k8s import k8s_job_executor


class EnsemblFtpResource(ConfigurableResource):
    """Resource for interacting with the Ensembl FTP server."""

    host: str = "ftp.ensembl.org"

    def download(self, remote_path: str) -> str:
        """Placeholder download implementation."""
        logger = get_dagster_logger()
        logger.info(f"Downloading {remote_path} from {self.host}")
        # In a real implementation, download the file and return the local path.
        return f"/tmp/{remote_path.split('/')[-1]}"


class S3DataLakeResource(ConfigurableResource):
    """Resource for writing data to an AWS S3 data lake."""

    bucket: str

    def upload(self, key: str, data: bytes) -> None:
        """Placeholder upload implementation."""
        logger = get_dagster_logger()
        logger.info(f"Uploading {key} to s3://{self.bucket}/{key}")
        # In a real implementation, upload the data to S3.


class SlackApiResource(ConfigurableResource):
    """Resource for sending Slack notifications."""

    webhook_url: str

    def send_message(self, text: str) -> None:
        """Placeholder Slack message implementation."""
        logger = get_dagster_logger()
        logger.info(f"Sending Slack message: {text}")
        # In a real implementation, POST to the webhook URL.


class KubernetesEtlResource(ConfigurableResource):
    """Resource for submitting jobs to a Kubernetes ETL cluster."""

    namespace: str = "default"

    def submit_job(self, job_spec: dict) -> None:
        """Placeholder job submission implementation."""
        logger = get_dagster_logger()
        logger.info(f"Submitting job to namespace {self.namespace}: {job_spec}")
        # In a real implementation, use the Kubernetes API to submit the job.


@op(
    required_resource_keys={"slack_api"},
    retry_policy=RetryPolicy(max_retries=1),
    description="Send a Slack notification when the DAG starts.",
    out=Out(Nothing),
)
def notify_slack_start(context) -> Nothing:
    context.resources.slack_api.send_message("üöÄ ETL pipeline `etl_import_ensembl` has started.")
    return Nothing


@op(
    required_resource_keys={"ensembl_ftp", "s3_data_lake"},
    retry_policy=RetryPolicy(max_retries=2),
    description="Check for and download Ensembl mapping files.",
    out=Out(Nothing),
)
def check_and_download_ensembl_files(context) -> Nothing:
    logger = get_dagster_logger()
    remote_path = "pub/current_release/ensembl_mapping.txt"
    local_path = context.resources.ensembl_ftp.download(remote_path)
    logger.info(f"Downloaded Ensembl file to {local_path}")

    # Simulate uploading to S3 data lake
    with open(local_path, "rb") as f:
        data = f.read()
    context.resources.s3_data_lake.upload(key="ensembl/mapping.txt", data=data)
    logger.info("Uploaded Ensembl mapping file to S3 data lake.")
    return Nothing


@op(
    required_resource_keys={"kubernetes_etl"},
    retry_policy=RetryPolicy(max_retries=3),
    description="Process the Ensembl mapping using a Spark job on Kubernetes.",
    out=Out(Nothing),
)
def process_ensembl_mapping_spark(context) -> Nothing:
    logger = get_dagster_logger()
    spark_job_spec = {
        "name": "ensembl-mapping-spark",
        "image": "myorg/spark-processor:latest",
        "command": ["spark-submit", "--class", "org.myorg.MappingProcessor", "app.jar"],
        "restartPolicy": "Never",
    }
    context.resources.kubernetes_etl.submit_job(spark_job_spec)
    logger.info("Submitted Spark job to Kubernetes ETL cluster.")
    return Nothing


@op(
    out={"success": Out(Nothing), "failure": Out(Nothing)},
    description="Branch based on the outcome of the previous step.",
)
def outcome_branch(context) -> dict:
    """
    In a production setting this op would inspect upstream results or
    execution status to decide which branch to follow. For this example
    we always follow the success branch.
    """
    return {"success": Nothing, "failure": Nothing}


@op(
    required_resource_keys={"slack_api"},
    retry_policy=RetryPolicy(max_retries=1),
    description="Send a Slack notification when the DAG completes successfully.",
    out=Out(Nothing),
)
def notify_slack_success(context) -> Nothing:
    context.resources.slack_api.send_message("‚úÖ ETL pipeline `etl_import_ensembl` completed successfully.")
    return Nothing


@op(
    required_resource_keys={"slack_api"},
    retry_policy=RetryPolicy(max_retries=1),
    description="Send a Slack notification when the DAG fails.",
    out=Out(Nothing),
)
def notify_slack_failure(context) -> Nothing:
    context.resources.slack_api.send_message("‚ùå ETL pipeline `etl_import_ensembl` failed.")
    return Nothing


@job(
    executor_def=k8s_job_executor,
    description="Comprehensive Pipeline Description",
    resource_defs={
        "ensembl_ftp": EnsemblFtpResource(),
        "s3_data_lake": S3DataLakeResource(bucket="my-data-lake"),
        "slack_api": SlackApiResource(webhook_url="https://hooks.slack.com/services/T00000000/B00000000/XXXXXXXXXXXXXXXXXXXXXXXX"),
        "kubernetes_etl": KubernetesEtlResource(namespace="etl-jobs"),
    },
)
def etl_import_ensembl():
    """
    Dagster job implementing the `etl_import_ensembl` pipeline.
    Execution order:
        1. notify_slack_start
        2. check_and_download_ensembl_files
        3. process_ensembl_mapping_spark
        4. outcome_branch
        5. notify_slack_success (on success)
        6. notify_slack_failure (on failure)
    """
    start = notify_slack_start()
    download = check_and_download_ensembl_files(start)
    process = process_ensembl_mapping_spark(download)
    branch = outcome_branch(process)

    # Success and failure notifications are wired to the appropriate branch outputs.
    notify_slack_success(branch.success)
    notify_slack_failure(branch.failure)