# Generated by Dagster Code Generator
# Generation Metadata:
# - Job Name: etl_import_ensembl
# - Description: No description provided.
# - Executor Type: in_process_executor
# - IO Manager: fs_io_manager
# - Dagster Version: 1.5.0
# - Use Assets: False
# - Required Resources: ["config_s3_conn_id"]

from dagster import (
    job,
    op,
    RetryPolicy,
    In,
    Out,
    ResourceDefinition,
    fs_io_manager,
    s3_resource,
    ResourceParam,
    String,
)

# Define resources
ensembl_ftp = ResourceDefinition.mock_resource()
s3_datalake = s3_resource
slack_notifications = ResourceDefinition.mock_resource()
k8s_etl_context = ResourceDefinition.mock_resource()

# Define ops
@op(
    required_resource_keys={"ensembl_ftp", "s3_datalake", "slack_notifications", "k8s_etl_context"},
    out=Out(String, description="Path to the downloaded Ensembl files"),
    retry_policy=RetryPolicy(max_retries=3),
)
def check_and_download_ensembl_files(context):
    """
    Check and download Ensembl files from the FTP server to the S3 Data Lake.
    """
    # Example implementation (replace with actual logic)
    ensembl_files_path = "path/to/ensembl/files"
    context.log.info(f"Downloading Ensembl files to {ensembl_files_path}")
    return ensembl_files_path

@op(
    required_resource_keys={"s3_datalake", "slack_notifications", "k8s_etl_context"},
    ins={"ensembl_files_path": In(String, description="Path to the downloaded Ensembl files")},
    retry_policy=RetryPolicy(max_retries=3),
)
def process_ensembl_files_with_spark(context, ensembl_files_path):
    """
    Process the downloaded Ensembl files using Spark.
    """
    # Example implementation (replace with actual logic)
    context.log.info(f"Processing Ensembl files from {ensembl_files_path} with Spark")
    # Processing logic here
    context.log.info("Processing complete")

# Define job
@job(
    name="etl_import_ensembl",
    description="No description provided.",
    executor_def=ResourceDefinition.in_process_executor(),
    resource_defs={
        "ensembl_ftp": ensembl_ftp,
        "s3_datalake": s3_datalake,
        "slack_notifications": slack_notifications,
        "k8s_etl_context": k8s_etl_context,
        "io_manager": fs_io_manager,
    },
)
def etl_import_ensembl():
    """
    ETL pipeline to import Ensembl files.
    """
    ensembl_files_path = check_and_download_ensembl_files()
    process_ensembl_files_with_spark(ensembl_files_path)