# Generated by Dagster Code Generator
# Generation Metadata:
# - Job Name: check_and_download_ensembl_files_pipeline
# - Description: No description provided.
# - Executor Type: in_process_executor
# - IO Manager: fs_io_manager
# - Dagster Version: 1.5.0
# - Use Assets: False
# - Required Resources: config_s3_conn_id

from dagster import job, op, Out, In, RetryPolicy, resource, fs_io_manager, in_process_executor, Field, String
from dagster_aws.s3 import s3_resource
from dagster_slack import slack_resource

@resource(config_schema={"config_s3_conn_id": Field(String)})
def config_s3_conn_id(init_context):
    return init_context.resource_config["config_s3_conn_id"]

@op(
    out=Out(String, description="Path to the downloaded Ensembl files"),
    required_resource_keys={"ensembl_ftp", "s3_datalake", "slack_notifications"},
    retry_policy=RetryPolicy(max_retries=3)
)
def check_and_download_ensembl_files(context):
    """
    Check and download Ensembl files from the FTP server to the S3 Data Lake.
    """
    # Example logic to check and download files
    ftp_path = "ftp://ftp.ensembl.org/pub/"
    s3_path = "s3://my-datalake/ensembl/"
    
    # Simulate file download
    context.log.info(f"Checking for new files at {ftp_path}")
    context.log.info(f"Downloading files to {s3_path}")
    
    # Notify Slack
    context.resources.slack_notifications.send_message("Ensembl files have been downloaded.")
    
    return s3_path

@op(
    ins={"s3_path": In(String, description="Path to the Ensembl files in S3")},
    required_resource_keys={"s3_datalake", "k8s_etl_context"},
    retry_policy=RetryPolicy(max_retries=3)
)
def process_ensembl_files_with_spark(context, s3_path):
    """
    Process the downloaded Ensembl files using Spark.
    """
    # Example logic to process files with Spark
    context.log.info(f"Processing files from {s3_path} using Spark")
    
    # Simulate Spark processing
    context.log.info("Spark processing completed.")
    
    # Notify Slack
    context.resources.slack_notifications.send_message("Ensembl files have been processed with Spark.")

@job(
    name="check_and_download_ensembl_files_pipeline",
    description="No description provided.",
    executor_def=in_process_executor,
    resource_defs={
        "ensembl_ftp": fs_io_manager,
        "s3_datalake": s3_resource,
        "slack_notifications": slack_resource,
        "k8s_etl_context": resource(config_field=Field(dict)),
        "config_s3_conn_id": config_s3_conn_id
    }
)
def check_and_download_ensembl_files_pipeline():
    s3_path = check_and_download_ensembl_files()
    process_ensembl_files_with_spark(s3_path)