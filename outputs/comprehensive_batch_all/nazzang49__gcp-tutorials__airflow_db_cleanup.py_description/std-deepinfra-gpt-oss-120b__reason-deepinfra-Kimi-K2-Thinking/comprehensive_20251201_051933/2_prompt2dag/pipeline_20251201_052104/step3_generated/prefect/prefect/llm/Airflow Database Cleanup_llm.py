# Generated by Prefect Pipeline Generator
# Date: 2024-06-13
# Prefect version: 2.14.0
# Flow: Airflow Database Cleanup

import os
import smtplib
from datetime import datetime, timedelta
from email.message import EmailMessage

import psycopg2
from psycopg2.extras import execute_values

from prefect import flow, task, get_run_logger
from prefect.task_runners import SequentialTaskRunner
from prefect.deployments import DeploymentSpec
from prefect.server.schemas.schedules import CronSchedule
from prefect.blocks.system import Secret


@task(retries=1, retry_delay_seconds=60, name="Extract Cleanup Configuration")
def extract_cleanup_configuration() -> dict:
    """
    Extracts cleanup configuration for the Airflow MetaStore database.

    Returns:
        dict: Configuration dictionary containing:
            - cutoff_date (datetime): Date before which records will be deleted.
            - tables (list[str]): List of tables to clean.
    """
    logger = get_run_logger()
    logger.info("Loading Airflow MetaStore DB secret.")
    db_secret = Secret.load("airflow_metastore_db")
    # Assume the secret contains a DSN string, e.g., "postgresql://user:pass@host:port/dbname"
    db_dsn = db_secret.get()

    # Example: keep 30 days of metadata
    retention_days = int(os.getenv("AIRFLOW_METADATA_RETENTION_DAYS", "30"))
    cutoff_date = datetime.utcnow() - timedelta(days=retention_days)

    # Tables commonly needing cleanup in Airflow MetaStore
    tables_to_cleanup = [
        "log",
        "task_instance",
        "dag_run",
        "job",
        "sla_miss",
        "xcom",
    ]

    config = {
        "db_dsn": db_dsn,
        "cutoff_date": cutoff_date,
        "tables": tables_to_cleanup,
    }

    logger.debug(f"Cleanup configuration: {config}")
    return config


def _execute_cleanup(conn, table: str, cutoff_date: datetime):
    """
    Executes a DELETE statement for a given table based on the cutoff date.

    Args:
        conn: psycopg2 connection object.
        table (str): Table name.
        cutoff_date (datetime): Delete rows older than this timestamp.
    """
    with conn.cursor() as cur:
        # Most Airflow tables have a `execution_date` or `start_date` column.
        # We'll attempt to delete based on common column names.
        date_columns = ["execution_date", "start_date", "timestamp", "created_at"]
        for col in date_columns:
            try:
                delete_sql = f"DELETE FROM {table} WHERE {col} < %s"
                cur.execute(delete_sql, (cutoff_date,))
                if cur.rowcount > 0:
                    return cur.rowcount
            except psycopg2.errors.UndefinedColumn:
                # Column does not exist in this table; try next
                conn.rollback()
                continue
        # If none of the columns matched, skip the table
        return 0


@task(retries=1, retry_delay_seconds=60, name="Cleanup Airflow MetaDB")
def cleanup_airflow_metadb(config: dict) -> dict:
    """
    Connects to the Airflow MetaStore database and removes old metadata entries.

    Args:
        config (dict): Configuration dictionary from `extract_cleanup_configuration`.

    Returns:
        dict: Summary of cleanup actions per table.
    """
    logger = get_run_logger()
    db_dsn = config["db_dsn"]
    cutoff_date = config["cutoff_date"]
    tables = config["tables"]

    logger.info(f"Connecting to Airflow MetaStore DB at {db_dsn}")
    cleanup_summary = {}

    try:
        with psycopg2.connect(dsn=db_dsn) as conn:
            conn.autocommit = False
            for table in tables:
                logger.info(f"Cleaning table `{table}` older than {cutoff_date.isoformat()}")
                deleted_rows = _execute_cleanup(conn, table, cutoff_date)
                conn.commit()
                cleanup_summary[table] = deleted_rows
                logger.info(f"Deleted {deleted_rows} rows from `{table}`")
    except Exception as exc:
        logger.error(f"Failed to clean Airflow MetaStore DB: {exc}")
        raise

    logger.debug(f"Cleanup summary: {cleanup_summary}")
    return cleanup_summary


def _send_alert_email(subject: str, body: str):
    """
    Sends an alert email using the configured SMTP server secret.

    Args:
        subject (str): Email subject.
        body (str): Email body content.
    """
    logger = get_run_logger()
    logger.info("Loading SMTP server secret for alert email.")
    smtp_secret = Secret.load("email_smtp_server")
    smtp_config = smtp_secret.get()  # Expected to be a dict with keys: host, port, username, password, from_addr, to_addrs

    if not isinstance(smtp_config, dict):
        raise ValueError("SMTP secret must contain a dictionary with SMTP configuration.")

    msg = EmailMessage()
    msg["Subject"] = subject
    msg["From"] = smtp_config["from_addr"]
    msg["To"] = ", ".join(smtp_config["to_addrs"])
    msg.set_content(body)

    try:
        with smtplib.SMTP(smtp_config["host"], smtp_config.get("port", 25)) as server:
            if smtp_config.get("tls", False):
                server.starttls()
            if smtp_config.get("username"):
                server.login(smtp_config["username"], smtp_config["password"])
            server.send_message(msg)
        logger.info("Alert email sent successfully.")
    except Exception as exc:
        logger.error(f"Failed to send alert email: {exc}")
        raise


@flow(
    name="airflow_database_cleanup",
    task_runner=SequentialTaskRunner(),
    retries=0,
)
def airflow_database_cleanup_flow():
    """
    Prefect flow that orchestrates the cleanup of Airflow MetaStore database tables.
    The flow extracts cleanup configuration, performs the cleanup, and sends an
    alert email if any step fails.
    """
    logger = get_run_logger()
    try:
        config = extract_cleanup_configuration()
        summary = cleanup_airflow_metadb(config)
        logger.info(f"Airflow MetaStore cleanup completed. Summary: {summary}")
    except Exception as exc:
        logger.error(f"Airflow Database Cleanup flow failed: {exc}")
        subject = "Airflow Database Cleanup Failure"
        body = f"The Airflow Database Cleanup flow encountered an error:\n\n{exc}"
        _send_alert_email(subject, body)
        raise  # Reâ€‘raise to mark the flow run as failed


# Deployment definition
DeploymentSpec(
    name="airflow_database_cleanup_deployment",
    flow=airflow_database_cleanup_flow,
    schedule=CronSchedule(cron="0 0 * * *", timezone="UTC"),  # @daily at midnight UTC
    tags=["airflow", "maintenance"],
    work_pool_name="default-agent-pool",
    enforce_parameter_schema=False,
    description="Daily maintenance workflow that cleans old metadata entries from Airflow's MetaStore database.",
)


if __name__ == "__main__":
    # Allows local execution for testing/debugging
    airflow_database_cleanup_flow()