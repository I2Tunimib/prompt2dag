# Generated by Airflow DAG Generator
# Date: 2024-06-28
# Description: Airflow Database Cleanup DAG

from __future__ import annotations

import logging
from datetime import datetime, timedelta

from airflow import DAG
from airflow.decorators import task
from airflow.exceptions import AirflowException
from airflow.models import Variable
from airflow.providers.email.operators.email import EmailOperator
from airflow.providers.postgres.hooks.postgres import PostgresHook
from airflow.utils.email import send_email
from airflow.utils.state import State
from pendulum import timezone

# ----------------------------------------------------------------------
# Default arguments for the DAG and tasks
# ----------------------------------------------------------------------
DEFAULT_ARGS = {
    "owner": "airflow",
    "depends_on_past": False,
    "retries": 1,
    "retry_delay": timedelta(minutes=5),
    "email_on_failure": False,
    "email_on_retry": False,
}


def _failure_callback(context: dict) -> None:
    """
    Callback executed on task failure.
    Sends an alert email using the ``email_smtp_server`` connection.
    """
    dag_id = context.get("dag").dag_id
    task_id = context.get("task_instance").task_id
    execution_date = context.get("execution_date")
    log_url = context.get("task_instance").log_url

    subject = f"[Airflow] DAG {dag_id} - Task {task_id} Failed"
    html_content = f"""
    <p>Task <strong>{task_id}</strong> in DAG <strong>{dag_id}</strong> has failed.</p>
    <p><strong>Execution date:</strong> {execution_date}</p>
    <p><strong>Log URL:</strong> <a href="{log_url}">{log_url}</a></p>
    """

    # Use EmailOperator to send the email
    email = EmailOperator(
        task_id="send_failure_email",
        to=["admin@example.com"],  # replace with real recipients
        subject=subject,
        html_content=html_content,
        smtp_conn_id="email_smtp_server",
    )
    email.execute(context=context)


# ----------------------------------------------------------------------
# DAG definition
# ----------------------------------------------------------------------
with DAG(
    dag_id="airflow_database_cleanup",
    description="Maintenance workflow that periodically cleans old metadata entries from Airflow's MetaStore database tables to prevent excessive data accumulation.",
    schedule_interval="@daily",
    start_date=datetime(2024, 1, 1, tzinfo=timezone("UTC")),
    catchup=False,
    default_args=DEFAULT_ARGS,
    tags=["maintenance", "cleanup"],
    on_failure_callback=_failure_callback,
    max_active_runs=1,
) as dag:

    @task(retries=1, retry_delay=timedelta(minutes=5))
    def extract_cleanup_configuration() -> dict:
        """
        Extracts cleanup configuration.
        In a real scenario this could read from Airflow Variables,
        a configuration file, or an external service.
        Returns a dictionary with configuration parameters.
        """
        try:
            # Example: read a JSON string stored in an Airflow Variable
            config_json = Variable.get("airflow_cleanup_config", default_var="{}")
            import json

            config = json.loads(config_json)

            # Provide sensible defaults if not defined
            config.setdefault("tables", ["dag_run", "task_instance", "log"])
            config.setdefault("retention_days", 30)

            logging.info("Cleanup configuration extracted: %s", config)
            return config
        except Exception as exc:
            logging.error("Failed to extract cleanup configuration: %s", exc)
            raise AirflowException("Error extracting cleanup configuration") from exc

    @task(retries=1, retry_delay=timedelta(minutes=5))
    def cleanup_airflow_metadb(config: dict) -> None:
        """
        Connects to the Airflow MetaStore DB and deletes rows older than the
        retention period for the specified tables.
        """
        try:
            tables: list[str] = config.get("tables", [])
            retention_days: int = int(config.get("retention_days", 30))

            cutoff_date = (datetime.utcnow() - timedelta(days=retention_days)).strftime(
                "%Y-%m-%d %H:%M:%S"
            )
            logging.info(
                "Starting cleanup. Retention days: %s, cutoff date: %s", retention_days, cutoff_date
            )

            hook = PostgresHook(postgres_conn_id="airflow_metastore_db")
            conn = hook.get_conn()
            cursor = conn.cursor()

            for table in tables:
                # Build a safe DELETE statement; assumes each table has a column named "execution_date"
                sql = f"""
                DELETE FROM {table}
                WHERE execution_date < %s
                """
                logging.info("Executing cleanup for table %s", table)
                cursor.execute(sql, (cutoff_date,))
                deleted_rows = cursor.rowcount
                logging.info("Deleted %s rows from %s", deleted_rows, table)

            conn.commit()
            cursor.close()
            conn.close()
            logging.info("Airflow MetaDB cleanup completed successfully.")
        except Exception as exc:
            logging.error("Error during Airflow MetaDB cleanup: %s", exc)
            raise AirflowException("Cleanup task failed") from exc

    # ------------------------------------------------------------------
    # Task pipeline
    # ------------------------------------------------------------------
    config = extract_cleanup_configuration()
    cleanup_airflow_metadb(config)

    # Define explicit dependencies (TaskFlow API already creates them via upstream/downstream)
    # However, for clarity we keep the chain expression.
    config >> cleanup_airflow_metadb

# End of DAG definition.