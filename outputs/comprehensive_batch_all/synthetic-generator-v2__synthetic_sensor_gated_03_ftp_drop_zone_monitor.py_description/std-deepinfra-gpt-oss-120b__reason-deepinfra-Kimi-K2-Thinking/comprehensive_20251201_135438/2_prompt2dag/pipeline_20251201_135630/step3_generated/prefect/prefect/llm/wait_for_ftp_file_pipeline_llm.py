# Generated by Prefect Pipeline Generator
# Date: 2024-06-13
# Prefect version: 2.14.0
# Pipeline: wait_for_ftp_file_pipeline

import os
import ftplib
import logging
from datetime import datetime
from pathlib import Path
from typing import Any, Dict

import pandas as pd
from prefect import flow, task, get_run_logger
from prefect.deployments import DeploymentSpec
from prefect.filesystems import LocalFileSystem
from prefect.blocks.system import Secret
from prefect.orion.schemas.schedules import CronSchedule
from prefect.task_runners import SequentialTaskRunner

# -------------------------------------------------------------------------
# Configuration and Block Loading
# -------------------------------------------------------------------------

# Load secret blocks (ensure they are created in Prefect UI or via CLI)
FTP_SERVER_SECRET = Secret.load("Vendor FTP Server")
INVENTORY_DB_SECRET = Secret.load("Internal Inventory System")

# Load local temporary filesystem block
LOCAL_TMP_FS = LocalFileSystem.load("Local Temporary Filesystem")

# Define constants (these could also be parameterized)
FTP_HOST = FTP_SERVER_SECRET.get()
FTP_USER = FTP_SERVER_SECRET.get("username")
FTP_PASSWORD = FTP_SERVER_SECRET.get("password")
TARGET_FILENAME = "vendor_data.csv"  # Expected file name on the FTP server
LOCAL_DOWNLOAD_PATH = Path(LOCAL_TMP_FS.basepath) / "downloads"
LOCAL_CLEANSED_PATH = Path(LOCAL_TMP_FS.basepath) / "cleansed"

# Ensure local directories exist
LOCAL_DOWNLOAD_PATH.mkdir(parents=True, exist_ok=True)
LOCAL_CLEANSED_PATH.mkdir(parents=True, exist_ok=True)


# -------------------------------------------------------------------------
# Tasks
# -------------------------------------------------------------------------

@task(retries=2, retry_delay_seconds=30, name="Wait for FTP File")
def wait_for_ftp_file(timeout_seconds: int = 300, poll_interval: int = 30) -> str:
    """
    Poll the FTP server until the target file appears or timeout is reached.

    Args:
        timeout_seconds: Maximum time to wait for the file.
        poll_interval: Seconds between successive polls.

    Returns:
        The name of the file that was detected.

    Raises:
        TimeoutError: If the file does not appear within the timeout.
    """
    logger = get_run_logger()
    deadline = datetime.utcnow().timestamp() + timeout_seconds
    logger.info(f"Starting to poll FTP server {FTP_HOST} for file '{TARGET_FILENAME}'.")

    while datetime.utcnow().timestamp() < deadline:
        try:
            with ftplib.FTP(FTP_HOST) as ftp:
                ftp.login(user=FTP_USER, passwd=FTP_PASSWORD)
                files = ftp.nlst()
                logger.debug(f"Current files on FTP: {files}")
                if TARGET_FILENAME in files:
                    logger.info(f"File '{TARGET_FILENAME}' found on FTP server.")
                    return TARGET_FILENAME
        except ftplib.all_errors as exc:
            logger.error(f"FTP error while checking for file: {exc}")

        logger.info(f"File not found yet. Sleeping for {poll_interval}s.")
        time.sleep(poll_interval)

    raise TimeoutError(f"File '{TARGET_FILENAME}' not found on FTP within {timeout_seconds}s.")


@task(retries=2, retry_delay_seconds=30, name="Download Vendor File")
def download_vendor_file(filename: str) -> Path:
    """
    Download the specified file from the FTP server to the local temporary filesystem.

    Args:
        filename: Name of the file to download.

    Returns:
        Path to the downloaded file on the local filesystem.
    """
    logger = get_run_logger()
    local_path = LOCAL_DOWNLOAD_PATH / filename
    logger.info(f"Downloading '{filename}' from FTP server to '{local_path}'.")

    try:
        with ftplib.FTP(FTP_HOST) as ftp:
            ftp.login(user=FTP_USER, passwd=FTP_PASSWORD)
            with open(local_path, "wb") as f:
                ftp.retrbinary(f"RETR {filename}", f.write)
        logger.info(f"Download completed: {local_path}")
    except ftplib.all_errors as exc:
        logger.error(f"Failed to download file '{filename}': {exc}")
        raise

    return local_path


@task(retries=2, retry_delay_seconds=30, name="Cleanse Vendor Data")
def cleanse_vendor_data(file_path: Path) -> Path:
    """
    Perform basic data cleansing on the downloaded CSV file.

    Steps include:
    - Removing rows with all null values.
    - Stripping whitespace from string columns.
    - Converting date columns to datetime.

    Args:
        file_path: Path to the raw vendor CSV file.

    Returns:
        Path to the cleansed CSV file.
    """
    logger = get_run_logger()
    logger.info(f"Starting data cleansing for file: {file_path}")

    try:
        df = pd.read_csv(file_path)
        logger.debug(f"Initial rows: {len(df)}")
        df.dropna(how="all", inplace=True)
        for col in df.select_dtypes(include=["object"]).columns:
            df[col] = df[col].str.strip()
        # Example date conversion (adjust column name as needed)
        if "date" in df.columns:
            df["date"] = pd.to_datetime(df["date"], errors="coerce")
        cleansed_path = LOCAL_CLEANSED_PATH / f"cleansed_{file_path.name}"
        df.to_csv(cleansed_path, index=False)
        logger.info(f"Cleansed data written to: {cleansed_path}")
    except Exception as exc:
        logger.error(f"Error during data cleansing: {exc}")
        raise

    return cleansed_path


@task(retries=2, retry_delay_seconds=30, name="Merge with Internal Inventory")
def merge_with_internal_inventory(cleansed_file: Path) -> Path:
    """
    Merge the cleansed vendor data with internal inventory data.

    The internal inventory data is fetched using credentials stored in a secret block.
    The merged result is saved as a new CSV file.

    Args:
        cleansed_file: Path to the cleansed vendor CSV file.

    Returns:
        Path to the merged CSV file.
    """
    logger = get_run_logger()
    logger.info(f"Merging cleansed vendor data '{cleansed_file}' with internal inventory.")

    try:
        # Load vendor data
        vendor_df = pd.read_csv(cleansed_file)

        # Simulate loading internal inventory data.
        # In a real scenario, you would connect to a DB using the secret.
        inventory_conn_str = INVENTORY_DB_SECRET.get()
        logger.debug(f"Using inventory connection string: {inventory_conn_str}")

        # Placeholder: create a dummy inventory DataFrame
        inventory_df = pd.DataFrame({
            "product_id": [1, 2, 3],
            "stock_qty": [100, 150, 200]
        })

        # Example merge on 'product_id' (adjust column names as needed)
        merged_df = pd.merge(vendor_df, inventory_df, on="product_id", how="left")
        merged_path = LOCAL_CLEANSED_PATH / f"merged_{cleansed_file.name}"
        merged_df.to_csv(merged_path, index=False)
        logger.info(f"Merged data written to: {merged_path}")
    except Exception as exc:
        logger.error(f"Failed to merge data: {exc}")
        raise

    return merged_path


# -------------------------------------------------------------------------
# Flow Definition
# -------------------------------------------------------------------------

@flow(
    name="wait_for_ftp_file_pipeline",
    task_runner=SequentialTaskRunner(),
    description="Pipeline that waits for a vendor FTP file, downloads, cleanses, and merges it with internal inventory."
)
def wait_for_ftp_file_pipeline() -> Dict[str, Any]:
    """
    Orchestrates the end‑to‑end process:
    1. Wait for the vendor file to appear on the FTP server.
    2. Download the file to a temporary location.
    3. Cleanse the raw data.
    4. Merge the cleansed data with internal inventory.

    Returns:
        Dictionary containing paths to the intermediate and final artifacts.
    """
    logger = get_run_logger()
    logger.info("Pipeline execution started.")

    # Step 1: Wait for the file
    filename = wait_for_ftp_file()

    # Step 2: Download the file
    downloaded_path = download_vendor_file(filename)

    # Step 3: Cleanse the data
    cleansed_path = cleanse_vendor_data(downloaded_path)

    # Step 4: Merge with internal inventory
    merged_path = merge_with_internal_inventory(cleansed_path)

    logger.info("Pipeline execution completed successfully.")
    return {
        "downloaded_file": str(downloaded_path),
        "cleansed_file": str(cleansed_path),
        "merged_file": str(merged_path),
    }


# -------------------------------------------------------------------------
# Deployment Specification
# -------------------------------------------------------------------------

# The deployment will run daily at midnight UTC without catch‑up.
DeploymentSpec(
    name="wait_for_ftp_file_pipeline_deployment",
    flow=wait_for_ftp_file_pipeline,
    schedule=CronSchedule(cron="0 0 * * *", timezone="UTC", day_or=True, catchup=False),
    tags=["daily", "ftp", "vendor"],
    work_pool_name="default-agent-pool",
    enforce_parameter_schema=False,
)


# -------------------------------------------------------------------------
# Entry Point
# -------------------------------------------------------------------------

if __name__ == "__main__":
    # Running the flow locally for testing/debugging purposes.
    wait_for_ftp_file_pipeline()