# Generated by Dagster code generator
# Date: 2024-06-13
# Dagster version: 1.5.0

from __future__ import annotations

import os
import time
from typing import Any, Dict, List

import pandas as pd
from dagster import (
    AssetKey,
    Config,
    In,
    Out,
    RetryPolicy,
    ResourceDefinition,
    String,
    asset,
    fs_io_manager,
    job,
    op,
    resource,
    schedule,
    sensor,
    InOut,
    InputDefinition,
    OutputDefinition,
    Nothing,
    get_dagster_logger,
    InProcessExecutor,
)

# ----------------------------------------------------------------------
# Resource definitions
# ----------------------------------------------------------------------


@resource(config_schema={"host": str, "username": str, "password": str, "remote_dir": str})
def ftp_server_connection(init_context) -> Any:
    """Simple FTP connection placeholder.

    In a real implementation this would wrap an FTP client (e.g., ftplib.FTP or a
    third‑party library). The resource provides two helper methods used by the
    ops below.
    """
    class _FTPClient:
        def __init__(self, cfg: Dict[str, str]) -> None:
            self.host = cfg["host"]
            self.username = cfg["username"]
            self.password = cfg["password"]
            self.remote_dir = cfg["remote_dir"]
            # Placeholder: pretend we are connected
            self.logger = get_dagster_logger()

        def list_files(self) -> List[str]:
            # In a real implementation, list files in remote_dir
            self.logger.info(f"Listing files on FTP server {self.host}/{self.remote_dir}")
            return ["vendor_inventory_2024_06_13.csv"]

        def download_file(self, filename: str, local_path: str) -> str:
            # In a real implementation, download the file.
            self.logger.info(f"Downloading {filename} to {local_path}")
            # Simulate download delay
            time.sleep(1)
            # Create a dummy CSV file for downstream ops
            dummy_data = pd.DataFrame(
                {
                    "sku": ["A001", "A002", "A003"],
                    "quantity": [10, 20, 30],
                    "price": [5.99, 9.99, 14.99],
                }
            )
            dummy_data.to_csv(local_path, index=False)
            return local_path

    return _FTPClient(init_context.resource_config)


@resource(config_schema={"connection_string": str})
def internal_inventory_db(init_context) -> Any:
    """Placeholder for an internal inventory database resource.

    Provides a ``merge_inventory`` method that would normally upsert data into
    the database.
    """
    class _InventoryDB:
        def __init__(self, cfg: Dict[str, str]) -> None:
            self.connection_string = cfg["connection_string"]
            self.logger = get_dagster_logger()

        def merge_inventory(self, df: pd.DataFrame) -> None:
            # In a real implementation, perform upserts/merges.
            self.logger.info(
                f"Merging {len(df)} rows into internal inventory DB at {self.connection_string}"
            )
            # Simulate DB latency
            time.sleep(1)

    return _InventoryDB(init_context.resource_config)


# ----------------------------------------------------------------------
# Ops
# ----------------------------------------------------------------------


@op(
    name="wait_for_ftp_file",
    out=Out(String),
    required_resource_keys={"ftp_server_connection"},
    retry_policy=RetryPolicy(max_retries=2),
    description="Polls the FTP server until the expected vendor file appears.",
)
def wait_for_ftp_file(context) -> str:
    """Wait for the vendor file to be present on the FTP server.

    Returns the filename that will be downloaded by the next op.
    """
    ftp = context.resources.ftp_server_connection
    expected_prefix = "vendor_inventory"
    timeout_seconds = 300
    poll_interval = 10
    elapsed = 0

    while elapsed < timeout_seconds:
        files = ftp.list_files()
        for f in files:
            if f.startswith(expected_prefix):
                context.log.info(f"Found expected file: {f}")
                return f
        context.log.info("Expected file not found yet; sleeping...")
        time.sleep(poll_interval)
        elapsed += poll_interval

    raise RuntimeError(
        f"Timed out after {timeout_seconds}s waiting for file with prefix '{expected_prefix}'."
    )


@op(
    name="download_vendor_file",
    ins={"filename": In(String)},
    out=Out(String),
    required_resource_keys={"ftp_server_connection"},
    retry_policy=RetryPolicy(max_retries=2),
    description="Downloads the vendor file from the FTP server to a temporary local path.",
)
def download_vendor_file(context, filename: str) -> str:
    """Download the specified file from FTP to a local temporary directory.

    Returns the local file path for downstream processing.
    """
    ftp = context.resources.ftp_server_connection
    tmp_dir = context.resources.fs_io_manager.get_base_dir()
    os.makedirs(tmp_dir, exist_ok=True)
    local_path = os.path.join(tmp_dir, filename)

    context.log.info(f"Downloading {filename} to temporary location {local_path}")
    downloaded_path = ftp.download_file(filename, local_path)
    return downloaded_path


@op(
    name="cleanse_vendor_data",
    ins={"local_path": In(String)},
    out=Out(pd.DataFrame),
    retry_policy=RetryPolicy(max_retries=2),
    description="Cleanses the raw vendor CSV into a well‑formed DataFrame.",
)
def cleanse_vendor_data(context, local_path: str) -> pd.DataFrame:
    """Read the CSV, perform basic cleansing, and return a DataFrame.

    Cleansing steps include:
    * Dropping rows with missing critical fields.
    * Converting data types.
    * Normalising column names.
    """
    context.log.info(f"Reading raw vendor file from {local_path}")
    df = pd.read_csv(local_path)

    # Basic cleansing
    original_len = len(df)
    df = df.dropna(subset=["sku", "quantity", "price"])
    df["sku"] = df["sku"].astype(str).str.upper()
    df["quantity"] = df["quantity"].astype(int)
    df["price"] = df["price"].astype(float)

    context.log.info(
        f"Cleansed vendor data: dropped {original_len - len(df)} rows; {len(df)} rows remain."
    )
    return df


@op(
    name="merge_with_internal_inventory",
    ins={"clean_data": In(pd.DataFrame)},
    required_resource_keys={"internal_inventory_db"},
    retry_policy=RetryPolicy(max_retries=2),
    description="Merges the cleansed vendor data into the internal inventory database.",
)
def merge_with_internal_inventory(context, clean_data: pd.DataFrame) -> Nothing:
    """Merge the cleaned vendor inventory into the internal system."""
    db = context.resources.internal_inventory_db
    context.log.info("Merging cleansed data into internal inventory database.")
    db.merge_inventory(clean_data)
    return Nothing


# ----------------------------------------------------------------------
# Job definition
# ----------------------------------------------------------------------


@job(
    name="ftp_vendor_inventory_processor",
    description="No description provided.",
    executor_def=InProcessExecutor(),
    resource_defs={
        "ftp_server_connection": ftp_server_connection,
        "internal_inventory_db": internal_inventory_db,
        "fs_io_manager": fs_io_manager,
    },
)
def ftp_vendor_inventory_processor():
    """Sequential pipeline that waits for a vendor file on FTP, downloads it,
    cleanses the data, and merges it into the internal inventory database.
    """
    filename = wait_for_ftp_file()
    local_path = download_vendor_file(filename)
    clean_df = cleanse_vendor_data(local_path)
    merge_with_internal_inventory(clean_df)


# ----------------------------------------------------------------------
# Schedule (disabled by default)
# ----------------------------------------------------------------------


@schedule(
    cron_schedule="@daily",
    job=ftp_vendor_inventory_processor,
    execution_timezone="UTC",
    default_status="inactive",  # Disabled
    description="Daily run of the FTP vendor inventory processor.",
)
def daily_ftp_vendor_inventory_processor_schedule():
    """Schedule placeholder – currently disabled."""
    return {}


# ----------------------------------------------------------------------
# Sensor (optional – can be used to trigger on FTP file arrival)
# ----------------------------------------------------------------------


@sensor(job=ftp_vendor_inventory_processor, minimum_interval_seconds=300)
def ftp_file_arrival_sensor(context):
    """Sensor that triggers the job when a new vendor file appears on the FTP server."""
    ftp = context.resources.ftp_server_connection
    files = ftp.list_files()
    for f in files:
        if f.startswith("vendor_inventory"):
            # Simple deduplication logic could be added here.
            context.log.info(f"Sensor detected new file: {f}")
            yield context.run_request_for_job(
                run_key=f,
                run_config={},
                tags={"triggered_by": "ftp_file_arrival_sensor"},
            )
            break  # Trigger once per evaluation cycle.