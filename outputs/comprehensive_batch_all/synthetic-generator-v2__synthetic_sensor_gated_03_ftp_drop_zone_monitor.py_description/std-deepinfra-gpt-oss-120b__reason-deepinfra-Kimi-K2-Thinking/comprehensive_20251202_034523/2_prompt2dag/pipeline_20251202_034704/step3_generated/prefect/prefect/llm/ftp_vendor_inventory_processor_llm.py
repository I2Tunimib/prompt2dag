# Generated by Prefect Pipeline Generator
# Date: 2024-06-28
# Pipeline: ftp_vendor_inventory_processor
# Description: Sequential processing of vendor inventory files from FTP.

import os
import ftplib
import logging
from datetime import datetime
from typing import List

import pandas as pd
from prefect import flow, task, get_run_logger
from prefect.task_runners import SequentialTaskRunner
from prefect.deployments import DeploymentSpec
from prefect.server.schemas.schedules import CronSchedule
from prefect.blocks.system import Secret
from prefect.filesystems import LocalFileSystem

# Configure logger
logger = logging.getLogger(__name__)


@task(retries=2, retry_delay_seconds=60, name="Wait for FTP File")
def wait_for_ftp_file(
    ftp_host: str,
    ftp_user: str,
    ftp_password: str,
    remote_dir: str,
    filename_pattern: str,
    poll_interval_seconds: int = 30,
    timeout_seconds: int = 600,
) -> str:
    """
    Poll the FTP server until a file matching ``filename_pattern`` appears
    in ``remote_dir``. Returns the full remote path of the first matching file.

    Args:
        ftp_host: FTP server hostname.
        ftp_user: Username for FTP login.
        ftp_password: Password for FTP login.
        remote_dir: Directory on the FTP server to watch.
        filename_pattern: Simple substring pattern to match filenames.
        poll_interval_seconds: Seconds to wait between polls.
        timeout_seconds: Maximum time to wait before raising an error.

    Returns:
        The remote file path (e.g., ``/incoming/file.csv``).

    Raises:
        TimeoutError: If the file does not appear within ``timeout_seconds``.
    """
    logger = get_run_logger()
    deadline = datetime.utcnow().timestamp() + timeout_seconds

    while datetime.utcnow().timestamp() < deadline:
        try:
            with ftplib.FTP(ftp_host) as ftp:
                ftp.login(user=ftp_user, passwd=ftp_password)
                ftp.cwd(remote_dir)
                files = ftp.nlst()
                logger.info("Files on FTP: %s", files)

                for f in files:
                    if filename_pattern in f:
                        remote_path = f"/{remote_dir}/{f}".replace("//", "/")
                        logger.info("Found matching file: %s", remote_path)
                        return remote_path
        except ftplib.all_errors as exc:
            logger.error("FTP error while polling: %s", exc)

        logger.info("File not found yet, sleeping %s seconds...", poll_interval_seconds)
        time.sleep(poll_interval_seconds)

    raise TimeoutError(
        f"File matching pattern '{filename_pattern}' not found in {remote_dir} within {timeout_seconds}s."
    )


@task(retries=2, retry_delay_seconds=60, name="Download Vendor File")
def download_vendor_file(
    ftp_host: str,
    ftp_user: str,
    ftp_password: str,
    remote_file_path: str,
    local_fs: LocalFileSystem,
    local_path: str = "vendor_raw.csv",
) -> str:
    """
    Download a file from the FTP server to the local temporary filesystem.

    Args:
        ftp_host: FTP server hostname.
        ftp_user: Username for FTP login.
        ftp_password: Password for FTP login.
        remote_file_path: Full path of the file on the FTP server.
        local_fs: Prefect LocalFileSystem block for temporary storage.
        local_path: Destination path relative to the block's base path.

    Returns:
        The absolute local path of the downloaded file.
    """
    logger = get_run_logger()
    # Ensure the base directory exists
    base_path = local_fs.basepath
    os.makedirs(base_path, exist_ok=True)

    local_full_path = os.path.join(base_path, local_path)

    try:
        with ftplib.FTP(ftp_host) as ftp:
            ftp.login(user=ftp_user, passwd=ftp_password)
            remote_dir, remote_file = os.path.split(remote_file_path)
            ftp.cwd(remote_dir)
            with open(local_full_path, "wb") as f:
                ftp.retrbinary(f"RETR {remote_file}", f.write)
        logger.info("Downloaded %s to %s", remote_file_path, local_full_path)
    except ftplib.all_errors as exc:
        logger.error("Failed to download %s: %s", remote_file_path, exc)
        raise

    return local_full_path


@task(retries=2, retry_delay_seconds=60, name="Cleanse Vendor Data")
def cleanse_vendor_data(local_file_path: str) -> pd.DataFrame:
    """
    Load the vendor CSV, perform basic cleansing, and return a DataFrame.

    Cleansing steps (example):
    * Trim whitespace from column names.
    * Drop rows with all NaN values.
    * Ensure required columns exist.

    Args:
        local_file_path: Path to the downloaded CSV file.

    Returns:
        A cleaned pandas DataFrame.
    """
    logger = get_run_logger()
    try:
        df = pd.read_csv(local_file_path)
        logger.info("Loaded %d rows and %d columns", df.shape[0], df.shape[1])
    except Exception as exc:
        logger.error("Error reading CSV %s: %s", local_file_path, exc)
        raise

    # Basic cleansing
    df.columns = [c.strip() for c in df.columns]
    df.dropna(how="all", inplace=True)

    required_columns = {"sku", "quantity", "price"}
    missing = required_columns - set(df.columns)
    if missing:
        raise ValueError(f"Missing required columns: {missing}")

    logger.info("Cleansed data now has %d rows", df.shape[0])
    return df


@task(retries=2, retry_delay_seconds=60, name="Merge with Internal Inventory")
def merge_with_internal_inventory(
    vendor_df: pd.DataFrame,
    db_connection_string: str,
    internal_table: str = "inventory",
) -> None:
    """
    Merge the vendor DataFrame with the internal inventory database.

    This example performs an upsert based on the ``sku`` column.

    Args:
        vendor_df: Cleaned vendor data.
        db_connection_string: SQLAlchemy-compatible connection string.
        internal_table: Target table name in the internal DB.

    Returns:
        None
    """
    logger = get_run_logger()
    from sqlalchemy import create_engine, text

    engine = create_engine(db_connection_string)

    with engine.begin() as conn:
        # Create a temporary staging table
        staging_table = "staging_vendor"
        vendor_df.to_sql(staging_table, conn, if_exists="replace", index=False)
        logger.info("Staged vendor data into temporary table %s", staging_table)

        # Upsert logic (PostgreSQL syntax as an example)
        upsert_sql = f"""
        INSERT INTO {internal_table} (sku, quantity, price)
        SELECT sku, quantity, price FROM {staging_table}
        ON CONFLICT (sku) DO UPDATE
        SET quantity = EXCLUDED.quantity,
            price = EXCLUDED.price;
        DROP TABLE {staging_table};
        """
        conn.execute(text(upsert_sql))
        logger.info("Merged vendor data into %s", internal_table)


@flow(
    name="ftp_vendor_inventory_processor",
    task_runner=SequentialTaskRunner(),
)
def ftp_vendor_inventory_processor():
    """
    Orchestrates the end‑to‑end processing of vendor inventory files from an FTP server.
    The flow follows these steps:

    1. Wait for the expected file to appear on the FTP server.
    2. Download the file to a temporary local filesystem.
    3. Cleanse and validate the CSV data.
    4. Merge the cleaned data into the internal inventory database.
    """
    logger = get_run_logger()

    # Load secrets and blocks
    ftp_secret: Secret = Secret.load("ftp_server_connection")
    db_secret: Secret = Secret.load("internal_inventory_db")
    local_fs: LocalFileSystem = LocalFileSystem.load("local_filesystem_tmp")

    # Expected secret payload format (JSON string)
    # {
    #   "host": "ftp.example.com",
    #   "user": "username",
    #   "password": "p@ssw0rd",
    #   "remote_dir": "incoming",
    #   "filename_pattern": "inventory_"
    # }
    ftp_cfg = ftp_secret.get()
    db_cfg = db_secret.get()

    # Parse configurations
    ftp_host = ftp_cfg["host"]
    ftp_user = ftp_cfg["user"]
    ftp_password = ftp_cfg["password"]
    remote_dir = ftp_cfg["remote_dir"]
    filename_pattern = ftp_cfg["filename_pattern"]

    db_connection_string = db_cfg["connection_string"]  # e.g., postgresql://user:pwd@host/db

    # Step 1: Wait for the file
    remote_file_path = wait_for_ftp_file(
        ftp_host=ftp_host,
        ftp_user=ftp_user,
        ftp_password=ftp_password,
        remote_dir=remote_dir,
        filename_pattern=filename_pattern,
    )

    # Step 2: Download the file
    local_file_path = download_vendor_file(
        ftp_host=ftp_host,
        ftp_user=ftp_user,
        ftp_password=ftp_password,
        remote_file_path=remote_file_path,
        local_fs=local_fs,
    )

    # Step 3: Cleanse data
    cleaned_df = cleanse_vendor_data(local_file_path)

    # Step 4: Merge with internal inventory
    merge_with_internal_inventory(
        vendor_df=cleaned_df,
        db_connection_string=db_connection_string,
    )

    logger.info("FTP vendor inventory processing completed successfully.")


# -------------------------------------------------------------------------
# Deployment specification (paused by default, i.e., schedule disabled)
# -------------------------------------------------------------------------
DeploymentSpec(
    name="ftp_vendor_inventory_processor_deployment",
    flow=ftp_vendor_inventory_processor,
    schedule=CronSchedule(cron="0 0 * * *", timezone="UTC", catchup=False),
    work_pool_name="default-agent-pool",
    paused=True,  # Disabled by default; enable manually when ready
    tags=["ftp", "vendor", "inventory"],
)

if __name__ == "__main__":
    # Running the flow locally for testing/debugging
    ftp_vendor_inventory_processor()