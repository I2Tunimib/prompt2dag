# Generated by Dagster Code Generator
# Generation Metadata:
# - Job Name: wait_for_ftp_file_pipeline
# - Description: No description provided.
# - Executor Type: in_process_executor
# - IO Manager: fs_io_manager
# - Dagster Version: 1.5.0
# - Use Assets: False
# - Required Resources: ftp_server, internal_inventory

from dagster import job, op, RetryPolicy, In, Out, fs_io_manager, in_process_executor, resource

@resource
def ftp_server():
    """FTP Server resource for file operations."""
    pass

@resource
def internal_inventory():
    """Internal Inventory System resource."""
    pass

@op(
    name="wait_for_ftp_file",
    description="Wait for FTP File",
    required_resource_keys={"ftp_server"},
    out=Out(str, description="Path to the FTP file"),
)
def wait_for_ftp_file(context):
    """Wait for the FTP file to be available."""
    # Simulate waiting for the file
    file_path = "path/to/ftp/file.csv"
    context.log.info(f"FTP file {file_path} is available.")
    return file_path

@op(
    name="download_vendor_file",
    description="Download Vendor File",
    required_resource_keys={"ftp_server", "local_filesystem"},
    ins={"ftp_file_path": In(str, description="Path to the FTP file")},
    out=Out(str, description="Path to the downloaded file"),
    retry_policy=RetryPolicy(max_retries=2),
)
def download_vendor_file(context, ftp_file_path):
    """Download the vendor file from the FTP server to the local filesystem."""
    # Simulate downloading the file
    local_file_path = "path/to/local/file.csv"
    context.log.info(f"Downloaded file from {ftp_file_path} to {local_file_path}.")
    return local_file_path

@op(
    name="cleanse_vendor_data",
    description="Cleanse Vendor Data",
    required_resource_keys={"local_filesystem"},
    ins={"local_file_path": In(str, description="Path to the downloaded file")},
    out=Out(str, description="Path to the cleansed file"),
    retry_policy=RetryPolicy(max_retries=2),
)
def cleanse_vendor_data(context, local_file_path):
    """Cleanse the vendor data in the downloaded file."""
    # Simulate data cleansing
    cleansed_file_path = "path/to/cleansed/file.csv"
    context.log.info(f"Cleaned data in file {local_file_path} and saved to {cleansed_file_path}.")
    return cleansed_file_path

@op(
    name="merge_with_internal_inventory",
    description="Merge with Internal Inventory",
    required_resource_keys={"internal_inventory", "local_filesystem"},
    ins={"cleansed_file_path": In(str, description="Path to the cleansed file")},
    out=Out(str, description="Path to the merged file"),
    retry_policy=RetryPolicy(max_retries=2),
)
def merge_with_internal_inventory(context, cleansed_file_path):
    """Merge the cleansed vendor data with the internal inventory."""
    # Simulate merging data
    merged_file_path = "path/to/merged/file.csv"
    context.log.info(f"Merged data from {cleansed_file_path} with internal inventory and saved to {merged_file_path}.")
    return merged_file_path

@job(
    name="wait_for_ftp_file_pipeline",
    description="No description provided.",
    executor_def=in_process_executor,
    resource_defs={"ftp_server": ftp_server, "local_filesystem": fs_io_manager, "internal_inventory": internal_inventory},
)
def wait_for_ftp_file_pipeline():
    """Dagster job to wait for an FTP file, download it, cleanse the data, and merge with internal inventory."""
    ftp_file_path = wait_for_ftp_file()
    local_file_path = download_vendor_file(ftp_file_path)
    cleansed_file_path = cleanse_vendor_data(local_file_path)
    merge_with_internal_inventory(cleansed_file_path)