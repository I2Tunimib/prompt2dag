# Generated by Prefect Pipeline Generator
# Date: 2024-06-28
# Pipeline: file_arrival_watcher
# Description: Monitors daily transaction file arrivals, validates schema, and loads data to PostgreSQL.

from __future__ import annotations

import os
import time
import subprocess
from pathlib import Path
from typing import List

import pandas as pd
from sqlalchemy import create_engine
from sqlalchemy.engine import Engine

from prefect import flow, task, get_run_logger
from prefect.task_runners import SequentialTaskRunner
from prefect.deployments import DeploymentSpec
from prefect.server.schemas.schedules import CronSchedule
from prefect.blocks.system import Secret
from prefect.filesystems import LocalFileSystem


# -------------------------------------------------------------------------
# Configuration Constants
# -------------------------------------------------------------------------
FILE_PATTERN = "transactions_*.csv"  # Adjust as needed
EXPECTED_COLUMNS = [
    "transaction_id",
    "account_id",
    "amount",
    "currency",
    "timestamp",
]  # Define the required schema


def _get_local_fs() -> LocalFileSystem:
    """
    Retrieve the LocalFileSystem block named ``local_filesystem``.
    """
    return LocalFileSystem.load("local_filesystem")


def _get_postgres_engine() -> Engine:
    """
    Build a SQLAlchemy engine from the ``postgres_local`` secret block.
    The secret must contain a full PostgreSQL connection URL.
    """
    secret: Secret = Secret.load("postgres_local")
    connection_url = secret.get()
    return create_engine(connection_url)


# -------------------------------------------------------------------------
# Tasks
# -------------------------------------------------------------------------

@task(retries=2, retry_delay_seconds=30, name="Wait for Transaction File")
def wait_for_file() -> Path:
    """
    Poll the local filesystem until a file matching ``FILE_PATTERN`` appears.
    Returns the path to the first matching file found.

    Raises:
        FileNotFoundError: If no file is found after a reasonable number of attempts.
    """
    logger = get_run_logger()
    fs = _get_local_fs()
    base_path = Path(fs.base_path or ".")
    logger.info("Monitoring directory %s for pattern %s", base_path, FILE_PATTERN)

    max_attempts = 48  # e.g., wait up to 24 hours checking every 30 minutes
    attempt = 0
    while attempt < max_attempts:
        matching_files = list(base_path.glob(FILE_PATTERN))
        if matching_files:
            file_path = matching_files[0]
            logger.info("Found transaction file: %s", file_path)
            return file_path
        logger.info("No file found yet (attempt %d/%d). Sleeping 30 minutes...", attempt + 1, max_attempts)
        time.sleep(1800)  # 30 minutes
        attempt += 1

    raise FileNotFoundError(f"No transaction file matching {FILE_PATTERN} was found after waiting.")


@task(retries=2, retry_delay_seconds=30, name="Validate Transaction File Schema")
def validate_schema(file_path: Path) -> pd.DataFrame:
    """
    Load the CSV file and validate that it contains the expected columns.
    Returns the loaded DataFrame for downstream processing.

    Args:
        file_path: Path to the CSV file to validate.

    Raises:
        ValueError: If the file schema does not match ``EXPECTED_COLUMNS``.
    """
    logger = get_run_logger()
    logger.info("Reading transaction file %s", file_path)

    try:
        df = pd.read_csv(file_path)
    except Exception as exc:
        logger.error("Failed to read CSV file: %s", exc)
        raise

    missing = set(EXPECTED_COLUMNS) - set(df.columns)
    extra = set(df.columns) - set(EXPECTED_COLUMNS)

    if missing:
        raise ValueError(f"Missing required columns: {missing}")
    if extra:
        logger.warning("File contains extra columns that will be ignored: %s", extra)

    # Reorder columns to match expected schema
    df = df[EXPECTED_COLUMNS]
    logger.info("Schema validation passed for %s rows.", len(df))
    return df


@task(retries=2, retry_delay_seconds=30, name="Load Transactions into PostgreSQL")
def load_db(df: pd.DataFrame) -> None:
    """
    Load the validated DataFrame into a PostgreSQL table named ``transactions``.
    If the table does not exist, it will be created.

    Args:
        df: Validated DataFrame containing transaction records.
    """
    logger = get_run_logger()
    engine = _get_postgres_engine()
    table_name = "transactions"

    logger.info("Loading %d records into PostgreSQL table '%s'.", len(df), table_name)

    try:
        df.to_sql(
            name=table_name,
            con=engine,
            if_exists="append",
            index=False,
            method="multi",
        )
    except Exception as exc:
        logger.error("Failed to load data into PostgreSQL: %s", exc)
        raise

    logger.info("Data successfully loaded into PostgreSQL.")


# -------------------------------------------------------------------------
# Flow
# -------------------------------------------------------------------------

@flow(
    name="file_arrival_watcher",
    task_runner=SequentialTaskRunner(),
)
def file_arrival_watcher() -> None:
    """
    Orchestrates the daily pipeline:
    1. Waits for a transaction file to arrive.
    2. Validates the file schema.
    3. Loads the data into PostgreSQL.
    """
    logger = get_run_logger()
    logger.info("Starting file_arrival_watcher flow.")

    file_path = wait_for_file()
    df = validate_schema(file_path)
    load_db(df)

    logger.info("file_arrival_watcher flow completed successfully.")


# -------------------------------------------------------------------------
# Deployment Specification
# -------------------------------------------------------------------------

DeploymentSpec(
    name="file_arrival_watcher_deployment",
    flow=file_arrival_watcher,
    schedule=CronSchedule(cron="0 0 * * *", timezone="UTC"),  # Runs daily at 00:00 UTC
    tags=["daily", "transactions"],
    parameters={},
    work_pool_name="default-agent-pool",
    task_runner=SequentialTaskRunner(),
    enforce_parameter_schema=False,
    description="Monitors daily transaction file arrivals, validates schema, and loads data to PostgreSQL.",
    catchup=False,
)