# Generated by Airflow DAG generator on 2024-06-12
# DAG: file_arrival_watcher
# Description: Monitors daily transaction file arrivals, validates schema, and loads data to PostgreSQL.

import os
import time
from datetime import datetime, timedelta

import pandas as pd
from airflow import DAG
from airflow.decorators import task
from airflow.utils.dates import days_ago
from airflow.exceptions import AirflowException
from airflow.providers.postgres.hooks.postgres import PostgresHook
from airflow.hooks.base import BaseHook

# -------------------------------------------------------------------------
# Default arguments applied to all tasks
# -------------------------------------------------------------------------
default_args = {
    "owner": "airflow",
    "depends_on_past": False,
    "retries": 2,
    "retry_delay": timedelta(minutes=5),
    "email_on_failure": False,
    "email_on_retry": False,
}

# -------------------------------------------------------------------------
# DAG definition
# -------------------------------------------------------------------------
with DAG(
    dag_id="file_arrival_watcher",
    description="Monitors daily transaction file arrivals, validates schema, and loads data to PostgreSQL.",
    schedule_interval="@daily",
    start_date=days_ago(1),
    catchup=False,
    default_args=default_args,
    tags=["monitoring", "postgres", "validation"],
    max_active_runs=1,
    timezone="UTC",
) as dag:

    @task(
        retries=2,
        retry_delay=timedelta(minutes=5),
        doc_md="Waits for the daily transaction file to appear in the incoming directory.",
    )
    def wait_for_file(**context):
        """
        Polls the local filesystem until the expected transaction file is present.
        Expected filename pattern: transactions_YYYY-MM-DD.csv
        """
        execution_date = context["ds"]  # e.g., '2024-06-12'
        filename = f"transactions_{execution_date}.csv"

        # Retrieve filesystem connection details (if any)
        try:
            conn = BaseHook.get_connection("local_filesystem")
            base_path = conn.extra_dejson.get("path", "/data/incoming")
        except Exception:
            # Fallback to a hardâ€‘coded path if the connection is missing
            base_path = "/data/incoming"

        file_path = os.path.join(base_path, filename)

        max_wait_seconds = 60 * 60  # 1 hour
        poll_interval = 60  # seconds
        waited = 0

        while not os.path.isfile(file_path):
            if waited >= max_wait_seconds:
                raise AirflowException(f"File {file_path} not found after waiting 1 hour.")
            time.sleep(poll_interval)
            waited += poll_interval

        # Push the file path to XCom for downstream tasks
        return file_path

    @task(
        retries=2,
        retry_delay=timedelta(minutes=5),
        doc_md="Validates the schema of the arrived transaction CSV file.",
    )
    def validate_schema(file_path: str):
        """
        Loads the CSV with pandas and checks that required columns exist.
        Raises AirflowException if validation fails.
        """
        required_columns = {"transaction_id", "account_id", "amount", "currency", "timestamp"}

        try:
            df = pd.read_csv(file_path, nrows=5)  # read a small sample for speed
        except Exception as exc:
            raise AirflowException(f"Unable to read CSV file {file_path}: {exc}")

        missing = required_columns.difference(df.columns)
        if missing:
            raise AirflowException(f"Schema validation failed. Missing columns: {missing}")

        # Return the validated file path (could also return a schema version, etc.)
        return file_path

    @task(
        retries=2,
        retry_delay=timedelta(minutes=5),
        doc_md="Loads validated transaction data into the PostgreSQL database.",
    )
    def load_db(file_path: str):
        """
        Reads the full CSV and inserts rows into the target PostgreSQL table.
        Uses PostgresHook for connection handling.
        """
        target_table = "public.transactions"

        # Initialize Postgres hook using the defined connection
        try:
            pg_hook = PostgresHook(postgres_conn_id="postgres_local")
        except Exception as exc:
            raise AirflowException(f"Failed to create PostgresHook: {exc}")

        # Load data in chunks to avoid memory overload
        chunk_size = 10_000
        try:
            for chunk in pd.read_csv(file_path, chunksize=chunk_size):
                # Convert DataFrame to list of tuples
                records = [tuple(row) for row in chunk.itertuples(index=False, name=None)]

                # Build INSERT statement with placeholders
                cols = ", ".join(chunk.columns)
                placeholders = ", ".join(["%s"] * len(chunk.columns))
                insert_sql = f"INSERT INTO {target_table} ({cols}) VALUES ({placeholders})"

                # Execute batch insert
                pg_hook.run(insert_sql, parameters=records, autocommit=True)
        except Exception as exc:
            raise AirflowException(f"Error loading data into PostgreSQL: {exc}")

        return f"Loaded data from {file_path} into {target_table}"

    # -------------------------------------------------------------------------
    # Task pipeline definition
    # -------------------------------------------------------------------------
    file_path = wait_for_file()
    validated_path = validate_schema(file_path)
    load_result = load_db(validated_path)

    # Define explicit dependencies (TaskFlow API also respects the order of arguments)
    file_path >> validated_path >> load_result