# Generated by Dagster code generator
# Date: 2024-06-13
# Description: wait_for_file_pipeline – sensor‑gated pipeline that monitors daily transaction file arrivals,
# validates the file schema, and loads the data into a PostgreSQL table.

import os
import csv
import datetime
from typing import List, Dict, Any

import psycopg2
from dagster import (
    op,
    job,
    In,
    Out,
    RetryPolicy,
    ResourceDefinition,
    fs_io_manager,
    InProcessExecutor,
    sensor,
    RunRequest,
    SkipReason,
    ScheduleDefinition,
    DefaultScheduleStatus,
    ConfigurableResource,
    InitResourceContext,
    OpExecutionContext,
)


# ----------------------------------------------------------------------
# Resources
# ----------------------------------------------------------------------


class PostgresResource(ConfigurableResource):
    """Resource for connecting to a local PostgreSQL database."""

    host: str = "localhost"
    port: int = 5432
    database: str = "transactions_db"
    user: str = "postgres"
    password: str = "postgres"

    def get_connection(self) -> psycopg2.extensions.connection:
        """Create and return a new PostgreSQL connection."""
        return psycopg2.connect(
            host=self.host,
            port=self.port,
            dbname=self.database,
            user=self.user,
            password=self.password,
        )


postgres_local = ResourceDefinition.resource(
    resource_fn=lambda init_context: PostgresResource()
)


# ----------------------------------------------------------------------
# Ops
# ----------------------------------------------------------------------


@op(
    name="wait_for_file",
    description="Passes the file path discovered by the sensor to downstream ops.",
    out=Out(str),
    retry_policy=RetryPolicy(max_retries=0),
    required_resource_keys=set(),
)
def wait_for_file(context: OpExecutionContext, file_path: str) -> str:
    """
    Simple pass‑through op that receives the file path from the run config
    (populated by the sensor) and returns it for downstream processing.
    """
    context.log.info(f"Received file path: {file_path}")
    if not os.path.isfile(file_path):
        raise FileNotFoundError(f"File not found at path: {file_path}")
    return file_path


@op(
    name="validate_schema",
    description="Validates the CSV schema of the transaction file.",
    ins={"file_path": In(str)},
    out=Out(List[Dict[str, Any]]),
    retry_policy=RetryPolicy(max_retries=2, delay=5),
)
def validate_schema(context: OpExecutionContext, file_path: str) -> List[Dict[str, Any]]:
    """
    Reads the CSV file, checks that required columns exist, and returns the
    parsed rows as a list of dictionaries.

    Expected columns: transaction_id, amount, currency, timestamp
    """
    required_columns = {"transaction_id", "amount", "currency", "timestamp"}
    rows: List[Dict[str, Any]] = []

    with open(file_path, newline="") as csvfile:
        reader = csv.DictReader(csvfile)
        header = set(reader.fieldnames or [])
        missing = required_columns - header
        if missing:
            raise ValueError(f"Missing required columns: {missing}")

        for row in reader:
            rows.append(row)

    context.log.info(f"Validated {len(rows)} rows from {file_path}")
    return rows


@op(
    name="load_db",
    description="Loads validated transaction rows into a PostgreSQL table.",
    ins={"transactions": In(List[Dict[str, Any]])},
    out=Out(None),
    retry_policy=RetryPolicy(max_retries=2, delay=5),
    required_resource_keys={"postgres_local"},
)
def load_db(context: OpExecutionContext, transactions: List[Dict[str, Any]]) -> None:
    """
    Inserts each transaction row into the `transactions` table.
    The table schema should match the CSV columns.
    """
    if not transactions:
        context.log.warning("No transactions to load.")
        return

    postgres: PostgresResource = context.resources.postgres_local
    insert_sql = """
        INSERT INTO transactions (transaction_id, amount, currency, timestamp)
        VALUES (%s, %s, %s, %s)
        ON CONFLICT (transaction_id) DO UPDATE
        SET amount = EXCLUDED.amount,
            currency = EXCLUDED.currency,
            timestamp = EXCLUDED.timestamp;
    """

    conn = postgres.get_connection()
    try:
        with conn.cursor() as cur:
            for txn in transactions:
                cur.execute(
                    insert_sql,
                    (
                        txn["transaction_id"],
                        float(txn["amount"]),
                        txn["currency"],
                        txn["timestamp"],
                    ),
                )
        conn.commit()
        context.log.info(f"Loaded {len(transactions)} transactions into PostgreSQL.")
    except Exception as exc:
        conn.rollback()
        context.log.error(f"Failed to load transactions: {exc}")
        raise
    finally:
        conn.close()


# ----------------------------------------------------------------------
# Job
# ----------------------------------------------------------------------


@job(
    name="wait_for_file_pipeline",
    description=(
        "Sensor‑gated pipeline that monitors daily transaction file arrivals, validates the file schema, "
        "and loads the data into a PostgreSQL table. The workflow is linear: a FileSensor gates the process, "
        "followed by a Python‑based schema validation and a Python‑based load to PostgreSQL."
    ),
    resource_defs={"io_manager": fs_io_manager, "postgres_local": postgres_local},
    executor_def=InProcessExecutor(),
)
def wait_for_file_pipeline():
    """
    Sequential pipeline:
        wait_for_file -> validate_schema -> load_db
    """
    file_path = wait_for_file()
    validated = validate_schema(file_path)
    load_db(validated)


# ----------------------------------------------------------------------
# Sensor
# ----------------------------------------------------------------------


@sensor(job=wait_for_file_pipeline, minimum_interval_seconds=60)
def transaction_file_sensor(context) -> RunRequest | SkipReason:
    """
    Monitors the incoming directory for a daily transaction CSV file.
    Expected naming pattern: transactions_YYYYMMDD.csv
    """
    incoming_dir = os.getenv("INCOMING_DIR", "/tmp/incoming")
    today_str = datetime.datetime.utcnow().strftime("%Y%m%d")
    expected_filename = f"transactions_{today_str}.csv"
    file_path = os.path.join(incoming_dir, expected_filename)

    if os.path.isfile(file_path):
        context.log.info(f"Detected file: {file_path}")
        run_config = {"ops": {"wait_for_file": {"config": {"file_path": file_path}}}}
        return RunRequest(run_key=expected_filename, run_config=run_config)
    else:
        return SkipReason(f"No file {expected_filename} in {incoming_dir}")


# ----------------------------------------------------------------------
# Schedule
# ----------------------------------------------------------------------


daily_schedule = ScheduleDefinition(
    job=wait_for_file_pipeline,
    cron_schedule="0 0 * * *",  # @daily at 00:00 UTC
    execution_timezone="UTC",
    default_status=DefaultScheduleStatus.RUNNING,
    description="Runs the wait_for_file_pipeline daily at midnight UTC.",
    # The sensor will decide whether a run is actually launched.
    # No additional run config is required for the schedule.
)

# End of file.