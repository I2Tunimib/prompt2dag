# Generated by Prefect Pipeline Generator
# Date: 2024-06-28
# Prefect version: 2.14.0
# Flow: wait_for_file_pipeline

import os
import time
from datetime import datetime
from pathlib import Path
from typing import Dict

import pandas as pd
import sqlalchemy
from prefect import flow, task, get_run_logger
from prefect.deployments import DeploymentSpec
from prefect.filesystems import LocalFileSystem
from prefect.blocks.system import Secret
from prefect.orion.schemas.schedules import CronSchedule
from prefect.task_runners import SequentialTaskRunner


# -------------------------------------------------------------------------
# Helper Functions
# -------------------------------------------------------------------------

def _load_db_credentials(secret_name: str) -> Dict[str, str]:
    """
    Retrieve PostgreSQL credentials from a Prefect Secret block.

    Args:
        secret_name: Name of the Secret block containing DB credentials.

    Returns:
        Dictionary with keys: username, password, host, port, database.
    """
    secret_block = Secret.load(secret_name)
    # The secret value is expected to be a JSON string or dict.
    credentials = secret_block.get()
    if isinstance(credentials, str):
        import json
        credentials = json.loads(credentials)
    required_keys = {"username", "password", "host", "port", "database"}
    if not required_keys.issubset(credentials):
        missing = required_keys - credentials.keys()
        raise ValueError(f"Missing DB credential fields: {missing}")
    return credentials


def _build_engine(creds: Dict[str, str]) -> sqlalchemy.engine.Engine:
    """
    Build a SQLAlchemy engine using the provided credentials.

    Args:
        creds: Dictionary of DB credentials.

    Returns:
        SQLAlchemy Engine instance.
    """
    url = (
        f"postgresql://{creds['username']}:{creds['password']}"
        f"@{creds['host']}:{creds['port']}/{creds['database']}"
    )
    return sqlalchemy.create_engine(url)


# -------------------------------------------------------------------------
# Tasks
# -------------------------------------------------------------------------

@task(retries=0, name="Wait for Transaction File")
def wait_for_file(
    filesystem_block: str,
    file_pattern: str = "{date}_transactions.csv",
    poll_interval_seconds: int = 30,
) -> Path:
    """
    Poll a local filesystem until a transaction file for the current date appears.

    Args:
        filesystem_block: Name of the Prefect LocalFileSystem block.
        file_pattern: Filename pattern using ``{date}`` placeholder (YYYY-MM-DD).
        poll_interval_seconds: Seconds to wait between polls.

    Returns:
        Path object pointing to the discovered file.

    Raises:
        FileNotFoundError: If the file never appears (should not happen with infinite polling).
    """
    logger = get_run_logger()
    fs = LocalFileSystem.load(filesystem_block)

    today_str = datetime.utcnow().strftime("%Y-%m-%d")
    target_filename = file_pattern.format(date=today_str)

    logger.info(f"Waiting for file '{target_filename}' in block '{filesystem_block}'.")

    while True:
        # Resolve the absolute path using the block's base_path
        base_path = Path(fs.base_path).expanduser().resolve()
        candidate_path = base_path / target_filename

        if candidate_path.is_file():
            logger.info(f"File found: {candidate_path}")
            return candidate_path

        logger.debug(f"File not found yet. Sleeping {poll_interval_seconds}s.")
        time.sleep(poll_interval_seconds)


@task(retries=2, name="Validate Transaction File Schema")
def validate_schema(file_path: Path, required_columns: list = None) -> pd.DataFrame:
    """
    Load the CSV file and validate that it contains the required columns.

    Args:
        file_path: Path to the transaction CSV file.
        required_columns: List of column names that must be present.

    Returns:
        pandas.DataFrame containing the loaded data.

    Raises:
        ValueError: If required columns are missing.
    """
    logger = get_run_logger()
    if required_columns is None:
        required_columns = ["transaction_id", "date", "amount", "currency", "account_id"]

    logger.info(f"Reading transaction file: {file_path}")
    df = pd.read_csv(file_path)

    missing = set(required_columns) - set(df.columns)
    if missing:
        raise ValueError(f"Missing required columns in {file_path.name}: {missing}")

    logger.info(f"Schema validation passed for {file_path.name}.")
    return df


@task(retries=2, name="Load Transactions into PostgreSQL")
def load_db(
    df: pd.DataFrame,
    secret_block_name: str,
    table_name: str = "transactions",
    if_exists: str = "append",
) -> None:
    """
    Load a DataFrame into a PostgreSQL table.

    Args:
        df: DataFrame to be loaded.
        secret_block_name: Name of the Prefect Secret block containing DB credentials.
        table_name: Destination table in PostgreSQL.
        if_exists: Behavior if the table already exists (default: 'append').

    Returns:
        None
    """
    logger = get_run_logger()
    logger.info("Fetching database credentials.")
    creds = _load_db_credentials(secret_block_name)

    logger.info("Creating SQLAlchemy engine.")
    engine = _build_engine(creds)

    logger.info(f"Loading {len(df)} records into table '{table_name}'.")
    df.to_sql(name=table_name, con=engine, if_exists=if_exists, index=False)
    logger.info("Data load completed successfully.")


# -------------------------------------------------------------------------
# Flow
# -------------------------------------------------------------------------

@flow(
    name="wait_for_file_pipeline",
    description=(
        "Sensor‑gated pipeline that monitors daily transaction file arrivals, "
        "validates the file schema, and loads the data into a PostgreSQL table."
    ),
    task_runner=SequentialTaskRunner(),
)
def wait_for_file_pipeline(
    filesystem_block: str = "local_filesystem",
    secret_block_name: str = "postgres_local",
) -> None:
    """
    Orchestrates the end‑to‑end pipeline:
    1. Wait for the daily transaction file.
    2. Validate its schema.
    3. Load the data into PostgreSQL.

    Args:
        filesystem_block: Prefect LocalFileSystem block name for incoming files.
        secret_block_name: Prefect Secret block name containing PostgreSQL credentials.
    """
    file_path = wait_for_file(filesystem_block=filesystem_block)
    df = validate_schema(file_path)
    load_db(df, secret_block_name=secret_block_name)


# -------------------------------------------------------------------------
# Deployment Specification
# -------------------------------------------------------------------------

# The deployment registers the flow with a daily schedule.
DeploymentSpec(
    name="wait_for_file_pipeline_deployment",
    flow=wait_for_file_pipeline,
    schedule=CronSchedule(cron="0 0 * * *", timezone="UTC", catchup=False),  # @daily at 00:00 UTC
    work_pool_name="default-agent-pool",
    tags=[],
    description="Daily run of the wait_for_file_pipeline.",
    enforce_parameter_schema=False,
)

if __name__ == "__main__":
    # Optional: run the flow locally for testing
    wait_for_file_pipeline()