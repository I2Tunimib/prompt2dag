# Generated by Dagster Code Generator
# Date: 2024-06-28
# Dagster version: 1.5.0
# Pipeline: file_arrival_watcher

from __future__ import annotations

import csv
import os
from typing import List, Dict, Any

import psycopg2
from dagster import (
    In,
    Out,
    op,
    job,
    resource,
    RetryRequested,
    Config,
    String,
    List as DagsterList,
    Dict as DagsterDict,
    fs_io_manager,
    in_process_executor,
    ScheduleDefinition,
    ScheduleStatus,
    Definitions,
)


# -------------------------------------------------------------------------
# Resources
# -------------------------------------------------------------------------

@resource(config_schema={"connection_string": str})
def postgresql_db(init_context) -> psycopg2.extensions.connection:
    """
    Provides a PostgreSQL connection using the supplied connection string.
    """
    conn_str = init_context.resource_config["connection_string"]
    try:
        conn = psycopg2.connect(conn_str)
        init_context.log.info("PostgreSQL connection established.")
        return conn
    except Exception as exc:
        init_context.log.error(f"Failed to connect to PostgreSQL: {exc}")
        raise


# -------------------------------------------------------------------------
# Ops
# -------------------------------------------------------------------------

class WaitForFileConfig(Config):
    """Configuration for the ``wait_for_file`` op."""
    directory: str
    filename_pattern: str = "*.csv"


@op(
    name="Wait for Transaction File",
    description="Polls a directory until a transaction file matching the pattern appears.",
    required_resource_keys=set(),
    config_schema=WaitForFileConfig,
    retries=2,
    out=Out(str, description="Path to the discovered transaction file."),
)
def wait_for_file(context, config: WaitForFileConfig) -> str:
    """
    Simple polling implementation that looks for the first file matching the pattern.
    In a production setting this would likely be replaced with an event‑driven trigger.
    """
    import time
    import glob

    directory = config.directory
    pattern = os.path.join(directory, config.filename_pattern)

    context.log.info(f"Watching directory: {directory} for pattern: {config.filename_pattern}")

    while True:
        matches = glob.glob(pattern)
        if matches:
            file_path = matches[0]
            context.log.info(f"Found file: {file_path}")
            return file_path
        context.log.debug("No matching file found; sleeping 30 seconds.")
        time.sleep(30)


class ValidateSchemaConfig(Config):
    """Configuration for the ``validate_schema`` op."""
    required_columns: List[str] = ["transaction_id", "amount", "timestamp"]


@op(
    name="Validate Transaction File Schema",
    description="Ensures the incoming CSV file contains the required columns.",
    required_resource_keys=set(),
    config_schema=ValidateSchemaConfig,
    retries=2,
    ins={"file_path": In(str)},
    out=Out(List[Dict[str, Any]], description="List of validated transaction records."),
)
def validate_schema(context, file_path: str, config: ValidateSchemaConfig) -> List[Dict[str, Any]]:
    """
    Reads the CSV file, checks that required columns exist, and returns the rows as dictionaries.
    Raises ``RetryRequested`` if validation fails.
    """
    context.log.info(f"Validating schema of file: {file_path}")

    with open(file_path, newline="") as csvfile:
        reader = csv.DictReader(csvfile)
        missing = [col for col in config.required_columns if col not in reader.fieldnames]
        if missing:
            msg = f"Missing required columns: {missing}"
            context.log.error(msg)
            raise RetryRequested(max_retries=2, seconds_to_wait=60)

        records = [row for row in reader]

    context.log.info(f"Schema validation passed. {len(records)} records loaded.")
    return records


@op(
    name="Load Validated Transactions to PostgreSQL",
    description="Inserts validated transaction records into the PostgreSQL database.",
    required_resource_keys={"postgresql_db"},
    retries=2,
    ins={"records": In(List[Dict[str, Any]])},
    out=Out(None),
)
def load_db(context, records: List[Dict[str, Any]]) -> None:
    """
    Inserts each transaction record into a target table.
    This implementation uses a simple INSERT statement; bulk loading strategies
    (e.g., COPY) could be employed for larger datasets.
    """
    conn: psycopg2.extensions.connection = context.resources.postgresql_db
    cursor = conn.cursor()

    insert_sql = """
        INSERT INTO transactions (transaction_id, amount, timestamp)
        VALUES (%s, %s, %s)
        ON CONFLICT (transaction_id) DO UPDATE
        SET amount = EXCLUDED.amount,
            timestamp = EXCLUDED.timestamp;
    """

    for rec in records:
        try:
            cursor.execute(
                insert_sql,
                (
                    rec["transaction_id"],
                    float(rec["amount"]),
                    rec["timestamp"],
                ),
            )
        except Exception as exc:
            context.log.error(f"Failed to insert record {rec['transaction_id']}: {exc}")
            conn.rollback()
            raise

    conn.commit()
    cursor.close()
    context.log.info(f"Inserted {len(records)} transaction records into PostgreSQL.")


# -------------------------------------------------------------------------
# Job
# -------------------------------------------------------------------------

@job(
    name="file_arrival_watcher",
    description="Monitors daily transaction file arrivals, validates schema, and loads data into PostgreSQL.",
    resource_defs={
        "io_manager": fs_io_manager,
        "postgresql_db": postgresql_db,
    },
    executor_def=in_process_executor,
)
def file_arrival_watcher():
    """
    Sequential pipeline:
    1. wait_for_file → 2. validate_schema → 3. load_db
    """
    file_path = wait_for_file()
    records = validate_schema(file_path)
    load_db(records)


# -------------------------------------------------------------------------
# Schedule
# -------------------------------------------------------------------------

daily_file_watcher_schedule = ScheduleDefinition(
    job=file_arrival_watcher,
    cron_schedule="@daily",
    execution_timezone="UTC",
    default_status=ScheduleStatus.RUNNING,
    description="Runs the file_arrival_watcher job once per day at midnight UTC.",
    # Dagster 1.5 supports `catchup` directly on ScheduleDefinition
    catchup=False,
)


# -------------------------------------------------------------------------
# Definitions (for Dagster UI)
# -------------------------------------------------------------------------

defs = Definitions(
    jobs=[file_arrival_watcher],
    schedules=[daily_file_watcher_schedule],
    resources={"postgresql_db": postgresql_db, "io_manager": fs_io_manager},
)