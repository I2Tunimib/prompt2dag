# Generated by Prefect Pipeline Generator
# Date: 2024-06-28
# Pipeline: PM2.5_Risk_Alert_Pipeline
# Description: Sequential ETL pipeline that scrapes Mahidol University AQI data, transforms it to JSON,
# loads it into PostgreSQL, and sends email alerts when PM2.5 exceeds thresholds.

import os
import json
import smtplib
import logging
from email.message import EmailMessage
from typing import List, Dict, Any

import requests
from bs4 import BeautifulSoup
import pandas as pd
import psycopg2
from psycopg2.extras import execute_values

from prefect import flow, task, get_run_logger
from prefect.task_runners import SequentialTaskRunner
from prefect.blocks.system import Secret

# -------------------------------------------------------------------------
# Helper functions
# -------------------------------------------------------------------------

def _get_secret(block_name: str) -> str:
    """Retrieve a secret value from a Prefect Secret block."""
    secret_block = Secret.load(block_name)
    return secret_block.get()


# -------------------------------------------------------------------------
# Tasks
# -------------------------------------------------------------------------

@task(retries=0, name="Extract Mahidol AQI HTML")
def extract_mahidol_aqi_html() -> str:
    """
    Download the raw HTML page containing AQI data from Mahidol University.

    Returns
    -------
    str
        The raw HTML content.
    """
    logger = get_run_logger()
    try:
        url = _get_secret("mahidol_aqi_website_api")  # Expected to contain the full URL
        logger.info(f"Fetching AQI data from {url}")
        response = requests.get(url, timeout=30)
        response.raise_for_status()
        logger.debug("HTML content fetched successfully.")
        return response.text
    except Exception as exc:
        logger.error(f"Failed to fetch AQI HTML: {exc}")
        raise


@task(retries=0, name="Transform HTML to Structured JSON")
def transform_html_to_json(html: str) -> List[Dict[str, Any]]:
    """
    Parse the HTML and convert the AQI table into a list of JSON‑serializable dicts.

    Parameters
    ----------
    html : str
        Raw HTML content.

    Returns
    -------
    List[Dict[str, Any]]
        Structured data where each dict represents a measurement point.
    """
    logger = get_run_logger()
    try:
        soup = BeautifulSoup(html, "html.parser")
        # The actual selector depends on the website; this is a generic example.
        table = soup.find("table", {"id": "aqi-table"})
        if not table:
            raise ValueError("AQI table not found in HTML.")

        rows = table.find_all("tr")
        data = []
        for row in rows[1:]:  # Skip header
            cols = row.find_all("td")
            if len(cols) < 4:
                continue
            record = {
                "station": cols[0].get_text(strip=True),
                "pm25": float(cols[1].get_text(strip=True)),
                "pm10": float(cols[2].get_text(strip=True)),
                "timestamp": pd.to_datetime(cols[3].get_text(strip=True)).isoformat(),
            }
            data.append(record)

        logger.info(f"Transformed {len(data)} rows into JSON.")
        return data
    except Exception as exc:
        logger.error(f"Failed to transform HTML to JSON: {exc}")
        raise


@task(retries=0, name="Duplicate Check Branch")
def duplicate_check_branch(data: List[Dict[str, Any]]) -> bool:
    """
    Determine whether the incoming records already exist in the warehouse.

    Parameters
    ----------
    data : List[Dict[str, Any]]
        The transformed AQI records.

    Returns
    -------
    bool
        ``True`` if at least one record is new (i.e., should be loaded), ``False`` otherwise.
    """
    logger = get_run_logger()
    try:
        conn_str = _get_secret("postgres_warehouse")
        conn = psycopg2.connect(conn_str)
        cur = conn.cursor()

        # Build a set of (station, timestamp) tuples from incoming data
        incoming_keys = {(rec["station"], rec["timestamp"]) for rec in data}

        # Query existing keys
        cur.execute(
            """
            SELECT station, timestamp
            FROM aqi_measurements
            WHERE (station, timestamp) IN %s
            """,
            (tuple(incoming_keys),)
        )
        existing = {(row[0], row[1].isoformat()) for row in cur.fetchall()}
        cur.close()
        conn.close()

        is_new = not incoming_keys.issubset(existing)
        logger.info(f"Duplicate check: {'new records found' if is_new else 'all records already exist'}.")
        return is_new
    except Exception as exc:
        logger.error(f"Duplicate check failed: {exc}")
        raise


@task(retries=0, name="Load AQI Data to PostgreSQL Warehouse")
def load_mahidol_aqi_to_warehouse(data: List[Dict[str, Any]]) -> None:
    """
    Insert new AQI records into the PostgreSQL warehouse.

    Parameters
    ----------
    data : List[Dict[str, Any]]
        The transformed AQI records.
    """
    logger = get_run_logger()
    try:
        conn_str = _get_secret("postgres_warehouse")
        conn = psycopg2.connect(conn_str)
        cur = conn.cursor()

        # Prepare data for bulk insert
        records = [
            (rec["station"], rec["pm25"], rec["pm10"], rec["timestamp"])
            for rec in data
        ]

        insert_sql = """
            INSERT INTO aqi_measurements (station, pm25, pm10, timestamp)
            VALUES %s
            ON CONFLICT (station, timestamp) DO NOTHING
        """
        execute_values(cur, insert_sql, records)
        conn.commit()
        cur.close()
        conn.close()
        logger.info(f"Inserted {len(records)} records into the warehouse.")
    except Exception as exc:
        logger.error(f"Failed to load data into warehouse: {exc}")
        raise


@task(retries=0, name="AQI Threshold Branch")
def aqi_threshold_branch(data: List[Dict[str, Any]], threshold: float = 35.0) -> List[Dict[str, Any]]:
    """
    Filter records where PM2.5 exceeds the supplied threshold.

    Parameters
    ----------
    data : List[Dict[str, Any]]
        The transformed AQI records.
    threshold : float, optional
        PM2.5 concentration (µg/m³) that triggers an alert. Default is 35.0.

    Returns
    -------
    List[Dict[str, Any]]
        Subset of records exceeding the threshold.
    """
    logger = get_run_logger()
    try:
        exceedances = [rec for rec in data if rec["pm25"] > threshold]
        logger.info(f"Found {len(exceedances)} stations exceeding PM2.5 threshold of {threshold}.")
        return exceedances
    except Exception as exc:
        logger.error(f"Threshold evaluation failed: {exc}")
        raise


@task(retries=0, name="PM2.5 Email Alert Notification")
def notify_pm25_alert(exceedances: List[Dict[str, Any]]) -> None:
    """
    Send an email alert containing stations with PM2.5 above the threshold.

    Parameters
    ----------
    exceedances : List[Dict[str, Any]]
        Records that triggered the alert.
    """
    logger = get_run_logger()
    if not exceedances:
        logger.info("No exceedances to notify; skipping email.")
        return

    try:
        smtp_user = _get_secret("smtp_gmail_user")
        smtp_pass = _get_secret("smtp_gmail_password")
        recipient = _get_secret("smtp_gmail_recipient")

        msg = EmailMessage()
        msg["Subject"] = "PM2.5 Alert – Mahidol University AQI"
        msg["From"] = smtp_user
        msg["To"] = recipient

        body_lines = ["The following stations have PM2.5 levels above the threshold:\n"]
        for rec in exceedances:
            line = f"- {rec['station']} at {rec['timestamp']}: {rec['pm25']} µg/m³"
            body_lines.append(line)
        msg.set_content("\n".join(body_lines))

        with smtplib.SMTP_SSL("smtp.gmail.com", 465) as smtp:
            smtp.login(smtp_user, smtp_pass)
            smtp.send_message(msg)

        logger.info(f"Sent PM2.5 alert email to {recipient}.")
    except Exception as exc:
        logger.error(f"Failed to send alert email: {exc}")
        raise


# -------------------------------------------------------------------------
# Flow definition
# -------------------------------------------------------------------------

@flow(
    name="PM2.5_Risk_Alert_Pipeline",
    task_runner=SequentialTaskRunner(),
    description="Sequential ETL pipeline that scrapes Mahidol University AQI data, transforms it to JSON, loads it into PostgreSQL, and sends email alerts when PM2.5 exceeds thresholds.",
)
def pm2_5_risk_alert_pipeline():
    """
    Orchestrates the end‑to‑end ETL and alerting workflow.
    """
    logger = get_run_logger()
    logger.info("Starting PM2.5 Risk Alert Pipeline")

    # 1. Extract
    raw_html = extract_mahidol_aqi_html()

    # 2. Transform
    json_data = transform_html_to_json(raw_html)

    # 3. Branch: duplicate check → load
    should_load = duplicate_check_branch(json_data)
    if should_load:
        load_mahidol_aqi_to_warehouse(json_data)
    else:
        logger.info("No new data to load; skipping warehouse insertion.")

    # 4. Branch: threshold check → notify
    exceedances = aqi_threshold_branch(json_data)
    notify_pm25_alert(exceedances)

    logger.info("PM2.5 Risk Alert Pipeline completed successfully.")


# -------------------------------------------------------------------------
# Deployment configuration (metadata only – actual deployment should be created via Prefect CLI/UI)
# -------------------------------------------------------------------------

deployment_metadata = {
    "flow_name": "pm2.5_risk_alert_pipeline",
    "deployment_name": "pm2.5_risk_alert_pipeline_deployment",
    "work_pool": "default-agent-pool",
    "schedule": {
        "enabled": False,
        "cron": None,
        "timezone": "UTC",
        "catchup": False,
    },
    "prefect_version": "2.14.0",
    "task_runner": "SequentialTaskRunner",
}

if __name__ == "__main__":
    # Running the flow locally for testing/debugging
    pm2_5_risk_alert_pipeline()