# Generated by Prefect 2.x Code Generator
# Generation Metadata:
# - Name: AirVisual_Pipeline_Lat_Long_v1
# - Description: No description provided.
# - Pattern: sequential
# - Schedule Configuration:
#   - Enabled: False
#   - Expression: None
#   - Timezone: UTC
#   - Catchup: False
# - Orchestrator-Specific Configuration:
#   - flow_name: airvisual_pipeline_lat_long_v1
#   - deployment_name: airvisual_pipeline_lat_long_v1_deployment
#   - work_pool: default-agent-pool
#   - task_runner: SequentialTaskRunner
#   - prefect_version: 2.14.0

from prefect import flow, task, get_run_logger
from prefect.tasks import task_input_hash
from prefect.blocks.system import Secret
from prefect.filesystems import LocalFileSystem
from prefect_sqlalchemy import DatabaseCredentials
from prefect_sqlalchemy.database import sqlalchemy_result
from prefect.exceptions import PrefectException
import requests
import pandas as pd

# Task: Fetch AirVisual Data
@task(retries=2, retry_delay_seconds=10)
def get_airvisual_data_hourly():
    """
    Fetches hourly AirVisual data using the AirVisual API.
    """
    logger = get_run_logger()
    airvisual_api = Secret.load("airvisual_api").get()
    url = f"https://api.airvisual.com/v2/nearest_city?key={airvisual_api}"
    response = requests.get(url)
    response.raise_for_status()
    data = response.json()
    logger.info("Fetched AirVisual data successfully.")
    return data

# Task: Read and Validate AirVisual Data
@task(retries=2, retry_delay_seconds=10)
def read_data_airvisual(data):
    """
    Reads and validates the fetched AirVisual data.
    """
    logger = get_run_logger()
    df = pd.DataFrame(data['data'])
    if df.empty:
        raise ValueError("No data to process.")
    logger.info("Read and validated AirVisual data successfully.")
    return df

# Task: Load AirVisual Data to PostgreSQL
@task(retries=2, retry_delay_seconds=10)
def load_data_airvisual_to_postgresql(df):
    """
    Loads the validated AirVisual data into a PostgreSQL database.
    """
    logger = get_run_logger()
    postgresql_db = DatabaseCredentials.load("postgresql_db")
    with postgresql_db.get_engine().begin() as connection:
        df.to_sql('airvisual_data', connection, if_exists='append', index=False)
    logger.info("Loaded AirVisual data to PostgreSQL successfully.")

# Flow: AirVisual_Pipeline_Lat_Long_v1
@flow(name="airvisual_pipeline_lat_long_v1", task_runner="SequentialTaskRunner")
def airvisual_pipeline_lat_long_v1():
    """
    Orchestrates the AirVisual data pipeline.
    """
    logger = get_run_logger()
    logger.info("Starting AirVisual data pipeline.")
    
    # Fetch AirVisual data
    data = get_airvisual_data_hourly()
    
    # Read and validate AirVisual data
    df = read_data_airvisual(data)
    
    # Load AirVisual data to PostgreSQL
    load_data_airvisual_to_postgresql(df)
    
    logger.info("AirVisual data pipeline completed successfully.")

if __name__ == "__main__":
    airvisual_pipeline_lat_long_v1()