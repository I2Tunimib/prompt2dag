# Generated by Prefect Pipeline Generator on 2024-06-28
# Pipeline: extract_airvisual_data_pipeline
# Description: Sequential pipeline to extract, validate, and load AirVisual data into PostgreSQL.

import json
import logging
import os
from pathlib import Path
from typing import Any, Dict, List

import requests
import psycopg2
import psycopg2.extras
from prefect import flow, task
from prefect.task_runners import SequentialTaskRunner
from prefect.blocks.system import Secret
from prefect.filesystems import LocalFileSystem

logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO)


@task(retries=2, retry_delay_seconds=60)
def extract_airvisual_data(
    api_secret_name: str = "airvisual_api",
    tmp_fs_block_name: str = "local_tmp_json",
    output_filename: str = "airvisual_raw.json",
) -> Path:
    """
    Pulls raw AirVisual data from the API and stores it as a JSON file
    in a local temporary filesystem.

    Args:
        api_secret_name: Name of the Prefect Secret block containing the API key.
        tmp_fs_block_name: Name of the LocalFileSystem block for temporary storage.
        output_filename: Filename for the raw JSON payload.

    Returns:
        Path to the written JSON file.
    """
    logger.info("Loading AirVisual API secret block: %s", api_secret_name)
    api_secret = Secret.load(api_secret_name)
    api_key = api_secret.get()  # assumes the secret value is the API key string

    # Example endpoint – replace with the actual AirVisual endpoint as needed
    endpoint = "https://api.airvisual.com/v2/cities"
    params = {"key": api_key}
    logger.info("Requesting AirVisual data from %s", endpoint)
    response = requests.get(endpoint, params=params, timeout=30)
    response.raise_for_status()
    data = response.json()

    logger.info("Loading temporary filesystem block: %s", tmp_fs_block_name)
    tmp_fs = LocalFileSystem.load(tmp_fs_block_name)
    output_path = Path(tmp_fs.base_path) / output_filename

    logger.info("Writing raw AirVisual data to %s", output_path)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    with output_path.open("w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

    return output_path


@task(retries=2, retry_delay_seconds=60)
def validate_airvisual_json(
    raw_json_path: Path,
    mapping_fs_block_name: str = "config_mapping_files",
    required_fields: List[str] = None,
) -> Path:
    """
    Validates the structure of the AirVisual JSON payload against a simple schema.
    The schema can be extended using mapping configuration files stored in a
    LocalFileSystem block.

    Args:
        raw_json_path: Path to the raw JSON file produced by ``extract_airvisual_data``.
        mapping_fs_block_name: Name of the LocalFileSystem block containing mapping files.
        required_fields: List of top‑level keys that must exist in the payload.

    Returns:
        Path to the validated JSON file (same as input if validation passes).

    Raises:
        ValueError: If validation fails.
    """
    if required_fields is None:
        required_fields = ["status", "data"]

    logger.info("Loading mapping configuration files from block: %s", mapping_fs_block_name)
    mapping_fs = LocalFileSystem.load(mapping_fs_block_name)
    # Example: you could load a JSON schema file here if needed.
    # schema_path = Path(mapping_fs.base_path) / "airvisual_schema.json"
    # with schema_path.open() as f:
    #     schema = json.load(f)

    logger.info("Reading raw AirVisual JSON from %s", raw_json_path)
    with raw_json_path.open("r", encoding="utf-8") as f:
        payload = json.load(f)

    logger.info("Validating required top‑level fields: %s", required_fields)
    missing = [field for field in required_fields if field not in payload]
    if missing:
        raise ValueError(f"Missing required fields in AirVisual payload: {missing}")

    # Additional validation logic can be added here (e.g., JSON schema validation).

    logger.info("Validation successful for %s", raw_json_path)
    return raw_json_path


@task(retries=2, retry_delay_seconds=60)
def load_airvisual_to_postgresql(
    validated_json_path: Path,
    db_secret_name: str = "postgres_db",
    tmp_fs_block_name: str = "local_tmp_json",
    table_name: str = "airvisual_data",
) -> None:
    """
    Loads validated AirVisual data into a PostgreSQL table.

    Args:
        validated_json_path: Path to the validated JSON file.
        db_secret_name: Name of the Prefect Secret block containing DB connection info.
        tmp_fs_block_name: Name of the LocalFileSystem block (used for consistency; not required here).
        table_name: Target PostgreSQL table.

    Raises:
        psycopg2.DatabaseError: If any database operation fails.
    """
    logger.info("Loading PostgreSQL secret block: %s", db_secret_name)
    db_secret = Secret.load(db_secret_name)
    # Expected secret format: a DSN string, e.g., "postgresql://user:pass@host:port/dbname"
    dsn = db_secret.get()

    logger.info("Reading validated JSON data from %s", validated_json_path)
    with validated_json_path.open("r", encoding="utf-8") as f:
        payload = json.load(f)

    # Extract the actual data portion; adjust according to AirVisual response structure.
    records = payload.get("data", [])
    if not isinstance(records, list):
        records = [records]

    if not records:
        logger.warning("No records found to load into PostgreSQL.")
        return

    # Example: flatten each record into a dict suitable for insertion.
    rows_to_insert = []
    for record in records:
        # This is a placeholder; adapt field extraction to your schema.
        rows_to_insert.append(
            {
                "city": record.get("city"),
                "state": record.get("state"),
                "country": record.get("country"),
                "location": json.dumps(record.get("location")),
                "current": json.dumps(record.get("current")),
            }
        )

    logger.info("Connecting to PostgreSQL.")
    conn = psycopg2.connect(dsn)
    try:
        with conn:
            with conn.cursor() as cur:
                # Ensure target table exists – simple DDL; replace with migrations in production.
                create_table_sql = f"""
                CREATE TABLE IF NOT EXISTS {table_name} (
                    id SERIAL PRIMARY KEY,
                    city TEXT,
                    state TEXT,
                    country TEXT,
                    location JSONB,
                    current JSONB,
                    inserted_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
                );
                """
                cur.execute(create_table_sql)

                # Insert rows
                insert_sql = f"""
                INSERT INTO {table_name} (city, state, country, location, current)
                VALUES %s
                """
                psycopg2.extras.execute_values(
                    cur,
                    insert_sql,
                    [(r["city"], r["state"], r["country"], r["location"], r["current"]) for r in rows_to_insert],
                )
                logger.info("Inserted %d rows into %s.", len(rows_to_insert), table_name)
    finally:
        conn.close()
        logger.info("PostgreSQL connection closed.")


@flow(
    name="extract_airvisual_data_pipeline",
    task_runner=SequentialTaskRunner(),
)
def extract_airvisual_data_pipeline() -> None:
    """
    Orchestrates the extraction, validation, and loading of AirVisual data
    into a PostgreSQL data warehouse.

    The flow follows a strict sequential pattern:
        1. Extract raw data from the AirVisual API.
        2. Validate the JSON payload.
        3. Load the validated data into PostgreSQL.
    """
    raw_path = extract_airvisual_data()
    validated_path = validate_airvisual_json(raw_path)
    load_airvisual_to_postgresql(validated_path)


if __name__ == "__main__":
    # Running the flow directly (useful for local testing)
    extract_airvisual_data_pipeline()