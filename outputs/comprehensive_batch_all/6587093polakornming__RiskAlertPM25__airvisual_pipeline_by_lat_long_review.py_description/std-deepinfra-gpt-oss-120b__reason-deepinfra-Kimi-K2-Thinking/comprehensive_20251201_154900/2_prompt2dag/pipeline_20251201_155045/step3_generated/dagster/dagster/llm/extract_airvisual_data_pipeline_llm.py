# Generated by Dagster code generator
# Date: 2024-06-13
# Description: Dagster job for extracting, validating, and loading AirVisual data.

from dagster import (
    op,
    job,
    In,
    Out,
    RetryPolicy,
    ResourceDefinition,
    fs_io_manager,
    InMemoryIOManager,
    ConfigurableResource,
    InitResourceContext,
    get_dagster_logger,
    executor,
    in_process_executor,
)
import json
import requests
from typing import Any, Dict


class AirVisualAPIResource(ConfigurableResource):
    """Resource for interacting with the AirVisual API."""

    api_key: str
    base_url: str = "https://api.airvisual.com/v2"

    def fetch_data(self, endpoint: str, params: Dict[str, Any] = None) -> Dict[str, Any]:
        """Fetch JSON data from a given AirVisual endpoint."""
        logger = get_dagster_logger()
        url = f"{self.base_url}/{endpoint}"
        params = params or {}
        params["key"] = self.api_key
        logger.info(f"Requesting AirVisual data from {url} with params {params}")
        response = requests.get(url, params=params, timeout=30)
        response.raise_for_status()
        return response.json()


class PostgresResource(ConfigurableResource):
    """Simple PostgreSQL resource placeholder."""

    connection_string: str

    def execute_query(self, query: str, parameters: tuple = None) -> None:
        """Execute a SQL query against the PostgreSQL database."""
        # In a real implementation, you would use a library like psycopg2 or asyncpg.
        # This placeholder simply logs the query.
        logger = get_dagster_logger()
        logger.info(f"Executing SQL query: {query}")
        if parameters:
            logger.info(f"With parameters: {parameters}")


@op(
    name="extract_airvisual_data",
    description="Extract raw AirVisual JSON data using the AirVisual API.",
    out=Out(dict, description="Raw JSON payload from AirVisual."),
    retry_policy=RetryPolicy(max_retries=2),
    required_resource_keys={"airvisual_api"},
)
def extract_airvisual_data(context: InitResourceContext) -> Dict[str, Any]:
    """Calls the AirVisual API and returns the raw JSON response."""
    api: AirVisualAPIResource = context.resources.airvisual_api
    # Example endpoint; adjust as needed.
    raw_data = api.fetch_data(endpoint="city")
    context.log.info("AirVisual data extraction completed.")
    return raw_data


@op(
    name="validate_airvisual_json",
    description="Validate the structure of the AirVisual JSON payload.",
    ins={"raw_json": In(dict)},
    out=Out(dict, description="Validated JSON payload."),
    retry_policy=RetryPolicy(max_retries=2),
)
def validate_airvisual_json(context: InitResourceContext, raw_json: Dict[str, Any]) -> Dict[str, Any]:
    """Performs minimal validation on the AirVisual JSON payload."""
    # Simple validation: ensure expected top‑level keys exist.
    required_keys = {"status", "data"}
    missing = required_keys - raw_json.keys()
    if missing:
        raise ValueError(f"Missing required keys in AirVisual payload: {missing}")

    if raw_json.get("status") != "success":
        raise ValueError(f"AirVisual API returned non‑success status: {raw_json.get('status')}")

    context.log.info("AirVisual JSON validation succeeded.")
    return raw_json


@op(
    name="load_airvisual_to_postgresql",
    description="Load validated AirVisual data into a PostgreSQL data warehouse.",
    ins={"validated_json": In(dict)},
    out=Out(None),
    retry_policy=RetryPolicy(max_retries=2),
    required_resource_keys={"postgres_db"},
)
def load_airvisual_to_postgresql(context: InitResourceContext, validated_json: Dict[str, Any]) -> None:
    """Transforms the validated JSON and inserts it into PostgreSQL."""
    pg: PostgresResource = context.resources.postgres_db

    # Example transformation: flatten the nested structure into a simple record.
    # Adjust according to your schema.
    records = []
    for city_info in validated_json["data"]:
        record = {
            "city": city_info.get("city"),
            "state": city_info.get("state"),
            "country": city_info.get("country"),
            "aqi": city_info.get("current", {}).get("pollution", {}).get("aqius"),
            "timestamp": city_info.get("current", {}).get("pollution", {}).get("ts"),
        }
        records.append(record)

    # Insert each record; in production use batch inserts.
    insert_sql = """
        INSERT INTO airvisual_measurements (city, state, country, aqi, timestamp)
        VALUES (%(city)s, %(state)s, %(country)s, %(aqi)s, %(timestamp)s)
        ON CONFLICT DO NOTHING;
    """
    for rec in records:
        pg.execute_query(insert_sql, parameters=tuple(rec.values()))

    context.log.info(f"Loaded {len(records)} AirVisual records into PostgreSQL.")


@job(
    name="extract_airvisual_data_pipeline",
    description="No description provided.",
    executor_def=in_process_executor,
    resource_defs={
        "airvisual_api": AirVisualAPIResource,
        "postgres_db": PostgresResource,
        "local_tmp_json": fs_io_manager,
        "config_mapping_files": fs_io_manager,
    },
)
def extract_airvisual_data_pipeline():
    """Sequential pipeline that extracts, validates, and loads AirVisual data."""
    raw = extract_airvisual_data()
    validated = validate_airvisual_json(raw)
    load_airvisual_to_postgresql(validated)