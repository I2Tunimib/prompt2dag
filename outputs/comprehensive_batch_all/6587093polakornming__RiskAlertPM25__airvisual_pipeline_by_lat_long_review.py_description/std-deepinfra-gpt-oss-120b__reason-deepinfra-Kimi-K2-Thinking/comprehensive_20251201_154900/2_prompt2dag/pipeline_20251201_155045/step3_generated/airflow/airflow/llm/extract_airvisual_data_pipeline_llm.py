# Generated by Airflow DAG generator on 2024-06-28
"""
DAG: extract_airvisual_data_pipeline
Description: No description provided.
Pattern: Sequential execution of extraction, validation, and loading tasks.
"""

from datetime import datetime, timedelta
import os
import json

import requests
from airflow import DAG
from airflow.decorators import task
from airflow.exceptions import AirflowException
from airflow.hooks.base import BaseHook
from airflow.providers.postgres.hooks.postgres import PostgresHook

# Default arguments applied to all tasks
DEFAULT_ARGS = {
    "owner": "airflow",
    "depends_on_past": False,
    "retries": 2,
    "retry_delay": timedelta(minutes=5),
    "email_on_failure": False,
    "email_on_retry": False,
}

# DAG definition
with DAG(
    dag_id="extract_airvisual_data_pipeline",
    description="No description provided.",
    schedule_interval=None,  # Disabled schedule
    start_date=days_ago(1),
    catchup=False,
    default_args=DEFAULT_ARGS,
    tags=["airvisual", "etl"],
    max_active_runs=1,
) as dag:

    @task(task_id="extract_airvisual_data", retries=2)
    def extract_airvisual_data() -> str:
        """
        Extract data from the AirVisual API and store it as a temporary JSON file.

        Returns:
            str: Path to the temporary JSON file.
        """
        # Retrieve connection details from Airflow
        conn = BaseHook.get_connection("airvisual_api")
        base_url = conn.host.rstrip("/")  # e.g., https://api.airvisual.com/v2/city
        api_key = conn.password  # Assume API key stored in password field

        # Build request
        params = {"key": api_key}
        try:
            response = requests.get(base_url, params=params, timeout=30)
            response.raise_for_status()
        except requests.RequestException as exc:
            raise AirflowException(f"Failed to fetch AirVisual data: {exc}") from exc

        data = response.json()

        # Determine temporary file location
        tmp_dir = os.getenv("AIRFLOW_TMP_DIR", "/tmp")
        tmp_path = os.path.join(tmp_dir, f"airvisual_{datetime.utcnow().strftime('%Y%m%d%H%M%S')}.json")

        # Write JSON to file
        try:
            with open(tmp_path, "w", encoding="utf-8") as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        except OSError as exc:
            raise AirflowException(f"Unable to write temporary file {tmp_path}: {exc}") from exc

        return tmp_path

    @task(task_id="validate_airvisual_json", retries=2)
    def validate_airvisual_json(json_path: str) -> str:
        """
        Validate the structure of the extracted AirVisual JSON file.

        Args:
            json_path: Path to the JSON file produced by the extraction task.

        Returns:
            str: The same path if validation succeeds.
        """
        if not os.path.isfile(json_path):
            raise AirflowException(f"JSON file not found at {json_path}")

        try:
            with open(json_path, "r", encoding="utf-8") as f:
                data = json.load(f)
        except (OSError, json.JSONDecodeError) as exc:
            raise AirflowException(f"Failed to read or parse JSON file {json_path}: {exc}") from exc

        # Basic validation rules – adjust as needed for real schema
        required_keys = {"status", "data"}
        missing = required_keys - data.keys()
        if missing:
            raise AirflowException(f"JSON validation error – missing keys: {missing}")

        # Example additional check
        if data.get("status") != "success":
            raise AirflowException(f"API returned non‑success status: {data.get('status')}")

        return json_path

    @task(task_id="load_airvisual_to_postgresql", retries=2)
    def load_airvisual_to_postgresql(json_path: str) -> str:
        """
        Load the validated JSON data into a PostgreSQL table.

        Args:
            json_path: Path to the validated JSON file.

        Returns:
            str: Confirmation message.
        """
        if not os.path.isfile(json_path):
            raise AirflowException(f"JSON file not found at {json_path}")

        try:
            with open(json_path, "r", encoding="utf-8") as f:
                data = json.load(f)
        except (OSError, json.JSONDecodeError) as exc:
            raise AirflowException(f"Failed to read JSON file {json_path}: {exc}") from exc

        # Insert JSON into PostgreSQL – assumes a table `airvisual_data` with a JSONB column `data`
        pg_hook = PostgresHook(postgres_conn_id="postgres_db")
        insert_sql = "INSERT INTO airvisual_data (data) VALUES (%s);"
        try:
            pg_hook.run(insert_sql, parameters=(json.dumps(data),))
        except Exception as exc:
            raise AirflowException(f"Failed to insert data into PostgreSQL: {exc}") from exc

        # Clean up temporary file
        try:
            os.remove(json_path)
        except OSError as exc:
            # Log but do not fail the DAG
            dag.log.warning("Could not delete temporary file %s: %s", json_path, exc)

        return "AirVisual data loaded successfully."

    # Define task pipeline
    extracted_path = extract_airvisual_data()
    validated_path = validate_airvisual_json(extracted_path)
    load_result = load_airvisual_to_postgresql(validated_path)

    # Explicit dependencies (optional, as TaskFlow handles them)
    extracted_path >> validated_path >> load_result