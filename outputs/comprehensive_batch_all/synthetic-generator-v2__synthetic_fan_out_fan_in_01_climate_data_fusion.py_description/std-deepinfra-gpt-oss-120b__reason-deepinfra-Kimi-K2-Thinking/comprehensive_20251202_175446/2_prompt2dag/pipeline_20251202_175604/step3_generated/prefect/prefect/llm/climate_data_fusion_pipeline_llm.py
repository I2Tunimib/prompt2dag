# Generated by Prefect Pipeline Generator
# Pipeline: climate_data_fusion_pipeline
# Description: Climate data fusion workflow that downloads, normalizes, and merges weather station data.
# Generation Date: 2024-06-20
# Prefect Version: 2.14.0

import os
import ftplib
import requests
import pandas as pd
from pathlib import Path
from typing import List

from prefect import flow, task, get_run_logger
from prefect.task_runners import ConcurrentTaskRunner
from prefect.deployments import DeploymentSpec
from prefect.server.schemas.schedules import CronSchedule
from prefect.blocks.system import Secret
from prefect.filesystems import LocalFileSystem

# -------------------------------------------------------------------------
# Configuration and helper utilities
# -------------------------------------------------------------------------

BASE_DIR = Path(__file__).parent
RAW_DIR = BASE_DIR / "data" / "raw"
NORMALIZED_DIR = BASE_DIR / "data" / "normalized"
FINAL_DIR = BASE_DIR / "data" / "final"

for directory in (RAW_DIR, NORMALIZED_DIR, FINAL_DIR):
    directory.mkdir(parents=True, exist_ok=True)

# Load secret blocks (these must be created in the Prefect UI/CLI beforehand)
NOAA_FTP_SECRET = Secret.load("noaa_ftp")
ECMWF_HTTPS_SECRET = Secret.load("ecmwf_https")
JMA_HTTPS_SECRET = Secret.load("jma_https")
METOFFICE_HTTPS_SECRET = Secret.load("metoffice_https")
BOM_HTTPS_SECRET = Secret.load("bom_https")

# Local filesystem block for final output
LOCAL_FS = LocalFileSystem.load("local_filesystem")


def _write_file(path: Path, content: bytes):
    """Utility to write binary content to a file."""
    with open(path, "wb") as f:
        f.write(content)


# -------------------------------------------------------------------------
# Download tasks
# -------------------------------------------------------------------------

@task(retries=3, retry_delay_seconds=60, name="Download BOM Weather Data")
def download_bom() -> Path:
    """Download BOM weather data via HTTPS."""
    logger = get_run_logger()
    url = BOM_HTTPS_SECRET.get()  # Expected to contain the full download URL
    logger.info("Downloading BOM data from %s", url)
    response = requests.get(url, timeout=120)
    response.raise_for_status()
    out_path = RAW_DIR / "bom_raw.csv"
    _write_file(out_path, response.content)
    logger.info("BOM data saved to %s", out_path)
    return out_path


@task(retries=3, retry_delay_seconds=60, name="Download ECMWF Weather Data")
def download_ecmwf() -> Path:
    """Download ECMWF weather data via HTTPS."""
    logger = get_run_logger()
    url = ECMWF_HTTPS_SECRET.get()
    logger.info("Downloading ECMWF data from %s", url)
    response = requests.get(url, timeout=120)
    response.raise_for_status()
    out_path = RAW_DIR / "ecmwf_raw.csv"
    _write_file(out_path, response.content)
    logger.info("ECMWF data saved to %s", out_path)
    return out_path


@task(retries=3, retry_delay_seconds=60, name="Download JMA Weather Data")
def download_jma() -> Path:
    """Download JMA weather data via HTTPS."""
    logger = get_run_logger()
    url = JMA_HTTPS_SECRET.get()
    logger.info("Downloading JMA data from %s", url)
    response = requests.get(url, timeout=120)
    response.raise_for_status()
    out_path = RAW_DIR / "jma_raw.csv"
    _write_file(out_path, response.content)
    logger.info("JMA data saved to %s", out_path)
    return out_path


@task(retries=3, retry_delay_seconds=60, name="Download MetOffice Weather Data")
def download_metoffice() -> Path:
    """Download MetOffice weather data via HTTPS."""
    logger = get_run_logger()
    url = METOFFICE_HTTPS_SECRET.get()
    logger.info("Downloading MetOffice data from %s", url)
    response = requests.get(url, timeout=120)
    response.raise_for_status()
    out_path = RAW_DIR / "metoffice_raw.csv"
    _write_file(out_path, response.content)
    logger.info("MetOffice data saved to %s", out_path)
    return out_path


@task(retries=3, retry_delay_seconds=60, name="Download NOAA Weather Data")
def download_noaa() -> Path:
    """Download NOAA weather data via FTP."""
    logger = get_run_logger()
    ftp_info = NOAA_FTP_SECRET.get()  # Expected format: ftp://user:pass@host/path/file.csv
    logger.info("Downloading NOAA data from %s", ftp_info)

    # Parse FTP URL
    from urllib.parse import urlparse

    parsed = urlparse(ftp_info)
    host = parsed.hostname
    username = parsed.username or "anonymous"
    password = parsed.password or ""
    remote_path = parsed.path.lstrip("/")

    with ftplib.FTP(host) as ftp:
        ftp.login(user=username, passwd=password)
        out_path = RAW_DIR / "noaa_raw.csv"
        with open(out_path, "wb") as f:
            ftp.retrbinary(f"RETR {remote_path}", f.write)
    logger.info("NOAA data saved to %s", out_path)
    return out_path


# -------------------------------------------------------------------------
# Normalization tasks
# -------------------------------------------------------------------------

def _normalize_dataframe(df: pd.DataFrame, source: str) -> pd.DataFrame:
    """Standardize column names and data types for climate datasets."""
    # Example normalization: rename columns to a common schema
    column_mapping = {
        "temp": "temperature",
        "temp_c": "temperature",
        "temperature_celsius": "temperature",
        "precip": "precipitation",
        "rainfall": "precipitation",
        "date": "timestamp",
        "datetime": "timestamp",
    }
    df = df.rename(columns=lambda c: column_mapping.get(c.lower(), c.lower()))
    # Ensure required columns exist
    required = ["timestamp", "temperature", "precipitation"]
    for col in required:
        if col not in df.columns:
            df[col] = pd.NA
    # Convert timestamp to datetime
    df["timestamp"] = pd.to_datetime(df["timestamp"], errors="coerce")
    # Cast numeric columns
    df["temperature"] = pd.to_numeric(df["temperature"], errors="coerce")
    df["precipitation"] = pd.to_numeric(df["precipitation"], errors="coerce")
    df["source"] = source
    return df[required + ["source"]]


@task(retries=3, retry_delay_seconds=30, name="Normalize BOM Weather Data")
def normalize_bom(raw_path: Path) -> Path:
    """Normalize BOM raw CSV into a clean Parquet file."""
    logger = get_run_logger()
    logger.info("Normalizing BOM data from %s", raw_path)
    df = pd.read_csv(raw_path)
    norm_df = _normalize_dataframe(df, source="BOM")
    out_path = NORMALIZED_DIR / "bom_normalized.parquet"
    norm_df.to_parquet(out_path, index=False)
    logger.info("BOM normalized data written to %s", out_path)
    return out_path


@task(retries=3, retry_delay_seconds=30, name="Normalize ECMWF Weather Data")
def normalize_ecmwf(raw_path: Path) -> Path:
    """Normalize ECMWF raw CSV into a clean Parquet file."""
    logger = get_run_logger()
    logger.info("Normalizing ECMWF data from %s", raw_path)
    df = pd.read_csv(raw_path)
    norm_df = _normalize_dataframe(df, source="ECMWF")
    out_path = NORMALIZED_DIR / "ecmwf_normalized.parquet"
    norm_df.to_parquet(out_path, index=False)
    logger.info("ECMWF normalized data written to %s", out_path)
    return out_path


@task(retries=3, retry_delay_seconds=30, name="Normalize JMA Weather Data")
def normalize_jma(raw_path: Path) -> Path:
    """Normalize JMA raw CSV into a clean Parquet file."""
    logger = get_run_logger()
    logger.info("Normalizing JMA data from %s", raw_path)
    df = pd.read_csv(raw_path)
    norm_df = _normalize_dataframe(df, source="JMA")
    out_path = NORMALIZED_DIR / "jma_normalized.parquet"
    norm_df.to_parquet(out_path, index=False)
    logger.info("JMA normalized data written to %s", out_path)
    return out_path


@task(retries=3, retry_delay_seconds=30, name="Normalize MetOffice Weather Data")
def normalize_metoffice(raw_path: Path) -> Path:
    """Normalize MetOffice raw CSV into a clean Parquet file."""
    logger = get_run_logger()
    logger.info("Normalizing MetOffice data from %s", raw_path)
    df = pd.read_csv(raw_path)
    norm_df = _normalize_dataframe(df, source="MetOffice")
    out_path = NORMALIZED_DIR / "metoffice_normalized.parquet"
    norm_df.to_parquet(out_path, index=False)
    logger.info("MetOffice normalized data written to %s", out_path)
    return out_path


@task(retries=3, retry_delay_seconds=30, name="Normalize NOAA Weather Data")
def normalize_noaa(raw_path: Path) -> Path:
    """Normalize NOAA raw CSV into a clean Parquet file."""
    logger = get_run_logger()
    logger.info("Normalizing NOAA data from %s", raw_path)
    df = pd.read_csv(raw_path)
    norm_df = _normalize_dataframe(df, source="NOAA")
    out_path = NORMALIZED_DIR / "noaa_normalized.parquet"
    norm_df.to_parquet(out_path, index=False)
    logger.info("NOAA normalized data written to %s", out_path)
    return out_path


# -------------------------------------------------------------------------
# Merge task
# -------------------------------------------------------------------------

@task(retries=3, retry_delay_seconds=60, name="Merge Climate Datasets")
def merge_climate_data(
    bom_path: Path,
    ecmwf_path: Path,
    jma_path: Path,
    metoffice_path: Path,
    noaa_path: Path,
) -> Path:
    """Merge all normalized climate datasets into a single Parquet file."""
    logger = get_run_logger()
    logger.info("Merging normalized datasets")
    paths: List[Path] = [bom_path, ecmwf_path, jma_path, metoffice_path, noaa_path]
    dfs = []
    for p in paths:
        logger.debug("Reading %s", p)
        dfs.append(pd.read_parquet(p))
    merged_df = pd.concat(dfs, ignore_index=True)
    merged_df.sort_values("timestamp", inplace=True)
    out_path = FINAL_DIR / "climate_merged.parquet"
    merged_df.to_parquet(out_path, index=False)
    logger.info("Merged climate dataset written to %s", out_path)

    # Upload to the configured LocalFileSystem block (optional)
    LOCAL_FS.upload_from_path(local_path=str(out_path), to_path="climate_merged.parquet")
    logger.info("Uploaded merged file to LocalFileSystem block '%s'", LOCAL_FS.block_name)

    return out_path


# -------------------------------------------------------------------------
# Main flow definition
# -------------------------------------------------------------------------

@flow(
    name="climate_data_fusion_pipeline",
    task_runner=ConcurrentTaskRunner(),
)
def climate_data_fusion_pipeline():
    """
    Orchestrates the climate data fusion pipeline:
    1. Downloads raw data from five agencies.
    2. Normalizes each dataset to a common schema.
    3. Merges the normalized datasets into a unified Parquet file.
    """
    # Download stage (fan-out)
    bom_raw = download_bom()
    ecmwf_raw = download_ecmwf()
    jma_raw = download_jma()
    metoffice_raw = download_metoffice()
    noaa_raw = download_noaa()

    # Normalization stage (fan-in per source)
    bom_norm = normalize_bom(bom_raw)
    ecmwf_norm = normalize_ecmwf(ecmwf_raw)
    jma_norm = normalize_jma(jma_raw)
    metoffice_norm = normalize_metoffice(metoffice_raw)
    noaa_norm = normalize_noaa(noaa_raw)

    # Merge stage (fan-in)
    merge_climate_data(
        bom_path=bom_norm,
        ecmwf_path=ecmwf_norm,
        jma_path=jma_norm,
        metoffice_path=metoffice_norm,
        noaa_path=noaa_norm,
    )


# -------------------------------------------------------------------------
# Deployment specification
# -------------------------------------------------------------------------

DeploymentSpec(
    name="climate_data_fusion_pipeline_deployment",
    flow=climate_data_fusion_pipeline,
    schedule=CronSchedule(cron="0 0 * * *", timezone="UTC", day_or=False),  # @daily UTC
    tags=["climate", "fusion"],
    parameters={},
    work_pool_name="default-agent-pool",
    enforce_parameter_schema=False,
)

if __name__ == "__main__":
    # Allows local execution for testing/debugging
    climate_data_fusion_pipeline()