# Generated by Airflow DAG generator on 2024-06-28 12:00:00 UTC
"""
DAG: climate_data_fusion_pipeline
Description: Implements a climate data fusion workflow that downloads weather station data from five
meteorological agencies, normalizes each dataset, and merges them into a unified climate dataset.
Pattern: fan-in
"""

from __future__ import annotations

import os
import json
import logging
import tempfile
from datetime import datetime, timedelta

import pandas as pd
import requests
from ftplib import FTP
from airflow import DAG
from airflow.exceptions import AirflowException
from airflow.utils.dates import days_ago
from airflow.utils.timezone import timezone
from airflow.decorators import task
from airflow.models import Variable
from airflow.hooks.base import BaseHook

# -------------------------------------------------------------------------
# Default arguments and DAG definition
# -------------------------------------------------------------------------
DEFAULT_ARGS = {
    "owner": "airflow",
    "depends_on_past": False,
    "retries": 3,
    "retry_delay": timedelta(minutes=5),
    "email_on_failure": False,
    "email_on_retry": False,
}

with DAG(
    dag_id="climate_data_fusion_pipeline",
    description="Download, normalize and merge climate data from multiple agencies",
    schedule_interval="@daily",
    start_date=days_ago(1),
    catchup=False,
    default_args=DEFAULT_ARGS,
    tags=["climate", "data-fusion"],
    max_active_runs=1,
    timezone=timezone("UTC"),
) as dag:

    # -------------------------------------------------------------------------
    # Helper utilities
    # -------------------------------------------------------------------------
    def _get_connection_uri(conn_id: str) -> str:
        """Retrieve a connection URI from Airflow's connection store."""
        try:
            conn = BaseHook.get_connection(conn_id)
            return conn.get_uri()
        except Exception as exc:
            raise AirflowException(f"Unable to retrieve connection {conn_id}: {exc}")

    def _save_to_tempfile(content: bytes, suffix: str = ".csv") -> str:
        """Write binary content to a temporary file and return its path."""
        fd, path = tempfile.mkstemp(suffix=suffix)
        with os.fdopen(fd, "wb") as tmp:
            tmp.write(content)
        return path

    # -------------------------------------------------------------------------
    # Download tasks
    # -------------------------------------------------------------------------
    @task(retries=3, retry_delay=timedelta(minutes=5), task_id="download_bom")
    def download_bom() -> str:
        """Download BOM weather data via HTTPS."""
        conn_uri = _get_connection_uri("bom_https")
        logging.info("Downloading BOM data from %s", conn_uri)
        try:
            response = requests.get(conn_uri, timeout=60)
            response.raise_for_status()
            file_path = _save_to_tempfile(response.content, suffix=".csv")
            logging.info("BOM data saved to %s", file_path)
            return file_path
        except Exception as exc:
            raise AirflowException(f"BOM download failed: {exc}")

    @task(retries=3, retry_delay=timedelta(minutes=5), task_id="download_ecmwf")
    def download_ecmwf() -> str:
        """Download ECMWF weather data via HTTPS."""
        conn_uri = _get_connection_uri("ecmwf_https")
        logging.info("Downloading ECMWF data from %s", conn_uri)
        try:
            response = requests.get(conn_uri, timeout=60)
            response.raise_for_status()
            file_path = _save_to_tempfile(response.content, suffix=".csv")
            logging.info("ECMWF data saved to %s", file_path)
            return file_path
        except Exception as exc:
            raise AirflowException(f"ECMWF download failed: {exc}")

    @task(retries=3, retry_delay=timedelta(minutes=5), task_id="download_jma")
    def download_jma() -> str:
        """Download JMA weather data via HTTPS."""
        conn_uri = _get_connection_uri("jma_https")
        logging.info("Downloading JMA data from %s", conn_uri)
        try:
            response = requests.get(conn_uri, timeout=60)
            response.raise_for_status()
            file_path = _save_to_tempfile(response.content, suffix=".csv")
            logging.info("JMA data saved to %s", file_path)
            return file_path
        except Exception as exc:
            raise AirflowException(f"JMA download failed: {exc}")

    @task(retries=3, retry_delay=timedelta(minutes=5), task_id="download_metoffice")
    def download_metoffice() -> str:
        """Download MetOffice weather data via HTTPS."""
        conn_uri = _get_connection_uri("metoffice_https")
        logging.info("Downloading MetOffice data from %s", conn_uri)
        try:
            response = requests.get(conn_uri, timeout=60)
            response.raise_for_status()
            file_path = _save_to_tempfile(response.content, suffix=".csv")
            logging.info("MetOffice data saved to %s", file_path)
            return file_path
        except Exception as exc:
            raise AirflowException(f"MetOffice download failed: {exc}")

    @task(retries=3, retry_delay=timedelta(minutes=5), task_id="download_noaa")
    def download_noaa() -> str:
        """Download NOAA weather data via FTP."""
        conn = BaseHook.get_connection("noaa_ftp")
        host = conn.host
        port = conn.port or 21
        user = conn.login or "anonymous"
        password = conn.password or ""
        remote_path = conn.extra_dejson.get("remote_path", "/")
        logging.info("Connecting to NOAA FTP %s:%s", host, port)

        try:
            with FTP() as ftp:
                ftp.connect(host, port, timeout=60)
                ftp.login(user=user, passwd=password)
                ftp.cwd(remote_path)
                filenames = ftp.nlst()
                # For simplicity, download the first file
                filename = filenames[0]
                logging.info("Downloading %s from NOAA FTP", filename)
                with tempfile.NamedTemporaryFile(delete=False, suffix=".csv") as tmp_file:
                    ftp.retrbinary(f"RETR {filename}", tmp_file.write)
                    tmp_path = tmp_file.name
                logging.info("NOAA data saved to %s", tmp_path)
                return tmp_path
        except Exception as exc:
            raise AirflowException(f"NOAA download failed: {exc}")

    # -------------------------------------------------------------------------
    # Normalization tasks
    # -------------------------------------------------------------------------
    @task(retries=3, retry_delay=timedelta(minutes=5), task_id="normalize_bom")
    def normalize_bom(raw_path: str) -> str:
        """Normalize BOM dataset to a standard schema."""
        logging.info("Normalizing BOM data from %s", raw_path)
        try:
            df = pd.read_csv(raw_path)
            # Example normalization steps
            df = df.rename(columns=lambda x: x.strip().lower())
            df["source"] = "bom"
            normalized_path = _save_to_tempfile(df.to_parquet(index=False), suffix=".parquet")
            logging.info("Normalized BOM data saved to %s", normalized_path)
            return normalized_path
        except Exception as exc:
            raise AirflowException(f"BOM normalization failed: {exc}")

    @task(retries=3, retry_delay=timedelta(minutes=5), task_id="normalize_ecmwf")
    def normalize_ecmwf(raw_path: str) -> str:
        """Normalize ECMWF dataset to a standard schema."""
        logging.info("Normalizing ECMWF data from %s", raw_path)
        try:
            df = pd.read_csv(raw_path)
            df = df.rename(columns=lambda x: x.strip().lower())
            df["source"] = "ecmwf"
            normalized_path = _save_to_tempfile(df.to_parquet(index=False), suffix=".parquet")
            logging.info("Normalized ECMWF data saved to %s", normalized_path)
            return normalized_path
        except Exception as exc:
            raise AirflowException(f"ECMWF normalization failed: {exc}")

    @task(retries=3, retry_delay=timedelta(minutes=5), task_id="normalize_jma")
    def normalize_jma(raw_path: str) -> str:
        """Normalize JMA dataset to a standard schema."""
        logging.info("Normalizing JMA data from %s", raw_path)
        try:
            df = pd.read_csv(raw_path)
            df = df.rename(columns=lambda x: x.strip().lower())
            df["source"] = "jma"
            normalized_path = _save_to_tempfile(df.to_parquet(index=False), suffix=".parquet")
            logging.info("Normalized JMA data saved to %s", normalized_path)
            return normalized_path
        except Exception as exc:
            raise AirflowException(f"JMA normalization failed: {exc}")

    @task(retries=3, retry_delay=timedelta(minutes=5), task_id="normalize_metoffice")
    def normalize_metoffice(raw_path: str) -> str:
        """Normalize MetOffice dataset to a standard schema."""
        logging.info("Normalizing MetOffice data from %s", raw_path)
        try:
            df = pd.read_csv(raw_path)
            df = df.rename(columns=lambda x: x.strip().lower())
            df["source"] = "metoffice"
            normalized_path = _save_to_tempfile(df.to_parquet(index=False), suffix=".parquet")
            logging.info("Normalized MetOffice data saved to %s", normalized_path)
            return normalized_path
        except Exception as exc:
            raise AirflowException(f"MetOffice normalization failed: {exc}")

    @task(retries=3, retry_delay=timedelta(minutes=5), task_id="normalize_noaa")
    def normalize_noaa(raw_path: str) -> str:
        """Normalize NOAA dataset to a standard schema."""
        logging.info("Normalizing NOAA data from %s", raw_path)
        try:
            df = pd.read_csv(raw_path)
            df = df.rename(columns=lambda x: x.strip().lower())
            df["source"] = "noaa"
            normalized_path = _save_to_tempfile(df.to_parquet(index=False), suffix=".parquet")
            logging.info("Normalized NOAA data saved to %s", normalized_path)
            return normalized_path
        except Exception as exc:
            raise AirflowException(f"NOAA normalization failed: {exc}")

    # -------------------------------------------------------------------------
    # Merge task
    # -------------------------------------------------------------------------
    @task(retries=3, retry_delay=timedelta(minutes=5), task_id="merge_climate_data")
    def merge_climate_data(
        bom_path: str,
        ecmwf_path: str,
        jma_path: str,
        metoffice_path: str,
        noaa_path: str,
    ) -> str:
        """Merge all normalized climate datasets into a single Parquet file."""
        logging.info("Merging normalized climate datasets")
        try:
            dfs = []
            for path in [bom_path, ecmwf_path, jma_path, metoffice_path, noaa_path]:
                df = pd.read_parquet(path)
                dfs.append(df)
            merged_df = pd.concat(dfs, ignore_index=True)
            # Example: ensure consistent datetime column
            if "date" in merged_df.columns:
                merged_df["date"] = pd.to_datetime(merged_df["date"])
            output_dir = Variable.get("climate_output_dir", default_var="/tmp/climate")
            os.makedirs(output_dir, exist_ok=True)
            output_path = os.path.join(output_dir, f"climate_merged_{datetime.utcnow().strftime('%Y%m%d')}.parquet")
            merged_df.to_parquet(output_path, index=False)
            logging.info("Merged climate dataset written to %s", output_path)
            return output_path
        except Exception as exc:
            raise AirflowException(f"Merging climate data failed: {exc}")

    # -------------------------------------------------------------------------
    # Task orchestration
    # -------------------------------------------------------------------------
    # Download tasks
    bom_raw = download_bom()
    ecmwf_raw = download_ecmwf()
    jma_raw = download_jma()
    metoffice_raw = download_metoffice()
    noaa_raw = download_noaa()

    # Normalization tasks (fanâ€‘in)
    bom_norm = normalize_bom(bom_raw)
    ecmwf_norm = normalize_ecmwf(ecmwf_raw)
    jma_norm = normalize_jma(jma_raw)
    metoffice_norm = normalize_metoffice(metoffice_raw)
    noaa_norm = normalize_noaa(noaa_raw)

    # Merge task
    merged_path = merge_climate_data(
        bom_path=bom_norm,
        ecmwf_path=ecmwf_norm,
        jma_path=jma_norm,
        metoffice_path=metoffice_norm,
        noaa_path=noaa_norm,
    )

    # Define explicit dependencies (optional, as they are already implied)
    (
        bom_raw
        >> bom_norm
    )
    (
        ecmwf_raw
        >> ecmwf_norm
    )
    (
        jma_raw
        >> jma_norm
    )
    (
        metoffice_raw
        >> metoffice_norm
    )
    (
        noaa_raw
        >> noaa_norm
    )
    (
        bom_norm,
        ecmwf_norm,
        jma_norm,
        metoffice_norm,
        noaa_norm,
    ) >> merged_path

# End of DAG definition.