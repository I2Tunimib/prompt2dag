# Generated by Airflow DAG generator on 2024-06-13
"""
Airflow DAG: climate_data_fusion_pipeline
Description: No description provided.
Pattern: fanin
"""

from datetime import datetime, timedelta
import logging
import json
import os
from typing import Dict, List

from airflow import DAG
from airflow.exceptions import AirflowException
from airflow.models import Variable
from airflow.utils.dates import days_ago
from airflow.utils.task_group import TaskGroup
from airflow.utils.trigger_rule import TriggerRule
from airflow.hooks.base import BaseHook
from airflow.operators.python import PythonOperator

# ----------------------------------------------------------------------
# Default arguments for all tasks
# ----------------------------------------------------------------------
default_args = {
    "owner": "airflow",
    "depends_on_past": False,
    "retries": 3,
    "retry_delay": timedelta(minutes=5),
    "email_on_failure": False,
    "email_on_retry": False,
}

# ----------------------------------------------------------------------
# Helper functions
# ----------------------------------------------------------------------
def _get_connection_uri(conn_id: str) -> str:
    """
    Retrieve the connection URI for a given Airflow connection id.
    """
    try:
        conn = BaseHook.get_connection(conn_id)
        return conn.get_uri()
    except Exception as exc:
        raise AirflowException(f"Failed to retrieve connection '{conn_id}': {exc}") from exc


def download_agency_data(**context) -> Dict[str, str]:
    """
    Download raw weather data from multiple agency endpoints.
    The function stores the downloaded files in a temporary directory
    and returns a mapping of agency name to local file path.
    """
    logging.info("Starting download of agency weather data")
    agencies = {
        "noaa": "noaa_ftp",
        "ecmwf": "ecmwf_https",
        "jma": "jma_https",
        "metoffice": "metoffice_https",
        "bom": "bom_https",
    }

    download_dir = Variable.get("climate_download_dir", default_var="/tmp/climate_download")
    os.makedirs(download_dir, exist_ok=True)

    result: Dict[str, str] = {}
    for agency, conn_id in agencies.items():
        try:
            uri = _get_connection_uri(conn_id)
            logging.info("Downloading data for %s from %s", agency, uri)

            # Placeholder for actual download logic.
            # For FTP we could use ftplib, for HTTPS requests we could use requests.
            # Here we simply create an empty file to simulate the download.
            file_path = os.path.join(download_dir, f"{agency}_raw.json")
            with open(file_path, "w", encoding="utf-8") as f:
                json.dump({"agency": agency, "data": []}, f)

            result[agency] = file_path
            logging.info("Saved %s data to %s", agency, file_path)
        except Exception as exc:
            logging.error("Error downloading data for %s: %s", agency, exc)
            raise AirflowException(f"Download failed for {agency}") from exc

    # Push the mapping to XCom for downstream tasks
    context["ti"].xcom_push(key="download_paths", value=result)
    return result


def normalize_agency_data(**context) -> Dict[str, str]:
    """
    Normalize raw agency data into a common schema.
    Reads the file paths from XCom (key: download_paths) and writes
    normalized JSON files to a separate directory.
    Returns a mapping of agency name to normalized file path.
    """
    logging.info("Starting normalization of agency weather data")
    ti = context["ti"]
    download_paths: Dict[str, str] = ti.xcom_pull(key="download_paths", task_ids="download_agency_data")
    if not download_paths:
        raise AirflowException("No download paths found in XCom")

    normalized_dir = Variable.get("climate_normalized_dir", default_var="/tmp/climate_normalized")
    os.makedirs(normalized_dir, exist_ok=True)

    normalized_paths: Dict[str, str] = {}
    for agency, raw_path in download_paths.items():
        try:
            with open(raw_path, "r", encoding="utf-8") as f:
                raw_data = json.load(f)

            # Placeholder for actual normalization logic.
            # Here we simply copy the structure and add a normalized flag.
            normalized_data = {
                "agency": agency,
                "normalized": True,
                "records": raw_data.get("data", []),
            }

            norm_path = os.path.join(normalized_dir, f"{agency}_normalized.json")
            with open(norm_path, "w", encoding="utf-8") as f:
                json.dump(normalized_data, f)

            normalized_paths[agency] = norm_path
            logging.info("Normalized data for %s written to %s", agency, norm_path)
        except Exception as exc:
            logging.error("Error normalizing data for %s: %s", agency, exc)
            raise AirflowException(f"Normalization failed for {agency}") from exc

    ti.xcom_push(key="normalized_paths", value=normalized_paths)
    return normalized_paths


def merge_climate_data(**context) -> str:
    """
    Merge all normalized agency datasets into a unified dataset.
    The merged file is stored on the local filesystem (as defined by the
    'local_filesystem' connection) and its path is returned.
    """
    logging.info("Starting merge of normalized climate datasets")
    ti = context["ti"]
    normalized_paths: Dict[str, str] = ti.xcom_pull(key="normalized_paths", task_ids="normalize_agency_data")
    if not normalized_paths:
        raise AirflowException("No normalized paths found in XCom")

    merged_dir = Variable.get("climate_merged_dir", default_var="/tmp/climate_merged")
    os.makedirs(merged_dir, exist_ok=True)
    merged_path = os.path.join(merged_dir, "unified_climate_dataset.json")

    merged_records: List[Dict] = []
    for agency, norm_path in normalized_paths.items():
        try:
            with open(norm_path, "r", encoding="utf-8") as f:
                data = json.load(f)
                records = data.get("records", [])
                merged_records.extend(records)
            logging.info("Loaded %d records from %s", len(records), agency)
        except Exception as exc:
            logging.error("Error reading normalized file %s: %s", norm_path, exc)
            raise AirflowException(f"Failed to read normalized data for {agency}") from exc

    unified_dataset = {
        "generated_at": datetime.utcnow().isoformat() + "Z",
        "source_agencies": list(normalized_paths.keys()),
        "total_records": len(merged_records),
        "records": merged_records,
    }

    try:
        with open(merged_path, "w", encoding="utf-8") as f:
            json.dump(unified_dataset, f)
        logging.info("Unified climate dataset written to %s", merged_path)
    except Exception as exc:
        raise AirflowException(f"Failed to write merged dataset: {exc}") from exc

    # Optionally, you could push the path to XCom for downstream consumption
    ti.xcom_push(key="merged_path", value=merged_path)
    return merged_path


# ----------------------------------------------------------------------
# DAG definition
# ----------------------------------------------------------------------
with DAG(
    dag_id="climate_data_fusion_pipeline",
    description="No description provided.",
    schedule_interval=None,  # Disabled schedule; enable manually or via UI
    start_date=days_ago(1),
    catchup=False,
    default_args=default_args,
    tags=["climate", "fusion", "fanin"],
    is_paused_upon_creation=True,
    render_template_as_native_obj=True,
) as dag:

    # Task: Download Agency Weather Data
    download_agency_data_task = PythonOperator(
        task_id="download_agency_data",
        python_callable=download_agency_data,
        provide_context=True,
        retries=3,
        retry_delay=timedelta(minutes=5),
    )

    # Task: Normalize Agency Weather Data
    normalize_agency_data_task = PythonOperator(
        task_id="normalize_agency_data",
        python_callable=normalize_agency_data,
        provide_context=True,
        retries=3,
        retry_delay=timedelta(minutes=5),
    )

    # Task: Merge Climate Datasets
    merge_climate_data_task = PythonOperator(
        task_id="merge_climate_data",
        python_callable=merge_climate_data,
        provide_context=True,
        retries=3,
        retry_delay=timedelta(minutes=5),
    )

    # ------------------------------------------------------------------
    # Set task dependencies (fan-in pattern)
    # ------------------------------------------------------------------
    download_agency_data_task >> normalize_agency_data_task >> merge_climate_data_task

# End of DAG definition.