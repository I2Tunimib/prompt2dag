# Generated by Prefect Pipeline Generator
# Date: 2024-06-28
# Pipeline: climate_data_fusion_pipeline
# Description: No description provided.
# Prefect version: 2.14.0

from __future__ import annotations

import os
import ftplib
import json
import logging
from pathlib import Path
from typing import List, Dict

import requests
from prefect import flow, task, get_run_logger
from prefect.task_runners import ConcurrentTaskRunner
from prefect.deployments import DeploymentSpec
from prefect.server.schemas.schedules import CronSchedule
from prefect.blocks.system import Secret
from prefect.filesystems import LocalFileSystem

# -------------------------------------------------------------------------
# Helper functions to retrieve connection credentials from Prefect Secret blocks
# -------------------------------------------------------------------------

def _get_secret(secret_name: str) -> str:
    """
    Retrieve a secret value from a Prefect Secret block.

    Args:
        secret_name: Name of the Prefect Secret block.

    Returns:
        The secret string.

    Raises:
        ValueError: If the secret block cannot be loaded.
    """
    try:
        secret_block = Secret.load(secret_name)
        return secret_block.get()
    except Exception as exc:
        raise ValueError(f"Unable to load secret block '{secret_name}': {exc}") from exc


# -------------------------------------------------------------------------
# Task definitions
# -------------------------------------------------------------------------

@task(name="Download Agency Weather Data", retries=3, retry_delay_seconds=60)
def download_agency_data(
    noaa_ftp_secret: str,
    ecmwf_https_secret: str,
    jma_https_secret: str,
    metoffice_https_secret: str,
    bom_https_secret: str,
    download_dir: Path,
) -> Dict[str, List[Path]]:
    """
    Download raw weather data from multiple agencies using provided credentials.

    The function creates a subâ€‘directory per agency inside ``download_dir`` and
    stores the retrieved files there.

    Returns:
        Mapping of agency name to list of downloaded file paths.
    """
    logger = get_run_logger()
    logger.info("Starting download of agency weather data.")
    download_dir.mkdir(parents=True, exist_ok=True)

    agency_files: Dict[str, List[Path]] = {}

    # NOAA (FTP)
    try:
        noaa_dir = download_dir / "noaa"
        noaa_dir.mkdir(exist_ok=True)
        ftp = ftplib.FTP("ftp.ncdc.noaa.gov")
        ftp.login(user="anonymous", passwd=noaa_ftp_secret)
        ftp.cwd("/pub/data/noaa")
        filenames = ftp.nlst()
        for fname in filenames[:5]:  # limit for demo purposes
            local_path = noaa_dir / fname
            with open(local_path, "wb") as f:
                ftp.retrbinary(f"RETR {fname}", f.write)
            agency_files.setdefault("noaa", []).append(local_path)
        ftp.quit()
        logger.info("NOAA data downloaded: %d files.", len(agency_files["noaa"]))
    except Exception as exc:
        logger.error("Failed to download NOAA data: %s", exc)
        raise

    # ECMWF (HTTPS)
    def _download_https(url: str, token: str, target_dir: Path, agency_key: str):
        target_dir.mkdir(parents=True, exist_ok=True)
        headers = {"Authorization": f"Bearer {token}"}
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        filename = url.split("/")[-1] or f"{agency_key}_data.json"
        file_path = target_dir / filename
        file_path.write_bytes(response.content)
        agency_files.setdefault(agency_key, []).append(file_path)

    try:
        _download_https(
            url="https://api.ecmwf.int/v1/data",
            token=ecmwf_https_secret,
            target_dir=download_dir / "ecmwf",
            agency_key="ecmwf",
        )
        logger.info("ECMWF data downloaded.")
    except Exception as exc:
        logger.error("Failed to download ECMWF data: %s", exc)
        raise

    # JMA (HTTPS)
    try:
        _download_https(
            url="https://www.jma.go.jp/en/data",
            token=jma_https_secret,
            target_dir=download_dir / "jma",
            agency_key="jma",
        )
        logger.info("JMA data downloaded.")
    except Exception as exc:
        logger.error("Failed to download JMA data: %s", exc)
        raise

    # MetOffice (HTTPS)
    try:
        _download_https(
            url="https://www.metoffice.gov.uk/api/data",
            token=metoffice_https_secret,
            target_dir=download_dir / "metoffice",
            agency_key="metoffice",
        )
        logger.info("MetOffice data downloaded.")
    except Exception as exc:
        logger.error("Failed to download MetOffice data: %s", exc)
        raise

    # BOM (HTTPS)
    try:
        _download_https(
            url="https://www.bom.gov.au/api/data",
            token=bom_https_secret,
            target_dir=download_dir / "bom",
            agency_key="bom",
        )
        logger.info("BOM data downloaded.")
    except Exception as exc:
        logger.error("Failed to download BOM data: %s", exc)
        raise

    logger.info("All agency data downloaded successfully.")
    return agency_files


@task(name="Normalize Agency Weather Data", retries=3, retry_delay_seconds=60)
def normalize_agency_data(
    raw_data_map: Dict[str, List[Path]],
    normalized_dir: Path,
) -> Dict[str, List[Path]]:
    """
    Normalize raw agency data into a common schema.

    For demonstration, the function reads each file, pretends to transform it,
    and writes a JSON file with a standardized structure.

    Returns:
        Mapping of agency name to list of normalized file paths.
    """
    logger = get_run_logger()
    logger.info("Starting normalization of agency data.")
    normalized_dir.mkdir(parents=True, exist_ok=True)

    normalized_map: Dict[str, List[Path]] = {}

    for agency, files in raw_data_map.items():
        agency_norm_dir = normalized_dir / agency
        agency_norm_dir.mkdir(parents=True, exist_ok=True)
        for raw_path in files:
            try:
                # Dummy transformation: wrap content in a dict with metadata
                content = raw_path.read_bytes()
                normalized_content = {
                    "agency": agency,
                    "source_file": raw_path.name,
                    "data": content.decode(errors="ignore"),
                }
                norm_path = agency_norm_dir / f"{raw_path.stem}_normalized.json"
                norm_path.write_text(json.dumps(normalized_content))
                normalized_map.setdefault(agency, []).append(norm_path)
                logger.debug("Normalized %s -> %s", raw_path, norm_path)
            except Exception as exc:
                logger.error("Failed to normalize %s: %s", raw_path, exc)
                raise

        logger.info("Agency %s normalized: %d files.", agency, len(normalized_map[agency]))

    logger.info("All agency data normalized successfully.")
    return normalized_map


@task(name="Merge Climate Datasets", retries=3, retry_delay_seconds=60)
def merge_climate_data(
    normalized_data_map: Dict[str, List[Path]],
    merged_output_path: Path,
) -> Path:
    """
    Merge all normalized agency datasets into a single unified dataset.

    The merged file is a JSON array where each element corresponds to a
    normalized record from any agency.

    Returns:
        Path to the merged dataset file.
    """
    logger = get_run_logger()
    logger.info("Starting merge of normalized climate data.")
    merged_output_path.parent.mkdir(parents=True, exist_ok=True)

    merged_records = []

    for agency, files in normalized_data_map.items():
        for file_path in files:
            try:
                record = json.loads(file_path.read_text())
                merged_records.append(record)
            except Exception as exc:
                logger.error("Failed to read normalized file %s: %s", file_path, exc)
                raise

    try:
        merged_output_path.write_text(json.dumps(merged_records, indent=2))
        logger.info("Merged dataset written to %s (%d records).", merged_output_path, len(merged_records))
    except Exception as exc:
        logger.error("Failed to write merged dataset: %s", exc)
        raise

    return merged_output_path


# -------------------------------------------------------------------------
# Flow definition
# -------------------------------------------------------------------------

@flow(
    name="climate_data_fusion_pipeline",
    task_runner=ConcurrentTaskRunner(),
)
def climate_data_fusion_pipeline():
    """
    Orchestrates the download, normalization, and merging of climate data from
    multiple weather agencies.
    """
    logger = get_run_logger()
    logger.info("Pipeline execution started.")

    # Load secrets
    noaa_secret = _get_secret("noaa_ftp")
    ecmwf_secret = _get_secret("ecmwf_https")
    jma_secret = _get_secret("jma_https")
    metoffice_secret = _get_secret("metoffice_https")
    bom_secret = _get_secret("bom_https")

    # Define filesystem block for final unified dataset
    local_fs = LocalFileSystem.load("local_filesystem")
    base_path = Path(local_fs.base_path) if hasattr(local_fs, "base_path") else Path.cwd() / "climate_data"

    # Directories for intermediate artifacts
    raw_dir = base_path / "raw"
    normalized_dir = base_path / "normalized"
    merged_dir = base_path / "merged"
    merged_file = merged_dir / "unified_climate_dataset.json"

    # Task execution
    raw_data = download_agency_data(
        noaa_ftp_secret=noaa_secret,
        ecmwf_https_secret=ecmwf_secret,
        jma_https_secret=jma_secret,
        metoffice_https_secret=metoffice_secret,
        bom_https_secret=bom_secret,
        download_dir=raw_dir,
    )

    normalized_data = normalize_agency_data(
        raw_data_map=raw_data,
        normalized_dir=normalized_dir,
    )

    merged_path = merge_climate_data(
        normalized_data_map=normalized_data,
        merged_output_path=merged_file,
    )

    logger.info("Pipeline completed successfully. Unified dataset at %s", merged_path)


# -------------------------------------------------------------------------
# Deployment specification
# -------------------------------------------------------------------------

DeploymentSpec(
    name="climate_data_fusion_pipeline_deployment",
    flow=climate_data_fusion_pipeline,
    schedule=CronSchedule(cron="0 0 * * *", timezone="UTC", day_or=True),  # @daily
    tags=["climate", "fusion"],
    parameters={},
    work_pool_name="default-agent-pool",
    enforce_parameter_schema=False,
    description="Deployment for the climate data fusion pipeline.",
    flow_runner=ConcurrentTaskRunner(),
    paused=True,  # schedule disabled as per specification
)

if __name__ == "__main__":
    # Allow running the flow directly for testing/debugging
    climate_data_fusion_pipeline()