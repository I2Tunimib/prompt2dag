# Generated by Dagster code generator
# Date: 2024-06-13
# Pipeline: climate_data_fusion_pipeline
# Description: No description provided.
# Executor: multiprocess_executor
# Dagster version: 1.5.0

from typing import Any, Dict, List

from dagster import (
    In,
    Out,
    RetryPolicy,
    ResourceDefinition,
    IOManager,
    io_manager,
    job,
    op,
    ConfigurableResource,
    resource,
    fs_io_manager,
    multiprocess_executor,
    in_process_executor,
    ScheduleDefinition,
    DefaultScheduleStatus,
)


# ----------------------------------------------------------------------
# Resource definitions
# ----------------------------------------------------------------------


class NOAAFTPResource(ConfigurableResource):
    """Placeholder resource for NOAA Weather Station FTP connection."""

    host: str = "ftp.noaa.gov"
    username: str = "anonymous"
    password: str = ""

    def get_data(self) -> List[Dict[str, Any]]:
        # In a real implementation, connect to the FTP server and retrieve data.
        return [{"station": "NOAA1", "temp": 15.0}]


class ECMWFHTTPSResource(ConfigurableResource):
    """Placeholder resource for ECMWF Weather Station HTTPS connection."""

    base_url: str = "https://api.ecmwf.int"

    def get_data(self) -> List[Dict[str, Any]]:
        # In a real implementation, perform HTTPS requests to fetch data.
        return [{"station": "ECMWF1", "temp": 16.2}]


class JMAHTTPSResource(ConfigurableResource):
    """Placeholder resource for JMA Weather Station HTTPS connection."""

    base_url: str = "https://api.jma.go.jp"

    def get_data(self) -> List[Dict[str, Any]]:
        return [{"station": "JMA1", "temp": 14.8}]


class MetOfficeHTTPSResource(ConfigurableResource):
    """Placeholder resource for MetOffice Weather Station HTTPS connection."""

    base_url: str = "https://api.metoffice.gov.uk"

    def get_data(self) -> List[Dict[str, Any]]:
        return [{"station": "MET1", "temp": 15.5}]


class BOMHTTPSResource(ConfigurableResource):
    """Placeholder resource for BOM Weather Station HTTPS connection."""

    base_url: str = "https://api.bom.gov.au"

    def get_data(self) -> List[Dict[str, Any]]:
        return [{"station": "BOM1", "temp": 17.0}]


# ----------------------------------------------------------------------
# IO Manager
# ----------------------------------------------------------------------


@io_manager(config_schema={"base_dir": str})
def local_filesystem_io_manager(init_context) -> IOManager:
    """Filesystem IO manager that writes assets to a local directory."""
    base_dir = init_context.resource_config["base_dir"]

    class LocalFSIOManager(IOManager):
        def handle_output(self, context, obj):
            path = f"{base_dir}/{context.step_key}.json"
            with open(path, "w", encoding="utf-8") as f:
                import json

                json.dump(obj, f)

        def load_input(self, context):
            path = f"{base_dir}/{context.upstream_output.step_key}.json"
            with open(path, "r", encoding="utf-8") as f:
                import json

                return json.load(f)

    return LocalFSIOManager()


# ----------------------------------------------------------------------
# Ops
# ----------------------------------------------------------------------


@op(
    name="download_agency_data",
    description="Download raw weather data from all configured agency resources.",
    out=Out(dagster_type=Dict[str, Any]),
    retry_policy=RetryPolicy(max_retries=3, delay=5),
    required_resource_keys={"noaa_ftp", "ecmwf_https", "jma_https", "metoffice_https", "bom_https"},
    executor_def=in_process_executor,
)
def download_agency_data(context) -> Dict[str, Any]:
    """Fetch raw data from each agency resource and aggregate into a single dict."""
    raw_data = {
        "noaa": context.resources.noaa_ftp.get_data(),
        "ecmwf": context.resources.ecmwf_https.get_data(),
        "jma": context.resources.jma_https.get_data(),
        "metoffice": context.resources.metoffice_https.get_data(),
        "bom": context.resources.bom_https.get_data(),
    }
    context.log.info(f"Downloaded raw data from agencies: {list(raw_data.keys())}")
    return raw_data


@op(
    name="normalize_agency_data",
    description="Normalize raw agency data into a common schema.",
    ins={"raw_data": In(dagster_type=Dict[str, Any])},
    out=Out(dagster_type=List[Dict[str, Any]]),
    retry_policy=RetryPolicy(max_retries=3, delay=5),
    executor_def=in_process_executor,
)
def normalize_agency_data(context, raw_data: Dict[str, Any]) -> List[Dict[str, Any]]:
    """Transform each agency's raw payload into a unified record format."""
    normalized = []
    for agency, records in raw_data.items():
        for rec in records:
            normalized.append(
                {
                    "agency": agency,
                    "station_id": rec.get("station"),
                    "temperature_c": rec.get("temp"),
                }
            )
    context.log.info(f"Normalized {len(normalized)} records.")
    return normalized


@op(
    name="merge_climate_data",
    description="Merge normalized climate datasets into a unified dataset.",
    ins={"normalized_data": In(dagster_type=List[Dict[str, Any]])},
    out=Out(dagster_type=Dict[str, Any]),
    retry_policy=RetryPolicy(max_retries=3, delay=5),
    executor_def=in_process_executor,
)
def merge_climate_data(context, normalized_data: List[Dict[str, Any]]) -> Dict[str, Any]:
    """Aggregate normalized records, deduplicate, and compute summary statistics."""
    # Simple deduplication based on (agency, station_id)
    unique = {}
    for rec in normalized_data:
        key = (rec["agency"], rec["station_id"])
        if key not in unique:
            unique[key] = rec

    merged = {
        "total_stations": len(unique),
        "records": list(unique.values()),
    }
    context.log.info(f"Merged dataset contains {merged['total_stations']} unique stations.")
    return merged


# ----------------------------------------------------------------------
# Job definition
# ----------------------------------------------------------------------


@job(
    name="climate_data_fusion_pipeline",
    description="No description provided.",
    executor_def=multiprocess_executor,
    resource_defs={
        "noaa_ftp": NOAAFTPResource(),
        "ecmwf_https": ECMWFHTTPSResource(),
        "jma_https": JMAHTTPSResource(),
        "metoffice_https": MetOfficeHTTPSResource(),
        "bom_https": BOMHTTPSResource(),
        "io_manager": local_filesystem_io_manager,
    },
    # The default IO manager is set to the filesystem manager defined above.
    # Dagster will automatically use the key "io_manager" for step outputs.
)
def climate_data_fusion_pipeline():
    """Orchestrates the download, normalization, and merging of climate data."""
    raw = download_agency_data()
    normalized = normalize_agency_data(raw)
    merge_climate_data(normalized)


# ----------------------------------------------------------------------
# Schedule (disabled)
# ----------------------------------------------------------------------


daily_schedule = ScheduleDefinition(
    job=climate_data_fusion_pipeline,
    cron_schedule="@daily",
    execution_timezone="UTC",
    default_status=DefaultScheduleStatus.INACTIVE,
    description="Daily execution of the climate data fusion pipeline (currently disabled).",
)

# End of generated pipeline code.