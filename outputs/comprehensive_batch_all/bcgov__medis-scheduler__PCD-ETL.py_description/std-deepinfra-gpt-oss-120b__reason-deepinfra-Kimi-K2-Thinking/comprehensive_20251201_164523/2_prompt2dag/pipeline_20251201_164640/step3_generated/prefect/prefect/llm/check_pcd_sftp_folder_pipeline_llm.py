# Generated by Prefect Pipeline Generator
# Pipeline: check_pcd_sftp_folder_pipeline
# Description: No description provided.
# Generated on: 2024-06-28
# Prefect version: 2.14.0

import subprocess
import json
from pathlib import Path
from typing import Any, Dict

from prefect import flow, task, get_run_logger
from prefect.task_runners import ConcurrentTaskRunner
from prefect.exceptions import PrefectException
from prefect.blocks.system import Secret
from prefect.filesystems import LocalFileSystem
from prefect.infrastructure.kubernetes import KubernetesJob

# -------------------------------------------------------------------------
# Helper Functions
# -------------------------------------------------------------------------

def _run_kubernetes_job(job_block_name: str, job_name: str, env: Dict[str, str] | None = None) -> None:
    """
    Execute a Prefect KubernetesJob block.

    Args:
        job_block_name: Name of the registered KubernetesJob block.
        job_name: Identifier for the job (used for logging).
        env: Optional environment variables to inject into the job.

    Raises:
        PrefectException: If the job fails to start or complete.
    """
    logger = get_run_logger()
    logger.info(f"Loading KubernetesJob block '{job_block_name}' for job '{job_name}'.")
    try:
        k8s_job: KubernetesJob = KubernetesJob.load(job_block_name)
    except Exception as exc:
        raise PrefectException(f"Failed to load KubernetesJob block '{job_block_name}': {exc}") from exc

    # Inject environment variables if provided
    if env:
        k8s_job.env = env

    logger.info(f"Submitting Kubernetes job '{job_name}'.")
    try:
        run = k8s_job.run()
        logger.info(f"Kubernetes job '{job_name}' completed with state: {run.state}.")
    except Exception as exc:
        raise PrefectException(f"Kubernetes job '{job_name}' failed: {exc}") from exc


def _run_subprocess(command: list[str]) -> None:
    """
    Execute a shell command using subprocess.run with error handling.

    Args:
        command: List of command arguments.

    Raises:
        PrefectException: If the subprocess exits with a non-zero status.
    """
    logger = get_run_logger()
    logger.info(f"Running subprocess: {' '.join(command)}")
    result = subprocess.run(command, capture_output=True, text=True)
    if result.returncode != 0:
        logger.error(f"Command failed with exit code {result.returncode}")
        logger.error(f"Stdout: {result.stdout}")
        logger.error(f"Stderr: {result.stderr}")
        raise PrefectException(f"Subprocess failed: {' '.join(command)}")
    logger.info(f"Command succeeded. Output: {result.stdout.strip()}")


# -------------------------------------------------------------------------
# Tasks
# -------------------------------------------------------------------------

@task(retries=3, retry_delay_seconds=60)
def check_pcd_sftp_folder() -> None:
    """
    Verify that the PCD SFTP folder contains the expected files.
    Executed as a Kubernetes job.
    """
    logger = get_run_logger()
    logger.info("Starting check of PCD SFTP folder.")
    # Load connections (example usage)
    sftp_fs: LocalFileSystem = LocalFileSystem.load("sftp_folder_conn")
    folder_path = Path(sftp_fs.base_path)  # Assuming base_path is set
    logger.debug(f"Resolved SFTP folder path: {folder_path}")

    # Example: run a script inside Kubernetes that checks the folder
    _run_kubernetes_job(
        job_block_name="k8s_check_pcd_sftp_folder",
        job_name="check-pcd-sftp-folder",
        env={"FOLDER_PATH": str(folder_path)},
    )
    logger.info("PCD SFTP folder check completed.")


@task(retries=3, retry_delay_seconds=60)
def check_pcd_shared_folder() -> None:
    """
    Verify that the PCD shared folder is ready for processing.
    Executed as a Kubernetes job.
    """
    logger = get_run_logger()
    logger.info("Starting check of PCD shared folder.")
    shared_fs: LocalFileSystem = LocalFileSystem.load("shared_folder_conn")
    folder_path = Path(shared_fs.base_path)
    logger.debug(f"Resolved shared folder path: {folder_path}")

    _run_kubernetes_job(
        job_block_name="k8s_check_pcd_shared_folder",
        job_name="check-pcd-shared-folder",
        env={"FOLDER_PATH": str(folder_path)},
    )
    logger.info("PCD shared folder check completed.")


@task(retries=2, retry_delay_seconds=120)
def process_and_load_pcd() -> None:
    """
    Process raw PCD files and load the transformed data into the target system.
    Executed as a Kubernetes job.
    """
    logger = get_run_logger()
    logger.info("Starting processing and loading of PCD data.")
    # Example environment variables that might be needed
    env_vars = {
        "SFTP_PATH": str(Path(LocalFileSystem.load("sftp_folder_conn").base_path)),
        "SHARED_PATH": str(Path(LocalFileSystem.load("shared_folder_conn").base_path)),
    }

    _run_kubernetes_job(
        job_block_name="k8s_process_and_load_pcd",
        job_name="process-and-load-pcd",
        env=env_vars,
    )
    logger.info("Processing and loading of PCD data completed.")


@task(retries=1, retry_delay_seconds=30)
def send_etl_notification() -> None:
    """
    Send an email notification indicating the ETL run status.
    Executed as a local process (e.g., using a Python script or CLI tool).
    """
    logger = get_run_logger()
    logger.info("Sending ETL completion notification.")
    email_secret: Secret = Secret.load("email_notification_conn")
    email_config = json.loads(email_secret.get())
    # Example command; replace with actual notification script/CLI
    command = [
        "python",
        "scripts/send_email.py",
        "--to", email_config["recipients"],
        "--subject", "PCD ETL Run Completed",
        "--body", "The PCD ETL pipeline has finished successfully."
    ]
    _run_subprocess(command)
    logger.info("ETL notification sent.")


@task(retries=2, retry_delay_seconds=60)
def extract_pcd_api() -> None:
    """
    Extract data from various PCD related APIs.
    Executed as a local process.
    """
    logger = get_run_logger()
    logger.info("Starting extraction of PCD API data.")
    # Example: load API secrets and call a script
    api_secret: Secret = Secret.load("api_financial_expense_conn")
    api_config = json.loads(api_secret.get())
    command = [
        "python",
        "scripts/extract_pcd_api.py",
        "--api-key", api_config["api_key"],
        "--endpoint", api_config["endpoint"]
    ]
    _run_subprocess(command)
    logger.info("PCD API data extraction completed.")


# -------------------------------------------------------------------------
# Flow Definition
# -------------------------------------------------------------------------

@flow(
    name="check_pcd_sftp_folder_pipeline",
    task_runner=ConcurrentTaskRunner(),
)
def check_pcd_sftp_folder_pipeline() -> None:
    """
    Orchestrates the PCD SFTP folder validation, shared folder validation,
    data processing, API extraction, and final ETL notification.
    """
    logger = get_run_logger()
    logger.info("Pipeline execution started.")

    # Step 1: Check SFTP folder
    sftp_check = check_pcd_sftp_folder.submit()

    # Step 2: Check shared folder (depends on SFTP check)
    shared_check = check_pcd_shared_folder.submit(wait_for=[sftp_check])

    # Step 3: Extract API data (can run after SFTP check)
    api_extract = extract_pcd_api.submit(wait_for=[sftp_check])

    # Step 4: Process and load PCD data (depends on shared folder check)
    process_load = process_and_load_pcd.submit(wait_for=[shared_check])

    # Step 5: Send ETL notification (depends on processing)
    send_etl_notification.submit(wait_for=[process_load])

    logger.info("Pipeline execution finished.")


# -------------------------------------------------------------------------
# Entry Point
# -------------------------------------------------------------------------

if __name__ == "__main__":
    # Execute the flow locally for testing/debugging
    check_pcd_sftp_folder_pipeline()