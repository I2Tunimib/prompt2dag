# Generated by Dagster Code Generator
# Date: 2024-06-13
# Description: Dagster job for the "check_pcd_sftp_folder_pipeline" ETL workflow.

from dagster import (
    op,
    job,
    RetryPolicy,
    In,
    Out,
    Nothing,
    ResourceDefinition,
    ConfigurableResource,
    InitResourceContext,
    get_dagster_logger,
)
from dagster_k8s import k8s_job_executor
from dagster import in_process_executor

# ----------------------------------------------------------------------
# Resource placeholders
# ----------------------------------------------------------------------


class FsIOManager(ConfigurableResource):
    """Placeholder for a filesystem I/O manager (e.g., SFTP or shared network folder)."""

    base_path: str

    def read(self, path: str) -> str:
        logger = get_dagster_logger()
        logger.info(f"Reading from {self.base_path}/{path}")
        # Implement actual read logic here
        return ""

    def write(self, path: str, data: str) -> None:
        logger = get_dagster_logger()
        logger.info(f"Writing to {self.base_path}/{path}")
        # Implement actual write logic here


class KubernetesCluster(ConfigurableResource):
    """Placeholder for a Kubernetes cluster used to launch jobs."""

    namespace: str

    def launch_job(self, job_name: str, image: str, command: list[str]) -> None:
        logger = get_dagster_logger()
        logger.info(
            f"Launching K8s job '{job_name}' in namespace '{self.namespace}' with image '{image}'"
        )
        # Implement actual job launch logic here


class EmailSMTP(ConfigurableResource):
    """Placeholder for an SMTP server used to send notifications."""

    host: str
    port: int
    username: str
    password: str

    def send_email(self, subject: str, body: str, to: list[str]) -> None:
        logger = get_dagster_logger()
        logger.info(f"Sending email to {to}: {subject}")
        # Implement actual email sending logic here


class HttpAPIResource(ConfigurableResource):
    """Generic HTTP API client placeholder."""

    base_url: str
    auth_token: str | None = None

    def get(self, endpoint: str, params: dict | None = None) -> dict:
        logger = get_dagster_logger()
        logger.info(f"GET {self.base_url}{endpoint} with params {params}")
        # Implement actual HTTP GET logic here
        return {}


# ----------------------------------------------------------------------
# Ops
# ----------------------------------------------------------------------


@op(
    name="check_pcd_sftp_folder",
    required_resource_keys={"sftp_server"},
    out=Out(Nothing),
    retry_policy=RetryPolicy(max_retries=0),
    tags={"executor": "k8s_job_executor"},
)
def check_pcd_sftp_folder(context: InitResourceContext) -> None:
    """
    Validate that the expected files exist in the PCD SFTP folder.
    """
    sftp: FsIOManager = context.resources.sftp_server
    logger = context.log
    logger.info("Checking PCD SFTP folder for required files.")
    # Example validation logic
    required_files = ["patients.csv", "visits.csv"]
    for file_name in required_files:
        content = sftp.read(file_name)
        if not content:
            raise RuntimeError(f"Required file {file_name} is missing or empty in SFTP folder.")


@op(
    name="check_pcd_shared_folder",
    required_resource_keys={"shared_network_folder"},
    ins={"_prev": In(Nothing)},
    out=Out(Nothing),
    retry_policy=RetryPolicy(max_retries=0),
    tags={"executor": "k8s_job_executor"},
)
def check_pcd_shared_folder(context: InitResourceContext, _prev: Nothing) -> None:
    """
    Validate that the shared network folder is accessible and contains expected data.
    """
    shared_folder: FsIOManager = context.resources.shared_network_folder
    logger = context.log
    logger.info("Checking PCD shared network folder for accessibility.")
    # Example check – attempt to list a known directory
    try:
        shared_folder.read("health_authority_hierarchy.json")
    except Exception as exc:
        raise RuntimeError("Shared network folder validation failed.") from exc


@op(
    name="extract_pcd_api",
    required_resource_keys={
        "http_api_financial_expense",
        "http_api_upcc_financial_reporting",
        "http_api_chc_financial_reporting",
        "http_api_pcn_financial_reporting",
        "http_api_nppcc_financial_reporting",
        "http_api_fiscal_year_reporting_dates",
        "http_api_upcc_primary_care_patient_services",
        "http_api_chc_primary_care_patient_services",
        "http_api_practitioner_role_mapping",
        "http_api_status_tracker",
        "http_api_hr_records",
        "http_api_provincial_risk_tracking",
        "http_api_decision_log",
        "http_api_ha_hierarchy",
        "http_api_uppc_budget",
        "http_api_chc_budget",
        "http_api_pcn_budget",
        "http_api_nppcc_budget",
    },
    ins={"_prev": In(Nothing)},
    out=Out(dict),
    retry_policy=RetryPolicy(max_retries=3),
    tags={"executor": "in_process_executor"},
)
def extract_pcd_api(context: InitResourceContext, _prev: Nothing) -> dict:
    """
    Pull data from a suite of HTTP APIs required for the PCD ETL process.
    Returns a dictionary aggregating the fetched payloads.
    """
    logger = context.log
    logger.info("Starting extraction of PCD data from multiple APIs.")

    api_resources = {
        "financial_expense": context.resources.http_api_financial_expense,
        "upcc_financial_reporting": context.resources.http_api_upcc_financial_reporting,
        "chc_financial_reporting": context.resources.http_api_chc_financial_reporting,
        "pcn_financial_reporting": context.resources.http_api_pcn_financial_reporting,
        "nppcc_financial_reporting": context.resources.http_api_nppcc_financial_reporting,
        "fiscal_year_dates": context.resources.http_api_fiscal_year_reporting_dates,
        "upcc_patient_services": context.resources.http_api_upcc_primary_care_patient_services,
        "chc_patient_services": context.resources.http_api_chc_primary_care_patient_services,
        "practitioner_role_mapping": context.resources.http_api_practitioner_role_mapping,
        "status_tracker": context.resources.http_api_status_tracker,
        "hr_records": context.resources.http_api_hr_records,
        "provincial_risk_tracking": context.resources.http_api_provincial_risk_tracking,
        "decision_log": context.resources.http_api_decision_log,
        "ha_hierarchy": context.resources.http_api_ha_hierarchy,
        "uppc_budget": context.resources.http_api_uppc_budget,
        "chc_budget": context.resources.http_api_chc_budget,
        "pcn_budget": context.resources.http_api_pcn_budget,
        "nppcc_budget": context.resources.http_api_nppcc_budget,
    }

    extracted_data = {}
    for name, api in api_resources.items():
        logger.info(f"Fetching data from API: {name}")
        try:
            payload = api.get("/data")
            extracted_data[name] = payload
        except Exception as exc:
            logger.error(f"Failed to fetch data from {name}: {exc}")
            raise

    logger.info("Completed API extraction.")
    return extracted_data


@op(
    name="process_and_load_pcd_data",
    required_resource_keys={"kubernetes_cluster"},
    ins={"api_data": In(dict)},
    out=Out(Nothing),
    retry_policy=RetryPolicy(max_retries=2),
    tags={"executor": "k8s_job_executor"},
)
def process_and_load_pcd_data(context: InitResourceContext, api_data: dict) -> None:
    """
    Submit a Kubernetes job that processes the extracted API data and loads it into the target system.
    """
    k8s: KubernetesCluster = context.resources.kubernetes_cluster
    logger = context.log
    logger.info("Launching Kubernetes job to process and load PCD data.")

    # Serialize the payload to a temporary location or pass via environment variables.
    # For illustration, we assume the job image knows how to retrieve the data from a shared store.
    job_name = "pcd-data-processing"
    image = "myregistry/pcd-processing:latest"
    command = ["python", "process_and_load.py"]

    try:
        k8s.launch_job(job_name=job_name, image=image, command=command)
    except Exception as exc:
        logger.error(f"Kubernetes job failed: {exc}")
        raise


@op(
    name="send_etl_notification",
    required_resource_keys={"email_smtp"},
    ins={"_prev": In(Nothing)},
    out=Out(Nothing),
    retry_policy=RetryPolicy(max_retries=1),
    tags={"executor": "in_process_executor"},
)
def send_etl_notification(context: InitResourceContext, _prev: Nothing) -> None:
    """
    Send an email notification indicating successful completion of the PCD ETL pipeline.
    """
    email: EmailSMTP = context.resources.email_smtp
    logger = context.log
    subject = "PCD ETL Pipeline Completed"
    body = "The Primary Care Data ETL pipeline has finished successfully."
    recipients = ["data-team@example.com"]

    logger.info("Sending ETL completion notification email.")
    try:
        email.send_email(subject=subject, body=body, to=recipients)
    except Exception as exc:
        logger.error(f"Failed to send notification email: {exc}")
        raise


# ----------------------------------------------------------------------
# Job definition
# ----------------------------------------------------------------------


@job(
    name="check_pcd_sftp_folder_pipeline",
    description=(
        "Staged ETL pipeline for Primary Care Data (PCD) processing with folder validation, "
        "parallel API extraction, Kubernetes‑based processing and email notification."
    ),
    executor_def=k8s_job_executor,
    resource_defs={
        "sftp_server": FsIOManager(base_path="/mnt/sftp/pcd"),
        "shared_network_folder": FsIOManager(base_path="/mnt/shared/pcd"),
        "kubernetes_cluster": KubernetesCluster(namespace="etl-jobs"),
        "email_smtp": EmailSMTP(
            host="smtp.example.com", port=587, username="etl_user", password="secret"
        ),
        # HTTP API resources – placeholders with dummy URLs
        "http_api_financial_expense": HttpAPIResource(base_url="https://api.example.com/financial_expense"),
        "http_api_upcc_financial_reporting": HttpAPIResource(base_url="https://api.example.com/upcc_financial_reporting"),
        "http_api_chc_financial_reporting": HttpAPIResource(base_url="https://api.example.com/chc_financial_reporting"),
        "http_api_pcn_financial_reporting": HttpAPIResource(base_url="https://api.example.com/pcn_financial_reporting"),
        "http_api_nppcc_financial_reporting": HttpAPIResource(base_url="https://api.example.com/nppcc_financial_reporting"),
        "http_api_fiscal_year_reporting_dates": HttpAPIResource(base_path="https://api.example.com/fiscal_year_dates"),
        "http_api_upcc_primary_care_patient_services": HttpAPIResource(base_url="https://api.example.com/upcc_patient_services"),
        "http_api_chc_primary_care_patient_services": HttpAPIResource(base_url="https://api.example.com/chc_patient_services"),
        "http_api_practitioner_role_mapping": HttpAPIResource(base_url="https://api.example.com/practitioner_role_mapping"),
        "http_api_status_tracker": HttpAPIResource(base_url="https://api.example.com/status_tracker"),
        "http_api_hr_records": HttpAPIResource(base_url="https://api.example.com/hr_records"),
        "http_api_provincial_risk_tracking": HttpAPIResource(base_url="https://api.example.com/provincial_risk_tracking"),
        "http_api_decision_log": HttpAPIResource(base_url="https://api.example.com/decision_log"),
        "http_api_ha_hierarchy": HttpAPIResource(base_url="https://api.example.com/ha_hierarchy"),
        "http_api_uppc_budget": HttpAPIResource(base_url="https://api.example.com/uppc_budget"),
        "http_api_chc_budget": HttpAPIResource(base_url="https://api.example.com/chc_budget"),
        "http_api_pcn_budget": HttpAPIResource(base_url="https://api.example.com/pcn_budget"),
        "http_api_nppcc_budget": HttpAPIResource(base_url="https://api.example.com/nppcc_budget"),
    },
)
def check_pcd_sftp_folder_pipeline():
    """
    Orchestrates the PCD ETL workflow:
    1. Validate SFTP folder.
    2. Validate shared network folder.
    3. Extract data from multiple APIs.
    4. Process and load data via a Kubernetes job.
    5. Send a completion notification email.
    """
    # Step 1
    sftp_check = check_pcd_sftp_folder()

    # Step 2 (depends on step 1)
    shared_check = check_pcd_shared_folder(sftp_check)

    # Step 3 (depends on step 2)
    api_payload = extract_pcd_api(shared_check)

    # Step 4 (depends on step 3)
    process_and_load_pcd_data(api_payload)

    # Step 5 (depends on step 4)
    send_etl_notification()


# ----------------------------------------------------------------------
# Schedule (disabled)
# ----------------------------------------------------------------------


from dagster import ScheduleDefinition, DefaultScheduleStatus

# The schedule is defined but disabled per specification.
schedule = ScheduleDefinition(
    job=check_pcd_sftp_folder_pipeline,
    cron_schedule=None,  # No cron expression provided
    default_status=DefaultScheduleStatus.INACTIVE,
    execution_timezone="UTC",
    name="check_pcd_sftp_folder_schedule",
)