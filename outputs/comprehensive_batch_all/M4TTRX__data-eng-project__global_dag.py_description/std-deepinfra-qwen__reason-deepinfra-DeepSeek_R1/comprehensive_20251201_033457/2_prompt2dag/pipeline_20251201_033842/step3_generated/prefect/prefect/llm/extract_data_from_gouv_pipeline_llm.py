# Generated by Prefect 2.x Code Generator
# Date: 2023-10-05
# Prefect Version: 2.14.0

from prefect import flow, task, get_run_logger
from prefect.task_runners import ConcurrentTaskRunner
from prefect.blocks.system import Secret
from prefect.filesystems import LocalFileSystem
from prefect.infrastructure.docker import DockerContainer
import os

# Secrets
data_gouv_api = Secret.load("data-gouv-api")
redis_cache = Secret.load("redis-cache")
postgresql_db = Secret.load("postgresql-db")

# Filesystems
ingestion_filesystem = LocalFileSystem.load("ingestion-filesystem")
staging_filesystem = LocalFileSystem.load("staging-filesystem")
sql_tmp_filesystem = LocalFileSystem.load("sql-tmp-filesystem")

@task(retries=1)
def fetch_thermal_data():
    logger = get_run_logger()
    logger.info("Fetching Thermal Power Plant Data")
    # Implementation here
    return "thermal_data"

@task(retries=1)
def store_plants_in_postgres(thermal_data):
    logger = get_run_logger()
    logger.info("Storing Plants in PostgreSQL")
    # Implementation here
    return "plants_stored"

@task(retries=1)
def extract_data_from_gouv():
    logger = get_run_logger()
    logger.info("Extracting Data from data.gouv.fr")
    # Implementation here
    return "gouv_data"

@task(retries=1)
def fetch_death_records():
    logger = get_run_logger()
    logger.info("Fetching Death Records")
    # Implementation here
    return "death_records"

@task(retries=1)
def load_death_records_to_redis(death_records):
    logger = get_run_logger()
    logger.info("Loading Death Records to Redis")
    # Implementation here
    return "death_records_loaded"

@task(retries=1)
def download_city_geo():
    logger = get_run_logger()
    logger.info("Downloading City Geographic Coordinates")
    # Implementation here
    return "city_geo_data"

@task(retries=1)
def create_death_table():
    logger = get_run_logger()
    logger.info("Creating Death Table")
    # Implementation here
    return "death_table_created"

@task(retries=1)
def cleanse_death_data(death_table_created, death_records_loaded):
    logger = get_run_logger()
    logger.info("Cleaning Death Data")
    # Implementation here
    return "death_data_cleaned"

@task(retries=1)
def check_death_data_emptiness(death_data_cleaned):
    logger = get_run_logger()
    logger.info("Checking Death Data Emptiness")
    # Implementation here
    return "death_data_not_empty"

@task(retries=1)
def staging_end(death_data_not_empty):
    logger = get_run_logger()
    logger.info("Staging End")
    # Implementation here
    return "staging_ended"

@task(retries=1)
def store_deaths_in_postgres(death_data_cleaned):
    logger = get_run_logger()
    logger.info("Storing Deaths in PostgreSQL")
    # Implementation here
    return "deaths_stored"

@task(retries=1)
def clean_tmp_death_files(deaths_stored, plants_stored):
    logger = get_run_logger()
    logger.info("Cleaning Temporary Death Files")
    # Implementation here
    return "tmp_files_cleaned"

@task(retries=1)
def cleanse_power_plant_data(thermal_data):
    logger = get_run_logger()
    logger.info("Cleaning Power Plant Data")
    # Implementation here
    return "power_plant_data_cleaned"

@task(retries=1)
def generate_plant_persist_sql(power_plant_data_cleaned):
    logger = get_run_logger()
    logger.info("Generating Plant Persist SQL")
    # Implementation here
    return "plant_persist_sql_generated"

@task(retries=1)
def create_power_plants_table():
    logger = get_run_logger()
    logger.info("Creating Power Plants Table")
    # Implementation here
    return "power_plants_table_created"

@task(retries=1)
def fetch_nuclear_data():
    logger = get_run_logger()
    logger.info("Fetching Nuclear Power Plant Data")
    # Implementation here
    return "nuclear_data"

@flow(name="extract_data_from_gouv_pipeline", task_runner=ConcurrentTaskRunner())
def extract_data_from_gouv_pipeline():
    logger = get_run_logger()
    logger.info("Starting extract_data_from_gouv_pipeline")

    ingestion_pipeline_start = extract_data_from_gouv.submit()
    download_city_geo.submit(wait_for=[ingestion_pipeline_start])
    fetch_nuclear_data.submit(wait_for=[ingestion_pipeline_start])
    fetch_thermal_data.submit(wait_for=[ingestion_pipeline_start])
    fetch_death_records.submit(wait_for=[ingestion_pipeline_start])

    staging_pipeline_start = create_death_table.submit()
    create_power_plants_table.submit(wait_for=[staging_pipeline_start])
    load_death_records_to_redis.submit(fetch_death_records, wait_for=[staging_pipeline_start])

    cleanse_death_data.submit(create_death_table, load_death_records_to_redis)
    cleanse_power_plant_data.submit(fetch_thermal_data)

    generate_plant_persist_sql.submit(cleanse_power_plant_data)
    check_death_data_emptiness.submit(cleanse_death_data)
    staging_end.submit(check_death_data_emptiness)

    store_deaths_in_postgres.submit(cleanse_death_data)
    store_plants_in_postgres.submit(generate_plant_persist_sql, staging_end)
    clean_tmp_death_files.submit(store_deaths_in_postgres, store_plants_in_postgres)

if __name__ == "__main__":
    extract_data_from_gouv_pipeline()