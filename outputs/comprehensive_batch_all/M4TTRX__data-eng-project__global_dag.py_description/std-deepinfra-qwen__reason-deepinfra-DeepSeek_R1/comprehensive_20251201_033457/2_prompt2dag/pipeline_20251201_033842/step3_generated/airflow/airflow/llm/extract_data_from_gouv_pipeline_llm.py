# Generated by Airflow DAG Generator
# Date: 2023-10-05
# Airflow Version: 2.x

from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
from airflow.providers.postgres.operators.postgres import PostgresOperator
from airflow.operators.docker_operator import DockerOperator
from airflow.utils.task_group import TaskGroup
from airflow.utils.dates import days_ago
from airflow.models import Variable
import pendulum

# Default arguments for the DAG
default_args = {
    'owner': 'airflow',
    'retries': 1,
    'retry_delay': pendulum.duration(minutes=5),
}

# Define the DAG
with DAG(
    dag_id='extract_data_from_gouv_pipeline',
    description='No description provided.',
    schedule_interval=None,
    start_date=days_ago(1),
    catchup=False,
    default_args=default_args,
    tags=['data_extraction', 'gouv'],
) as dag:

    # Task: Ingestion Pipeline Start
    ingestion_pipeline_start = PythonOperator(
        task_id='ingestion_pipeline_start',
        python_callable=lambda: print("Ingestion Pipeline Started"),
    )

    # Task: Staging Pipeline Start
    staging_pipeline_start = PythonOperator(
        task_id='staging_pipeline_start',
        python_callable=lambda: print("Staging Pipeline Started"),
    )

    # Task: Extract Data from data.gouv.fr
    extract_data_from_gouv = PythonOperator(
        task_id='extract_data_from_gouv',
        python_callable=lambda: print("Extracting data from data.gouv.fr"),
    )

    # Task: Download City Geographic Coordinates
    download_city_geo = BashOperator(
        task_id='download_city_geo',
        bash_command='curl -o /tmp/city_geo.json http://data.gouv.fr/api/1/datasets/city-geo-coordinates/',
    )

    # Task: Fetch Nuclear Power Plant Data
    fetch_nuclear_data = PythonOperator(
        task_id='fetch_nuclear_data',
        python_callable=lambda: print("Fetching nuclear power plant data"),
    )

    # Task: Fetch Thermal Power Plant Data
    fetch_thermal_data = PythonOperator(
        task_id='fetch_thermal_data',
        python_callable=lambda: print("Fetching thermal power plant data"),
    )

    # Task: Fetch Death Records
    fetch_death_records = PythonOperator(
        task_id='fetch_death_records',
        python_callable=lambda: print("Fetching death records"),
    )

    # Task: Create Death Table
    create_death_table = PostgresOperator(
        task_id='create_death_table',
        postgres_conn_id='postgresql_db',
        sql="""
        CREATE TABLE IF NOT EXISTS deaths (
            id SERIAL PRIMARY KEY,
            name VARCHAR(255),
            date DATE,
            location VARCHAR(255)
        );
        """,
    )

    # Task: Create Power Plants Table
    create_power_plants_table = PostgresOperator(
        task_id='create_power_plants_table',
        postgres_conn_id='postgresql_db',
        sql="""
        CREATE TABLE IF NOT EXISTS power_plants (
            id SERIAL PRIMARY KEY,
            name VARCHAR(255),
            type VARCHAR(255),
            location VARCHAR(255)
        );
        """,
    )

    # Task: Load Death Records to Redis
    load_death_records_to_redis = PythonOperator(
        task_id='load_death_records_to_redis',
        python_callable=lambda: print("Loading death records to Redis"),
    )

    # Task: Cleanse Death Data
    cleanse_death_data = PythonOperator(
        task_id='cleanse_death_data',
        python_callable=lambda: print("Cleansing death data"),
    )

    # Task: Cleanse Power Plant Data
    cleanse_power_plant_data = PythonOperator(
        task_id='cleanse_power_plant_data',
        python_callable=lambda: print("Cleansing power plant data"),
    )

    # Task: Generate Plant Persist SQL
    generate_plant_persist_sql = PythonOperator(
        task_id='generate_plant_persist_sql',
        python_callable=lambda: print("Generating plant persist SQL"),
    )

    # Task: Check Death Data Emptiness
    check_death_data_emptiness = PythonOperator(
        task_id='check_death_data_emptiness',
        python_callable=lambda: print("Checking death data emptiness"),
    )

    # Task: Store Deaths in PostgreSQL
    store_deaths_in_postgres = PostgresOperator(
        task_id='store_deaths_in_postgres',
        postgres_conn_id='postgresql_db',
        sql="""
        INSERT INTO deaths (name, date, location)
        VALUES ('John Doe', '2023-10-01', 'Paris');
        """,
    )

    # Task: Staging End
    staging_end = PythonOperator(
        task_id='staging_end',
        python_callable=lambda: print("Staging Pipeline Ended"),
    )

    # Task: Store Plants in PostgreSQL
    store_plants_in_postgres = PostgresOperator(
        task_id='store_plants_in_postgres',
        postgres_conn_id='postgresql_db',
        sql="""
        INSERT INTO power_plants (name, type, location)
        VALUES ('Nuclear Plant 1', 'Nuclear', 'Lyon');
        """,
    )

    # Task: Clean Temporary Death Files
    clean_tmp_death_files = PythonOperator(
        task_id='clean_tmp_death_files',
        python_callable=lambda: print("Cleaning temporary death files"),
    )

    # Set task dependencies
    ingestion_pipeline_start >> [extract_data_from_gouv, download_city_geo, fetch_nuclear_data, fetch_thermal_data, fetch_death_records]
    staging_pipeline_start >> [create_death_table, create_power_plants_table, load_death_records_to_redis]
    create_death_table >> cleanse_death_data
    load_death_records_to_redis >> cleanse_death_data
    create_power_plants_table >> cleanse_power_plant_data
    cleanse_power_plant_data >> generate_plant_persist_sql
    cleanse_death_data >> check_death_data_emptiness
    check_death_data_emptiness >> [store_deaths_in_postgres, staging_end]
    staging_end >> store_plants_in_postgres
    generate_plant_persist_sql >> store_plants_in_postgres
    store_deaths_in_postgres >> clean_tmp_death_files
    store_plants_in_postgres >> clean_tmp_death_files