# Generated by Prefect Pipeline Generator
# Generation Timestamp: 2024-06-28T12:00:00Z
# Prefect version: 2.14.0
# Flow name: global_dag
# Deployment name: global_dag_deployment
# Work pool: default-agent-pool
# Task runner: ConcurrentTaskRunner

import json
import logging
import os
import shutil
import subprocess
from pathlib import Path
from typing import Any, Dict

import requests
from prefect import flow, task, get_run_logger
from prefect.task_runners import ConcurrentTaskRunner
from prefect.blocks.system import Secret
from prefect.filesystems import LocalFileSystem
from redis import Redis
from sqlalchemy import create_engine, text

# Configure logger
logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO)


@task(retries=3, retry_delay_seconds=10)
def load_secret(secret_name: str) -> str:
    """
    Load a secret value from a Prefect Secret block.

    Args:
        secret_name: Name of the Secret block.

    Returns:
        The secret value as a string.

    Raises:
        ValueError: If the secret cannot be loaded.
    """
    logger = get_run_logger()
    try:
        secret_block = Secret.load(secret_name)
        secret_value = secret_block.get()
        logger.info(f"Loaded secret '{secret_name}'.")
        return secret_value
    except Exception as exc:
        logger.error(f"Failed to load secret '{secret_name}': {exc}")
        raise ValueError(f"Unable to load secret '{secret_name}'.") from exc


@task(retries=3, retry_delay_seconds=10)
def download_json(api_url: str, api_key: str) -> Dict[str, Any]:
    """
    Download JSON data from a given API endpoint using an API key.

    Args:
        api_url: The full URL of the API endpoint.
        api_key: API key or token for authentication.

    Returns:
        Parsed JSON data as a dictionary.

    Raises:
        RuntimeError: If the request fails or returns a non‑200 status.
    """
    logger = get_run_logger()
    headers = {"Authorization": f"Bearer {api_key}"}
    try:
        response = requests.get(api_url, headers=headers, timeout=30)
        response.raise_for_status()
        logger.info(f"Successfully fetched data from {api_url}.")
        return response.json()
    except Exception as exc:
        logger.error(f"Error fetching data from {api_url}: {exc}")
        raise RuntimeError(f"Failed to download data from {api_url}.") from exc


@task(retries=2, retry_delay_seconds=5)
def write_json_to_file(data: Dict[str, Any], destination: Path) -> Path:
    """
    Write JSON data to a file on the local filesystem.

    Args:
        data: JSON‑serializable dictionary.
        destination: Path object where the file will be written.

    Returns:
        Path to the written file.

    Raises:
        IOError: If writing fails.
    """
    logger = get_run_logger()
    try:
        destination.parent.mkdir(parents=True, exist_ok=True)
        with destination.open("w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        logger.info(f"Wrote JSON data to {destination}.")
        return destination
    except Exception as exc:
        logger.error(f"Failed to write JSON to {destination}: {exc}")
        raise IOError(f"Unable to write JSON to {destination}.") from exc


@task
def move_file_to_staging(source: Path, staging_dir: Path) -> Path:
    """
    Move a file from the ingestion directory to the staging directory.

    Args:
        source: Path to the source file.
        staging_dir: Directory where the file should be placed.

    Returns:
        Path to the file in the staging directory.

    Raises:
        FileNotFoundError: If the source file does not exist.
    """
    logger = get_run_logger()
    if not source.is_file():
        raise FileNotFoundError(f"Source file {source} does not exist.")
    staging_dir.mkdir(parents=True, exist_ok=True)
    destination = staging_dir / source.name
    shutil.move(str(source), str(destination))
    logger.info(f"Moved {source} to staging at {destination}.")
    return destination


@task(retries=2, retry_delay_seconds=5)
def transform_json(input_path: Path, output_path: Path) -> Path:
    """
    Placeholder transformation: read JSON, perform minimal processing,
    and write to a new file.

    Args:
        input_path: Path to the raw JSON file.
        output_path: Path where the transformed JSON will be saved.

    Returns:
        Path to the transformed file.

    Raises:
        RuntimeError: If transformation fails.
    """
    logger = get_run_logger()
    try:
        with input_path.open("r", encoding="utf-8") as f:
            data = json.load(f)

        # Example transformation: keep only records with a non‑null 'id' field
        transformed = [record for record in data if record.get("id")]

        output_path.parent.mkdir(parents=True, exist_ok=True)
        with output_path.open("w", encoding="utf-8") as f:
            json.dump(transformed, f, ensure_ascii=False, indent=2)

        logger.info(f"Transformed data written to {output_path}.")
        return output_path
    except Exception as exc:
        logger.error(f"Transformation failed for {input_path}: {exc}")
        raise RuntimeError(f"Failed to transform {input_path}.") from exc


@task(retries=3, retry_delay_seconds=10)
def cache_to_redis(redis_url: str, key: str, data_path: Path) -> None:
    """
    Cache the contents of a JSON file in Redis.

    Args:
        redis_url: Redis connection URL.
        key: Redis key under which the data will be stored.
        data_path: Path to the JSON file to cache.

    Raises:
        redis.exceptions.RedisError: If caching fails.
    """
    logger = get_run_logger()
    try:
        r = Redis.from_url(redis_url)
        with data_path.open("r", encoding="utf-8") as f:
            payload = f.read()
        r.set(key, payload)
        logger.info(f"Cached {data_path.name} in Redis under key '{key}'.")
    except Exception as exc:
        logger.error(f"Redis caching failed for {data_path}: {exc}")
        raise


@task(retries=3, retry_delay_seconds=10)
def load_to_postgres(pg_url: str, sql_script_dir: Path, table_name: str, data_path: Path) -> None:
    """
    Load transformed JSON data into a PostgreSQL table using a SQL script.

    Args:
        pg_url: SQLAlchemy-compatible PostgreSQL connection URL.
        sql_script_dir: Directory containing SQL scripts.
        table_name: Target table name.
        data_path: Path to the transformed JSON file.

    Raises:
        sqlalchemy.exc.SQLAlchemyError: If loading fails.
    """
    logger = get_run_logger()
    engine = create_engine(pg_url)
    try:
        # Example: Use COPY FROM STDIN for bulk insert
        with engine.begin() as conn:
            # Ensure target table exists (run a create script if needed)
            create_script_path = sql_script_dir / f"create_{table_name}.sql"
            if create_script_path.is_file():
                with create_script_path.open("r", encoding="utf-8") as f:
                    conn.execute(text(f.read()))
                logger.info(f"Executed table creation script for {table_name}.")

            # Load JSON data
            with data_path.open("r", encoding="utf-8") as f:
                records = json.load(f)

            # Insert rows
            for record in records:
                columns = ", ".join(record.keys())
                placeholders = ", ".join([f":{k}" for k in record.keys()])
                insert_stmt = text(
                    f"INSERT INTO {table_name} ({columns}) VALUES ({placeholders})"
                )
                conn.execute(insert_stmt, **record)

        logger.info(f"Loaded data from {data_path.name} into PostgreSQL table '{table_name}'.")
    except Exception as exc:
        logger.error(f"Failed to load data into PostgreSQL: {exc}")
        raise


@flow(
    name="global_dag",
    task_runner=ConcurrentTaskRunner(),
    description="ETL pipeline processes French government death records and power plant data using a staged ETL pattern with mixed topology.",
)
def global_dag():
    """
    Main Prefect flow orchestrating the ETL pipeline.

    The flow performs the following high‑level steps for each dataset:
    1. Load API credentials from Secret blocks.
    2. Download raw JSON data from the respective API.
    3. Persist raw data to the local ingestion directory.
    4. Move raw files to the staging directory.
    5. Transform staged data.
    6. Cache transformed data in Redis for downstream consumption.
    7. Load transformed data into PostgreSQL using SQL scripts.
    """
    # ----------------------------------------------------------------------
    # Configuration – Load blocks
    # ----------------------------------------------------------------------
    ingestion_fs = LocalFileSystem.load("local_filesystem_ingestion")
    staging_fs = LocalFileSystem.load("local_filesystem_staging")
    sql_scripts_fs = LocalFileSystem.load("sql_scripts_directory")

    redis_url = load_secret.submit("redis_intermediate")
    pg_url = load_secret.submit("postgres_target")

    # ----------------------------------------------------------------------
    # Dataset definitions
    # ----------------------------------------------------------------------
    datasets = [
        {
            "name": "city_geo",
            "api_secret": "data_gouv_fr_city_geo_api",
            "api_url": "https://static.data.gouv.fr/api/v1/cities.geojson",
            "table": "city_geography",
        },
        {
            "name": "nuclear_power",
            "api_secret": "data_gouv_fr_nuclear_api",
            "api_url": "https://data.gouv.fr/api/v1/nuclear_plants",
            "table": "nuclear_power_plants",
        },
        {
            "name": "thermal_power",
            "api_secret": "data_gouv_fr_thermal_api",
            "api_url": "https://data.gouv.fr/api/v1/thermal_plants",
            "table": "thermal_power_plants",
        },
        {
            "name": "death_records",
            "api_secret": "data_gouv_fr_death_api",
            "api_url": "https://data.gouv.fr/api/v1/death_records",
            "table": "death_records",
        },
    ]

    # ----------------------------------------------------------------------
    # Process each dataset
    # ----------------------------------------------------------------------
    for ds in datasets:
        # Load API key
        api_key = load_secret.submit(ds["api_secret"])

        # Download raw JSON
        raw_json = download_json.submit(ds["api_url"], api_key)

        # Write raw JSON to ingestion directory
        ingestion_path = Path(ingestion_fs.base_path) / f"{ds['name']}_raw.json"
        raw_file = write_json_to_file.submit(raw_json, ingestion_path)

        # Move file to staging
        staging_path = Path(staging_fs.base_path)
        staged_file = move_file_to_staging.submit(raw_file, staging_path)

        # Transform data
        transformed_path = Path(staging_fs.base_path) / f"{ds['name']}_transformed.json"
        transformed_file = transform_json.submit(staged_file, transformed_path)

        # Cache transformed data in Redis
        cache_key = f"{ds['name']}:latest"
        cache_to_redis.submit(redis_url, cache_key, transformed_file)

        # Load into PostgreSQL
        load_to_postgres.submit(
            pg_url,
            Path(sql_scripts_fs.base_path),
            ds["table"],
            transformed_file,
        )


if __name__ == "__main__":
    # Execute the flow locally (useful for debugging)
    global_dag()