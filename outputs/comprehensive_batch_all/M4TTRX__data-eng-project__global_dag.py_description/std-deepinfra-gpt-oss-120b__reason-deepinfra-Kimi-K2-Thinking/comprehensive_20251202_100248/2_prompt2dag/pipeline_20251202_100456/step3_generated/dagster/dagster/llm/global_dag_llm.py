# Generated by Dagster code generator
# Date: 2024-06-13
# Description: Dagster job for ETL pipeline processing French government death records and power plant data.

from dagster import (
    op,
    job,
    In,
    Out,
    RetryPolicy,
    ResourceDefinition,
    fs_io_manager,
    MultiprocessExecutor,
    ConfigurableResource,
    InitResourceContext,
    get_dagster_logger,
)
from typing import Any, Dict, List
import requests
import json
import redis
import psycopg2
import os


# -------------------------------------------------------------------------
# Resource definitions
# -------------------------------------------------------------------------

class HttpApiResource(ConfigurableResource):
    """Simple HTTP API client resource."""

    base_url: str

    def get(self, endpoint: str, params: Dict[str, Any] = None) -> Any:
        url = f"{self.base_url.rstrip('/')}/{endpoint.lstrip('/')}"
        response = requests.get(url, params=params, timeout=30)
        response.raise_for_status()
        return response.json()


class RedisCacheResource(ConfigurableResource):
    """Redis client for intermediate caching."""

    host: str = "localhost"
    port: int = 6379
    db: int = 0

    def get_client(self) -> redis.Redis:
        return redis.Redis(host=self.host, port=self.port, db=self.db)


class PostgresResource(ConfigurableResource):
    """PostgreSQL connection resource."""

    host: str = "localhost"
    port: int = 5432
    dbname: str = "postgres"
    user: str = "postgres"
    password: str = "postgres"

    def get_connection(self) -> psycopg2.extensions.connection:
        return psycopg2.connect(
            host=self.host,
            port=self.port,
            dbname=self.dbname,
            user=self.user,
            password=self.password,
        )


# -------------------------------------------------------------------------
# Ops
# -------------------------------------------------------------------------

@op(
    out=Out(dict, description="Raw city geographic data"),
    required_resource_keys={"data_gouv_fr_city_geo_api"},
    tags={"op_name": "ingest_city_geo"},
)
def ingest_city_geo(context) -> Dict:
    """Ingest city geographic data from the static.data.gouv.fr API."""
    logger = get_dagster_logger()
    api: HttpApiResource = context.resources.data_gouv_fr_city_geo_api
    logger.info("Fetching city geographic data...")
    data = api.get("api/records/1.0/search/", params={"dataset": "georef-france-communes"})
    logger.info("Fetched %d city records.", len(data.get("records", [])))
    return data


@op(
    out=Out(dict, description="Raw nuclear power plant data"),
    required_resource_keys={"data_gouv_fr_nuclear_api"},
    tags={"op_name": "ingest_nuclear"},
)
def ingest_nuclear(context) -> Dict:
    """Ingest nuclear power plant dataset from data.gouv.fr."""
    logger = get_dagster_logger()
    api: HttpApiResource = context.resources.data_gouv_fr_nuclear_api
    logger.info("Fetching nuclear power plant data...")
    data = api.get("api/records/1.0/search/", params={"dataset": "nuclear-plants-france"})
    logger.info("Fetched %d nuclear plant records.", len(data.get("records", [])))
    return data


@op(
    out=Out(dict, description="Raw thermal power plant data"),
    required_resource_keys={"data_gouv_fr_thermal_api"},
    tags={"op_name": "ingest_thermal"},
)
def ingest_thermal(context) -> Dict:
    """Ingest thermal power plant dataset from data.gouv.fr."""
    logger = get_dagster_logger()
    api: HttpApiResource = context.resources.data_gouv_fr_thermal_api
    logger.info("Fetching thermal power plant data...")
    data = api.get("api/records/1.0/search/", params={"dataset": "thermal-plants-france"})
    logger.info("Fetched %d thermal plant records.", len(data.get("records", [])))
    return data


@op(
    out=Out(dict, description="Raw death records data"),
    required_resource_keys={"data_gouv_fr_death_api"},
    tags={"op_name": "ingest_death"},
)
def ingest_death(context) -> Dict:
    """Ingest death records dataset from data.gouv.fr."""
    logger = get_dagster_logger()
    api: HttpApiResource = context.resources.data_gouv_fr_death_api
    logger.info("Fetching death records data...")
    data = api.get("api/records/1.0/search/", params={"dataset": "death-records-france"})
    logger.info("Fetched %d death records.", len(data.get("records", [])))
    return data


@op(
    ins={
        "city_geo": In(dict),
        "nuclear": In(dict),
        "thermal": In(dict),
        "death": In(dict),
    },
    out=Out(dict, description="Staged data ready for loading"),
    required_resource_keys={"redis_intermediate"},
    tags={"op_name": "stage_data"},
)
def stage_data(context, city_geo: Dict, nuclear: Dict, thermal: Dict, death: Dict) -> Dict:
    """
    Combine raw datasets, perform light transformations, and cache the staged
    result in Redis for downstream consumption.
    """
    logger = get_dagster_logger()
    redis_res: RedisCacheResource = context.resources.redis_intermediate
    client = redis_res.get_client()

    logger.info("Staging data...")
    staged = {
        "city_geo": city_geo,
        "nuclear": nuclear,
        "thermal": thermal,
        "death": death,
    }

    # Store as JSON string in Redis under a known key
    key = "global_dag:staged_data"
    client.set(key, json.dumps(staged))
    logger.info("Staged data cached in Redis under key %s.", key)

    return staged


@op(
    ins={"staged": In(dict)},
    required_resource_keys={"postgres_target"},
    tags={"op_name": "load_to_postgres"},
)
def load_to_postgres(context, staged: Dict) -> None:
    """
    Load the staged data into the target PostgreSQL database.
    This op demonstrates a simple insert; in a real pipeline you would
    execute parameterized SQL scripts located in `sql_scripts_directory`.
    """
    logger = get_dagster_logger()
    pg_res: PostgresResource = context.resources.postgres_target
    conn = pg_res.get_connection()
    cur = conn.cursor()

    logger.info("Loading staged data into PostgreSQL...")

    # Example: insert count of records per source into a tracking table
    insert_sql = """
        INSERT INTO etl_load_audit (source, record_count, loaded_at)
        VALUES (%s, %s, NOW())
        ON CONFLICT (source) DO UPDATE
        SET record_count = EXCLUDED.record_count,
            loaded_at = EXCLUDED.loaded_at;
    """

    for source_name, payload in staged.items():
        count = len(payload.get("records", []))
        cur.execute(insert_sql, (source_name, count))
        logger.info("Inserted audit row for %s with %d records.", source_name, count)

    conn.commit()
    cur.close()
    conn.close()
    logger.info("Data load complete.")


# -------------------------------------------------------------------------
# Job definition
# -------------------------------------------------------------------------

@job(
    executor=MultiprocessExecutor(),
    resource_defs={
        "data_gouv_fr_city_geo_api": HttpApiResource(base_url="https://static.data.gouv.fr"),
        "data_gouv_fr_nuclear_api": HttpApiResource(base_url="https://www.data.gouv.fr"),
        "data_gouv_fr_thermal_api": HttpApiResource(base_url="https://www.data.gouv.fr"),
        "data_gouv_fr_death_api": HttpApiResource(base_url="https://www.data.gouv.fr"),
        "redis_intermediate": RedisCacheResource(),
        "postgres_target": PostgresResource(),
        "local_filesystem_ingestion": fs_io_manager,
        "local_filesystem_staging": fs_io_manager,
        "sql_scripts_directory": fs_io_manager,
    },
    description=(
        "ETL pipeline processes French government death records and power plant data "
        "using a staged ETL pattern with mixed topology."
    ),
    tags={"job_name": "global_dag"},
)
def global_dag():
    """
    Dagster job orchestrating the ETL workflow:
    1. Ingest raw datasets from various public APIs.
    2. Stage and cache combined data in Redis.
    3. Load transformed data into PostgreSQL.
    """
    city_geo = ingest_city_geo()
    nuclear = ingest_nuclear()
    thermal = ingest_thermal()
    death = ingest_death()

    staged = stage_data(city_geo=city_geo, nuclear=nuclear, thermal=thermal, death=death)

    load_to_postgres(staged=staged)