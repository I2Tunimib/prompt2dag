# Generated by Dagster Code Generator
# Date: 2024-06-28
# Description: Production-ready Dagster job for ETL pipeline "global_dag"

from typing import Dict, Any, List

import pandas as pd
from dagster import (
    op,
    job,
    In,
    Out,
    RetryPolicy,
    ResourceDefinition,
    ConfigurableResource,
    InitResourceContext,
    Output,
    MultiDependencyDefinition,
    Nothing,
    fs_io_manager,
    multiprocess_executor,
    get_dagster_logger,
)


# ----------------------------------------------------------------------
# Resource Definitions
# ----------------------------------------------------------------------


class CityGeoAPIResource(ConfigurableResource):
    """Resource for accessing the City Geographic Data API (static.data.gouv.fr)."""

    base_url: str = "https://static.data.gouv.fr/api/cities"

    def fetch(self) -> pd.DataFrame:
        # Placeholder implementation; replace with real API call.
        logger = get_dagster_logger()
        logger.info("Fetching city geographic data from %s", self.base_url)
        return pd.DataFrame({"city_id": [1, 2], "name": ["Paris", "Lyon"], "lat": [48.8566, 45.7640], "lon": [2.3522, 4.8357]})


class NuclearPlantsAPIResource(ConfigurableResource):
    """Resource for accessing the Nuclear Power Plant Dataset API (data.gouv.fr)."""

    base_url: str = "https://data.gouv.fr/api/nuclear_plants"

    def fetch(self) -> pd.DataFrame:
        logger = get_dagster_logger()
        logger.info("Fetching nuclear power plant data from %s", self.base_url)
        return pd.DataFrame({"plant_id": [101], "name": ["Plant A"], "capacity_mw": [1200]})


class ThermalPlantsAPIResource(ConfigurableResource):
    """Resource for accessing the Thermal Power Plant Dataset API (data.gouv.fr)."""

    base_url: str = "https://data.gouv.fr/api/thermal_plants"

    def fetch(self) -> pd.DataFrame:
        logger = get_dagster_logger()
        logger.info("Fetching thermal power plant data from %s", self.base_url)
        return pd.DataFrame({"plant_id": [201], "name": ["Plant B"], "capacity_mw": [800]})


class DeathRecordsAPIResource(ConfigurableResource):
    """Resource for accessing the Death Records Dataset API (data.gouv.fr)."""

    base_url: str = "https://data.gouv.fr/api/death_records"

    def fetch(self) -> pd.DataFrame:
        logger = get_dagster_logger()
        logger.info("Fetching death records data from %s", self.base_url)
        return pd.DataFrame({"record_id": [1, 2], "city_id": [1, 2], "deaths": [5, 3]})


class RedisCacheResource(ConfigurableResource):
    """Simple Redis cache wrapper."""

    host: str = "redis"
    port: int = 6379

    def set(self, key: str, value: Any) -> None:
        logger = get_dagster_logger()
        logger.info("Setting cache key %s", key)
        # Placeholder: implement real Redis set.

    def get(self, key: str) -> Any:
        logger = get_dagster_logger()
        logger.info("Getting cache key %s", key)
        # Placeholder: implement real Redis get.
        return None


class PostgresDBResource(ConfigurableResource):
    """PostgreSQL database connection."""

    conn_str: str = "postgresql://postgres:postgres@postgres_default:5432/postgres"

    def execute(self, query: str, params: Dict[str, Any] = None) -> None:
        logger = get_dagster_logger()
        logger.info("Executing SQL query: %s", query)
        # Placeholder: implement real DB execution.


# ----------------------------------------------------------------------
# Ops
# ----------------------------------------------------------------------


@op(
    required_resource_keys={"city_geo_api"},
    out=Out(pd.DataFrame, description="Raw city geographic data"),
    retry_policy=RetryPolicy(max_retries=3, delay=5),
    description="Ingest city geographic data from the City Geo API.",
)
def ingest_city_geo(context) -> pd.DataFrame:
    df = context.resources.city_geo_api.fetch()
    context.log.info("Ingested %d city records.", len(df))
    return df


@op(
    required_resource_keys={"nuclear_plants_api"},
    out=Out(pd.DataFrame, description="Raw nuclear power plant data"),
    retry_policy=RetryPolicy(max_retries=3, delay=5),
    description="Ingest nuclear power plant data from the Nuclear Plants API.",
)
def ingest_nuclear_plants(context) -> pd.DataFrame:
    df = context.resources.nuclear_plants_api.fetch()
    context.log.info("Ingested %d nuclear plant records.", len(df))
    return df


@op(
    required_resource_keys={"thermal_plants_api"},
    out=Out(pd.DataFrame, description="Raw thermal power plant data"),
    retry_policy=RetryPolicy(max_retries=3, delay=5),
    description="Ingest thermal power plant data from the Thermal Plants API.",
)
def ingest_thermal_plants(context) -> pd.DataFrame:
    df = context.resources.thermal_plants_api.fetch()
    context.log.info("Ingested %d thermal plant records.", len(df))
    return df


@op(
    required_resource_keys={"death_records_api"},
    out=Out(pd.DataFrame, description="Raw death records data"),
    retry_policy=RetryPolicy(max_retries=3, delay=5),
    description="Ingest death records data from the Death Records API.",
)
def ingest_death_records(context) -> pd.DataFrame:
    df = context.resources.death_records_api.fetch()
    context.log.info("Ingested %d death record entries.", len(df))
    return df


@op(
    ins={
        "city_geo": In(pd.DataFrame),
        "nuclear": In(pd.DataFrame),
        "thermal": In(pd.DataFrame),
        "death_records": In(pd.DataFrame),
    },
    out=Out(pd.DataFrame, description="Combined and transformed dataset"),
    required_resource_keys={"redis_cache"},
    description="Combine all ingested datasets, perform transformations, and cache intermediate results.",
)
def combine_and_transform(context, city_geo: pd.DataFrame, nuclear: pd.DataFrame, thermal: pd.DataFrame, death_records: pd.DataFrame) -> pd.DataFrame:
    logger = context.log
    logger.info("Starting data combination and transformation.")

    # Example join: attach death records to city data
    merged = city_geo.merge(death_records, on="city_id", how="left")
    merged["deaths"] = merged["deaths"].fillna(0)

    # Append plant data (simple concatenation for illustration)
    plants = pd.concat([nuclear, thermal], ignore_index=True, sort=False)
    merged["plant_count"] = len(plants)

    # Cache the intermediate result
    cache_key = "combined_dataset"
    context.resources.redis_cache.set(cache_key, merged.to_json())

    logger.info("Transformation complete. Resulting rows: %d", len(merged))
    return merged


@op(
    ins={"dataset": In(pd.DataFrame)},
    out=Out(bool, description="Flag indicating whether dataset passes quality checks"),
    description="Perform simple data quality checks; returns True if data is acceptable.",
)
def data_quality_check(context, dataset: pd.DataFrame) -> bool:
    logger = context.log
    if dataset.empty:
        logger.warning("Dataset is empty after transformation.")
        return False
    if dataset["deaths"].isnull().any():
        logger.warning("Found null values in 'deaths' column.")
        return False
    logger.info("Data quality check passed.")
    return True


@op(
    ins={"dataset": In(pd.DataFrame), "quality_flag": In(bool)},
    required_resource_keys={"postgres_db"},
    out=Out(Nothing),
    description="Load the transformed dataset into PostgreSQL if quality checks passed.",
)
def load_to_postgres(context, dataset: pd.DataFrame, quality_flag: bool) -> Nothing:
    if not quality_flag:
        context.log.info("Quality check failed; skipping load to PostgreSQL.")
        return Nothing

    # Example: create a temporary table and insert data
    insert_query = """
        INSERT INTO public.french_death_records (city_id, name, lat, lon, deaths, plant_count)
        VALUES (%(city_id)s, %(name)s, %(lat)s, %(lon)s, %(deaths)s, %(plant_count)s)
        ON CONFLICT (city_id) DO UPDATE SET
            deaths = EXCLUDED.deaths,
            plant_count = EXCLUDED.plant_count;
    """
    for _, row in dataset.iterrows():
        params = {
            "city_id": row["city_id"],
            "name": row["name"],
            "lat": row["lat"],
            "lon": row["lon"],
            "deaths": int(row["deaths"]),
            "plant_count": int(row["plant_count"]),
        }
        context.resources.postgres_db.execute(insert_query, params)

    context.log.info("Loaded %d rows into PostgreSQL.", len(dataset))
    return Nothing


# ----------------------------------------------------------------------
# Job Definition
# ----------------------------------------------------------------------


@job(
    executor_def=multiprocess_executor,
    resource_defs={
        "city_geo_api": CityGeoAPIResource(),
        "nuclear_plants_api": NuclearPlantsAPIResource(),
        "thermal_plants_api": ThermalPlantsAPIResource(),
        "death_records_api": DeathRecordsAPIResource(),
        "redis_cache": RedisCacheResource(),
        "postgres_db": PostgresDBResource(),
        "io_manager": fs_io_manager,
    },
    description=(
        "ETL pipeline that processes French government death records and power plant data "
        "using a staged ETL pattern with fan‑out/fan‑in parallelism, conditional branching, "
        "and PostgreSQL loading."
    ),
    name="global_dag",
)
def global_dag():
    # Fan‑out ingestion ops (run in parallel)
    city_geo = ingest_city_geo()
    nuclear = ingest_nuclear_plants()
    thermal = ingest_thermal_plants()
    death_records = ingest_death_records()

    # Fan‑in transformation
    transformed = combine_and_transform(
        city_geo=city_geo,
        nuclear=nuclear,
        thermal=thermal,
        death_records=death_records,
    )

    # Conditional branching based on quality check
    quality_ok = data_quality_check(transformed)

    # Load step (executes only if quality_ok is True)
    load_to_postgres(dataset=transformed, quality_flag=quality_ok)


# ----------------------------------------------------------------------
# End of file
# ----------------------------------------------------------------------