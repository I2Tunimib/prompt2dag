# Generated by Prefect Pipeline Generator
# Pipeline: start_pipeline_pipeline
# Description: No description provided.
# Pattern: fanout_fanin
# Prefect version: 2.14.0
# Schedule (disabled): @daily, UTC, catchup=False

import pandas as pd
from pathlib import Path
from typing import List

from prefect import flow, task, get_run_logger
from prefect.task_runners import ConcurrentTaskRunner
from prefect.exceptions import PrefectException
from prefect.filesystems import LocalFileSystem
from prefect.blocks.system import Secret

# -------------------------------------------------------------------------
# Load infrastructure blocks (connections/resources)
# -------------------------------------------------------------------------
regional_sales_us_east: LocalFileSystem = LocalFileSystem.load("regional_sales_us_east")
regional_sales_us_west: LocalFileSystem = LocalFileSystem.load("regional_sales_us_west")
regional_sales_eu: LocalFileSystem = LocalFileSystem.load("regional_sales_eu")
regional_sales_apac: LocalFileSystem = LocalFileSystem.load("regional_sales_apac")
global_report_storage: LocalFileSystem = LocalFileSystem.load("global_report_storage")
email_notification_smtp: Secret = Secret.load("email_notification_smtp")


# -------------------------------------------------------------------------
# Helper functions
# -------------------------------------------------------------------------
def _read_csv_from_block(block: LocalFileSystem, pattern: str = "*.csv") -> pd.DataFrame:
    """
    Reads all CSV files matching ``pattern`` from the given ``LocalFileSystem`` block
    and concatenates them into a single DataFrame.

    Args:
        block: Prefect LocalFileSystem block pointing to a directory.
        pattern: Glob pattern for CSV files.

    Returns:
        DataFrame containing concatenated CSV data.
    """
    logger = get_run_logger()
    base_path = Path(block.basepath) if block.basepath else Path(".")
    csv_files = list(base_path.glob(pattern))
    if not csv_files:
        logger.warning(f"No CSV files found in {base_path}")
        return pd.DataFrame()
    dfs = []
    for file in csv_files:
        logger.info(f"Reading {file}")
        dfs.append(pd.read_csv(file))
    return pd.concat(dfs, ignore_index=True)


def _write_report(df: pd.DataFrame, block: LocalFileSystem, filename: str = "global_revenue_report.csv"):
    """
    Writes the provided DataFrame to a CSV file in the given ``LocalFileSystem`` block.

    Args:
        df: DataFrame to write.
        block: Prefect LocalFileSystem block where the file will be stored.
        filename: Name of the output CSV file.
    """
    logger = get_run_logger()
    output_path = Path(block.basepath) / filename if block.basepath else Path(filename)
    df.to_csv(output_path, index=False)
    logger.info(f"Global revenue report written to {output_path}")


# -------------------------------------------------------------------------
# Tasks
# -------------------------------------------------------------------------
@task(retries=0, name="Start Pipeline")
def start_pipeline() -> None:
    """
    Entry point task for the pipeline. Performs any required initialization.
    """
    logger = get_run_logger()
    logger.info("Pipeline started.")


@task(retries=2, name="Ingest APAC Sales Data")
def ingest_apac() -> pd.DataFrame:
    """
    Ingests APAC regional sales CSV files.
    """
    logger = get_run_logger()
    logger.info("Ingesting APAC sales data.")
    return _read_csv_from_block(regional_sales_apac)


@task(retries=2, name="Ingest EU Sales Data")
def ingest_eu() -> pd.DataFrame:
    """
    Ingests EU regional sales CSV files.
    """
    logger = get_run_logger()
    logger.info("Ingesting EU sales data.")
    return _read_csv_from_block(regional_sales_eu)


@task(retries=2, name="Ingest US‑East Sales Data")
def ingest_us_east() -> pd.DataFrame:
    """
    Ingests US‑East regional sales CSV files.
    """
    logger = get_run_logger()
    logger.info("Ingesting US‑East sales data.")
    return _read_csv_from_block(regional_sales_us_east)


@task(retries=2, name="Ingest US‑West Sales Data")
def ingest_us_west() -> pd.DataFrame:
    """
    Ingests US‑West regional sales CSV files.
    """
    logger = get_run_logger()
    logger.info("Ingesting US‑West sales data.")
    return _read_csv_from_block(regional_sales_us_west)


@task(retries=2, name="Convert APAC Currency to USD")
def convert_currency_apac(df: pd.DataFrame) -> pd.DataFrame:
    """
    Converts APAC local currency to USD using a fixed conversion rate.
    """
    logger = get_run_logger()
    if df.empty:
        logger.warning("APAC dataframe is empty; skipping conversion.")
        return df
    conversion_rate = 0.013  # Example: 1 local unit = 0.013 USD
    df["revenue_usd"] = df["revenue_local"] * conversion_rate
    logger.info("Converted APAC revenue to USD.")
    return df


@task(retries=2, name="Convert EU Currency to USD")
def convert_currency_eu(df: pd.DataFrame) -> pd.DataFrame:
    """
    Converts EU Euro revenue to USD using a fixed conversion rate.
    """
    logger = get_run_logger()
    if df.empty:
        logger.warning("EU dataframe is empty; skipping conversion.")
        return df
    conversion_rate = 1.10  # Example: 1 EUR = 1.10 USD
    df["revenue_usd"] = df["revenue_eur"] * conversion_rate
    logger.info("Converted EU revenue to USD.")
    return df


@task(retries=2, name="Convert US‑East Currency to USD")
def convert_currency_us_east(df: pd.DataFrame) -> pd.DataFrame:
    """
    US‑East data is already in USD; ensures column naming consistency.
    """
    logger = get_run_logger()
    if df.empty:
        logger.warning("US‑East dataframe is empty; skipping conversion.")
        return df
    df["revenue_usd"] = df["revenue_usd"]
    logger.info("US‑East revenue already in USD.")
    return df


@task(retries=2, name="Convert US‑West Currency to USD")
def convert_currency_us_west(df: pd.DataFrame) -> pd.DataFrame:
    """
    US‑West data is already in USD; ensures column naming consistency.
    """
    logger = get_run_logger()
    if df.empty:
        logger.warning("US‑West dataframe is empty; skipping conversion.")
        return df
    df["revenue_usd"] = df["revenue_usd"]
    logger.info("US‑West revenue already in USD.")
    return df


@task(retries=2, name="Aggregate Global Revenue")
def aggregate_global_revenue(dfs: List[pd.DataFrame]) -> pd.DataFrame:
    """
    Concatenates all regional DataFrames and computes total global revenue.
    """
    logger = get_run_logger()
    if not dfs:
        raise PrefectException("No dataframes provided for aggregation.")
    combined = pd.concat(dfs, ignore_index=True)
    total_revenue = combined["revenue_usd"].sum()
    report = pd.DataFrame({"total_global_revenue_usd": [total_revenue]})
    logger.info(f"Aggregated global revenue: {total_revenue:.2f} USD")
    return report


@task(retries=0, name="End Pipeline")
def end_pipeline(report: pd.DataFrame) -> None:
    """
    Final task that writes the global revenue report and optionally sends a notification.
    """
    logger = get_run_logger()
    _write_report(report, global_report_storage)
    # Example placeholder for email notification
    try:
        smtp_credentials = email_notification_smtp.get()
        logger.info("Email notification would be sent using SMTP credentials.")
        # Actual email sending logic would go here.
    except Exception as exc:
        logger.warning(f"Failed to retrieve SMTP credentials: {exc}")
    logger.info("Pipeline completed successfully.")


# -------------------------------------------------------------------------
# Flow definition
# -------------------------------------------------------------------------
@flow(
    name="start_pipeline_pipeline",
    task_runner=ConcurrentTaskRunner(),
    description="No description provided."
)
def start_pipeline_pipeline() -> None:
    """
    Orchestrates the end‑to‑end data pipeline:
    1. Starts the pipeline.
    2. Ingests regional sales data in parallel.
    3. Converts each region's currency to USD.
    4. Aggregates the global revenue.
    5. Writes the final report and ends the pipeline.
    """
    # Step 1: start
    start = start_pipeline()

    # Step 2: parallel ingestion (fan‑out)
    apac_df = ingest_apac(wait_for=[start])
    eu_df = ingest_eu(wait_for=[start])
    us_east_df = ingest_us_east(wait_for=[start])
    us_west_df = ingest_us_west(wait_for=[start])

    # Step 3: parallel conversion (fan‑out)
    apac_usd = convert_currency_apac(apac_df)
    eu_usd = convert_currency_eu(eu_df)
    us_east_usd = convert_currency_us_east(us_east_df)
    us_west_usd = convert_currency_us_west(us_west_df)

    # Step 4: aggregation (fan‑in)
    aggregated_report = aggregate_global_revenue(
        dfs=[apac_usd, eu_usd, us_east_usd, us_west_usd]
    )

    # Step 5: end
    end_pipeline(aggregated_report)


# -------------------------------------------------------------------------
# Entry point for local execution
# -------------------------------------------------------------------------
if __name__ == "__main__":
    start_pipeline_pipeline()