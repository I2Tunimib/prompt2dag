# Generated by Prefect Pipeline Generator
# Pipeline: load_and_modify_data_pipeline
# Prefect version: 2.14.0
# Date: 2024-06-13

"""Prefect flow that runs a series of Docker containers to load, modify,
reconcile, enrich, extend and finally save data.

The flow is executed sequentially using a ``SequentialTaskRunner`` and
relies on several Prefect blocks for configuration:

* ``data_dir`` – ``LocalFileSystem`` block that points to the host directory
  containing the data (mounted into the containers at ``/app/data``).
* ``app_network`` – ``Secret`` block that stores the name of the Docker
  network the containers should join.
* ``here_geocoding_service`` – ``Secret`` block that stores the HERE API token
  used by the reconciliation step.

All Docker tasks are configured with a single retry (``retries=1``) and
propagate any exception raised by the container execution.
"""

from __future__ import annotations

from typing import Dict, Any

from prefect import flow, task
from prefect.task_runners import SequentialTaskRunner
from prefect.filesystems import LocalFileSystem
from prefect.blocks import Secret
from prefect_docker import DockerContainer


def _load_secret(secret_name: str) -> str:
    """Utility to load a secret block and return its value.

    Args:
        secret_name: Name of the Prefect ``Secret`` block.

    Returns:
        The secret value as a string.

    Raises:
        ValueError: If the secret cannot be loaded.
    """
    try:
        secret_block = Secret.load(secret_name)
        return secret_block.get()
    except Exception as exc:
        raise ValueError(f"Unable to load secret '{secret_name}': {exc}") from exc


def _load_filesystem(fs_name: str) -> LocalFileSystem:
    """Utility to load a filesystem block.

    Args:
        fs_name: Name of the Prefect ``LocalFileSystem`` block.

    Returns:
        The loaded ``LocalFileSystem`` block.

    Raises:
        ValueError: If the block cannot be loaded.
    """
    try:
        return LocalFileSystem.load(fs_name)
    except Exception as exc:
        raise ValueError(f"Unable to load filesystem block '{fs_name}': {exc}") from exc


def _docker_task(
    image: str,
    env: Dict[str, str],
    network: str,
    volumes: list[str],
) -> Dict[str, Any]:
    """Runs a Docker container using ``prefect_docker.DockerContainer``.

    Args:
        image: Docker image to run.
        env: Environment variables passed to the container.
        network: Docker network name.
        volumes: List of volume bindings in the form ``host_path:container_path``.

    Returns:
        The result dictionary returned by ``DockerContainer.run``.
    """
    container = DockerContainer(
        image=image,
        env=env,
        network=network,
        volumes=volumes,
        # The containers are expected to run and exit on their own.
        # No explicit command is required unless the image overrides it.
    )
    return container.run()


@task(retries=1, name="Load and Modify Data")
def load_and_modify_data(data_dir_path: str, network: str) -> Dict[str, Any]:
    """Runs the ``load-and-modify`` Docker image.

    The container expects the data directory to be mounted at ``/app/data`` and
    receives a few configuration variables via the environment.

    Args:
        data_dir_path: Host path to the data directory.
        network: Docker network name.

    Returns:
        Result of the Docker container execution.
    """
    env = {
        "DATA_DIR": "/app/data",
        "DATASET_ID": "2",
        "DATE_COLUMN": "Fecha_id",
        "TABLE_NAMING": "JOT_{}",
    }
    volumes = [f"{data_dir_path}:/app/data"]
    return _docker_task(
        image="i2t-backendwithintertwino6-load-and-modify:latest",
        env=env,
        network=network,
        volumes=volumes,
    )


@task(retries=1, name="Data Reconciliation")
def reconcile_city_names(data_dir_path: str, network: str, here_api_token: str) -> Dict[str, Any]:
    """Runs the ``reconciliation`` Docker image to harmonise city names.

    Args:
        data_dir_path: Host path to the data directory.
        network: Docker network name.
        here_api_token: HERE Geocoding API token.

    Returns:
        Result of the Docker container execution.
    """
    env = {
        "RECONCILIATOR_ID": "geocodingHere",
        "PRIMARY_COLUMN": "City",
        "OPTIONAL_COLUMNS": "County,Country",
        "API_TOKEN": here_api_token,
    }
    volumes = [f"{data_dir_path}:/app/data"]
    return _docker_task(
        image="i2t-backendwithintertwino6-reconciliation:latest",
        env=env,
        network=network,
        volumes=volumes,
    )


@task(retries=1, name="OpenMeteo Data Extension")
def enrich_with_openmeteo(data_dir_path: str, network: str) -> Dict[str, Any]:
    """Runs the ``openmeteo-extension`` Docker image to add weather data.

    Args:
        data_dir_path: Host path to the data directory.
        network: Docker network name.

    Returns:
        Result of the Docker container execution.
    """
    env = {
        "WEATHER_ATTRIBUTES": "apparent_temperature_max,apparent_temperature_min,precipitation_sum,precipitation_hours",
        "DATE_SEPARATOR": "-",
    }
    volumes = [f"{data_dir_path}:/app/data"]
    return _docker_task(
        image="i2t-backendwithintertwino6-openmeteo-extension:latest",
        env=env,
        network=network,
        volumes=volumes,
    )


@task(retries=1, name="Column Extension")
def extend_columns(data_dir_path: str, network: str) -> Dict[str, Any]:
    """Runs the ``column-extension`` Docker image to add extra columns.

    Args:
        data_dir_path: Host path to the data directory.
        network: Docker network name.

    Returns:
        Result of the Docker container execution.
    """
    env = {"EXTENDER_ID": "reconciledColumnExt"}
    volumes = [f"{data_dir_path}:/app/data"]
    return _docker_task(
        image="i2t-backendwithintertwino6-column-extension:latest",
        env=env,
        network=network,
        volumes=volumes,
    )


@task(retries=1, name="Save Final Data")
def save_final_data(data_dir_path: str, network: str) -> Dict[str, Any]:
    """Runs the ``save`` Docker image to write the final dataset.

    Args:
        data_dir_path: Host path to the data directory.
        network: Docker network name.

    Returns:
        Result of the Docker container execution.
    """
    env = {"OUTPUT_DIR": "/app/data"}
    volumes = [f"{data_dir_path}:/app/data"]
    return _docker_task(
        image="i2t-backendwithintertwino6-save:latest",
        env=env,
        network=network,
        volumes=volumes,
    )


@flow(
    name="load_and_modify_data_pipeline",
    task_runner=SequentialTaskRunner(),
)
def load_and_modify_data_pipeline() -> None:
    """Orchestrates the full data processing pipeline.

    The flow loads configuration from Prefect blocks, then executes each
    Docker‑based step in the required order.
    """
    # Load configuration blocks
    data_fs = _load_filesystem("data_dir")
    data_dir_path = data_fs.basepath  # Host path to be mounted

    app_network = _load_secret("app_network")
    here_api_token = _load_secret("here_geocoding_service")

    # Execute tasks sequentially respecting dependencies
    load_and_modify_data_result = load_and_modify_data(
        data_dir_path=data_dir_path,
        network=app_network,
    )

    reconcile_result = reconcile_city_names(
        data_dir_path=data_dir_path,
        network=app_network,
        here_api_token=here_api_token,
    )

    openmeteo_result = enrich_with_openmeteo(
        data_dir_path=data_dir_path,
        network=app_network,
    )

    extend_result = extend_columns(
        data_dir_path=data_dir_path,
        network=app_network,
    )

    save_result = save_final_data(
        data_dir_path=data_dir_path,
        network=app_network,
    )

    # Optionally, you could return a summary dictionary
    return {
        "load_and_modify": load_and_modify_data_result,
        "reconcile": reconcile_result,
        "openmeteo": openmeteo_result,
        "extend": extend_result,
        "save": save_result,
    }


if __name__ == "__main__":
    # Running the flow directly (useful for local testing)
    load_and_modify_data_pipeline()