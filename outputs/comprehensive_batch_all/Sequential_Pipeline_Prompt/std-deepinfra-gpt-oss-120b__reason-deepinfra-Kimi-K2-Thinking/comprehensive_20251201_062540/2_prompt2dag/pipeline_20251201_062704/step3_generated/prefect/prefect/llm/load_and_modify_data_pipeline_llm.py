# Generated by Prefect Pipeline Generator
# Pipeline: load_and_modify_data_pipeline
# Generation Timestamp: 2024-06-28T12:00:00Z
# Prefect version: 2.14.0

"""Prefect flow that orchestrates loading, reconciling, enriching,
extending, and saving data using Docker containers."""

from __future__ import annotations

from pathlib import Path
from typing import Dict

from prefect import flow, task
from prefect.task_runners import SequentialTaskRunner
from prefect.filesystems import LocalFileSystem
from prefect.blocks.system import Secret
from prefect_docker import DockerContainer


def _load_secret(secret_name: str) -> str:
    """Utility to load a secret block and return its value.

    Args:
        secret_name: The name of the Prefect Secret block.

    Returns:
        The secret value as a string.
    """
    secret = Secret.load(secret_name)
    return secret.get()


def _docker_network_name() -> str:
    """Retrieve the Docker network name from the secret block."""
    return _load_secret("docker_network_app")


def _shared_volume_path() -> Path:
    """Return the path to the shared data directory volume."""
    fs = LocalFileSystem.load("data_dir_volume")
    # ``basepath`` is the root directory configured for the block.
    return Path(fs.basepath).resolve()


def _run_container(
    image: str,
    env: Dict[str, str] | None = None,
    command: str | None = None,
) -> None:
    """Run a Docker container with the provided configuration.

    The container is attached to the shared data volume and the custom
    Docker network defined in the ``docker_network_app`` secret.

    Args:
        image: Docker image to run.
        env: Optional environment variables to pass to the container.
        command: Optional command to override the image's default CMD.
    """
    volume_path = _shared_volume_path()
    container = DockerContainer(
        image=image,
        command=command,
        env=env or {},
        # Mount the shared volume at ``/data`` inside the container.
        volumes={str(volume_path): "/data"},
        # Attach to the custom Docker network.
        network=_docker_network_name(),
        # Ensure the container is removed after execution.
        remove=True,
    )
    container.run()


@task(retries=1, retry_delay_seconds=60)
def load_and_modify_data() -> None:
    """Execute the *Load and Modify Data* Docker container.

    The container reads raw inputs from the shared volume, performs
    transformations, and writes the intermediate result back to the
    same volume.
    """
    env = {
        "LOAD_MODIFY_API_KEY": _load_secret("load_modify_api"),
        "INTERTWINO_API_KEY": _load_secret("intertwino_api"),
        "MONGODB_URI": _load_secret("mongodb"),
    }
    _run_container(
        image="i2t-backendwithintertwino6-load-and-modify:latest",
        env=env,
    )


@task(retries=1, retry_delay_seconds=60)
def reconcile_city_names() -> None:
    """Execute the *Data Reconciliation* Docker container.

    This step reconciles city names using the HERE Geocoding service.
    """
    env = {
        "RECONCILIATION_API_KEY": _load_secret("reconciliation_api"),
    }
    _run_container(
        image="i2t-backendwithintertwino6-reconciliation:latest",
        env=env,
    )


@task(retries=1, retry_delay_seconds=60)
def enrich_with_openmeteo() -> None:
    """Execute the *OpenMeteo Data Extension* Docker container.

    The container enriches the dataset with weather information from
    the OpenMeteo service.
    """
    env = {
        "OPENMETEO_API_KEY": _load_secret("openmeteo_api"),
    }
    _run_container(
        image="i2t-backendwithintertwino6-openmeteo-extension:latest",
        env=env,
    )


@task(retries=1, retry_delay_seconds=60)
def extend_columns() -> None:
    """Execute the *Column Extension* Docker container.

    Adds any additional columns required for downstream processing.
    """
    _run_container(
        image="i2t-backendwithintertwino6-column-extension:latest",
    )


@task(retries=1, retry_delay_seconds=60)
def save_final_data() -> None:
    """Execute the *Save Final Data* Docker container.

    Persists the final dataset to the configured MongoDB instance.
    """
    env = {
        "MONGODB_URI": _load_secret("mongodb"),
    }
    _run_container(
        image="i2t-backendwithintertwino6-save:latest",
        env=env,
    )


@flow(
    name="load_and_modify_data_pipeline",
    task_runner=SequentialTaskRunner(),
)
def load_and_modify_data_pipeline() -> None:
    """Orchestrates the full data processing pipeline.

    The flow runs each Dockerâ€‘based step sequentially, respecting the
    defined dependencies:

    1. Load and modify raw data.
    2. Reconcile city names.
    3. Enrich with OpenMeteo weather data.
    4. Extend the dataset with additional columns.
    5. Save the final dataset to MongoDB.
    """
    # Step 1
    load_and_modify_data()
    # Step 2
    reconcile_city_names()
    # Step 3
    enrich_with_openmeteo()
    # Step 4
    extend_columns()
    # Step 5
    save_final_data()


# The flow can be registered and deployed via Prefect CLI or UI.
# Example registration command:
#   prefect deployment build -n load_and_modify_data_pipeline_deployment \
#       -p default-agent-pool -a load_and_modify_data_pipeline.py:load_and_modify_data_pipeline \
#       --apply