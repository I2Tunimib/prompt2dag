# Generated by Airflow DAG Code Generator
# Date: 2023-10-05
# Author: Airflow Expert
# Description: Fetch user data from Reqres API, process it, and store it in a PostgreSQL database.

from airflow import DAG
from airflow.operators.http_operator import SimpleHttpOperator
from airflow.operators.python import PythonOperator
from airflow.providers.postgres.operators.postgres import PostgresOperator
from airflow.utils.dates import days_ago
from airflow.exceptions import AirflowException
import requests
import json

# Default arguments for the DAG
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 0,
    'retry_delay': 30,
}

# Define the DAG
with DAG(
    dag_id='fetch_user_data_pipeline',
    description='Fetch user data from Reqres API, process it, and store it in a PostgreSQL database.',
    schedule_interval='@daily',
    start_date=days_ago(1),
    catchup=True,
    default_args=default_args,
    tags=['data_pipeline', 'reqres', 'postgres'],
) as dag:

    # Task: Fetch User Data
    def fetch_user_data(**kwargs):
        try:
            response = requests.get('https://reqres.in/api/users')
            response.raise_for_status()
            return response.json()
        except requests.exceptions.RequestException as e:
            raise AirflowException(f"Error fetching user data: {e}")

    fetch_user_data_task = SimpleHttpOperator(
        task_id='fetch_user_data',
        http_conn_id='reqres_api',
        endpoint='/api/users',
        method='GET',
        response_filter=lambda response: response.json(),
        log_response=True,
    )

    # Task: Process User Data
    def process_user_data(**kwargs):
        ti = kwargs['ti']
        user_data = ti.xcom_pull(task_ids='fetch_user_data')
        processed_data = []
        for user in user_data['data']:
            processed_data.append({
                'id': user['id'],
                'first_name': user['first_name'],
                'last_name': user['last_name'],
                'email': user['email'],
            })
        ti.xcom_push(key='processed_user_data', value=processed_data)

    process_user_data_task = PythonOperator(
        task_id='process_user_data',
        python_callable=process_user_data,
    )

    # Task: Create User Table
    create_user_table_task = PostgresOperator(
        task_id='create_user_table',
        postgres_conn_id='postgres_db',
        sql="""
        CREATE TABLE IF NOT EXISTS users (
            id SERIAL PRIMARY KEY,
            first_name VARCHAR(255) NOT NULL,
            last_name VARCHAR(255) NOT NULL,
            email VARCHAR(255) UNIQUE NOT NULL
        );
        """,
    )

    # Task: Insert User Data
    def insert_user_data(**kwargs):
        ti = kwargs['ti']
        processed_data = ti.xcom_pull(task_ids='process_user_data', key='processed_user_data')
        if not processed_data:
            raise AirflowException("No processed user data found.")
        values = [f"({user['id']}, '{user['first_name']}', '{user['last_name']}', '{user['email']}')" for user in processed_data]
        sql = f"INSERT INTO users (id, first_name, last_name, email) VALUES {', '.join(values)} ON CONFLICT (email) DO NOTHING;"
        return sql

    insert_user_data_task = PostgresOperator(
        task_id='insert_user_data',
        postgres_conn_id='postgres_db',
        sql=insert_user_data,
    )

    # Set task dependencies
    fetch_user_data_task >> process_user_data_task >> create_user_table_task >> insert_user_data_task