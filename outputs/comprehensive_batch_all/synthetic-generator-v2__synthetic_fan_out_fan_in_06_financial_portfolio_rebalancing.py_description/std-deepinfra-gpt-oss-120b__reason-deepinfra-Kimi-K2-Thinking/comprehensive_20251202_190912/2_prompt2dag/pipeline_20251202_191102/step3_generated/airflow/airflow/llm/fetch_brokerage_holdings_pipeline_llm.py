# Generated by AI on 2024-06-13
# DAG: fetch_brokerage_holdings_pipeline
# Description: Portfolio Rebalancing DAG

import json
import logging
from datetime import datetime

import pendulum
import requests
from airflow import DAG
from airflow.exceptions import AirflowException
from airflow.models import Variable
from airflow.hooks.base import BaseHook
from airflow.decorators import task

# Default arguments applied to all tasks
default_args = {
    "owner": "airflow",
    "depends_on_past": False,
    "retries": 0,  # individual tasks override this
    "retry_delay": pendulum.duration(minutes=5),
    "email_on_failure": False,
    "email_on_retry": False,
}

# DAG definition
with DAG(
    dag_id="fetch_brokerage_holdings_pipeline",
    description="Portfolio Rebalancing DAG",
    schedule_interval="@daily",
    start_date=datetime(2024, 1, 1),
    catchup=False,
    default_args=default_args,
    tags=["portfolio", "rebalancing"],
    max_active_runs=1,
    timezone=pendulum.timezone("UTC"),
) as dag:

    @task(retries=2, retry_delay=pendulum.duration(minutes=5))
    def fetch_brokerage_holdings() -> dict:
        """
        Pull current holdings from the simulated brokerage API.
        Returns a dictionary representing the portfolio.
        """
        try:
            conn = BaseHook.get_connection("brokerage_api")
            api_url = conn.host.rstrip("/") + "/holdings"
            auth = (conn.login, conn.password) if conn.login else None

            logging.info("Requesting holdings from %s", api_url)
            response = requests.get(api_url, auth=auth, timeout=30)
            response.raise_for_status()
            holdings = response.json()
            logging.debug("Holdings received: %s", json.dumps(holdings, indent=2))
            return holdings
        except Exception as exc:
            logging.error("Failed to fetch brokerage holdings: %s", exc)
            raise AirflowException(f"Error fetching holdings: {exc}")

    @task(retries=2, retry_delay=pendulum.duration(minutes=5))
    def analyze_portfolio(holdings: dict) -> dict:
        """
        Analyze the current portfolio and compute metrics needed for rebalancing.
        Returns analysis results.
        """
        try:
            logging.info("Analyzing portfolio with %d assets", len(holdings.get("assets", [])))
            # Placeholder analysis â€“ compute total market value
            total_value = sum(asset.get("market_value", 0) for asset in holdings.get("assets", []))
            analysis = {"total_market_value": total_value, "asset_count": len(holdings.get("assets", []))}
            logging.debug("Analysis result: %s", analysis)
            return analysis
        except Exception as exc:
            logging.error("Portfolio analysis failed: %s", exc)
            raise AirflowException(f"Error analyzing portfolio: {exc}")

    @task(retries=2, retry_delay=pendulum.duration(minutes=5))
    def aggregate_and_rebalance(analysis: dict) -> dict:
        """
        Aggregate analysis data and compute target allocations for rebalancing.
        Returns a rebalance plan.
        """
        try:
            target_allocation = 0.10  # Example: each asset should be 10% of total value
            rebalance_plan = {
                "target_allocation_per_asset": target_allocation,
                "total_market_value": analysis["total_market_value"],
            }
            logging.info("Rebalance plan generated")
            logging.debug("Rebalance plan: %s", rebalance_plan)
            return rebalance_plan
        except Exception as exc:
            logging.error("Rebalancing computation failed: %s", exc)
            raise AirflowException(f"Error in rebalance computation: {exc}")

    @task(retries=2, retry_delay=pendulum.duration(minutes=5))
    def generate_trade_orders(rebalance_plan: dict) -> str:
        """
        Generate trade order files based on the rebalance plan.
        Returns the path to the generated order file.
        """
        try:
            conn = BaseHook.get_connection("local_filesystem")
            base_path = conn.host  # Assuming host contains the directory path
            filename = f"trade_orders_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}.json"
            file_path = f"{base_path.rstrip('/')}/{filename}"

            orders = {
                "generated_at": datetime.utcnow().isoformat(),
                "plan": rebalance_plan,
                "orders": [],  # Placeholder for actual order details
            }

            with open(file_path, "w", encoding="utf-8") as f:
                json.dump(orders, f, indent=2)

            logging.info("Trade orders written to %s", file_path)
            return file_path
        except Exception as exc:
            logging.error("Failed to generate trade orders: %s", exc)
            raise AirflowException(f"Error generating trade orders: {exc}")

    # Define task pipeline
    holdings = fetch_brokerage_holdings()
    analysis = analyze_portfolio(holdings)
    rebalance = aggregate_and_rebalance(analysis)
    trade_orders_path = generate_trade_orders(rebalance)

    # Set explicit dependencies (optional, as TaskFlow handles them)
    holdings >> analysis >> rebalance >> trade_orders_path