# -*- coding: utf-8 -*-
"""
Generated Airflow DAG for Data Transformation Pipeline
Author: Auto-generated by OpenAI ChatGPT
Date: 2024-06-13
"""

from __future__ import annotations

import json
import logging
from datetime import datetime, timedelta

from airflow import DAG
from airflow.decorators import task
from airflow.exceptions import AirflowException
from airflow.utils.dates import days_ago
from airflow.hooks.base import BaseHook

# ----------------------------------------------------------------------
# Default arguments applied to all tasks
# ----------------------------------------------------------------------
default_args = {
    "owner": "airflow",
    "depends_on_past": False,
    "retries": 0,                     # As per specification
    "retry_delay": timedelta(minutes=5),
    "email_on_failure": False,
    "email_on_retry": False,
}

# ----------------------------------------------------------------------
# DAG definition
# ----------------------------------------------------------------------
with DAG(
    dag_id="data_transformation_pipeline",
    description="Comprehensive Pipeline Description",
    schedule_interval=None,               # No periodic schedule (manual trigger)
    start_date=days_ago(1),
    catchup=False,
    default_args=default_args,
    tags=["dataform", "bigquery", "snowflake"],
    max_active_runs=1,
    render_template_as_native_obj=True,
) as dag:

    # ------------------------------------------------------------------
    # Helper: fetch connection details (used by multiple tasks)
    # ------------------------------------------------------------------
    def get_connection(conn_id: str) -> dict:
        """
        Retrieve connection information from Airflow metadata DB.
        Returns a dictionary with connection attributes.
        """
        try:
            conn = BaseHook.get_connection(conn_id)
            return {
                "conn_id": conn.conn_id,
                "host": conn.host,
                "login": conn.login,
                "password": conn.password,
                "schema": conn.schema,
                "port": conn.port,
                "extra": json.loads(conn.extra) if conn.extra else {},
            }
        except Exception as exc:
            raise AirflowException(f"Failed to retrieve connection '{conn_id}': {exc}")

    # ------------------------------------------------------------------
    # Task: Initialize Pipeline
    # ------------------------------------------------------------------
    @task(task_id="initialize_pipeline", retries=0, retry_delay=timedelta(minutes=5))
    def initialize_pipeline() -> dict:
        """
        Perform any required initialization steps.
        Returns a context dictionary that can be passed downstream.
        """
        logging.info("Initializing pipeline...")
        # Example: load configuration from Airflow Variables
        try:
            config = {
                "run_id": f"run_{datetime.utcnow().isoformat()}",
                "start_time": datetime.utcnow().isoformat(),
            }
            logging.debug(f"Pipeline config: {config}")
            return config
        except Exception as exc:
            raise AirflowException(f"Initialization failed: {exc}")

    # ------------------------------------------------------------------
    # Task: Parse Input Parameters
    # ------------------------------------------------------------------
    @task(task_id="parse_input_params", retries=0, retry_delay=timedelta(minutes=5))
    def parse_input_params(init_context: dict) -> dict:
        """
        Parse and validate input parameters required for the Dataform workflow.
        """
        logging.info("Parsing input parameters...")
        try:
            # Placeholder: In a real scenario, pull parameters from Variables,
            # XComs, or external services.
            params = {
                "project_id": "my-gcp-project",
                "location": "us-central1",
                "repository_id": "dataform-repo",
                "commit_sha": "latest",
            }
            logging.debug(f"Parsed parameters: {params}")
            # Merge with init_context for downstream use
            init_context.update(params)
            return init_context
        except Exception as exc:
            raise AirflowException(f"Parameter parsing failed: {exc}")

    # ------------------------------------------------------------------
    # Task: Create Dataform Compilation Result
    # ------------------------------------------------------------------
    @task(task_id="create_compilation_result", retries=0, retry_delay=timedelta(minutes=5))
    def create_compilation_result(context: dict) -> dict:
        """
        Trigger a Dataform compilation via the GCP Dataform API.
        Stores the compilation result ID for later steps.
        """
        logging.info("Creating Dataform compilation result...")
        try:
            dataform_conn = get_connection("gcp_dataform_api")
            # Placeholder for API call – replace with actual HTTP request logic.
            compilation_result_id = "compilation_12345"
            logging.info(
                f"Compilation triggered using connection '{dataform_conn['conn_id']}'. "
                f"Result ID: {compilation_result_id}"
            )
            context["compilation_result_id"] = compilation_result_id
            return context
        except Exception as exc:
            raise AirflowException(f"Compilation creation failed: {exc}")

    # ------------------------------------------------------------------
    # Task: Create Dataform Workflow Invocation
    # ------------------------------------------------------------------
    @task(task_id="create_workflow_invocation", retries=0, retry_delay=timedelta(minutes=5))
    def create_workflow_invocation(context: dict) -> dict:
        """
        Starts a Dataform workflow invocation using the previously created compilation.
        """
        logging.info("Creating Dataform workflow invocation...")
        try:
            dataform_conn = get_connection("gcp_dataform_api")
            # Placeholder for API call – replace with actual HTTP request logic.
            workflow_invocation_id = "invocation_67890"
            logging.info(
                f"Workflow invocation started using connection '{dataform_conn['conn_id']}'. "
                f"Invocation ID: {workflow_invocation_id}"
            )
            context["workflow_invocation_id"] = workflow_invocation_id
            return context
        except Exception as exc:
            raise AirflowException(f"Workflow invocation creation failed: {exc}")

    # ------------------------------------------------------------------
    # Task: Monitor Workflow Invocation State
    # ------------------------------------------------------------------
    @task(task_id="monitor_workflow_state", retries=0, retry_delay=timedelta(minutes=5))
    def monitor_workflow_state(context: dict) -> dict:
        """
        Polls the Dataform workflow invocation until it reaches a terminal state.
        """
        logging.info("Monitoring workflow invocation state...")
        try:
            dataform_conn = get_connection("gcp_dataform_api")
            invocation_id = context.get("workflow_invocation_id")
            if not invocation_id:
                raise AirflowException("Missing workflow_invocation_id in context.")

            # Placeholder polling logic – replace with real status checks.
            # For demonstration, we assume the workflow succeeds instantly.
            workflow_state = "SUCCEEDED"
            logging.info(
                f"Workflow invocation '{invocation_id}' completed with state: {workflow_state}"
            )
            context["workflow_state"] = workflow_state
            return context
        except Exception as exc:
            raise AirflowException(f"Workflow monitoring failed: {exc}")

    # ------------------------------------------------------------------
    # Task: Finalize Pipeline
    # ------------------------------------------------------------------
    @task(task_id="finalize_pipeline", retries=0, retry_delay=timedelta(minutes=5))
    def finalize_pipeline(context: dict) -> None:
        """
        Perform any cleanup, logging, or downstream notifications.
        """
        logging.info("Finalizing pipeline...")
        try:
            # Example: write final status to a BigQuery table using the Snowflake connection.
            snowflake_conn = get_connection("bigquery_dataset_trigger")
            workflow_state = context.get("workflow_state", "UNKNOWN")
            logging.info(
                f"Pipeline finished. Workflow state: {workflow_state}. "
                f"Using connection '{snowflake_conn['conn_id']}' for any post‑processing."
            )
            # Placeholder for actual finalization logic.
        except Exception as exc:
            raise AirflowException(f"Finalization failed: {exc}")

    # ------------------------------------------------------------------
    # Define task pipeline (sequential execution)
    # ------------------------------------------------------------------
    init = initialize_pipeline()
    parsed = parse_input_params(init)
    compiled = create_compilation_result(parsed)
    invoked = create_workflow_invocation(compiled)
    monitored = monitor_workflow_state(invoked)
    finalize = finalize_pipeline(monitored)

    # Explicit dependencies (TaskFlow API handles XCom passing automatically)
    init >> parsed >> compiled >> invoked >> monitored >> finalize