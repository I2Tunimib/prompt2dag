# Generated by AI Assistant on 2024-06-28
# Airflow DAG: Supply Chain Shipment ETL
# Description: Sequential ETL pipeline for shipment data processing.

import datetime
import pendulum
import pandas as pd
from airflow import DAG
from airflow.decorators import task
from airflow.providers.postgres.hooks.postgres import PostgresHook
from airflow.providers.smtp.operators.email import EmailOperator
from airflow.utils.task_group import TaskGroup
from airflow.utils.email import send_email
from airflow.models import Variable
from airflow.exceptions import AirflowFailException
from airflow.utils.dates import days_ago
from airflow.utils.trigger_rule import TriggerRule
from airflow.operators.empty import EmptyOperator
from airflow.operators.python import get_current_context
from airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator

# Default arguments for all tasks
default_args = {
    "owner": "airflow",
    "depends_on_past": False,
    "retries": 2,
    "retry_delay": datetime.timedelta(minutes=5),
    "email_on_failure": True,
    "email_on_retry": False,
}

# Define the DAG
with DAG(
    dag_id="supply_chain_shipment_etl",
    description="Sequential ETL pipeline for shipment data processing.",
    schedule_interval="@daily",
    start_date=days_ago(1),
    catchup=False,
    default_args=default_args,
    tags=["supply_chain", "etl"],
    max_active_runs=1,
    timezone=pendulum.timezone("UTC"),
    is_paused_upon_creation=True,  # Disabled by default
) as dag:

    # -------------------------------------------------------------------------
    # Placeholder for the upstream extraction task (join_extraction)
    # In a real implementation this would read raw shipment files from the
    # vendor_filesystem connection and produce a DataFrame or temporary file.
    # -------------------------------------------------------------------------
    @task(task_id="join_extraction", retries=2)
    def join_extraction():
        """
        Extract raw shipment CSV files from the vendor filesystem.
        Returns a pandas DataFrame with the raw data.
        """
        ctx = get_current_context()
        # Retrieve connection details (example, adjust as needed)
        vendor_conn_id = "vendor_filesystem"
        # For illustration, we assume the CSV path is stored in an Airflow Variable.
        csv_path = Variable.get("vendor_shipment_csv_path", default_var="/tmp/shipments_raw.csv")
        try:
            df = pd.read_csv(csv_path)
            return df.to_json(date_format="iso", orient="records")
        except Exception as exc:
            raise AirflowFailException(f"Failed to read shipment CSV: {exc}")

    # -------------------------------------------------------------------------
    # Cleanse and Normalize Shipments
    # -------------------------------------------------------------------------
    @task(task_id="cleanse_and_normalize_shipments", retries=2)
    def cleanse_and_normalize_shipments(raw_json: str):
        """
        Cleanse and normalize shipment data.
        Expects raw data as JSON string (produced by join_extraction).
        Returns normalized data as JSON string.
        """
        try:
            df = pd.read_json(raw_json, orient="records")
            # Example cleansing steps
            df.dropna(subset=["shipment_id", "origin", "destination"], inplace=True)
            df["shipment_date"] = pd.to_datetime(df["shipment_date"], errors="coerce")
            df = df[df["shipment_date"].notna()]
            # Normalization: standardize column names
            df.rename(columns=lambda x: x.strip().lower().replace(" ", "_"), inplace=True)
            return df.to_json(date_format="iso", orient="records")
        except Exception as exc:
            raise AirflowFailException(f"Data cleansing failed: {exc}")

    # -------------------------------------------------------------------------
    # Load Shipments to Inventory Database
    # -------------------------------------------------------------------------
    @task(task_id="load_shipments_to_inventory", retries=2)
    def load_shipments_to_inventory(normalized_json: str):
        """
        Load the normalized shipment data into the PostgreSQL inventory database.
        """
        pg_conn_id = "inventory_postgres"
        hook = PostgresHook(postgres_conn_id=pg_conn_id)
        try:
            df = pd.read_json(normalized_json, orient="records")
            # Create a temporary table name
            temp_table = "tmp_shipments"
            # Write DataFrame to PostgreSQL using the hook's get_sqlalchemy_engine
            engine = hook.get_sqlalchemy_engine()
            df.to_sql(temp_table, con=engine, if_exists="replace", index=False)
            # Upsert into the target table (example assumes target table 'shipments')
            upsert_sql = """
                INSERT INTO shipments (shipment_id, origin, destination, shipment_date, quantity, weight)
                SELECT shipment_id, origin, destination, shipment_date, quantity, weight FROM tmp_shipments
                ON CONFLICT (shipment_id) DO UPDATE SET
                    origin = EXCLUDED.origin,
                    destination = EXCLUDED.destination,
                    shipment_date = EXCLUDED.shipment_date,
                    quantity = EXCLUDED.quantity,
                    weight = EXCLUDED.weight;
                DROP TABLE IF EXISTS tmp_shipments;
            """
            hook.run(upsert_sql)
        except Exception as exc:
            raise AirflowFailException(f"Failed to load shipments into inventory DB: {exc}")

    # -------------------------------------------------------------------------
    # Send ETL Summary Email
    # -------------------------------------------------------------------------
    @task(task_id="send_etl_summary_email", retries=2)
    def send_etl_summary_email():
        """
        Send a summary email to the supply chain team after successful ETL.
        """
        smtp_conn_id = "email_smtp"
        # Email details can be stored in Variables or hardâ€‘coded
        to = Variable.get("supply_chain_email_recipients", default_var="supply.chain@example.com")
        subject = "Supply Chain Shipment ETL Summary"
        html_content = """
        <h3>ETL Run Completed Successfully</h3>
        <p>The shipment data has been processed and loaded into the inventory database.</p>
        """
        try:
            send_email(to=to, subject=subject, html_content=html_content, conn_id=smtp_conn_id)
        except Exception as exc:
            raise AirflowFailException(f"Failed to send ETL summary email: {exc}")

    # -------------------------------------------------------------------------
    # Define task flow
    # -------------------------------------------------------------------------
    raw_data = join_extraction()
    cleaned_data = cleanse_and_normalize_shipments(raw_data)
    load_shipments_to_inventory(cleaned_data)
    send_etl_summary_email()

    # End of DAG definition.