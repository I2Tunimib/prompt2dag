# Generated by Prefect Pipeline Generator
# Date: 2024-06-28
# Prefect version: 2.14.0
# Flow: supply_chain_shipment_etl

import os
import smtplib
from datetime import datetime
from email.message import EmailMessage
from typing import List

import pandas as pd
from sqlalchemy import create_engine
from prefect import flow, task, get_run_logger
from prefect.task_runners import ConcurrentTaskRunner
from prefect.blocks.system import Secret
from prefect.filesystems import LocalFileSystem


@task(retries=2, retry_delay_seconds=60)
def join_extraction(
    vendor_fs: LocalFileSystem,
    reference_fs: LocalFileSystem,
) -> pd.DataFrame:
    """
    Extract shipment CSV files from the vendor filesystem and join them with
    location reference tables.

    Args:
        vendor_fs: Prefect LocalFileSystem block pointing to vendor CSV files.
        reference_fs: Prefect LocalFileSystem block pointing to reference tables.

    Returns:
        A pandas DataFrame containing the joined raw shipment data.
    """
    logger = get_run_logger()
    try:
        # List CSV files in the vendor directory
        vendor_path = vendor_fs.basepath
        csv_files = [
            f for f in vendor_fs.get_directory(separator="/")
            if f.lower().endswith(".csv")
        ]

        if not csv_files:
            raise FileNotFoundError("No CSV files found in vendor filesystem.")

        # Read and concatenate all vendor CSVs
        df_list: List[pd.DataFrame] = []
        for file_name in csv_files:
            file_path = os.path.join(vendor_path, file_name)
            logger.info(f"Reading vendor file: {file_path}")
            df = pd.read_csv(file_path)
            df_list.append(df)

        raw_shipments = pd.concat(df_list, ignore_index=True)

        # Load location reference table (assume a single CSV named 'locations.csv')
        ref_path = os.path.join(reference_fs.basepath, "locations.csv")
        logger.info(f"Reading location reference table: {ref_path}")
        locations = pd.read_csv(ref_path)

        # Join on a common column, e.g., 'location_id'
        joined = raw_shipments.merge(
            locations,
            how="left",
            left_on="location_id",
            right_on="location_id",
            suffixes=("", "_ref"),
        )
        logger.info("Extraction and join completed.")
        return joined
    except Exception as exc:
        logger.error(f"Error in join_extraction: {exc}")
        raise


@task(retries=2, retry_delay_seconds=60)
def cleanse_and_normalize_shipments(raw_df: pd.DataFrame) -> pd.DataFrame:
    """
    Cleanse and normalize the extracted shipment data.

    Steps performed:
    - Drop rows with critical missing values.
    - Standardize column names to snake_case.
    - Convert dates to datetime objects.
    - Ensure numeric fields are correctly typed.

    Args:
        raw_df: DataFrame returned from join_extraction.

    Returns:
        Cleaned and normalized DataFrame ready for loading.
    """
    logger = get_run_logger()
    try:
        df = raw_df.copy()

        # Drop rows where essential columns are missing
        essential_cols = ["shipment_id", "location_id", "quantity", "shipment_date"]
        df.dropna(subset=essential_cols, inplace=True)

        # Standardize column names
        df.columns = [col.strip().lower().replace(" ", "_") for col in df.columns]

        # Convert dates
        if "shipment_date" in df.columns:
            df["shipment_date"] = pd.to_datetime(df["shipment_date"], errors="coerce")
            df = df.dropna(subset=["shipment_date"])

        # Ensure numeric types
        if "quantity" in df.columns:
            df["quantity"] = pd.to_numeric(df["quantity"], errors="coerce")
            df = df.dropna(subset=["quantity"])

        logger.info("Data cleansing and normalization completed.")
        return df
    except Exception as exc:
        logger.error(f"Error in cleanse_and_normalize_shipments: {exc}")
        raise


@task(retries=2, retry_delay_seconds=60)
def load_shipments_to_inventory(
    clean_df: pd.DataFrame,
    db_secret: Secret,
    table_name: str = "shipments",
) -> int:
    """
    Load the cleaned shipment data into the PostgreSQL inventory database.

    Args:
        clean_df: Cleaned DataFrame from cleanse_and_normalize_shipments.
        db_secret: Prefect Secret block containing the database URL.
        table_name: Destination table name in the inventory database.

    Returns:
        Number of rows inserted.
    """
    logger = get_run_logger()
    try:
        db_url = db_secret.get()
        engine = create_engine(db_url)

        with engine.begin() as conn:
            clean_df.to_sql(
                name=table_name,
                con=conn,
                if_exists="append",
                index=False,
                method="multi",
            )
        rows_inserted = len(clean_df)
        logger.info(f"Inserted {rows_inserted} rows into {table_name}.")
        return rows_inserted
    except Exception as exc:
        logger.error(f"Error in load_shipments_to_inventory: {exc}")
        raise


@task(retries=2, retry_delay_seconds=60)
def send_etl_summary_email(
    rows_loaded: int,
    email_secret: Secret,
    recipients: List[str],
    subject: str = "Supply Chain Shipment ETL Summary",
) -> None:
    """
    Send an email summarizing the ETL run.

    Args:
        rows_loaded: Number of rows successfully loaded into the database.
        email_secret: Prefect Secret block containing SMTP credentials in JSON:
            {"host": "...", "port": ..., "username": "...", "password": "..."}
        recipients: List of email addresses to notify.
        subject: Email subject line.
    """
    logger = get_run_logger()
    try:
        creds = email_secret.get()
        host = creds["host"]
        port = creds["port"]
        username = creds["username"]
        password = creds["password"]

        msg = EmailMessage()
        msg["From"] = username
        msg["To"] = ", ".join(recipients)
        msg["Subject"] = subject
        body = (
            f"ETL Run Summary - {datetime.utcnow().isoformat()} UTC\n\n"
            f"Rows loaded into inventory: {rows_loaded}\n"
            "Status: SUCCESS\n"
        )
        msg.set_content(body)

        with smtplib.SMTP(host, port) as server:
            server.starttls()
            server.login(username, password)
            server.send_message(msg)

        logger.info("ETL summary email sent successfully.")
    except Exception as exc:
        logger.error(f"Error in send_etl_summary_email: {exc}")
        raise


@flow(
    name="Supply Chain Shipment ETL",
    task_runner=ConcurrentTaskRunner(),
)
def supply_chain_shipment_etl():
    """
    Orchestrates the Supply Chain Shipment ETL pipeline.

    Execution order:
        1. join_extraction
        2. cleanse_and_normalize_shipments
        3. load_shipments_to_inventory
        4. send_etl_summary_email
    """
    logger = get_run_logger()

    # Load Prefect blocks
    vendor_fs = LocalFileSystem.load("vendor_filesystem")
    reference_fs = LocalFileSystem.load("location_reference_tables")
    db_secret = Secret.load("inventory_postgres")
    email_secret = Secret.load("email_smtp")

    # Step 1: Extraction & Join
    raw_df = join_extraction(vendor_fs, reference_fs)

    # Step 2: Cleanse & Normalize
    clean_df = cleanse_and_normalize_shipments(raw_df)

    # Step 3: Load to Inventory DB
    rows_loaded = load_shipments_to_inventory(clean_df, db_secret)

    # Step 4: Send Summary Email
    recipients = ["supply.chain.team@example.com"]
    send_etl_summary_email(rows_loaded, email_secret, recipients)

    logger.info("Supply Chain Shipment ETL flow completed successfully.")


if __name__ == "__main__":
    # Running the flow locally (useful for testing)
    supply_chain_shipment_etl()