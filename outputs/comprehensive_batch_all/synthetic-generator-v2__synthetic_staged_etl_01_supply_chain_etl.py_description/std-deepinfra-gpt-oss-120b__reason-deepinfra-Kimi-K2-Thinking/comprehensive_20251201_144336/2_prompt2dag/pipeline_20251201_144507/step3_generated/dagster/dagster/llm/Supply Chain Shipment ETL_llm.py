# Generated by Dagster ETL code generator
# Date: 2024-06-13
# Dagster version: 1.5.0

from typing import Any, List, Dict

from dagster import (
    op,
    job,
    In,
    Out,
    RetryPolicy,
    ConfigurableResource,
    InitResourceContext,
    get_dagster_logger,
    schedule,
    DefaultScheduleStatus,
    multiprocess_executor,
    fs_io_manager,
)


class PostgresInventoryResource(ConfigurableResource):
    """Resource for connecting to the PostgreSQL inventory database."""

    host: str
    port: int
    database: str
    user: str
    password: str

    def get_connection(self) -> Any:
        """Create and return a database connection.
        In a real implementation this would return a psycopg2 or asyncpg connection.
        """
        logger = get_dagster_logger()
        logger.info(
            f"Connecting to Postgres inventory DB at {self.host}:{self.port}/{self.database}"
        )
        # Placeholder for a real connection object
        return None


class EmailSMTPResource(ConfigurableResource):
    """Resource for sending emails via an SMTP server."""

    smtp_host: str
    smtp_port: int
    username: str
    password: str
    from_address: str

    def send_email(self, to: List[str], subject: str, body: str) -> None:
        """Send an email.
        This is a stub; replace with actual SMTP logic.
        """
        logger = get_dagster_logger()
        logger.info(f"Sending email to {to} with subject '{subject}'")
        # Real implementation would use smtplib or a third‑party email library.


@op(
    out=Out(dagster_type=List[Dict[str, Any]]),
    retry_policy=RetryPolicy(max_retries=2),
    description="Extract shipment CSV files and join with location reference tables.",
)
def extract_and_join_shipments(context: InitResourceContext) -> List[Dict[str, Any]]:
    """Read raw shipment CSV files from the vendor filesystem and enrich them with
    location reference data.

    Returns:
        A list of dictionaries representing the joined shipment records.
    """
    logger = get_dagster_logger()
    # In a real implementation you would use the filesystem IO manager to read files.
    logger.info("Extracting vendor shipment CSV files from 'vendor_filesystem'")
    logger.info("Loading location reference tables from 'location_reference_tables'")
    # Placeholder data
    joined_shipments = [
        {"shipment_id": "S001", "origin_code": "NYC", "dest_code": "LAX", "weight": 100},
        {"shipment_id": "S002", "origin_code": "CHI", "dest_code": "MIA", "weight": 200},
    ]
    logger.info(f"Extracted and joined {len(joined_shipments)} shipment records")
    return joined_shipments


@op(
    ins={"raw_shipments": In(dagster_type=List[Dict[str, Any]])},
    out=Out(dagster_type=List[Dict[str, Any]]),
    retry_policy=RetryPolicy(max_retries=2),
    description="Cleanse and normalize shipment data.",
)
def cleanse_and_normalize_shipments(
    context: InitResourceContext, raw_shipments: List[Dict[str, Any]]
) -> List[Dict[str, Any]]:
    """Perform data quality checks, remove invalid rows, and normalize fields.

    Args:
        raw_shipments: List of raw shipment dictionaries.

    Returns:
        List of cleaned and normalized shipment dictionaries.
    """
    logger = get_dagster_logger()
    cleaned = []
    for record in raw_shipments:
        # Simple placeholder cleansing logic
        if record.get("weight") and record["weight"] > 0:
            normalized = {
                "shipment_id": record["shipment_id"],
                "origin": record["origin_code"],
                "destination": record["dest_code"],
                "weight_kg": float(record["weight"]),
            }
            cleaned.append(normalized)
    logger.info(f"Cleaned {len(cleaned)} out of {len(raw_shipments)} records")
    return cleaned


@op(
    ins={"clean_shipments": In(dagster_type=List[Dict[str, Any]])},
    out=Out(dagster_type=int),
    required_resource_keys={"inventory_postgres"},
    retry_policy=RetryPolicy(max_retries=2),
    description="Load cleaned shipments into the inventory PostgreSQL database.",
)
def load_shipments_to_inventory(
    context: InitResourceContext, clean_shipments: List[Dict[str, Any]]
) -> int:
    """Insert the cleaned shipment records into the inventory database.

    Args:
        clean_shipments: List of cleaned shipment dictionaries.

    Returns:
        The number of rows successfully inserted.
    """
    logger = get_dagster_logger()
    pg_resource: PostgresInventoryResource = context.resources.inventory_postgres
    conn = pg_resource.get_connection()
    # Placeholder for insert logic
    rows_inserted = len(clean_shipments)
    logger.info(f"Inserted {rows_inserted} shipment records into inventory database")
    return rows_inserted


@op(
    ins={"rows_loaded": In(dagster_type=int)},
    required_resource_keys={"email_smtp"},
    retry_policy=RetryPolicy(max_retries=2),
    description="Send an email summarizing the ETL run.",
)
def send_etl_summary_email(context: InitResourceContext, rows_loaded: int) -> None:
    """Compose and send an ETL summary email to the supply chain team.

    Args:
        rows_loaded: Number of rows that were loaded into the inventory database.
    """
    logger = get_dagster_logger()
    email_resource: EmailSMTPResource = context.resources.email_smtp
    subject = "Supply Chain Shipment ETL Summary"
    body = f"The ETL job completed successfully. {rows_loaded} shipment records were loaded."
    recipients = ["supply.chain.team@example.com"]
    email_resource.send_email(to=recipients, subject=subject, body=body)
    logger.info("ETL summary email sent")


@job(
    name="supply_chain_shipment_etl",
    description="No description provided.",
    executor_def=multiprocess_executor,
    resource_defs={
        "vendor_filesystem": fs_io_manager,
        "location_reference_tables": fs_io_manager,
        "inventory_postgres": PostgresInventoryResource,
        "email_smtp": EmailSMTPResource,
    },
    io_manager_def=fs_io_manager,
)
def supply_chain_shipment_etl():
    """Dagster job that extracts, cleanses, loads shipment data and notifies the team."""
    raw = extract_and_join_shipments()
    cleaned = cleanse_and_normalize_shipments(raw_shipments=raw)
    rows = load_shipments_to_inventory(clean_shipments=cleaned)
    send_etl_summary_email(rows_loaded=rows)


# Schedule definition (disabled by default)
@schedule(
    cron_schedule="@daily",
    job=supply_chain_shipment_etl,
    execution_timezone="UTC",
    default_status=DefaultScheduleStatus.INACTIVE,
    description="Daily schedule for the Supply Chain Shipment ETL job (disabled).",
)
def daily_supply_chain_shipment_etl_schedule():
    """Schedule placeholder – currently inactive."""
    return {}