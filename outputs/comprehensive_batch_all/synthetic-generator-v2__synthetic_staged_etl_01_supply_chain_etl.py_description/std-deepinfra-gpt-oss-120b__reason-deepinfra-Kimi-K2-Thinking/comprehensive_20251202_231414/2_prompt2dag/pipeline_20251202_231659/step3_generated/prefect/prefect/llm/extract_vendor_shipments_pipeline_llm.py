# Generated by Prefect Pipeline Generator
# Date: 2024-06-28
# Prefect version: 2.14.0
# Pipeline: extract_vendor_shipments_pipeline

import os
import smtplib
import ssl
from email.message import EmailMessage
from pathlib import Path
from typing import List

import pandas as pd
import sqlalchemy
from prefect import flow, task, get_run_logger
from prefect.task_runners import ConcurrentTaskRunner
from prefect.blocks.system import Secret
from prefect.filesystems import LocalFileSystem


@task(retries=2, retry_delay_seconds=60)
def extract_vendor_shipments() -> pd.DataFrame:
    """
    Extract vendor shipment CSV files from the configured LocalFileSystem block.

    Returns:
        pd.DataFrame: Concatenated DataFrame containing all shipment records.
    """
    logger = get_run_logger()
    logger.info("Loading LocalFileSystem block 'vendor_csv_files'")
    fs_block: LocalFileSystem = LocalFileSystem.load("vendor_csv_files")

    base_path = Path(fs_block.base_path)  # type: ignore
    logger.info(f"Scanning directory: {base_path}")

    csv_files: List[Path] = [
        p for p in base_path.rglob("*.csv") if p.is_file()
    ]

    if not csv_files:
        logger.warning("No CSV files found in the vendor directory.")
        return pd.DataFrame()

    data_frames = []
    for csv_path in csv_files:
        logger.info(f"Reading CSV file: {csv_path}")
        df = pd.read_csv(csv_path)
        data_frames.append(df)

    combined_df = pd.concat(data_frames, ignore_index=True)
    logger.info(f"Extracted {len(combined_df)} shipment records.")
    return combined_df


@task(retries=2, retry_delay_seconds=60)
def cleanse_and_normalize_shipments(raw_df: pd.DataFrame) -> pd.DataFrame:
    """
    Cleanse and normalize the raw shipment DataFrame.

    Steps performed:
    - Drop rows where required columns are missing.
    - Standardize column names to snake_case.
    - Convert date columns to datetime objects.
    - Ensure numeric columns are of appropriate dtype.

    Args:
        raw_df (pd.DataFrame): Raw shipment data.

    Returns:
        pd.DataFrame: Cleaned and normalized shipment data.
    """
    logger = get_run_logger()
    if raw_df.empty:
        logger.warning("Received empty DataFrame for cleansing.")
        return raw_df

    logger.info("Starting data cleansing and normalization.")

    # Example required columns; adjust as needed.
    required_columns = ["shipment_id", "vendor_id", "product_sku", "quantity", "shipment_date"]
    missing_cols = [col for col in required_columns if col not in raw_df.columns]
    if missing_cols:
        logger.error(f"Missing required columns: {missing_cols}")
        raise ValueError(f"Missing required columns: {missing_cols}")

    # Drop rows with missing required fields
    df = raw_df.dropna(subset=required_columns)

    # Standardize column names
    df = df.rename(columns=lambda x: x.strip().lower().replace(" ", "_"))

    # Convert dates
    if "shipment_date" in df.columns:
        df["shipment_date"] = pd.to_datetime(df["shipment_date"], errors="coerce")
        df = df.dropna(subset=["shipment_date"])

    # Ensure numeric types
    if "quantity" in df.columns:
        df["quantity"] = pd.to_numeric(df["quantity"], errors="coerce").fillna(0).astype(int)

    logger.info(f"Cleansed DataFrame now has {len(df)} records.")
    return df


@task(retries=2, retry_delay_seconds=60)
def load_shipment_data(clean_df: pd.DataFrame) -> int:
    """
    Load the cleansed shipment data into the inventory PostgreSQL database.

    Args:
        clean_df (pd.DataFrame): Cleaned shipment data.

    Returns:
        int: Number of rows successfully inserted.
    """
    logger = get_run_logger()
    if clean_df.empty:
        logger.warning("No data to load into the database.")
        return 0

    logger.info("Loading Secret block 'inventory_postgres'")
    secret_block: Secret = Secret.load("inventory_postgres")
    conn_info = secret_block.get()  # Expected to be a dict with connection details

    # Expected keys: user, password, host, port, database
    required_keys = {"user", "password", "host", "port", "database"}
    if not required_keys.issubset(conn_info):
        missing = required_keys - conn_info.keys()
        logger.error(f"Missing PostgreSQL connection keys: {missing}")
        raise ValueError(f"Missing PostgreSQL connection keys: {missing}")

    connection_string = (
        f"postgresql://{conn_info['user']}:{conn_info['password']}"
        f"@{conn_info['host']}:{conn_info['port']}/{conn_info['database']}"
    )
    engine = sqlalchemy.create_engine(connection_string)

    table_name = "vendor_shipments"
    logger.info(f"Inserting data into table '{table_name}'")
    with engine.begin() as conn:
        clean_df.to_sql(
            name=table_name,
            con=conn,
            if_exists="append",
            index=False,
            method="multi",
        )
    rows_inserted = len(clean_df)
    logger.info(f"Inserted {rows_inserted} rows into the inventory database.")
    return rows_inserted


@task(retries=2, retry_delay_seconds=60)
def send_summary_email(rows_loaded: int) -> None:
    """
    Send an email summarizing the ETL run.

    Args:
        rows_loaded (int): Number of rows successfully loaded into the database.
    """
    logger = get_run_logger()
    logger.info("Loading Secret block 'email_smtp'")
    secret_block: Secret = Secret.load("email_smtp")
    smtp_info = secret_block.get()  # Expected to be a dict with SMTP details

    required_keys = {"host", "port", "username", "password", "from_addr", "to_addrs"}
    if not required_keys.issubset(smtp_info):
        missing = required_keys - smtp_info.keys()
        logger.error(f"Missing SMTP configuration keys: {missing}")
        raise ValueError(f"Missing SMTP configuration keys: {missing}")

    message = EmailMessage()
    message["Subject"] = "ETL Summary: Vendor Shipments Load"
    message["From"] = smtp_info["from_addr"]
    message["To"] = ", ".join(smtp_info["to_addrs"])
    message.set_content(
        f"The ETL pipeline has completed successfully.\n"
        f"Rows loaded into the inventory database: {rows_loaded}\n"
    )

    context = ssl.create_default_context()
    try:
        with smtplib.SMTP_SSL(smtp_info["host"], smtp_info["port"], context=context) as server:
            server.login(smtp_info["username"], smtp_info["password"])
            server.send_message(message)
        logger.info("Summary email sent successfully.")
    except Exception as exc:
        logger.error(f"Failed to send summary email: {exc}")
        raise


@flow(
    name="extract_vendor_shipments_pipeline",
    task_runner=ConcurrentTaskRunner(),
)
def extract_vendor_shipments_pipeline() -> None:
    """
    Orchestrates the extraction, cleansing, loading, and notification steps for
    vendor shipment data.
    """
    raw_shipments = extract_vendor_shipments()
    cleaned_shipments = cleanse_and_normalize_shipments(raw_shipments)
    rows_inserted = load_shipment_data(cleaned_shipments)
    send_summary_email(rows_inserted)


if __name__ == "__main__":
    # Running the flow locally; in production this would be triggered via a deployment.
    extract_vendor_shipments_pipeline()