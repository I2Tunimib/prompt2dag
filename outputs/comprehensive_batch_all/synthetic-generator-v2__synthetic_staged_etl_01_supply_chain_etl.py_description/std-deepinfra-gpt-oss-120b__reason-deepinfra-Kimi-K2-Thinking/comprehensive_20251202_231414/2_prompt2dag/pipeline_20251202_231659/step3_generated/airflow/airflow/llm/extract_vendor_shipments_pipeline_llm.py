# Generated by Airflow DAG generator on 2024-06-13
"""
DAG: extract_vendor_shipments_pipeline
Description: No description provided.
Schedule: @daily (paused by default)
"""

from datetime import datetime, timedelta
from airflow import DAG
from airflow.decorators import task, dag
from airflow.utils.task_group import TaskGroup
from airflow.utils.trigger_rule import TriggerRule
from airflow.models import Variable
from airflow.providers.common.sql.hooks.sql import DbApiHook
from airflow.providers.postgres.hooks.postgres import PostgresHook
from airflow.providers.smtp.hooks.smtp import SmtpHook
from airflow.operators.dummy import DummyOperator
from airflow.exceptions import AirflowFailException
import pandas as pd
import logging
import smtplib
from email.mime.text import MIMEText

# Default arguments for all tasks
default_args = {
    "owner": "airflow",
    "depends_on_past": False,
    "retries": 2,
    "retry_delay": timedelta(minutes=5),
    "email_on_failure": False,
    "email_on_retry": False,
}


@dag(
    dag_id="extract_vendor_shipments_pipeline",
    default_args=default_args,
    schedule_interval="@daily",
    start_date=datetime(2024, 1, 1),
    catchup=False,
    tags=["etl", "vendor", "shipments"],
    is_paused_upon_creation=True,  # pipeline disabled by default
    render_template_as_native_obj=True,
)
def pipeline():
    """
    Sequential ETL pipeline that extracts vendor shipment CSVs,
    cleanses & normalizes the data, loads it into the inventory
    PostgreSQL database and finally sends a summary email.
    """

    # ----------------------------------------------------------------------
    # External dependency placeholder (extract_join)
    # ----------------------------------------------------------------------
    extract_join = DummyOperator(task_id="extract_join")

    # ----------------------------------------------------------------------
    # Task: Extract Vendor Shipment CSVs
    # ----------------------------------------------------------------------
    @task(
        retries=2,
        retry_delay=timedelta(minutes=5),
        task_id="extract_vendor_shipments",
    )
    def extract_vendor_shipments():
        """
        Reads CSV files from the vendor_csv_files connection (filesystem)
        and returns a list of pandas DataFrames.
        """
        from airflow.hooks.base import BaseHook

        conn = BaseHook.get_connection("vendor_csv_files")
        base_path = conn.extra_dejson.get("path", "/tmp/vendor_shipments")
        logging.info("Reading CSV files from %s", base_path)

        try:
            # Simple glob pattern – adjust as needed
            import glob, os

            csv_files = glob.glob(os.path.join(base_path, "*.csv"))
            if not csv_files:
                raise AirflowFailException("No CSV files found in %s" % base_path)

            data_frames = []
            for file in csv_files:
                logging.info("Loading %s", file)
                df = pd.read_csv(file)
                data_frames.append(df)

            # Return concatenated DataFrame for downstream tasks
            combined_df = pd.concat(data_frames, ignore_index=True)
            return combined_df.to_json(date_format="iso", orient="split")
        except Exception as e:
            logging.exception("Failed to extract vendor shipments")
            raise

    # ----------------------------------------------------------------------
    # Task: Cleanse and Normalize Shipment Data
    # ----------------------------------------------------------------------
    @task(
        retries=2,
        retry_delay=timedelta(minutes=5),
        task_id="cleanse_and_normalize_shipments",
    )
    def cleanse_and_normalize_shipments(raw_json):
        """
        Performs basic cleansing and normalization on the extracted data.
        Expects JSON string from upstream task.
        """
        try:
            df = pd.read_json(raw_json, orient="split")
            logging.info("Rows before cleansing: %d", len(df))

            # Example cleansing steps
            df.dropna(subset=["shipment_id", "vendor_id"], inplace=True)
            df["shipment_date"] = pd.to_datetime(df["shipment_date"], errors="coerce")
            df = df[df["shipment_date"].notna()]

            # Normalization example: ensure consistent column names
            df.rename(columns=lambda x: x.strip().lower(), inplace=True)

            logging.info("Rows after cleansing: %d", len(df))
            return df.to_json(date_format="iso", orient="split")
        except Exception as e:
            logging.exception("Cleansing failed")
            raise

    # ----------------------------------------------------------------------
    # Task: Load Cleansed Shipments to Inventory Database
    # ----------------------------------------------------------------------
    @task(
        retries=2,
        retry_delay=timedelta(minutes=5),
        task_id="load_shipment_data",
    )
    def load_shipment_data(clean_json):
        """
        Loads the cleansed shipment data into the inventory_postgres database.
        """
        try:
            df = pd.read_json(clean_json, orient="split")
            pg_hook = PostgresHook(postgres_conn_id="inventory_postgres")
            engine = pg_hook.get_sqlalchemy_engine()
            table_name = "vendor_shipments"

            # Upsert logic – replace with appropriate strategy
            df.to_sql(
                name=table_name,
                con=engine,
                if_exists="append",
                index=False,
                method="multi",
                chunksize=1000,
            )
            logging.info("Loaded %d rows into %s", len(df), table_name)
            return f"Loaded {len(df)} rows into {table_name}"
        except Exception as e:
            logging.exception("Loading to PostgreSQL failed")
            raise

    # ----------------------------------------------------------------------
    # Task: Send ETL Summary Email
    # ----------------------------------------------------------------------
    @task(
        retries=2,
        retry_delay=timedelta(minutes=5),
        trigger_rule=TriggerRule.ALL_SUCCESS,
        task_id="send_summary_email",
    )
    def send_summary_email(load_message: str):
        """
        Sends a summary email using the email_smtp connection.
        """
        try:
            smtp_conn = SmtpHook(smtp_conn_id="email_smtp")
            subject = "Vendor Shipments ETL Summary"
            body = f"""
            Hello,

            The vendor shipments ETL pipeline has completed successfully.

            Details:
            {load_message}

            Regards,
            Airflow
            """
            msg = MIMEText(body, "plain")
            msg["Subject"] = subject
            msg["From"] = smtp_conn.mail_from
            msg["To"] = smtp_conn.to

            smtp_conn.send_email(to=smtp_conn.to, subject=subject, html_content=body)
            logging.info("Summary email sent to %s", smtp_conn.to)
        except Exception as e:
            logging.exception("Failed to send summary email")
            raise

    # ----------------------------------------------------------------------
    # Define task pipeline
    # ----------------------------------------------------------------------
    raw = extract_vendor_shipments()
    cleaned = cleanse_and_normalize_shipments(raw)
    load_msg = load_shipment_data(cleaned)
    send_summary_email(load_msg)

    # ----------------------------------------------------------------------
    # Set explicit dependencies (including external placeholder)
    # ----------------------------------------------------------------------
    extract_join >> raw

# Instantiate the DAG
dag = pipeline()