# Generated by Airflow DAG generator on 2024-06-28
# DAG: extract_vendor_a_pipeline
# Description: No description provided.
# Pattern: fanout_fanin

from datetime import datetime, timedelta
from airflow import DAG
from airflow.decorators import task
from airflow.operators.dummy import DummyOperator
from airflow.exceptions import AirflowException
from airflow.providers.postgres.hooks.postgres import PostgresHook
from airflow.providers.smtp.hooks.smtp import SmtpHook
from airflow.providers.filesystem.hooks.filesystem import FSHook

# Default arguments applied to all tasks
default_args = {
    "owner": "airflow",
    "depends_on_past": False,
    "email_on_failure": False,
    "email_on_retry": False,
    "retries": 0,  # Individual tasks set their own retry count
    "retry_delay": timedelta(minutes=5),
}

# DAG definition
with DAG(
    dag_id="extract_vendor_a_pipeline",
    description="No description provided.",
    schedule_interval="@daily",
    start_date=datetime(2024, 1, 1),
    catchup=False,
    default_args=default_args,
    tags=["fanout_fanin", "etl"],
    max_active_runs=1,
    timezone="UTC",
) as dag:

    # ----------------------------------------------------------------------
    # Dummy start node to model the fan‑out point (extract_parallel)
    # ----------------------------------------------------------------------
    extract_parallel = DummyOperator(task_id="extract_parallel")

    # ----------------------------------------------------------------------
    # Extraction tasks
    # ----------------------------------------------------------------------
    @task(retries=2, retry_delay=timedelta(minutes=5))
    def extract_vendor_a():
        """
        Extract Vendor A shipment CSV files from the filesystem.
        """
        try:
            fs_hook = FSHook(fs_conn_id="vendor_csv_fs")
            # Placeholder: list and read CSV files for Vendor A
            files = fs_hook.get_path("vendor_a/")
            # Simulate extraction logic
            # data_a = ...
            return {"vendor": "A", "files_processed": len(files)}
        except Exception as exc:
            raise AirflowException(f"Vendor A extraction failed: {exc}")

    @task(retries=2, retry_delay=timedelta(minutes=5))
    def extract_vendor_b():
        """
        Extract Vendor B shipment CSV files from the filesystem.
        """
        try:
            fs_hook = FSHook(fs_conn_id="vendor_csv_fs")
            files = fs_hook.get_path("vendor_b/")
            return {"vendor": "B", "files_processed": len(files)}
        except Exception as exc:
            raise AirflowException(f"Vendor B extraction failed: {exc}")

    @task(retries=2, retry_delay=timedelta(minutes=5))
    def extract_vendor_c():
        """
        Extract Vendor C shipment CSV files from the filesystem.
        """
        try:
            fs_hook = FSHook(fs_conn_id="vendor_csv_fs")
            files = fs_hook.get_path("vendor_c/")
            return {"vendor": "C", "files_processed": len(files)}
        except Exception as exc:
            raise AirflowException(f"Vendor C extraction failed: {exc}")

    # ----------------------------------------------------------------------
    # Data cleansing / enrichment task
    # ----------------------------------------------------------------------
    @task(retries=2, retry_delay=timedelta(minutes=5))
    def cleanse_shipment_data(vendor_a, vendor_b, vendor_c):
        """
        Combine and cleanse data from all vendors.
        """
        try:
            # Placeholder: merge and clean data
            combined = {
                "A": vendor_a,
                "B": vendor_b,
                "C": vendor_c,
            }
            # Simulate cleansing logic
            # cleaned_data = ...
            return {"status": "cleaned", "details": combined}
        except Exception as exc:
            raise AirflowException(f"Data cleansing failed: {exc}")

    # ----------------------------------------------------------------------
    # Load task
    # ----------------------------------------------------------------------
    @task(retries=2, retry_delay=timedelta(minutes=5))
    def load_shipment_to_inventory(cleaned_data):
        """
        Load the cleansed shipment data into the inventory PostgreSQL database.
        """
        try:
            pg_hook = PostgresHook(postgres_conn_id="inventory_postgres")
            # Placeholder: insert cleaned_data into target tables
            # pg_hook.run(insert_sql, parameters=...)
            return {"status": "loaded", "records": 0}
        except Exception as exc:
            raise AirflowException(f"Loading to inventory failed: {exc}")

    # ----------------------------------------------------------------------
    # Email notification task
    # ----------------------------------------------------------------------
    @task(retries=2, retry_delay=timedelta(minutes=5))
    def send_etl_summary_email(load_result):
        """
        Send an email summarising the ETL run.
        """
        try:
            smtp_hook = SmtpHook(smtp_conn_id="smtp_email")
            subject = "ETL Summary: Shipment Load Completed"
            html_content = f"""
            <h3>ETL Run Summary</h3>
            <p>Load Status: {load_result.get('status')}</p>
            <p>Records Loaded: {load_result.get('records')}</p>
            """
            smtp_hook.send_email(
                to=["data-team@example.com"],
                subject=subject,
                html_content=html_content,
            )
            return {"email_sent": True}
        except Exception as exc:
            raise AirflowException(f"Sending summary email failed: {exc}")

    # ----------------------------------------------------------------------
    # Task instantiation and dependencies
    # ----------------------------------------------------------------------
    # Extraction tasks (fan‑out)
    vendor_a_task = extract_vendor_a()
    vendor_b_task = extract_vendor_b()
    vendor_c_task = extract_vendor_c()

    # Fan‑out dependencies
    extract_parallel >> [vendor_a_task, vendor_b_task, vendor_c_task]

    # Fan‑in cleansing
    cleanse_task = cleanse_shipment_data(vendor_a_task, vendor_b_task, vendor_c_task)

    # Load and notification
    load_task = load_shipment_to_inventory(cleanse_task)
    email_task = send_etl_summary_email(load_task)

    # Linear downstream chain
    cleanse_task >> load_task >> email_task