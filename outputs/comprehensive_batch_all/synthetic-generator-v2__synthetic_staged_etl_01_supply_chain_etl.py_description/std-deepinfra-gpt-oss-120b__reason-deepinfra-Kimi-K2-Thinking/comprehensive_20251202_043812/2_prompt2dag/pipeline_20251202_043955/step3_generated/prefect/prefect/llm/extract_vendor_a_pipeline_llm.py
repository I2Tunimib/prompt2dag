# Generated by Prefect Pipeline Generator
# Date: 2024-06-28
# Pipeline: extract_vendor_a_pipeline
# Description: No description provided.
# Pattern: fanout_fanin
# Prefect version: 2.14.0

from __future__ import annotations

import os
import smtplib
import logging
from datetime import datetime
from email.message import EmailMessage
from typing import List

import pandas as pd
from sqlalchemy import create_engine
from prefect import flow, task, get_run_logger
from prefect.task_runners import ConcurrentTaskRunner, SequentialTaskRunner
from prefect.deployments import DeploymentSpec
from prefect.server.schemas.schedules import CronSchedule
from prefect.blocks.system import Secret
from prefect.filesystems import LocalFileSystem

# -------------------------------------------------------------------------
# Block loading helpers
# -------------------------------------------------------------------------

def _load_local_filesystem(block_name: str) -> LocalFileSystem:
    """Load a LocalFileSystem block by name."""
    return LocalFileSystem.load(block_name)


def _load_secret(block_name: str) -> Secret:
    """Load a Secret block by name."""
    return Secret.load(block_name)


# -------------------------------------------------------------------------
# Tasks
# -------------------------------------------------------------------------

@task(retries=2, retry_delay_seconds=60, name="Extract Parallel")
def extract_parallel() -> None:
    """
    Dummy upstream task that enables fan‑out of the vendor extraction tasks.
    It does not perform any work but provides a single dependency point.
    """
    logger = get_run_logger()
    logger.info("Starting parallel extraction phase.")
    # No return value needed; downstream tasks will run in parallel.


@task(retries=2, retry_delay_seconds=60, name="Extract Vendor A Shipments")
def extract_vendor_a(vendor_fs: LocalFileSystem) -> pd.DataFrame:
    """
    Extract shipment data for Vendor A from CSV files stored in a local filesystem.

    Args:
        vendor_fs: Prefect LocalFileSystem block pointing to the CSV directory.

    Returns:
        DataFrame containing Vendor A shipment records.
    """
    logger = get_run_logger()
    logger.info("Extracting Vendor A shipments.")
    csv_path = vendor_fs.get_directory() + "/vendor_a"
    files = vendor_fs.get_directory() + "/vendor_a"
    df_list: List[pd.DataFrame] = []

    for file_name in vendor_fs.listdir(csv_path):
        if file_name.endswith(".csv"):
            file_path = os.path.join(csv_path, file_name)
            logger.debug(f"Reading file {file_path}")
            with vendor_fs.open(file_path, "r") as f:
                df = pd.read_csv(f)
                df["vendor"] = "A"
                df_list.append(df)

    if not df_list:
        logger.warning("No CSV files found for Vendor A.")
        return pd.DataFrame()

    result = pd.concat(df_list, ignore_index=True)
    logger.info(f"Extracted {len(result)} rows for Vendor A.")
    return result


@task(retries=2, retry_delay_seconds=60, name="Extract Vendor B Shipments")
def extract_vendor_b(vendor_fs: LocalFileSystem) -> pd.DataFrame:
    """
    Extract shipment data for Vendor B from CSV files stored in a local filesystem.

    Args:
        vendor_fs: Prefect LocalFileSystem block pointing to the CSV directory.

    Returns:
        DataFrame containing Vendor B shipment records.
    """
    logger = get_run_logger()
    logger.info("Extracting Vendor B shipments.")
    csv_path = vendor_fs.get_directory() + "/vendor_b"
    df_list: List[pd.DataFrame] = []

    for file_name in vendor_fs.listdir(csv_path):
        if file_name.endswith(".csv"):
            file_path = os.path.join(csv_path, file_name)
            logger.debug(f"Reading file {file_path}")
            with vendor_fs.open(file_path, "r") as f:
                df = pd.read_csv(f)
                df["vendor"] = "B"
                df_list.append(df)

    if not df_list:
        logger.warning("No CSV files found for Vendor B.")
        return pd.DataFrame()

    result = pd.concat(df_list, ignore_index=True)
    logger.info(f"Extracted {len(result)} rows for Vendor B.")
    return result


@task(retries=2, retry_delay_seconds=60, name="Extract Vendor C Shipments")
def extract_vendor_c(vendor_fs: LocalFileSystem) -> pd.DataFrame:
    """
    Extract shipment data for Vendor C from CSV files stored in a local filesystem.

    Args:
        vendor_fs: Prefect LocalFileSystem block pointing to the CSV directory.

    Returns:
        DataFrame containing Vendor C shipment records.
    """
    logger = get_run_logger()
    logger.info("Extracting Vendor C shipments.")
    csv_path = vendor_fs.get_directory() + "/vendor_c"
    df_list: List[pd.DataFrame] = []

    for file_name in vendor_fs.listdir(csv_path):
        if file_name.endswith(".csv"):
            file_path = os.path.join(csv_path, file_name)
            logger.debug(f"Reading file {file_path}")
            with vendor_fs.open(file_path, "r") as f:
                df = pd.read_csv(f)
                df["vendor"] = "C"
                df_list.append(df)

    if not df_list:
        logger.warning("No CSV files found for Vendor C.")
        return pd.DataFrame()

    result = pd.concat(df_list, ignore_index=True)
    logger.info(f"Extracted {len(result)} rows for Vendor C.")
    return result


@task(retries=2, retry_delay_seconds=60, name="Cleanse and Enrich Shipment Data")
def cleanse_shipment_data(
    df_a: pd.DataFrame,
    df_b: pd.DataFrame,
    df_c: pd.DataFrame,
) -> pd.DataFrame:
    """
    Combine, cleanse, and enrich shipment data from all vendors.

    Args:
        df_a: DataFrame from Vendor A.
        df_b: DataFrame from Vendor B.
        df_c: DataFrame from Vendor C.

    Returns:
        A cleaned and enriched DataFrame ready for loading.
    """
    logger = get_run_logger()
    logger.info("Starting data cleansing and enrichment.")

    # Concatenate all vendor data
    combined = pd.concat([df_a, df_b, df_c], ignore_index=True)
    logger.debug(f"Combined rows count: {len(combined)}")

    if combined.empty:
        logger.warning("Combined DataFrame is empty after concatenation.")
        return combined

    # Example cleansing steps
    combined.drop_duplicates(inplace=True)
    combined["shipment_date"] = pd.to_datetime(combined["shipment_date"], errors="coerce")
    combined = combined.dropna(subset=["shipment_date"])

    # Enrichment: add processing timestamp
    combined["processed_at"] = datetime.utcnow()
    logger.info(f"Cleaned and enriched {len(combined)} rows.")
    return combined


@task(retries=2, retry_delay_seconds=60, name="Load Shipment Data to Inventory Database")
def load_shipment_to_inventory(
    cleaned_df: pd.DataFrame,
    db_secret: Secret,
) -> None:
    """
    Load the cleaned shipment data into the inventory PostgreSQL database.

    Args:
        cleaned_df: The DataFrame to load.
        db_secret: Secret block containing the database connection URL.
    """
    logger = get_run_logger()
    logger.info("Loading shipment data into inventory database.")

    if cleaned_df.empty:
        logger.warning("Received empty DataFrame; skipping load.")
        return

    # Retrieve connection URL from secret
    connection_url = db_secret.get()
    engine = create_engine(connection_url)

    # Load data into a table named 'shipments'
    with engine.begin() as conn:
        cleaned_df.to_sql(
            name="shipments",
            con=conn,
            if_exists="append",
            index=False,
            method="multi",
        )
    logger.info(f"Successfully loaded {len(cleaned_df)} rows into the inventory database.")


@task(retries=2, retry_delay_seconds=60, name="Send ETL Summary Email")
def send_etl_summary_email(
    cleaned_df: pd.DataFrame,
    email_secret: Secret,
) -> None:
    """
    Send an email summarizing the ETL run.

    Args:
        cleaned_df: The DataFrame that was loaded.
        email_secret: Secret block containing SMTP credentials and recipient list.
    """
    logger = get_run_logger()
    logger.info("Preparing ETL summary email.")

    # Extract SMTP configuration from secret (expected JSON string)
    smtp_config = email_secret.get()
    # Example expected format:
    # {
    #   "host": "smtp.example.com",
    #   "port": 587,
    #   "username": "user",
    #   "password": "pass",
    #   "from": "etl@example.com",
    #   "to": ["ops@example.com"]
    # }
    import json

    cfg = json.loads(smtp_config)

    msg = EmailMessage()
    msg["Subject"] = f"ETL Summary - {datetime.utcnow().isoformat()} UTC"
    msg["From"] = cfg["from"]
    msg["To"] = ", ".join(cfg["to"])

    body = (
        f"ETL run completed at {datetime.utcnow().isoformat()} UTC.\n"
        f"Total rows loaded: {len(cleaned_df)}\n"
        f"Sample rows:\n{cleaned_df.head().to_csv(index=False)}"
    )
    msg.set_content(body)

    try:
        with smtplib.SMTP(cfg["host"], cfg["port"]) as server:
            server.starttls()
            server.login(cfg["username"], cfg["password"])
            server.send_message(msg)
        logger.info("ETL summary email sent successfully.")
    except Exception as exc:
        logger.error(f"Failed to send ETL summary email: {exc}")
        raise


# -------------------------------------------------------------------------
# Flow definition
# -------------------------------------------------------------------------

@flow(
    name="extract_vendor_a_pipeline",
    task_runner=ConcurrentTaskRunner(),
)
def extract_vendor_a_pipeline() -> None:
    """
    Prefect flow that extracts shipment data from three vendors, cleanses
    and enriches the data, loads it into the inventory database, and sends
    a summary email.
    """
    logger = get_run_logger()
    logger.info("Starting extract_vendor_a_pipeline flow.")

    # Load infrastructure blocks
    vendor_fs = _load_local_filesystem("vendor_csv_fs")
    db_secret = _load_secret("inventory_postgres")
    email_secret = _load_secret("smtp_email")

    # Fan‑out: parallel extraction
    extract_parallel.submit()

    df_a = extract_vendor_a.submit(vendor_fs)
    df_b = extract_vendor_b.submit(vendor_fs)
    df_c = extract_vendor_c.submit(vendor_fs)

    # Fan‑in: cleansing after all extracts complete
    cleaned = cleanse_shipment_data.wait_for(df_a, df_b, df_c).submit(
        df_a.result(),
        df_b.result(),
        df_c.result(),
    )

    # Load and email steps (sequential)
    load_shipment_to_inventory.submit(cleaned.result(), db_secret)
    send_etl_summary_email.submit(cleaned.result(), email_secret)

    logger.info("extract_vendor_a_pipeline flow completed.")


# -------------------------------------------------------------------------
# Deployment specification
# -------------------------------------------------------------------------

DeploymentSpec(
    name="extract_vendor_a_pipeline_deployment",
    flow=extract_vendor_a_pipeline,
    schedule=CronSchedule(cron="0 0 * * *", timezone="UTC"),
    tags=["daily", "etl"],
    parameters={},
    work_pool_name="default-agent-pool",
    enforce_parameter_schema=False,
    flow_runner=ConcurrentTaskRunner(),
    description="Daily ETL pipeline for vendor shipment data.",
    catchup=False,
)

# End of file.