# Generated by Dagster Pipeline Generator
# Date: 2024-06-13
# Pipeline: extract_vendor_a_pipeline
# Description: No description provided.

from typing import Dict, List

from dagster import (
    Config,
    In,
    Out,
    RetryPolicy,
    ResourceDefinition,
    ConfigurableResource,
    op,
    job,
    ScheduleDefinition,
    ScheduleStatus,
    multiprocess_executor,
    fs_io_manager,
)


class PostgresResource(ConfigurableResource):
    """Placeholder resource for PostgreSQL interactions."""

    connection_string: str

    def get_connection(self):
        # In a real implementation, return a DB connection object.
        return f"Connected to Postgres at {self.connection_string}"


class EmailResource(ConfigurableResource):
    """Placeholder resource for sending emails."""

    smtp_server: str
    smtp_port: int
    username: str
    password: str

    def send_email(self, subject: str, body: str, recipients: List[str]) -> None:
        # In a real implementation, send an email via SMTP.
        print(f"Sending email to {recipients}: {subject}\n{body}")


class VendorExtractConfig(Config):
    """Configuration for vendor CSV extraction."""

    csv_path: str


@op(
    name="extract_vendor_a",
    out=Out(dict),
    retry_policy=RetryPolicy(max_retries=2),
    config_schema=VendorExtractConfig,
)
def extract_vendor_a(context) -> Dict:
    """Extract shipments from Vendor A CSV file."""
    csv_path = context.op_config.csv_path
    context.log.info(f"Reading Vendor A CSV from {csv_path}")
    # Placeholder: replace with actual CSV parsing logic.
    return {"vendor": "A", "records": []}


@op(
    name="extract_vendor_b",
    out=Out(dict),
    retry_policy=RetryPolicy(max_retries=2),
    config_schema=VendorExtractConfig,
)
def extract_vendor_b(context) -> Dict:
    """Extract shipments from Vendor B CSV file."""
    csv_path = context.op_config.csv_path
    context.log.info(f"Reading Vendor B CSV from {csv_path}")
    return {"vendor": "B", "records": []}


@op(
    name="extract_vendor_c",
    out=Out(dict),
    retry_policy=RetryPolicy(max_retries=2),
    config_schema=VendorExtractConfig,
)
def extract_vendor_c(context) -> Dict:
    """Extract shipments from Vendor C CSV file."""
    csv_path = context.op_config.csv_path
    context.log.info(f"Reading Vendor C CSV from {csv_path}")
    return {"vendor": "C", "records": []}


@op(
    name="cleanse_shipment_data",
    ins={
        "vendor_a": In(dict),
        "vendor_b": In(dict),
        "vendor_c": In(dict),
    },
    out=Out(dict),
    retry_policy=RetryPolicy(max_retries=2),
)
def cleanse_shipment_data(context, vendor_a: Dict, vendor_b: Dict, vendor_c: Dict) -> Dict:
    """Combine and cleanse shipment data from all vendors."""
    context.log.info("Starting data cleansing and enrichment")
    combined = {
        "vendors": [vendor_a, vendor_b, vendor_c],
        "cleaned_records": [],  # Placeholder for cleaned data.
    }
    # Insert real cleansing logic here.
    return combined


@op(
    name="load_shipment_to_inventory",
    ins={"cleaned_data": In(dict)},
    out=Out(None),
    retry_policy=RetryPolicy(max_retries=2),
    required_resource_keys={"inventory_postgres"},
)
def load_shipment_to_inventory(context, cleaned_data: Dict) -> None:
    """Load cleansed shipment data into the inventory PostgreSQL database."""
    conn = context.resources.inventory_postgres.get_connection()
    context.log.info(f"Loading data into inventory DB using {conn}")
    # Placeholder: replace with actual DB insert logic.
    context.log.info("Data load complete")


@op(
    name="send_etl_summary_email",
    ins={"load_result": In(None)},
    out=Out(None),
    retry_policy=RetryPolicy(max_retries=2),
    required_resource_keys={"smtp_email"},
)
def send_etl_summary_email(context) -> None:
    """Send an email summarizing the ETL run."""
    subject = "ETL Summary: Shipment Load Completed"
    body = "The shipment data has been successfully loaded into the inventory database."
    recipients = ["supplychain@example.com"]
    context.resources.smtp_email.send_email(subject, body, recipients)
    context.log.info("ETL summary email sent")


@job(
    name="extract_vendor_a_pipeline",
    description="No description provided.",
    executor_def=multiprocess_executor,
    resource_defs={
        "vendor_csv_fs": fs_io_manager,
        "inventory_postgres": ResourceDefinition.hardcoded_resource(
            PostgresResource(connection_string="postgresql://user:pass@localhost:5432/inventory")
        ),
        "smtp_email": ResourceDefinition.hardcoded_resource(
            EmailResource(
                smtp_server="smtp.example.com",
                smtp_port=587,
                username="etl_user",
                password="secure_password",
            )
        ),
        # Additional resources referenced in the specification.
        "db_location_ref": ResourceDefinition.none_resource(),
        "email_smtp": ResourceDefinition.none_resource(),
        "fs_vendor_a": fs_io_manager,
        "fs_vendor_b": fs_io_manager,
        "fs_vendor_c": fs_io_manager,
        "postgres_inventory": ResourceDefinition.none_resource(),
    },
)
def extract_vendor_a_pipeline():
    """Orchestrates extraction, cleansing, loading, and notification for vendor shipments."""
    # Parallel extraction
    vendor_a_data = extract_vendor_a()
    vendor_b_data = extract_vendor_b()
    vendor_c_data = extract_vendor_c()

    # Data cleansing
    cleaned = cleanse_shipment_data(
        vendor_a=vendor_a_data,
        vendor_b=vendor_b_data,
        vendor_c=vendor_c_data,
    )

    # Load to inventory
    load_result = load_shipment_to_inventory(cleaned)

    # Send summary email
    send_etl_summary_email(load_result)


daily_schedule = ScheduleDefinition(
    job=extract_vendor_a_pipeline,
    cron_schedule="@daily",
    execution_timezone="UTC",
    default_status=ScheduleStatus.RUNNING,
    name="extract_vendor_a_daily_schedule",
)