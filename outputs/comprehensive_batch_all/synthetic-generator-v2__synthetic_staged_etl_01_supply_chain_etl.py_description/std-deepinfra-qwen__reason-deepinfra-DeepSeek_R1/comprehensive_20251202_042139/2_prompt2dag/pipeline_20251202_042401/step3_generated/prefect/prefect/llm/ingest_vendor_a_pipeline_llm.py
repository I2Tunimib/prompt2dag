# Generated by Prefect 2.x Code Generator
# Date: 2023-10-05
# Prefect Version: 2.14.0

from prefect import flow, task, get_run_logger
from prefect.task_runners import ConcurrentTaskRunner
from prefect.deployments import Deployment
from prefect.infrastructure import DockerContainer, Process
from prefect.filesystems import LocalFileSystem
from prefect.blocks.system import Secret
from prefect.orion.schemas.schedules import CronSchedule
from prefect.exceptions import PrefectException

# Load the required resources
vendor_a_data_source = LocalFileSystem.load("vendor_a_data_source")
vendor_b_data_source = LocalFileSystem.load("vendor_b_data_source")
vendor_c_data_source = LocalFileSystem.load("vendor_c_data_source")
location_reference_tables = Secret.load("location_reference_tables")
inventory_database = Secret.load("inventory_database")
email_system = Secret.load("email_system")

@task(retries=2, name="Ingest Vendor A Data")
def ingest_vendor_a():
    logger = get_run_logger()
    logger.info("Ingesting data from Vendor A")
    # Simulate data ingestion
    data_a = vendor_a_data_source.read_path("vendor_a_data.csv")
    logger.info(f"Data from Vendor A: {data_a}")
    return data_a

@task(retries=2, name="Ingest Vendor B Data")
def ingest_vendor_b():
    logger = get_run_logger()
    logger.info("Ingesting data from Vendor B")
    # Simulate data ingestion
    data_b = vendor_b_data_source.read_path("vendor_b_data.csv")
    logger.info(f"Data from Vendor B: {data_b}")
    return data_b

@task(retries=2, name="Ingest Vendor C Data")
def ingest_vendor_c():
    logger = get_run_logger()
    logger.info("Ingesting data from Vendor C")
    # Simulate data ingestion
    data_c = vendor_c_data_source.read_path("vendor_c_data.csv")
    logger.info(f"Data from Vendor C: {data_c}")
    return data_c

@task(retries=2, name="Cleanse and Normalize Data")
def cleanse_data(data_a, data_b, data_c):
    logger = get_run_logger()
    logger.info("Cleaning and normalizing data")
    # Simulate data cleansing and normalization
    cleaned_data = f"Cleaned: {data_a}, {data_b}, {data_c}"
    logger.info(f"Cleaned data: {cleaned_data}")
    return cleaned_data

@task(retries=2, name="Load to Database")
def load_to_db(cleaned_data):
    logger = get_run_logger()
    logger.info("Loading data to database")
    # Simulate loading data to the database
    db_connection = inventory_database.get()
    db_connection.execute(f"INSERT INTO data_table VALUES ('{cleaned_data}')")
    logger.info("Data loaded to database")

@task(retries=2, name="Send Summary Email")
def send_summary_email():
    logger = get_run_logger()
    logger.info("Sending summary email")
    # Simulate sending an email
    email_system.get().send("Summary of Data Ingestion", "Data ingestion completed successfully.")
    logger.info("Summary email sent")

@flow(name="ingest_vendor_a_pipeline", task_runner=ConcurrentTaskRunner(), schedule=CronSchedule(cron="0 0 * * *", timezone="UTC", catchup=False))
def ingest_vendor_a_pipeline():
    logger = get_run_logger()
    logger.info("Starting ingest_vendor_a_pipeline")

    data_a = ingest_vendor_a()
    data_b = ingest_vendor_b()
    data_c = ingest_vendor_c()

    cleaned_data = cleanse_data(data_a, data_b, data_c)

    load_to_db(cleaned_data)

    send_summary_email()

if __name__ == "__main__":
    deployment = Deployment.build_from_flow(
        flow=ingest_vendor_a_pipeline,
        name="ingest_vendor_a_pipeline_deployment",
        work_pool_name="default-agent-pool",
    )
    deployment.apply()
```
This code defines a Prefect 2.x flow named `ingest_vendor_a_pipeline` with the specified tasks and dependencies. The flow is scheduled to run daily and uses a concurrent task runner. It also includes error handling with retries for each task. The required resources are loaded from Prefect blocks, and the flow is deployed to a work pool named `default-agent-pool`.