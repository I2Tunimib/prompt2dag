# Generated by Dagster Code Generator
# Generation Metadata:
# - Job Name: ingest_vendor_a_pipeline
# - Description: No description provided.
# - Executor Type: multiprocess_executor
# - IO Manager: fs_io_manager
# - Dagster Version: 1.5.0
# - Use Assets: False
# - Required Resources: email_system, vendor_a_filesystem, location_reference_tables, inventory_db, vendor_c_filesystem, vendor_b_filesystem

from dagster import (
    job,
    op,
    Out,
    In,
    RetryPolicy,
    multiprocess_executor,
    fs_io_manager,
    resource,
    schedule,
)

# Resources
@resource
def email_system():
    """Resource for the email system."""
    pass

@resource
def vendor_a_filesystem():
    """Resource for Vendor A Data Source."""
    pass

@resource
def vendor_b_filesystem():
    """Resource for Vendor B Data Source."""
    pass

@resource
def vendor_c_filesystem():
    """Resource for Vendor C Data Source."""
    pass

@resource
def location_reference_tables():
    """Resource for Location Reference Tables."""
    pass

@resource
def inventory_db():
    """Resource for Inventory Database."""
    pass

# Ops
@op(
    required_resource_keys={"vendor_a_filesystem"},
    out={"vendor_a_data": Out()},
    retry_policy=RetryPolicy(max_retries=2),
)
def ingest_vendor_a(context):
    """Ingest data from Vendor A."""
    # Simulate data ingestion
    vendor_a_data = "Vendor A Data"
    context.log.info(f"Ingested data from Vendor A: {vendor_a_data}")
    return vendor_a_data

@op(
    required_resource_keys={"vendor_b_filesystem"},
    out={"vendor_b_data": Out()},
    retry_policy=RetryPolicy(max_retries=2),
)
def ingest_vendor_b(context):
    """Ingest data from Vendor B."""
    # Simulate data ingestion
    vendor_b_data = "Vendor B Data"
    context.log.info(f"Ingested data from Vendor B: {vendor_b_data}")
    return vendor_b_data

@op(
    required_resource_keys={"vendor_c_filesystem"},
    out={"vendor_c_data": Out()},
    retry_policy=RetryPolicy(max_retries=2),
)
def ingest_vendor_c(context):
    """Ingest data from Vendor C."""
    # Simulate data ingestion
    vendor_c_data = "Vendor C Data"
    context.log.info(f"Ingested data from Vendor C: {vendor_c_data}")
    return vendor_c_data

@op(
    ins={
        "vendor_a_data": In(),
        "vendor_b_data": In(),
        "vendor_c_data": In(),
    },
    out={"cleaned_data": Out()},
    retry_policy=RetryPolicy(max_retries=2),
)
def cleanse_data(context, vendor_a_data, vendor_b_data, vendor_c_data):
    """Cleanse and normalize data from all vendors."""
    # Simulate data cleansing
    cleaned_data = f"Cleaned Data: {vendor_a_data}, {vendor_b_data}, {vendor_c_data}"
    context.log.info(f"Cleaned and normalized data: {cleaned_data}")
    return cleaned_data

@op(
    required_resource_keys={"inventory_db"},
    ins={"cleaned_data": In()},
    out={"db_load_status": Out()},
    retry_policy=RetryPolicy(max_retries=2),
)
def load_to_db(context, cleaned_data):
    """Load cleaned data to the database."""
    # Simulate data loading
    db_load_status = "Data loaded successfully"
    context.log.info(f"Loaded data to database: {db_load_status}")
    return db_load_status

@op(
    required_resource_keys={"email_system"},
    ins={"db_load_status": In()},
    retry_policy=RetryPolicy(max_retries=2),
)
def send_summary_email(context, db_load_status):
    """Send a summary email after data loading."""
    # Simulate sending an email
    context.log.info(f"Sent summary email: {db_load_status}")

# Job
@job(
    name="ingest_vendor_a_pipeline",
    description="No description provided.",
    executor_def=multiprocess_executor,
    resource_defs={
        "email_system": email_system,
        "vendor_a_filesystem": vendor_a_filesystem,
        "vendor_b_filesystem": vendor_b_filesystem,
        "vendor_c_filesystem": vendor_c_filesystem,
        "location_reference_tables": location_reference_tables,
        "inventory_db": inventory_db,
    },
    config={
        "ops": {
            "ingest_vendor_a": {"config": {}},
            "ingest_vendor_b": {"config": {}},
            "ingest_vendor_c": {"config": {}},
            "cleanse_data": {"config": {}},
            "load_to_db": {"config": {}},
            "send_summary_email": {"config": {}},
        }
    },
)
def ingest_vendor_a_pipeline():
    """Ingest, cleanse, and load data from multiple vendors and send a summary email."""
    vendor_a_data = ingest_vendor_a()
    vendor_b_data = ingest_vendor_b()
    vendor_c_data = ingest_vendor_c()
    cleaned_data = cleanse_data(vendor_a_data, vendor_b_data, vendor_c_data)
    db_load_status = load_to_db(cleaned_data)
    send_summary_email(db_load_status)

# Schedule
@schedule(
    job=ingest_vendor_a_pipeline,
    cron_schedule="@daily",
    execution_timezone="UTC",
    name="ingest_vendor_a_pipeline_schedule",
    should_execute=lambda: True,
    tags={"dagster/skip_reason": "Not running due to catchup being disabled"},
    execution_time_window=None,
    catchup=False,
)
def ingest_vendor_a_pipeline_schedule(_context):
    return {}