# Generated by Prefect Pipeline Generator
# Pipeline: retail_inventory_reconciliation
# Date: 2024-06-28
# Prefect version: 2.14.0

from __future__ import annotations

import io
import logging
from typing import List

import pandas as pd
import requests
from prefect import flow, task, get_run_logger
from prefect.task_runners import ConcurrentTaskRunner
from prefect.blocks.system import Secret

# -------------------------------------------------------------------------
# Secret blocks – replace the block names with the actual names in your
# Prefect workspace.
# -------------------------------------------------------------------------
NORTH_SECRET = Secret.load("north_warehouse_api")
SOUTH_SECRET = Secret.load("south_warehouse_api")
EAST_SECRET = Secret.load("east_warehouse_api")
WEST_SECRET = Secret.load("west_warehouse_api")


def _download_csv(api_secret: Secret) -> pd.DataFrame:
    """
    Helper to download a CSV file from a warehouse API.

    The secret is expected to contain a JSON string with at least the keys
    ``url`` and ``token``.  Adjust the parsing logic if your secret format
    differs.

    Args:
        api_secret: Prefect Secret block containing the API credentials.

    Returns:
        pandas.DataFrame with the raw CSV data.
    """
    logger = get_run_logger()
    try:
        credentials = api_secret.get()
        # Expected format: {"url": "...", "token": "..."}
        if isinstance(credentials, str):
            import json

            credentials = json.loads(credentials)
        url = credentials["url"]
        token = credentials["token"]
        headers = {"Authorization": f"Bearer {token}"}
        logger.info("Downloading CSV from %s", url)
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        return pd.read_csv(io.StringIO(response.text))
    except Exception as exc:
        logger.error("Failed to download CSV: %s", exc)
        raise


def _standardize_sku(df: pd.DataFrame, sku_column: str = "sku") -> pd.DataFrame:
    """
    Normalise SKU values to a consistent format.

    This implementation trims whitespace and forces uppercase.  Extend the
    logic as required for your data quality rules.

    Args:
        df: DataFrame containing a SKU column.
        sku_column: Name of the column that holds SKU values.

    Returns:
        DataFrame with a cleaned ``sku`` column.
    """
    df = df.copy()
    df[sku_column] = (
        df[sku_column].astype(str).str.strip().str.upper()
    )
    return df


# -------------------------------------------------------------------------
# Fetch tasks – each runs in its own process and can be retried.
# -------------------------------------------------------------------------
@task(retries=2, retry_delay_seconds=60)
def fetch_east_warehouse_csv() -> pd.DataFrame:
    """Fetch the raw CSV export from the East warehouse."""
    return _download_csv(EAST_SECRET)


@task(retries=2, retry_delay_seconds=60)
def fetch_north_warehouse_csv() -> pd.DataFrame:
    """Fetch the raw CSV export from the North warehouse."""
    return _download_csv(NORTH_SECRET)


@task(retries=2, retry_delay_seconds=60)
def fetch_south_warehouse_csv() -> pd.DataFrame:
    """Fetch the raw CSV export from the South warehouse."""
    return _download_csv(SOUTH_SECRET)


@task(retries=2, retry_delay_seconds=60)
def fetch_west_warehouse_csv() -> pd.DataFrame:
    """Fetch the raw CSV export from the West warehouse."""
    return _download_csv(WEST_SECRET)


# -------------------------------------------------------------------------
# Normalisation tasks – depend on their respective fetch tasks.
# -------------------------------------------------------------------------
@task(retries=2, retry_delay_seconds=60)
def normalize_east_skus(raw_df: pd.DataFrame) -> pd.DataFrame:
    """Normalise SKU values for the East warehouse data."""
    return _standardize_sku(raw_df)


@task(retries=2, retry_delay_seconds=60)
def normalize_north_skus(raw_df: pd.DataFrame) -> pd.DataFrame:
    """Normalise SKU values for the North warehouse data."""
    return _standardize_sku(raw_df)


@task(retries=2, retry_delay_seconds=60)
def normalize_south_skus(raw_df: pd.DataFrame) -> pd.DataFrame:
    """Normalise SKU values for the South warehouse data."""
    return _standardize_sku(raw_df)


@task(retries=2, retry_delay_seconds=60)
def normalize_west_skus(raw_df: pd.DataFrame) -> pd.DataFrame:
    """Normalise SKU values for the West warehouse data."""
    return _standardize_sku(raw_df)


# -------------------------------------------------------------------------
# Reconciliation task – fan‑in of all normalised data.
# -------------------------------------------------------------------------
@task(retries=2, retry_delay_seconds=60)
def reconcile_all_inventories(
    north_df: pd.DataFrame,
    south_df: pd.DataFrame,
    east_df: pd.DataFrame,
    west_df: pd.DataFrame,
) -> pd.DataFrame:
    """
    Combine inventories from all warehouses.

    The function performs an outer join on the ``sku`` column and aggregates
    the quantity columns (assumed to be named ``quantity``).  Adjust column
    names if your source data differs.
    """
    logger = get_run_logger()
    logger.info("Starting reconciliation of %d warehouses", 4)

    # Ensure each dataframe has the expected columns
    for df, name in zip(
        [north_df, south_df, east_df, west_df],
        ["north", "south", "east", "west"],
    ):
        if "sku" not in df.columns or "quantity" not in df.columns:
            raise ValueError(
                f"Dataframe from {name} missing required columns 'sku' or 'quantity'"
            )

    # Rename quantity columns to be warehouse‑specific before merging
    north = north_df.rename(columns={"quantity": "north_quantity"})
    south = south_df.rename(columns={"quantity": "south_quantity"})
    east = east_df.rename(columns={"quantity": "east_quantity"})
    west = west_df.rename(columns={"quantity": "west_quantity"})

    # Perform successive outer merges on SKU
    merged = pd.merge(north, south, on="sku", how="outer")
    merged = pd.merge(merged, east, on="sku", how="outer")
    merged = pd.merge(merged, west, on="sku", how="outer")

    # Fill missing quantities with zero and compute total
    qty_cols = [
        "north_quantity",
        "south_quantity",
        "east_quantity",
        "west_quantity",
    ]
    merged[qty_cols] = merged[qty_cols].fillna(0)
    merged["total_quantity"] = merged[qty_cols].sum(axis=1)

    logger.info("Reconciliation complete – %d unique SKUs", len(merged))
    return merged


# -------------------------------------------------------------------------
# Reporting task – final fan‑out.
# -------------------------------------------------------------------------
@task(retries=2, retry_delay_seconds=60)
def generate_final_report(reconciled_df: pd.DataFrame) -> str:
    """
    Generate a CSV report from the reconciled inventory.

    Returns:
        Path to the generated report file.
    """
    logger = get_run_logger()
    report_path = "final_inventory_report.csv"
    try:
        reconciled_df.to_csv(report_path, index=False)
        logger.info("Report written to %s", report_path)
        return report_path
    except Exception as exc:
        logger.error("Failed to write report: %s", exc)
        raise


# -------------------------------------------------------------------------
# Main flow definition – fan‑in pattern with a concurrent task runner.
# -------------------------------------------------------------------------
@flow(
    name="retail_inventory_reconciliation",
    task_runner=ConcurrentTaskRunner(),
)
def retail_inventory_reconciliation() -> str:
    """
    Orchestrates the end‑to‑end inventory reconciliation across four warehouses.

    The flow follows a fan‑in pattern:
        * Four fetch tasks run concurrently.
        * Each fetch is followed by a normalisation task.
        * Normalised data are merged in a single reconciliation step.
        * A final report is generated from the merged result.

    Returns:
        Path to the generated CSV report.
    """
    logger = get_run_logger()
    logger.info("Starting retail inventory reconciliation flow")

    # Fetch raw CSVs concurrently
    east_raw = fetch_east_warehouse_csv()
    north_raw = fetch_north_warehouse_csv()
    south_raw = fetch_south_warehouse_csv()
    west_raw = fetch_west_warehouse_csv()

    # Normalise each warehouse's data (fan‑in)
    east_norm = normalize_east_skus(east_raw)
    north_norm = normalize_north_skus(north_raw)
    south_norm = normalize_south_skus(south_raw)
    west_norm = normalize_west_skus(west_raw)

    # Reconcile inventories (depends on all normalised data)
    reconciled = reconcile_all_inventories(
        north_df=north_norm,
        south_df=south_norm,
        east_df=east_norm,
        west_df=west_norm,
    )

    # Generate final report
    report_path = generate_final_report(reconciled)

    logger.info("Flow completed successfully")
    return report_path


if __name__ == "__main__":
    # Running the flow locally (useful for debugging)
    retail_inventory_reconciliation()