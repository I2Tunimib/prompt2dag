# Generated by Dagster Pipeline Generator
# Pipeline: data_quality_gate
# Description: Comprehensive Pipeline Description
# Generation Timestamp: 2024-06-28T12:00:00Z

from typing import Any, Dict, List, Tuple

import pandas as pd
from dagster import (
    ConfigurableResource,
    In,
    Out,
    RetryPolicy,
    ScheduleDefinition,
    ScheduleStatus,
    asset,
    job,
    op,
    resource,
    get_dagster_logger,
    multiprocess_executor,
    in_process_executor,
)


# -------------------------------------------------------------------------
# Resource Definitions
# -------------------------------------------------------------------------

class FileSystemResource(ConfigurableResource):
    """Simple filesystem resource providing a base directory."""

    base_path: str

    def path(self, *subpaths: str) -> str:
        """Construct a full path under the base directory."""
        return "/".join([self.base_path, *subpaths])


class SMTPEmailResource(ConfigurableResource):
    """Placeholder SMTP email resource."""

    smtp_server: str
    smtp_port: int
    username: str
    password: str

    def send_email(self, subject: str, body: str, to: List[str]) -> None:
        logger = get_dagster_logger()
        logger.info(
            f"Sending email via {self.smtp_server}:{self.smtp_port} to {to}: {subject}"
        )
        # In a real implementation, integrate with an SMTP library here.
        # For this example we simply log the action.


class ProductionDBResource(ConfigurableResource):
    """Placeholder production database resource."""

    connection_string: str

    def load_data(self, table_name: str, data: pd.DataFrame) -> None:
        logger = get_dagster_logger()
        logger.info(f"Loading {len(data)} rows into {table_name} on {self.connection_string}")
        # Real implementation would execute INSERT statements or use an ORM.


# -------------------------------------------------------------------------
# Op Definitions
# -------------------------------------------------------------------------

@op(
    name="Ingest Customer CSV",
    description="Read raw customer CSV data from the raw CSV filesystem.",
    out=Out(pd.DataFrame),
    retry_policy=RetryPolicy(max_retries=1),
    required_resource_keys={"raw_csv_fs"},
    tags={"executor": "in_process_executor"},
)
def ingest_csv(context) -> pd.DataFrame:
    """Load CSV data from the raw CSV filesystem."""
    raw_path = context.resources.raw_csv_fs.path("customers.csv")
    logger = context.log
    logger.info(f"Reading raw CSV from {raw_path}")
    # In a real job, you'd read from disk; here we mock with an empty DataFrame.
    df = pd.read_csv(raw_path) if False else pd.DataFrame(
        {"customer_id": [], "name": [], "email": []}
    )
    return df


@op(
    name="Assess Data Quality",
    description="Separate high‑quality and low‑quality rows based on simple validation rules.",
    ins={"raw_data": In(pd.DataFrame)},
    out=Out(Dict[str, pd.DataFrame]),
    retry_policy=RetryPolicy(max_retries=1),
    tags={"executor": "in_process_executor"},
)
def quality_check(context, raw_data: pd.DataFrame) -> Dict[str, pd.DataFrame]:
    """Perform data quality checks and split the dataset."""
    logger = context.log
    logger.info(f"Running quality checks on {len(raw_data)} rows")

    # Placeholder validation: rows with non‑empty email are high quality.
    high_quality = raw_data[raw_data["email"].notna()]
    low_quality = raw_data[raw_data["email"].isna()]

    logger.info(
        f"Quality check results – high quality: {len(high_quality)}, low quality: {len(low_quality)}"
    )
    return {"high_quality": high_quality, "low_quality": low_quality}


@op(
    name="Quarantine Low‑Quality Data",
    description="Write low‑quality rows to quarantine storage.",
    ins={"low_quality": In(pd.DataFrame)},
    out=Out(str),
    retry_policy=RetryPolicy(max_retries=1),
    required_resource_keys={"quarantine_fs"},
    tags={"executor": "in_process_executor"},
)
def quarantine_and_alert(context, low_quality: pd.DataFrame) -> str:
    """Persist low‑quality data to a quarantine location."""
    logger = context.log
    if low_quality.empty:
        logger.info("No low‑quality data to quarantine.")
        return ""

    quarantine_path = context.resources.quarantine_fs.path("low_quality_quarantine.csv")
    logger.info(f"Writing low‑quality data to {quarantine_path}")
    # In a real implementation, write to CSV:
    # low_quality.to_csv(quarantine_path, index=False)
    return quarantine_path


@op(
    name="Send Quality Alert Email",
    description="Notify stakeholders about quarantined data via email.",
    ins={"quarantine_path": In(str)},
    out=Out(None),
    retry_policy=RetryPolicy(max_retries=1),
    required_resource_keys={"smtp_email"},
    tags={"executor": "in_process_executor"},
)
def send_alert_email(context, quarantine_path: str) -> None:
    """Send an email alert if low‑quality data was quarantined."""
    logger = context.log
    if not quarantine_path:
        logger.info("No quarantine file generated; skipping email alert.")
        return

    subject = "Data Quality Alert: Low‑Quality Records Quarantined"
    body = f"The following file contains low‑quality records and has been quarantined:\n{quarantine_path}"
    recipients = ["data-team@example.com"]
    context.resources.smtp_email.send_email(subject=subject, body=body, to=recipients)
    logger.info("Alert email sent.")


@op(
    name="Load High‑Quality Data to Production",
    description="Insert high‑quality records into the production database.",
    ins={"high_quality": In(pd.DataFrame)},
    out=Out(None),
    retry_policy=RetryPolicy(max_retries=1),
    required_resource_keys={"prod_db"},
    tags={"executor": "in_process_executor"},
)
def production_load(context, high_quality: pd.DataFrame) -> None:
    """Load validated data into the production database."""
    logger = context.log
    if high_quality.empty:
        logger.info("No high‑quality data to load.")
        return

    table_name = "customer"
    context.resources.prod_db.load_data(table_name=table_name, data=high_quality)
    logger.info(f"Loaded {len(high_quality)} rows into production table {table_name}.")


@op(
    name="Final Cleanup",
    description="Perform any necessary cleanup after processing.",
    ins={"load_result": In(None), "email_result": In(None)},
    out=Out(None),
    retry_policy=RetryPolicy(max_retries=1),
    tags={"executor": "in_process_executor"},
)
def cleanup(context, load_result: Any, email_result: Any) -> None:
    """Placeholder cleanup operation."""
    logger = context.log
    logger.info("Running final cleanup steps.")
    # Insert any required cleanup logic here (e.g., delete temp files).


# -------------------------------------------------------------------------
# Job Definition
# -------------------------------------------------------------------------

@job(
    name="data_quality_gate",
    description="Comprehensive Pipeline Description",
    executor_def=multiprocess_executor,
    resource_defs={
        "raw_csv_fs": FileSystemResource(base_path="/data/raw"),
        "quarantine_fs": FileSystemResource(base_path="/data/quarantine"),
        "smtp_email": SMTPEmailResource(
            smtp_server="smtp.example.com",
            smtp_port=587,
            username="alert@example.com",
            password="supersecret",
        ),
        "prod_db": ProductionDBResource(connection_string="postgresql://user:pass@prod-db:5432/app"),
    },
)
def data_quality_gate():
    """Dagster job orchestrating CSV ingestion, quality assessment, quarantine, alerting, and loading."""
    raw_df = ingest_csv()
    quality_dict = quality_check(raw_df)

    # Branching based on quality check output
    high_quality = quality_dict["high_quality"]
    low_quality = quality_dict["low_quality"]

    quarantine_path = quarantine_and_alert(low_quality)
    send_alert_email(quarantine_path)

    production_load(high_quality)

    # Cleanup depends on both production_load and send_alert_email
    cleanup(
        load_result=production_load(high_quality),
        email_result=send_alert_email(quarantine_path),
    )


# -------------------------------------------------------------------------
# Schedule Definition
# -------------------------------------------------------------------------

daily_schedule = ScheduleDefinition(
    job=data_quality_gate,
    cron_schedule="@daily",
    execution_timezone="UTC",
    default_status=ScheduleStatus.RUNNING,
    description="Daily execution of the data_quality_gate pipeline.",
    catchup=False,
)

# Export symbols for Dagster discovery
__all__ = [
    "data_quality_gate",
    "daily_schedule",
    "FileSystemResource",
    "SMTPEmailResource",
    "ProductionDBResource",
]