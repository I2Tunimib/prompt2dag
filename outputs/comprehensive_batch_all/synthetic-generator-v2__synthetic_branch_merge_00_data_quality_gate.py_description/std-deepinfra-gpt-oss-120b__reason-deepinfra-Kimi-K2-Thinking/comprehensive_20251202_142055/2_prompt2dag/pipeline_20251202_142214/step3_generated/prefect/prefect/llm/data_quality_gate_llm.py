# Generated by Prefect Pipeline Generator
# Pipeline: data_quality_gate
# Description: Comprehensive Pipeline Description
# Generated on: 2024-06-13

from __future__ import annotations

import os
import datetime
import pandas as pd
from typing import Any

from prefect import flow, task, get_run_logger
from prefect.task_runners import ConcurrentTaskRunner
from prefect.deployments import Deployment
from prefect.server.schemas.schedules import CronSchedule

from prefect.filesystems import LocalFileSystem
from prefect.blocks.core import Secret


# -------------------------------------------------------------------------
# Resource Blocks
# -------------------------------------------------------------------------
RAW_CSV_FS = LocalFileSystem.load("raw_csv_fs")
PROD_DB = Secret.load("prod_db")
QUARANTINE_FS = LocalFileSystem.load("quarantine_fs")
SMTP_EMAIL = Secret.load("smtp_email")


# -------------------------------------------------------------------------
# Tasks
# -------------------------------------------------------------------------
@task(retries=1, name="Ingest Customer CSV")
def ingest_csv(file_path: str = "customers.csv") -> pd.DataFrame:
    """
    Load the raw customer CSV from the configured file system.

    Args:
        file_path: Relative path to the CSV file within the raw CSV file system.

    Returns:
        pandas.DataFrame containing the raw data.
    """
    logger = get_run_logger()
    logger.info("Reading CSV from %s", file_path)

    # Resolve the absolute path using the block's base path
    full_path = os.path.join(RAW_CSV_FS.basepath, file_path)

    if not os.path.exists(full_path):
        raise FileNotFoundError(f"CSV file not found at {full_path}")

    df = pd.read_csv(full_path)
    logger.info("Loaded %d rows", len(df))
    return df


@task(retries=1, name="Assess Data Quality")
def quality_check(df: pd.DataFrame) -> bool:
    """
    Perform a simple data quality assessment.

    The check currently verifies that there are no missing values in the
    ``email`` column and that the ``age`` column contains only positive integers.

    Args:
        df: DataFrame to assess.

    Returns:
        True if data passes quality checks, False otherwise.
    """
    logger = get_run_logger()
    logger.info("Running data quality checks")

    # Example checks
    missing_email = df["email"].isna().any()
    invalid_age = (df["age"] <= 0).any()

    if missing_email or invalid_age:
        logger.warning(
            "Data quality issues detected: missing_email=%s, invalid_age=%s",
            missing_email,
            invalid_age,
        )
        return False

    logger.info("Data quality passed")
    return True


@task(retries=1, name="Load High‑Quality Data to Production")
def production_load(df: pd.DataFrame) -> None:
    """
    Load high‑quality data into the production database.

    This implementation is a placeholder; replace with actual DB logic.

    Args:
        df: DataFrame containing high‑quality records.
    """
    logger = get_run_logger()
    logger.info("Loading %d high‑quality rows into production", len(df))

    # Retrieve DB credentials from the secret block
    db_credentials = PROD_DB.get()
    # Placeholder for DB insertion logic
    logger.debug("Using DB credentials: %s", db_credentials)

    # Simulate load
    logger.info("Data successfully loaded into production")


@task(retries=1, name="Quarantine Low‑Quality Data")
def quarantine_and_alert(df: pd.DataFrame) -> pd.DataFrame:
    """
    Store low‑quality data in a quarantine location and return the quarantined
    DataFrame for downstream alerting.

    Args:
        df: Original DataFrame (contains low‑quality rows).

    Returns:
        DataFrame of the quarantined rows.
    """
    logger = get_run_logger()
    timestamp = datetime.datetime.utcnow().strftime("%Y%m%dT%H%M%SZ")
    quarantine_path = f"quarantine/customers_{timestamp}.csv"
    full_path = os.path.join(QUARANTINE_FS.basepath, quarantine_path)

    logger.info("Writing low‑quality data to %s", full_path)
    os.makedirs(os.path.dirname(full_path), exist_ok=True)
    df.to_csv(full_path, index=False)

    logger.info("Quarantine complete")
    return df


@task(retries=1, name="Send Quality Alert Email")
def send_alert_email(quarantined_df: pd.DataFrame) -> None:
    """
    Send an alert email summarizing the quarantine event.

    Args:
        quarantined_df: DataFrame of the quarantined records.
    """
    logger = get_run_logger()
    logger.info("Preparing alert email for %d quarantined rows", len(quarantined_df))

    # Retrieve SMTP credentials from the secret block
    smtp_config = SMTP_EMAIL.get()
    logger.debug("SMTP configuration retrieved")

    # Placeholder email content
    subject = "Data Quality Alert: Quarantined Records"
    body = (
        f"The latest data ingestion produced {len(quarantined_df)} low‑quality rows "
        "which have been moved to quarantine."
    )

    # Simulate sending email
    logger.info("Sending email with subject: %s", subject)
    logger.debug("Email body: %s", body)
    logger.info("Alert email sent successfully")


@task(retries=1, name="Final Cleanup")
def cleanup() -> None:
    """
    Perform any necessary cleanup after the pipeline run.
    """
    logger = get_run_logger()
    logger.info("Running final cleanup tasks")
    # Placeholder for cleanup logic (e.g., delete temp files)
    logger.info("Cleanup completed")


# -------------------------------------------------------------------------
# Flow Definition
# -------------------------------------------------------------------------
@flow(
    name="data_quality_gate",
    task_runner=ConcurrentTaskRunner(),
    description="Comprehensive Pipeline Description",
)
def data_quality_gate_flow() -> None:
    """
    Orchestrates the data quality gate pipeline.

    Execution order:
        ingest_csv → quality_check → (production_load | quarantine_and_alert → send_alert_email)
        → cleanup
    """
    logger = get_run_logger()
    logger.info("Starting data_quality_gate flow")

    # Step 1: Ingest raw CSV
    raw_df = ingest_csv()

    # Step 2: Assess quality
    is_high_quality = quality_check(raw_df)

    # Branching based on quality result
    if is_high_quality:
        # High‑quality path
        production_load(raw_df)
    else:
        # Low‑quality path
        quarantined_df = quarantine_and_alert(raw_df)
        send_alert_email(quarantined_df)

    # Final cleanup (runs after whichever branch completed)
    cleanup()
    logger.info("data_quality_gate flow completed")


# -------------------------------------------------------------------------
# Deployment Configuration
# -------------------------------------------------------------------------
def build_deployment() -> Deployment:
    """
    Build a Prefect deployment for the data_quality_gate flow.

    The deployment is scheduled to run daily at midnight UTC without catch‑up.
    """
    schedule = CronSchedule(cron="0 0 * * *", timezone="UTC", day_or=False)  # @daily
    deployment = Deployment.build_from_flow(
        flow=data_quality_gate_flow,
        name="data_quality_gate_deployment",
        schedule=schedule,
        work_pool_name="default-agent-pool",
        parameters={},  # No external parameters required
        tags=["data-quality", "daily"],
        description="Daily data quality gate pipeline",
    )
    return deployment


if __name__ == "__main__":
    # Register the deployment when this script is executed directly.
    # This is safe for production environments where the script is invoked
    # by CI/CD pipelines or deployment tooling.
    deployment = build_deployment()
    deployment.apply()
    # Optionally, trigger a run for immediate testing:
    # data_quality_gate_flow()