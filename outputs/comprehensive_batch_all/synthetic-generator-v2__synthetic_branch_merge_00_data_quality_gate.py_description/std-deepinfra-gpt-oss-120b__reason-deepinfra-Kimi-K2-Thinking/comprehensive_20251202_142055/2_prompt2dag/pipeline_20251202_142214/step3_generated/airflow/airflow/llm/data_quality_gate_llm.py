# Generated by Airflow DAG generator on 2024-06-13
"""
Airflow DAG: data_quality_gate
Description: Comprehensive Pipeline Description
Pattern: fanout_fanin
"""

from datetime import timedelta

from airflow import DAG
from airflow.decorators import task
from airflow.exceptions import AirflowFailException
from airflow.models import Variable
from airflow.providers.http.hooks.http import HttpHook
from airflow.providers.common.sql.hooks.sql import DbApiHook
from airflow.providers.standard.operators.python import PythonOperator, BranchPythonOperator
from airflow.utils.dates import days_ago
from airflow.utils.trigger_rule import TriggerRule
import logging
import json
import fsspec


DEFAULT_ARGS = {
    "owner": "airflow",
    "depends_on_past": False,
    "retries": 1,
    "retry_delay": timedelta(minutes=5),
    "email_on_failure": False,
    "email_on_retry": False,
}


def get_connection(conn_id):
    """Utility to fetch a connection using Airflow's BaseHook."""
    from airflow.hooks.base import BaseHook

    try:
        return BaseHook.get_connection(conn_id)
    except Exception as exc:
        logging.error("Failed to retrieve connection %s: %s", conn_id, exc)
        raise AirflowFailException(f"Connection {conn_id} not found") from exc


with DAG(
    dag_id="data_quality_gate",
    description="Comprehensive Pipeline Description",
    schedule_interval="@daily",
    start_date=days_ago(1),
    catchup=False,
    default_args=DEFAULT_ARGS,
    tags=["fanout_fanin", "data_quality"],
    max_active_runs=1,
) as dag:

    @task(task_id="ingest_csv", retries=1)
    def ingest_csv():
        """
        Ingest Customer CSV from the raw CSV file system.
        Stores the raw data in XCom for downstream tasks.
        """
        try:
            conn = get_connection("raw_csv_fs")
            fs = fsspec.filesystem(conn.conn_type, **json.loads(conn.extra or "{}"))
            csv_path = conn.host  # assuming host contains the path/prefix
            logging.info("Reading CSV from %s", csv_path)
            with fs.open(csv_path, mode="r") as f:
                data = f.read()
            # Push raw data to XCom
            return data
        except Exception as exc:
            logging.exception("Failed to ingest CSV")
            raise AirflowFailException("Ingest CSV failed") from exc

    @task(task_id="quality_check", retries=1)
    def quality_check(raw_data: str):
        """
        Assess Data Quality.
        Returns a boolean flag indicating high quality.
        """
        try:
            # Placeholder quality logic: ensure file is not empty and contains header
            if not raw_data:
                raise ValueError("CSV data is empty")
            lines = raw_data.splitlines()
            if len(lines) < 2:
                raise ValueError("CSV does not contain data rows")
            # Simple heuristic: if more than 90% rows have same number of columns as header
            header_cols = len(lines[0].split(","))
            valid_rows = sum(1 for line in lines[1:] if len(line.split(",")) == header_cols)
            quality_score = valid_rows / (len(lines) - 1)
            is_high_quality = quality_score >= 0.9
            logging.info("Quality score: %.2f, high quality: %s", quality_score, is_high_quality)
            return is_high_quality
        except Exception as exc:
            logging.exception("Quality check failed")
            raise AirflowFailException("Quality check failed") from exc

    def branch_quality(**context):
        """
        Branch based on quality check result.
        Returns the task_id of the next path.
        """
        ti = context["ti"]
        is_high_quality = ti.xcom_pull(task_ids="quality_check")
        if is_high_quality:
            return "production_load"
        return "quarantine_and_alert"

    quality_check_branch = BranchPythonOperator(
        task_id="quality_check_branch",
        python_callable=branch_quality,
        provide_context=True,
        retries=1,
    )

    @task(task_id="production_load", retries=1)
    def production_load(raw_data: str):
        """
        Load High‑Quality Data to Production Database.
        """
        try:
            conn = get_connection("prod_db")
            db_hook = DbApiHook(conn_id="prod_db")
            # Example: insert raw CSV into a staging table
            logging.info("Loading data into production database")
            # Placeholder: actual implementation depends on DB schema
            # db_hook.run("INSERT INTO target_table ...", parameters={...})
            return "load_success"
        except Exception as exc:
            logging.exception("Production load failed")
            raise AirflowFailException("Production load failed") from exc

    @task(task_id="quarantine_and_alert", retries=1)
    def quarantine_and_alert(raw_data: str):
        """
        Quarantine Low‑Quality Data to quarantine storage.
        """
        try:
            conn = get_connection("quarantine_fs")
            fs = fsspec.filesystem(conn.conn_type, **json.loads(conn.extra or "{}"))
            quarantine_path = conn.host  # assuming host contains the target path
            logging.info("Writing low-quality data to %s", quarantine_path)
            with fs.open(quarantine_path, mode="w") as f:
                f.write(raw_data)
            return "quarantine_success"
        except Exception as exc:
            logging.exception("Quarantine step failed")
            raise AirflowFailException("Quarantine failed") from exc

    @task(task_id="send_alert_email", retries=1)
    def send_alert_email():
        """
        Send Quality Alert Email using SMTP service.
        """
        try:
            conn = get_connection("smtp_email")
            http_hook = HttpHook(http_conn_id="smtp_email", method="POST")
            # Placeholder payload; actual email service may differ
            payload = {
                "to": Variable.get("alert_email_recipients", default_var="data-team@example.com"),
                "subject": "Data Quality Alert",
                "body": "Low-quality data has been quarantined.",
            }
            logging.info("Sending alert email")
            response = http_hook.run(endpoint="/send", json=payload)
            if response.status_code != 200:
                raise ValueError(f"Email service returned {response.status_code}")
            return "email_sent"
        except Exception as exc:
            logging.exception("Failed to send alert email")
            raise AirflowFailException("Alert email failed") from exc

    @task(task_id="cleanup", retries=1)
    def cleanup():
        """
        Final Cleanup task.
        """
        try:
            logging.info("Performing final cleanup actions")
            # Placeholder for any cleanup logic, e.g., removing temp files
            return "cleanup_done"
        except Exception as exc:
            logging.exception("Cleanup failed")
            raise AirflowFailException("Cleanup failed") from exc

    # Define task dependencies
    raw_data = ingest_csv()
    quality_flag = quality_check(raw_data)
    quality_check_branch.set_upstream(quality_flag)

    prod_load = production_load(raw_data)
    quarantine = quarantine_and_alert(raw_data)

    prod_load.set_upstream(quality_check_branch)
    quarantine.set_upstream(quality_check_branch)

    email = send_alert_email()
    email.set_upstream(quarantine)

    cleanup_task = cleanup()
    cleanup_task.set_upstream([prod_load, email])
    cleanup_task.trigger_rule = TriggerRule.ALL_DONE  # ensure it runs even if upstream fails

    # Expose tasks for Airflow UI
    (
        raw_data
        >> quality_flag
        >> quality_check_branch
        >> [prod_load, quarantine]
    )
    quarantine >> email >> cleanup_task
    prod_load >> cleanup_task