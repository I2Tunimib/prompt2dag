# Generated by Dagster code generator on 2024-06-28
# Pipeline: data_quality_gate
# Description: Implements a data quality gate for customer CSV data that ingests raw data,
# assesses quality, and conditionally routes to production or quarantine based on quality scores.

from __future__ import annotations

import os
import csv
import smtplib
from dataclasses import dataclass
from typing import List, Dict, Any

from dagster import (
    op,
    job,
    RetryPolicy,
    In,
    Out,
    Nothing,
    ResourceDefinition,
    ConfigurableResource,
    InitResourceContext,
    get_dagster_logger,
    multiprocess_executor,
    ScheduleDefinition,
    DefaultScheduleStatus,
)


# -------------------------------------------------------------------------
# Resource definitions
# -------------------------------------------------------------------------

class FSIOManager(ConfigurableResource):
    """Simple filesystem I/O manager for reading and writing CSV files."""

    base_path: str

    def _full_path(self, relative_path: str) -> str:
        return os.path.join(self.base_path, relative_path)

    def read_csv(self, relative_path: str) -> List[Dict[str, Any]]:
        """Read a CSV file and return a list of rows as dictionaries."""
        full_path = self._full_path(relative_path)
        logger = get_dagster_logger()
        logger.info(f"Reading CSV from {full_path}")

        with open(full_path, mode="r", newline="", encoding="utf-8") as f:
            reader = csv.DictReader(f)
            return [row for row in reader]

    def write_csv(self, relative_path: str, rows: List[Dict[str, Any]]) -> None:
        """Write a list of dictionaries to a CSV file."""
        if not rows:
            return
        full_path = self._full_path(relative_path)
        logger = get_dagster_logger()
        logger.info(f"Writing CSV to {full_path}")

        os.makedirs(os.path.dirname(full_path), exist_ok=True)
        with open(full_path, mode="w", newline="", encoding="utf-8") as f:
            writer = csv.DictWriter(f, fieldnames=rows[0].keys())
            writer.writeheader()
            writer.writerows(rows)


class ProductionDB(ConfigurableResource):
    """Placeholder resource for a production database."""

    connection_string: str

    def load_data(self, table_name: str, rows: List[Dict[str, Any]]) -> None:
        """Pretend to load data into a production table."""
        logger = get_dagster_logger()
        logger.info(f"Loading {len(rows)} rows into production table '{table_name}'")
        # In a real implementation, this would execute INSERT statements.


class SMTPEmail(ConfigurableResource):
    """Simple SMTP email sender."""

    smtp_host: str
    smtp_port: int
    username: str
    password: str
    from_addr: str

    def send_email(self, to_addr: str, subject: str, body: str) -> None:
        """Send an email via SMTP."""
        logger = get_dagster_logger()
        logger.info(f"Sending email to {to_addr} with subject '{subject}'")
        message = f"From: {self.from_addr}\nTo: {to_addr}\nSubject: {subject}\n\n{body}"
        with smtplib.SMTP(self.smtp_host, self.smtp_port) as server:
            server.starttls()
            server.login(self.username, self.password)
            server.sendmail(self.from_addr, [to_addr], message)


class TempFilesCleanup(ConfigurableResource):
    """Resource responsible for cleaning up temporary files."""

    temp_dir: str

    def cleanup(self) -> None:
        """Delete all files in the temporary directory."""
        logger = get_dagster_logger()
        logger.info(f"Cleaning up temporary directory: {self.temp_dir}")
        if not os.path.isdir(self.temp_dir):
            logger.warning("Temp directory does not exist; nothing to clean.")
            return
        for root, dirs, files in os.walk(self.temp_dir, topdown=False):
            for name in files:
                os.remove(os.path.join(root, name))
            for name in dirs:
                os.rmdir(os.path.join(root, name))


# -------------------------------------------------------------------------
# Op definitions
# -------------------------------------------------------------------------

@op(
    name="Ingest Customer CSV",
    description="Read raw customer CSV data from the filesystem.",
    out=Out(List[Dict[str, Any]]),
    retry_policy=RetryPolicy(max_retries=1),
    required_resource_keys={"raw_csv_filesystem"},
)
def ingest_csv(context: InitResourceContext) -> List[Dict[str, Any]]:
    """Ingest raw CSV data."""
    raw_fs: FSIOManager = context.resources.raw_csv_filesystem
    # Assuming a fixed filename; in practice this could be parameterized.
    data = raw_fs.read_csv("customers_raw.csv")
    context.log.info(f"Ingested {len(data)} rows.")
    return data


@op(
    name="Assess Data Quality",
    description="Compute a quality score for the ingested data.",
    ins={"data": In(List[Dict[str, Any]])},
    out=Out(Dict[str, Any]),
    retry_policy=RetryPolicy(max_retries=1),
    required_resource_keys=set(),
)
def quality_check(context: InitResourceContext, data: List[Dict[str, Any]]) -> Dict[str, Any]:
    """Assess data quality and return a dict with the original data and a score."""
    # Simple heuristic: percentage of rows with non‑empty 'email' field.
    total = len(data)
    if total == 0:
        score = 0.0
    else:
        valid = sum(1 for row in data if row.get("email"))
        score = valid / total

    context.log.info(f"Quality score computed: {score:.2%}")
    return {"data": data, "score": score}


@op(
    name="Load High-Quality Data to Production",
    description="Load data with sufficient quality into the production database.",
    ins={"qc": In(Dict[str, Any])},
    out=Out(Nothing),
    retry_policy=RetryPolicy(max_retries=1),
    required_resource_keys={"production_db"},
)
def production_load(context: InitResourceContext, qc: Dict[str, Any]) -> Nothing:
    """Load high‑quality data into production."""
    score = qc["score"]
    data = qc["data"]
    if score >= 0.95:
        prod_db: ProductionDB = context.resources.production_db
        prod_db.load_data(table_name="customers", rows=data)
        context.log.info("Data loaded into production.")
    else:
        context.log.info(
            f"Data quality ({score:.2%}) below threshold; skipping production load."
        )
    return Nothing


@op(
    name="Quarantine Low-Quality Data",
    description="Write low‑quality data to quarantine storage for later inspection.",
    ins={"qc": In(Dict[str, Any])},
    out=Out(str),  # Returns the path of the quarantined file
    retry_policy=RetryPolicy(max_retries=1),
    required_resource_keys={"quarantine_storage"},
)
def quarantine_and_alert(context: InitResourceContext, qc: Dict[str, Any]) -> str:
    """Quarantine data that fails quality checks."""
    score = qc["score"]
    data = qc["data"]
    if score >= 0.95:
        context.log.info("Data meets quality threshold; no quarantine needed.")
        return ""
    quarantine_fs: FSIOManager = context.resources.quarantine_storage
    quarantine_path = f"quarantine/customers_quarantined_{context.run_id}.csv"
    quarantine_fs.write_csv(quarantine_path, data)
    context.log.info(f"Quarantined low‑quality data to {quarantine_path}")
    return quarantine_path


@op(
    name="Send Quality Alert Email",
    description="Notify stakeholders about low‑quality data via email.",
    ins={"quarantine_path": In(str)},
    out=Out(Nothing),
    retry_policy=RetryPolicy(max_retries=1),
    required_resource_keys={"smtp_email"},
)
def send_alert_email(context: InitResourceContext, quarantine_path: str) -> Nothing:
    """Send an alert email if data was quarantined."""
    if not quarantine_path:
        context.log.info("No quarantine file; skipping alert email.")
        return Nothing

    email: SMTPEmail = context.resources.smtp_email
    subject = "Data Quality Alert: Low‑Quality Customer CSV Detected"
    body = (
        f"The recent customer CSV ingestion failed quality checks.\n"
        f"The problematic file has been stored at: {quarantine_path}\n"
        f"Please review and remediate."
    )
    # In a real scenario, the recipient list would be configurable.
    email.send_email(to_addr="data-team@example.com", subject=subject, body=body)
    context.log.info("Alert email sent.")
    return Nothing


@op(
    name="Cleanup Temporary Resources",
    description="Remove temporary files generated during the pipeline run.",
    ins={"_": In(Nothing), "___": In(Nothing)},
    out=Out(Nothing),
    retry_policy=RetryPolicy(max_retries=1),
    required_resource_keys={"temp_files_cleanup"},
)
def cleanup(context: InitResourceContext, _: Nothing, ___: Nothing) -> Nothing:
    """Cleanup temporary files after processing."""
    cleaner: TempFilesCleanup = context.resources.temp_files_cleanup
    cleaner.cleanup()
    context.log.info("Temporary resources cleaned up.")
    return Nothing


# -------------------------------------------------------------------------
# Job definition
# -------------------------------------------------------------------------

@job(
    name="data_quality_gate",
    description=(
        "Implements a data quality gate for customer CSV data that ingests raw data, "
        "performs quality assessment, and conditionally routes to production or quarantine "
        "based on quality scores."
    ),
    executor_def=multiprocess_executor,
    resource_defs={
        "raw_csv_filesystem": FSIOManager(base_path="/data/raw"),
        "production_db": ProductionDB(connection_string="postgresql://user:pass@prod-db:5432/app"),
        "quarantine_storage": FSIOManager(base_path="/data/quarantine"),
        "smtp_email": SMTPEmail(
            smtp_host="smtp.example.com",
            smtp_port=587,
            username="alert@example.com",
            password="supersecret",
            from_addr="alert@example.com",
        ),
        "temp_files_cleanup": TempFilesCleanup(temp_dir="/tmp/data_quality_gate"),
    },
)
def data_quality_gate():
    """Orchestrates the data quality gate pipeline."""
    raw_data = ingest_csv()
    qc_result = quality_check(raw_data)

    # Fan‑out: production load and quarantine path
    prod = production_load(qc_result)
    quarantine_path = quarantine_and_alert(qc_result)

    # Alert email depends on quarantine result
    alert = send_alert_email(quarantine_path)

    # Cleanup runs after both production load and alert email
    cleanup(prod, alert)


# -------------------------------------------------------------------------
# Schedule (disabled)
# -------------------------------------------------------------------------

daily_schedule = ScheduleDefinition(
    job=data_quality_gate,
    cron_schedule="@daily",
    execution_timezone="UTC",
    default_status=DefaultScheduleStatus.INACTIVE,  # Disabled
    description="Daily execution of the data_quality_gate job.",
)