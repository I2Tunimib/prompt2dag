# -*- coding: utf-8 -*-
"""
Generated Airflow DAG for data_quality_gate
Author: Auto-generated by OpenAI ChatGPT
Date: 2024-06-28
"""

from __future__ import annotations

import os
import smtplib
import logging
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, Any

import pandas as pd
from airflow import DAG
from airflow.models import Variable, Connection
from airflow.utils.dates import days_ago
from airflow.utils.email import send_email
from airflow.exceptions import AirflowSkipException
from airflow.decorators import task, dag
from airflow.providers.common.sql.hooks.sql import DbApiHook
from airflow.hooks.base import BaseHook

# ----------------------------------------------------------------------
# Default arguments and DAG definition
# ----------------------------------------------------------------------
default_args = {
    "owner": "airflow",
    "depends_on_past": False,
    "retries": 1,
    "retry_delay": timedelta(minutes=5),
    "email_on_failure": False,
    "email_on_retry": False,
}

# The DAG is created in a paused state (disabled) as per specification.
@dag(
    dag_id="data_quality_gate",
    description="Implements a data quality gate for customer CSV data that ingests raw data, "
                "performs quality assessment, and conditionally routes to production or quarantine "
                "based on quality scores.",
    schedule_interval="@daily",
    start_date=days_ago(1),
    catchup=False,
    default_args=default_args,
    tags=["data_quality", "fanout_fanin"],
    is_paused_upon_creation=True,
    max_active_runs=1,
    render_template_as_native_obj=True,
)
def data_quality_gate():
    """
    DAG definition using TaskFlow API.
    """

    # ------------------------------------------------------------------
    # Helper functions
    # ------------------------------------------------------------------
    def get_connection_uri(conn_id: str) -> str:
        """Retrieve a connection URI from Airflow metadata DB."""
        conn: Connection = BaseHook.get_connection(conn_id)
        return conn.get_uri()

    # ------------------------------------------------------------------
    # Tasks
    # ------------------------------------------------------------------

    @task(retries=1, retry_delay=timedelta(minutes=5))
    def ingest_csv() -> str:
        """
        Ingest raw customer CSV from the filesystem connection ``raw_csv_filesystem``.
        Returns the local temporary path of the downloaded file.
        """
        logging.info("Starting CSV ingestion.")
        try:
            # Retrieve the filesystem base path from Airflow Variable or Connection
            conn_uri = get_connection_uri("raw_csv_filesystem")
            # Assuming the connection URI is of the form `fs://<base_path>`
            base_path = conn_uri.replace("fs://", "")
            raw_file = Path(base_path) / "customer_data.csv"

            if not raw_file.is_file():
                raise FileNotFoundError(f"Raw CSV not found at {raw_file}")

            # Copy to a temporary working directory
            temp_dir = Path(Variable.get("temp_files_cleanup", default_var="/tmp/airflow"))
            temp_dir.mkdir(parents=True, exist_ok=True)
            temp_file = temp_dir / f"customer_data_{datetime.utcnow().strftime('%Y%m%d%H%M%S')}.csv"
            temp_file.write_bytes(raw_file.read_bytes())

            logging.info("CSV ingested to temporary location: %s", temp_file)
            return str(temp_file)
        except Exception as e:
            logging.exception("Failed to ingest CSV: %s", e)
            raise

    @task(retries=1, retry_delay=timedelta(minutes=5))
    def quality_check(csv_path: str) -> Dict[str, Any]:
        """
        Assess data quality of the ingested CSV.
        Returns a dictionary with quality metrics and the original CSV path.
        """
        logging.info("Starting data quality assessment for %s", csv_path)
        try:
            df = pd.read_csv(csv_path)

            # Simple quality metrics (example)
            total_rows = len(df)
            missing_ratio = df.isnull().mean().mean()  # average missing per column
            duplicate_ratio = df.duplicated().mean()

            # Compute a composite quality score (0-100)
            score = max(
                0,
                100 - (missing_ratio * 50 + duplicate_ratio * 50)
            )

            metrics = {
                "total_rows": total_rows,
                "missing_ratio": missing_ratio,
                "duplicate_ratio": duplicate_ratio,
                "quality_score": round(score, 2),
                "csv_path": csv_path,
            }

            logging.info("Quality metrics: %s", metrics)
            return metrics
        except Exception as e:
            logging.exception("Quality check failed: %s", e)
            raise

    @task(retries=1, retry_delay=timedelta(minutes=5))
    def production_load(quality_info: Dict[str, Any]) -> None:
        """
        Load high-quality data into the production database.
        Skips execution if quality_score is below the threshold.
        """
        threshold = float(Variable.get("quality_threshold", default_var=80))
        score = quality_info["quality_score"]
        csv_path = quality_info["csv_path"]

        if score < threshold:
            logging.info(
                "Quality score %.2f is below threshold %.2f. Skipping production load.",
                score,
                threshold,
            )
            raise AirflowSkipException("Data quality insufficient for production load.")

        logging.info(
            "Quality score %.2f meets threshold %.2f. Loading data to production.",
            score,
            threshold,
        )
        try:
            # Load CSV into production DB using generic connection
            conn_uri = get_connection_uri("production_db")
            hook = DbApiHook.get_hook(conn_id="production_db")
            df = pd.read_csv(csv_path)

            # Example: insert into a table named `customer`
            # This uses the hook's get_sqlalchemy_engine for bulk insert
            engine = hook.get_sqlalchemy_engine()
            df.to_sql(name="customer", con=engine, if_exists="append", index=False)
            logging.info("Data successfully loaded into production database.")
        except Exception as e:
            logging.exception("Failed to load data into production: %s", e)
            raise

    @task(retries=1, retry_delay=timedelta(minutes=5))
    def quarantine_and_alert(quality_info: Dict[str, Any]) -> str:
        """
        Quarantine low-quality data and prepare for alerting.
        Returns the path of the quarantined file.
        """
        threshold = float(Variable.get("quality_threshold", default_var=80))
        score = quality_info["quality_score"]
        csv_path = quality_info["csv_path"]

        if score >= threshold:
            logging.info(
                "Quality score %.2f meets threshold %.2f. Skipping quarantine.",
                score,
                threshold,
            )
            raise AirflowSkipException("Data quality sufficient; no quarantine needed.")

        logging.info(
            "Quality score %.2f below threshold %.2f. Quarantining data.",
            score,
            threshold,
        )
        try:
            # Retrieve quarantine storage base path
            conn_uri = get_connection_uri("quarantine_storage")
            base_path = conn_uri.replace("fs://", "")
            quarantine_dir = Path(base_path)
            quarantine_dir.mkdir(parents=True, exist_ok=True)

            quarantine_file = quarantine_dir / Path(csv_path).name
            Path(csv_path).rename(quarantine_file)

            logging.info("File moved to quarantine: %s", quarantine_file)
            return str(quarantine_file)
        except Exception as e:
            logging.exception("Failed to quarantine file: %s", e)
            raise

    @task(retries=1, retry_delay=timedelta(minutes=5))
    def send_alert_email(quarantined_path: str) -> None:
        """
        Send an email alert about low-quality data.
        """
        logging.info("Preparing to send alert email for quarantined file %s", quarantined_path)
        try:
            # Retrieve SMTP connection details
            smtp_conn: Connection = BaseHook.get_connection("smtp_email")
            smtp_host = smtp_conn.host
            smtp_port = smtp_conn.port or 587
            smtp_user = smtp_conn.login
            smtp_password = smtp_conn.password
            use_tls = smtp_conn.extra_dejson.get("use_tls", True)

            # Email content
            subject = "Data Quality Alert: Low-Quality Customer CSV Quarantined"
            html_content = f"""
            <p>Dear Team,</p>
            <p>The latest customer CSV file failed the quality gate and has been moved to quarantine.</p>
            <p><strong>File:</strong> {quarantined_path}</p>
            <p>Please review and take appropriate actions.</p>
            <p>Regards,<br/>Data Quality Pipeline</p>
            """

            # Build email message
            message = f"Subject: {subject}\nTo: {smtp_conn.extra_dejson.get('to')}\n" \
                      f"From: {smtp_user}\nMIME-Version: 1.0\nContent-Type: text/html\n\n{html_content}"

            # Send email using smtplib
            with smtplib.SMTP(smtp_host, smtp_port) as server:
                if use_tls:
                    server.starttls()
                if smtp_user and smtp_password:
                    server.login(smtp_user, smtp_password)
                server.sendmail(smtp_user, smtp_conn.extra_dejson.get("to").split(","), message)

            logging.info("Alert email sent successfully.")
        except Exception as e:
            logging.exception("Failed to send alert email: %s", e)
            raise

    @task(retries=1, retry_delay=timedelta(minutes=5))
    def cleanup(*paths: str) -> None:
        """
        Cleanup temporary resources (files, directories).
        Accepts any number of file paths to delete.
        """
        logging.info("Starting cleanup of temporary resources.")
        for p in paths:
            try:
                path_obj = Path(p)
                if path_obj.is_file():
                    path_obj.unlink()
                    logging.info("Deleted temporary file: %s", p)
                elif path_obj.is_dir():
                    for child in path_obj.iterdir():
                        if child.is_file():
                            child.unlink()
                    path_obj.rmdir()
                    logging.info("Deleted temporary directory: %s", p)
            except FileNotFoundError:
                logging.warning("File not found during cleanup: %s", p)
            except Exception as e:
                logging.exception("Error cleaning up %s: %s", p, e)

    # ------------------------------------------------------------------
    # Define the workflow
    # ------------------------------------------------------------------
    csv_path = ingest_csv()
    quality_info = quality_check(csv_path)

    # Fan‑out: both production load and quarantine run after quality check
    prod_load = production_load(quality_info)
    quarantine_file = quarantine_and_alert(quality_info)

    # Alert email runs after quarantine (if it was not skipped)
    alert = send_alert_email(quarantine_file)

    # Fan‑in: cleanup runs after both production load and alert email
    cleanup_task = cleanup(csv_path, quarantine_file)

    # Set explicit dependencies (TaskFlow API automatically creates them via >>)
    quality_info >> prod_load
    quality_info >> quarantine_file
    quarantine_file >> alert
    [prod_load, alert] >> cleanup_task

# Instantiate the DAG
dag = data_quality_gate()