# Generated by Prefect Pipeline Generator
# Pipeline: data_quality_gate
# Description: Implements a data quality gate for customer CSV data that ingests raw data,
# assesses quality, and routes data to production or quarantine with alerts.
# Prefect version: 2.14.0
# Flow name: data_quality_gate
# Deployment name: data_quality_gate_deployment
# Work pool: default-agent-pool
# Task runner: ConcurrentTaskRunner

import os
import shutil
import smtplib
from datetime import datetime
from email.message import EmailMessage
from pathlib import Path
from typing import Optional

import pandas as pd
from prefect import flow, task, get_run_logger
from prefect.task_runners import ConcurrentTaskRunner
from prefect.filesystems import LocalFileSystem
from prefect.blocks.system import Secret


QUALITY_THRESHOLD = 0.9  # Minimum score to consider data high quality


@task(retries=1, retry_delay_seconds=60)
def ingest_csv(raw_fs: LocalFileSystem, temp_dir: Path) -> Path:
    """
    Ingest raw CSV files from the configured filesystem into a temporary directory.

    Args:
        raw_fs: Prefect LocalFileSystem block pointing to the raw CSV location.
        temp_dir: Path to a temporary directory where combined CSV will be stored.

    Returns:
        Path to the combined CSV file in the temporary directory.
    """
    logger = get_run_logger()
    logger.info("Starting CSV ingestion from %s", raw_fs.base_path)

    # Ensure temporary directory exists
    temp_dir.mkdir(parents=True, exist_ok=True)

    # List CSV files in the raw filesystem root
    raw_path = Path(raw_fs.base_path)
    csv_files = [p for p in raw_path.iterdir() if p.is_file() and p.suffix.lower() == ".csv"]

    if not csv_files:
        raise FileNotFoundError("No CSV files found in raw CSV filesystem.")

    combined_df = pd.concat([pd.read_csv(p) for p in csv_files], ignore_index=True)

    combined_path = temp_dir / "combined_raw.csv"
    combined_df.to_csv(combined_path, index=False)
    logger.info("Combined CSV written to %s", combined_path)

    return combined_path


@task(retries=1, retry_delay_seconds=60)
def quality_check(csv_path: Path) -> tuple[float, Path]:
    """
    Assess data quality of the ingested CSV.

    The quality score is a simple heuristic: proportion of non‑missing values.

    Args:
        csv_path: Path to the CSV file to assess.

    Returns:
        A tuple containing the quality score (0‑1) and the path to the processed CSV.
    """
    logger = get_run_logger()
    logger.info("Running quality assessment on %s", csv_path)

    df = pd.read_csv(csv_path)

    # Simple quality metric: ratio of non‑null cells to total cells
    total_cells = df.shape[0] * df.shape[1]
    non_null_cells = df.count().sum()
    score = non_null_cells / total_cells if total_cells > 0 else 0.0

    # Write a cleaned version (e.g., drop rows with >50% missing)
    row_missing_ratio = df.isnull().mean(axis=1)
    cleaned_df = df[row_missing_ratio <= 0.5]
    processed_path = csv_path.parent / "processed.csv"
    cleaned_df.to_csv(processed_path, index=False)

    logger.info("Quality score: %.4f", score)
    logger.info("Processed CSV written to %s", processed_path)

    return score, processed_path


@task(retries=1, retry_delay_seconds=60)
def production_load(
    score: float,
    processed_path: Path,
    prod_db_secret: Secret,
) -> Optional[str]:
    """
    Load high‑quality data into the production database.

    The task is a no‑op if the quality score is below the threshold.

    Args:
        score: Quality score from the previous step.
        processed_path: Path to the processed CSV file.
        prod_db_secret: Secret block containing the DB connection string.

    Returns:
        A message indicating success, or ``None`` if the load was skipped.
    """
    logger = get_run_logger()
    if score < QUALITY_THRESHOLD:
        logger.info(
            "Score %.4f below threshold %.2f – skipping production load.", score, QUALITY_THRESHOLD
        )
        return None

    # Placeholder for actual DB loading logic
    conn_str = prod_db_secret.get()
    logger.info("Loading data from %s into production DB (%s)", processed_path, conn_str)

    # Simulate load
    # In real implementation, use sqlalchemy or appropriate DB client
    logger.info("Data successfully loaded into production database.")
    return "production_load_success"


@task(retries=1, retry_delay_seconds=60)
def quarantine_and_alert(
    score: float,
    processed_path: Path,
    quarantine_fs: LocalFileSystem,
) -> Optional[Path]:
    """
    Quarantine low‑quality data and return its location for alerting.

    The task is a no‑op if the quality score meets the threshold.

    Args:
        score: Quality score from the previous step.
        processed_path: Path to the processed CSV file.
        quarantine_fs: Filesystem block where quarantined files are stored.

    Returns:
        Path to the quarantined file, or ``None`` if no quarantine occurred.
    """
    logger = get_run_logger()
    if score >= QUALITY_THRESHOLD:
        logger.info(
            "Score %.4f meets threshold %.2f – no quarantine needed.", score, QUALITY_THRESHOLD
        )
        return None

    quarantine_dir = Path(quarantine_fs.base_path)
    quarantine_dir.mkdir(parents=True, exist_ok=True)

    timestamp = datetime.utcnow().strftime("%Y%m%d%H%M%S")
    quarantine_path = quarantine_dir / f"quarantined_{timestamp}.csv"
    shutil.copy(processed_path, quarantine_path)

    logger.info("Low‑quality data quarantined at %s", quarantine_path)
    return quarantine_path


@task(retries=1, retry_delay_seconds=60)
def send_alert_email(
    quarantine_path: Optional[Path],
    smtp_secret: Secret,
) -> Optional[str]:
    """
    Send an email alert when data has been quarantined.

    Args:
        quarantine_path: Path to the quarantined file, if any.
        smtp_secret: Secret block containing SMTP credentials in JSON format:
            {"host": "...", "port": ..., "username": "...", "password": "...", "from": "...", "to": "..."}

    Returns:
        A message indicating email status, or ``None`` if no email was sent.
    """
    logger = get_run_logger()
    if quarantine_path is None:
        logger.info("No quarantine file – skipping email alert.")
        return None

    smtp_cfg = smtp_secret.get()
    required_keys = {"host", "port", "username", "password", "from", "to"}
    if not required_keys.issubset(smtp_cfg):
        raise ValueError("SMTP secret is missing required configuration keys.")

    msg = EmailMessage()
    msg["Subject"] = "Data Quality Alert: Low‑Quality Customer CSV Quarantined"
    msg["From"] = smtp_cfg["from"]
    msg["To"] = smtp_cfg["to"]
    msg.set_content(
        f"The latest customer CSV data failed quality checks and has been quarantined at {quarantine_path}."
    )

    # Attach the quarantined file
    with open(quarantine_path, "rb") as f:
        file_data = f.read()
        msg.add_attachment(
            file_data,
            maintype="text",
            subtype="csv",
            filename=quarantine_path.name,
        )

    logger.info("Sending alert email to %s via %s:%s", smtp_cfg["to"], smtp_cfg["host"], smtp_cfg["port"])
    with smtplib.SMTP(smtp_cfg["host"], smtp_cfg["port"]) as server:
        server.starttls()
        server.login(smtp_cfg["username"], smtp_cfg["password"])
        server.send_message(msg)

    logger.info("Alert email sent successfully.")
    return "email_sent"


@task(retries=1, retry_delay_seconds=60)
def cleanup(temp_dir: Path) -> None:
    """
    Remove temporary files and directories used during the flow execution.

    Args:
        temp_dir: Path to the temporary directory to delete.
    """
    logger = get_run_logger()
    if temp_dir.exists() and temp_dir.is_dir():
        shutil.rmtree(temp_dir)
        logger.info("Temporary directory %s removed.", temp_dir)
    else:
        logger.warning("Temporary directory %s does not exist; nothing to clean.", temp_dir)


@flow(
    name="data_quality_gate",
    task_runner=ConcurrentTaskRunner(),
)
def data_quality_gate() -> None:
    """
    Main flow orchestrating the data quality gate.

    The flow follows a fan‑out/fan‑in pattern:
    1. Ingest raw CSV files.
    2. Assess data quality.
    3. Parallel branches:
       - Load high‑quality data to production.
       - Quarantine low‑quality data.
    4. Send an alert email if quarantine occurred.
    5. Cleanup temporary resources.
    """
    logger = get_run_logger()

    # Load infrastructure blocks
    raw_fs = LocalFileSystem.load("raw_csv_filesystem")
    prod_db_secret = Secret.load("production_db")
    quarantine_fs = LocalFileSystem.load("quarantine_storage")
    smtp_secret = Secret.load("smtp_email")
    temp_fs = LocalFileSystem.load("temp_files_cleanup")

    # Create a unique temporary working directory
    temp_dir = Path(temp_fs.base_path) / f"tmp_{datetime.utcnow().strftime('%Y%m%d%H%M%S')}"
    temp_dir.mkdir(parents=True, exist_ok=True)
    logger.info("Temporary working directory created at %s", temp_dir)

    # Step 1: Ingest
    raw_csv_path = ingest_csv(raw_fs, temp_dir)

    # Step 2: Quality check
    quality_score, processed_path = quality_check(raw_csv_path)

    # Step 3: Fan‑out
    prod_load_result = production_load(
        score=quality_score,
        processed_path=processed_path,
        prod_db_secret=prod_db_secret,
    )
    quarantine_path = quarantine_and_alert(
        score=quality_score,
        processed_path=processed_path,
        quarantine_fs=quarantine_fs,
    )

    # Step 4: Alert email (depends on quarantine)
    email_result = send_alert_email(
        quarantine_path=quarantine_path,
        smtp_secret=smtp_secret,
    )

    # Step 5: Cleanup (depends on both branches)
    cleanup(temp_dir)


if __name__ == "__main__":
    # Execute the flow locally for testing/debugging purposes.
    data_quality_gate()