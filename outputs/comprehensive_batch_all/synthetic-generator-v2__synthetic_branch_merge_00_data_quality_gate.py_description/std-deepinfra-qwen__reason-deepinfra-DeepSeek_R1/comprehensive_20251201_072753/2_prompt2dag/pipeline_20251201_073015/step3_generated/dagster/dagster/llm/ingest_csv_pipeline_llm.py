# Generated by Dagster Code Generator
# Generation Metadata:
# - Job Name: ingest_csv_pipeline
# - Description: Comprehensive Pipeline Description: This DAG implements a data quality gate for customer CSV data that ingests raw data, performs quality assessment, and conditionally routes to production or quarantine based on quality scores.
# - Executor Type: multiprocess_executor
# - IO Manager: fs_io_manager
# - Dagster Version: 1.5.0
# - Use Assets: False
# - Required Resources: file_system, production_database, email_system, quarantine_storage

from dagster import (
    job,
    op,
    Out,
    In,
    RetryPolicy,
    multiprocess_executor,
    fs_io_manager,
    resource,
    schedule,
)

# Define resources
@resource
def file_system():
    return {"description": "Raw CSV Source"}

@resource
def production_database():
    return {"description": "Production Database"}

@resource
def email_system():
    return {"description": "Email System"}

@resource
def quarantine_storage():
    return {"description": "Quarantine Storage"}

# Define ops
@op(
    out={"csv_data": Out()},
    retry_policy=RetryPolicy(max_retries=1),
    required_resource_keys={"file_system"},
)
def ingest_csv(context):
    """Ingest CSV data from the raw CSV source."""
    # Simulate CSV ingestion
    csv_data = "sample_csv_data"
    context.log.info(f"Ingested CSV data: {csv_data}")
    return csv_data

@op(
    in_={"csv_data": In()},
    out={"quality_score": Out()},
    retry_policy=RetryPolicy(max_retries=1),
)
def quality_check(context, csv_data):
    """Perform quality check on the ingested CSV data."""
    # Simulate quality check
    quality_score = 0.95
    context.log.info(f"Quality score for CSV data: {quality_score}")
    return quality_score

@op(
    in_={"quality_score": In()},
    out={"load_status": Out()},
    retry_policy=RetryPolicy(max_retries=1),
    required_resource_keys={"production_database"},
)
def production_load(context, quality_score):
    """Load data into the production database if quality score is above a threshold."""
    threshold = 0.9
    if quality_score >= threshold:
        load_status = "success"
        context.log.info("Data loaded into production database.")
    else:
        load_status = "failure"
        context.log.info("Data quality below threshold, not loaded into production database.")
    return load_status

@op(
    in_={"quality_score": In()},
    out={"quarantine_status": Out()},
    retry_policy=RetryPolicy(max_retries=1),
    required_resource_keys={"quarantine_storage"},
)
def quarantine_and_alert(context, quality_score):
    """Quarantine data and send an alert if quality score is below a threshold."""
    threshold = 0.9
    if quality_score < threshold:
        quarantine_status = "quarantined"
        context.log.info("Data quarantined.")
    else:
        quarantine_status = "not_quarantined"
        context.log.info("Data not quarantined.")
    return quarantine_status

@op(
    in_={"quarantine_status": In()},
    retry_policy=RetryPolicy(max_retries=1),
    required_resource_keys={"email_system"},
)
def send_alert_email(context, quarantine_status):
    """Send an alert email if data is quarantined."""
    if quarantine_status == "quarantined":
        context.log.info("Sending alert email.")
    else:
        context.log.info("No alert email sent.")

@op(
    in_={"load_status": In(), "alert_status": In()},
    retry_policy=RetryPolicy(max_retries=1),
    required_resource_keys={"file_system"},
)
def cleanup(context, load_status, alert_status):
    """Clean up the filesystem after processing."""
    context.log.info("Cleaning up filesystem.")
    # Simulate cleanup
    return "cleanup_complete"

# Define job
@job(
    name="ingest_csv_pipeline",
    description="Comprehensive Pipeline Description: This DAG implements a data quality gate for customer CSV data that ingests raw data, performs quality assessment, and conditionally routes to production or quarantine based on quality scores.",
    executor_def=multiprocess_executor,
    resource_defs={
        "file_system": file_system,
        "production_database": production_database,
        "email_system": email_system,
        "quarantine_storage": quarantine_storage,
    },
    op_retry_policy=RetryPolicy(max_retries=1),
)
def ingest_csv_pipeline():
    csv_data = ingest_csv()
    quality_score = quality_check(csv_data)
    load_status = production_load(quality_score)
    quarantine_status = quarantine_and_alert(quality_score)
    alert_status = send_alert_email(quarantine_status)
    cleanup(load_status, alert_status)

# Define schedule
@schedule(
    job=ingest_csv_pipeline,
    cron_schedule="@daily",
    execution_timezone="UTC",
    name="ingest_csv_pipeline_schedule",
    catchup=False,
)
def ingest_csv_pipeline_schedule(_context):
    return {}