# Generated by Prefect 2.x Code Generator
# Date: 2023-10-04
# Prefect Version: 2.14.0

from prefect import flow, task, get_run_logger
from prefect.task_runners import ConcurrentTaskRunner
from prefect.deployments import Deployment
from prefect.infrastructure import LocalFileSystem, Secret
from prefect.blocks.system import Secret as PrefectSecret
from prefect.blocks.notifications import EmailNotificationBlock
from prefect.exceptions import PrefectException
import os

# Define the tasks
@task(retries=1, name="Ingest CSV")
def ingest_csv(raw_csv_source: LocalFileSystem):
    logger = get_run_logger()
    logger.info("Ingesting CSV from local file system.")
    # Simulate CSV ingestion
    csv_path = raw_csv_source.read_path("path/to/your/csv/file.csv")
    logger.info(f"CSV file ingested from {csv_path}")
    return csv_path

@task(retries=1, name="Quality Check")
def quality_check(csv_path: str):
    logger = get_run_logger()
    logger.info("Performing quality check on CSV data.")
    # Simulate quality check
    if not os.path.exists(csv_path):
        raise PrefectException("CSV file not found.")
    logger.info("Quality check passed.")
    return True

@task(retries=1, name="Production Load")
def production_load(quality_check_passed: bool, production_database: Secret):
    logger = get_run_logger()
    logger.info("Loading data into production database.")
    # Simulate production load
    if not quality_check_passed:
        raise PrefectException("Quality check failed, cannot load data.")
    logger.info("Data loaded into production database.")
    return True

@task(retries=1, name="Quarantine and Alert")
def quarantine_and_alert(quality_check_passed: bool, quarantine_storage: LocalFileSystem):
    logger = get_run_logger()
    logger.info("Quarantining and alerting on failed quality check.")
    # Simulate quarantine and alert
    if not quality_check_passed:
        logger.warning("Quality check failed, quarantining data.")
        quarantine_storage.write_path("path/to/quarantine/file.csv", content="Quarantined data")
        logger.warning("Data quarantined.")
    return not quality_check_passed

@task(retries=1, name="Send Alert Email")
def send_alert_email(quarantine_alert: bool, email_system: PrefectSecret):
    logger = get_run_logger()
    logger.info("Sending alert email.")
    # Simulate sending alert email
    if quarantine_alert:
        email_block = EmailNotificationBlock.load(email_system.get())
        email_block.notify("Data quality check failed, data quarantined.")
        logger.info("Alert email sent.")
    return True

@task(retries=1, name="Cleanup")
def cleanup(production_load_success: bool, alert_email_sent: bool):
    logger = get_run_logger()
    logger.info("Performing cleanup.")
    # Simulate cleanup
    if production_load_success and alert_email_sent:
        logger.info("Cleanup completed successfully.")
    else:
        logger.warning("Cleanup failed.")
    return True

# Define the flow
@flow(name="ingest_csv_pipeline", task_runner=ConcurrentTaskRunner(), schedule="@daily", timezone="UTC", catchup=False)
def ingest_csv_pipeline():
    logger = get_run_logger()
    logger.info("Starting ingest CSV pipeline.")

    # Load resources
    raw_csv_source = LocalFileSystem.load("raw_csv_source")
    production_database = Secret.load("production_database")
    quarantine_storage = LocalFileSystem.load("quarantine_storage")
    email_system = PrefectSecret.load("email_system")

    # Execute tasks
    csv_path = ingest_csv(raw_csv_source)
    quality_check_passed = quality_check(csv_path)
    production_load_success = production_load(quality_check_passed, production_database)
    quarantine_alert = quarantine_and_alert(quality_check_passed, quarantine_storage)
    alert_email_sent = send_alert_email(quarantine_alert, email_system)
    cleanup(production_load_success, alert_email_sent)

    logger.info("Ingest CSV pipeline completed.")

# Define the deployment
Deployment.build_from_flow(
    flow=ingest_csv_pipeline,
    name="ingest_csv_pipeline_deployment",
    work_pool_name="default-agent-pool",
    work_queue_name="default",
    parameters={},
    tags=[],
    version="1.0.0",
    description="No description provided.",
    schedule="@daily",
    timezone="UTC",
    catchup=False,
    infra_overrides={},
    path=None,
    entrypoint="path/to/your/flow.py:ingest_csv_pipeline",
    work_pool="default-agent-pool",
    work_queue="default",
    enforce_parameter_schema=False,
    skip_upload=True,
)