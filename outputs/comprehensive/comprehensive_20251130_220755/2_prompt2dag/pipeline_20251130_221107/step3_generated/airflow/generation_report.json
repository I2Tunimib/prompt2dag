{
  "generation_timestamp": "2025-11-30T22:18:44.983445",
  "pipeline_name": "check_pcd_sftp_folder_pipeline",
  "orchestrator": "airflow",
  "detected_pattern": "fanout_fanin",
  "task_count": 7,
  "strategies": {
    "template": {
      "strategy": "template",
      "orchestrator": "airflow",
      "success": true,
      "output_path": "/Users/abubakarialidu/Desktop/Prompt2DAG/New_Experiments_2/outputs/comprehensive/comprehensive_20251130_220755/2_prompt2dag/pipeline_20251130_221107/step3_generated/airflow/airflow/template/check_pcd_sftp_folder_pipeline_template.py",
      "generation_time_ms": 18.507957458496094,
      "token_usage": {},
      "warnings": [],
      "errors": [],
      "metadata": {
        "pattern": "fanout_fanin",
        "template_used": "pattern_fanout.py.jinja2",
        "task_count": 7
      }
    },
    "llm": {
      "strategy": "llm",
      "orchestrator": "airflow",
      "success": true,
      "output_path": "/Users/abubakarialidu/Desktop/Prompt2DAG/New_Experiments_2/outputs/comprehensive/comprehensive_20251130_220755/2_prompt2dag/pipeline_20251130_221107/step3_generated/airflow/airflow/llm/check_pcd_sftp_folder_pipeline_llm.py",
      "generation_time_ms": 29469.77400779724,
      "token_usage": {
        "input_tokens": 544,
        "output_tokens": 860
      },
      "warnings": [],
      "errors": [],
      "metadata": {
        "pattern": "fanout_fanin",
        "llm_model": "Qwen/Qwen2.5-72B-Instruct",
        "task_count": 7
      }
    },
    "hybrid": {
      "strategy": "hybrid",
      "orchestrator": "airflow",
      "success": true,
      "output_path": "/Users/abubakarialidu/Desktop/Prompt2DAG/New_Experiments_2/outputs/comprehensive/comprehensive_20251130_220755/2_prompt2dag/pipeline_20251130_221107/step3_generated/airflow/airflow/hybrid/check_pcd_sftp_folder_pipeline_hybrid.py",
      "generation_time_ms": 19855.762243270874,
      "token_usage": {
        "input_tokens": 881,
        "output_tokens": 664
      },
      "warnings": [],
      "errors": [],
      "metadata": {
        "pattern": "fanout_fanin",
        "task_snippets_count": 7,
        "llm_model": "Qwen/Qwen2.5-72B-Instruct",
        "task_count": 7
      }
    }
  },
  "summary": {
    "total_strategies": 3,
    "successful": 3,
    "failed": 0,
    "fastest_ms": 18.507957458496094,
    "total_tokens": 2949
  }
}